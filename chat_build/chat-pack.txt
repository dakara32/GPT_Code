# === Chat Paste Pack (single file) ===
# Repo: dakara32/GPT_Code @ main
# Files: factor.py, scorer.py, drift.py, .github/workflows/weekly-report.yml, .github/workflows/daily-report.yml, documents/README.md, documents/drift_design.md, documents/factor_design.md, current_tickers.csv, candidate_tickers.csv
# ä½¿ã„æ–¹: ä¸‹è¨˜ã‚’ä¸¸ã”ã¨ã‚³ãƒ”ãƒ¼ã—ã¦ãƒãƒ£ãƒƒãƒˆã«è²¼ã‚Šä»˜ã‘ï¼ˆiPhoneã¯GitHubã‚¢ãƒ—ãƒªã®ã€ŒCopy file contentsã€ãŒä¾¿åˆ©ï¼‰ã€‚
---

## <factor.py>
```text
L1 """
L2 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
L3 â”ƒ ROLE of factor.py                                     â”ƒ
L4 â”ƒ  - Orchestration ONLYï¼ˆå¤–éƒ¨I/Oãƒ»SSOTãƒ»Slackå‡ºåŠ›ï¼‰     â”ƒ
L5 â”ƒ  - è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆæ¡ç‚¹/ãƒ•ã‚£ãƒ«ã‚¿/ç›¸é–¢ä½æ¸›ï¼‰ã¯ scorer.py â”ƒ
L6 â”ƒ  - ã“ã“ã§ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…/å¤‰æ›´ã—ãªã„                   â”ƒ
L7 â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
L8 """
L9 # === NOTE: æ©Ÿèƒ½ãƒ»å…¥å‡ºåŠ›ãƒ»ãƒ­ã‚°æ–‡è¨€ãƒ»ä¾‹å¤–æŒ™å‹•ã¯ä¸å¤‰ã€‚å®‰å…¨ãªçŸ­ç¸®ï¼ˆimportçµ±åˆ/è¤‡æ•°ä»£å…¥/å†…åŒ…è¡¨è¨˜/ãƒ¡ã‚½ãƒƒãƒ‰ãƒã‚§ãƒ¼ãƒ³/ä¸€è¡ŒåŒ–/ç©ºè¡Œåœ§ç¸®ãªã©ï¼‰ã®ã¿é©ç”¨ ===
L10 import yfinance as yf, pandas as pd, numpy as np, os, requests, time, json
L11 from scipy.stats import zscore
L12 from dataclasses import dataclass
L13 from typing import Dict, List
L14 from scorer import Scorer, ttm_div_yield_portfolio
L15 import os
L16 import requests
L17 from time import perf_counter
L18
L19
L20 class T:
L21     t = perf_counter()
L22
L23     @staticmethod
L24     def log(tag: str):
L25         now = perf_counter()
L26         print(f"[T] {tag}: {now - T.t:.2f}s")
L27         T.t = now
L28
L29
L30 T.log("start")
L31
L32 # ===== ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã¨å®šæ•°ï¼ˆå†’é ­ã«å›ºå®šï¼‰ =====
L33 exist, cand = [pd.read_csv(f, header=None)[0].tolist() for f in ("current_tickers.csv","candidate_tickers.csv")]
L34 T.log(f"csv loaded: exist={len(exist)} cand={len(cand)}")
L35 CAND_PRICE_MAX, bench = 450, '^GSPC'  # ä¾¡æ ¼ä¸Šé™ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
L36 N_G, N_D = 12, 13  # G/Dæ ã‚µã‚¤ã‚º
L37 g_weights = {'GRW':0.40,'MOM':0.45,'VOL':-0.15}
L38 D_BETA_MAX = float(os.environ.get("D_BETA_MAX", "0.8"))
L39 FILTER_SPEC = {"G":{"pre_mask":["trend_template"]},"D":{"pre_filter":{"beta_max":D_BETA_MAX}}}
L40 D_weights = {'QAL':0.15,'YLD':0.15,'VOL':-0.45,'TRD':0.25}
L41 def _fmt_w(w): return " ".join(f"{k}{int(v*100)}" for k,v in w.items())
L42
L43 # DRRS åˆæœŸãƒ—ãƒ¼ãƒ«ãƒ»å„ç¨®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
L44 corrM = 45
L45 DRRS_G, DRRS_D = dict(lookback=252,n_pc=3,gamma=1.2,lam=0.68,eta=0.8), dict(lookback=504,n_pc=4,gamma=0.8,lam=0.85,eta=0.5)
L46 DRRS_SHRINK = 0.10  # æ®‹å·®ç›¸é–¢ã®å¯¾è§’ã‚·ãƒ¥ãƒªãƒ³ã‚¯ï¼ˆåŸºç¤ï¼‰
L47
L48 # ã‚¯ãƒ­ã‚¹ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆæœªå®šç¾©ãªã‚‰è¨­å®šï¼‰
L49 try: CROSS_MU_GD
L50 except NameError: CROSS_MU_GD = 0.40  # æ¨å¥¨ 0.35â€“0.45ï¼ˆlam=0.85æƒ³å®šï¼‰
L51
L52 # å‡ºåŠ›é–¢é€£
L53 RESULTS_DIR, G_PREV_JSON, D_PREV_JSON = "results", os.path.join("results","G_selection.json"), os.path.join("results","D_selection.json")
L54 os.makedirs(RESULTS_DIR, exist_ok=True)
L55
L56 # ãã®ä»–
L57 debug_mode, FINNHUB_API_KEY = False, os.environ.get("FINNHUB_API_KEY")
L58
L59
L60 # ===== å…±æœ‰DTOï¼ˆã‚¯ãƒ©ã‚¹é–“I/Oå¥‘ç´„ï¼‰ï¼‹ Config =====
L61 @dataclass(frozen=True)
L62 class InputBundle:
L63     # Input â†’ Scorer ã§å—ã‘æ¸¡ã™ç´ æï¼ˆI/Oç¦æ­¢ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ï¼‰
L64     cand: List[str]
L65     tickers: List[str]
L66     bench: str
L67     data: pd.DataFrame              # yfinance downloadçµæœï¼ˆ'Close','Volume'ç­‰ã®éšå±¤åˆ—ï¼‰
L68     px: pd.DataFrame                # data['Close']
L69     spx: pd.Series                  # data['Close'][bench]
L70     tickers_bulk: object            # yfinance.Tickers
L71     info: Dict[str, dict]           # yfinance info per ticker
L72     eps_df: pd.DataFrame            # ['eps_ttm','eps_q_recent',...]
L73     fcf_df: pd.DataFrame            # ['fcf_ttm', ...]
L74     returns: pd.DataFrame           # px[tickers].pct_change()
L75
L76 @dataclass(frozen=True)
L77 class FeatureBundle:
L78     df: pd.DataFrame
L79     df_z: pd.DataFrame
L80     g_score: pd.Series
L81     d_score_all: pd.Series
L82     missing_logs: pd.DataFrame
L83
L84 @dataclass(frozen=True)
L85 class SelectionBundle:
L86     resG: dict
L87     resD: dict
L88     top_G: List[str]
L89     top_D: List[str]
L90     init_G: List[str]
L91     init_D: List[str]
L92
L93 @dataclass(frozen=True)
L94 class WeightsConfig:
L95     g: Dict[str,float]
L96     d: Dict[str,float]
L97
L98 @dataclass(frozen=True)
L99 class DRRSParams:
L100     corrM: int
L101     shrink: float
L102     G: Dict[str,float]   # lookback, n_pc, gamma, lam, eta
L103     D: Dict[str,float]
L104     cross_mu_gd: float
L105
L106 @dataclass(frozen=True)
L107 class PipelineConfig:
L108     weights: WeightsConfig
L109     drrs: DRRSParams
L110     price_max: float
L111
L112
L113 # ===== å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆè¤‡æ•°ã‚¯ãƒ©ã‚¹ã§ä½¿ç”¨ï¼‰ =====
L114 def winsorize_s(s: pd.Series, p=0.02):
L115     if s is None or s.dropna().empty: return s
L116     lo, hi = np.nanpercentile(s.astype(float), [100*p, 100*(1-p)]); return s.clip(lo, hi)
L117
L118 def robust_z(s: pd.Series, p=0.02):
L119     s2 = winsorize_s(s, p); return np.nan_to_num(zscore(s2.fillna(s2.mean())))
L120
L121 def _safe_div(a, b):
L122     try:
L123         if b is None or float(b)==0 or pd.isna(b): return np.nan
L124         return float(a)/float(b)
L125     except Exception: return np.nan
L126
L127 def _safe_last(series: pd.Series, default=np.nan):
L128     try: return float(series.iloc[-1])
L129     except Exception: return default
L130
L131 def _load_prev(path: str):
L132     try: return json.load(open(path)).get("tickers")
L133     except Exception: return None
L134
L135 def _save_sel(path: str, tickers: list[str], avg_r: float, sum_score: float, objective: float):
L136     with open(path,"w") as f:
L137         json.dump({"tickers":tickers,"avg_res_corr":round(avg_r,6),"sum_score":round(sum_score,6),"objective":round(objective,6)}, f, indent=2)
L138
L139 def _env_true(name: str, default=False):
L140     v = os.getenv(name)
L141     return default if v is None else v.strip().lower() == "true"
L142
L143 def _slack(message, code=False):
L144     url = os.getenv("SLACK_WEBHOOK_URL")
L145     if not url:
L146         print("âš ï¸ SLACK_WEBHOOK_URL æœªè¨­å®š"); return
L147     try:
L148         requests.post(url, json={"text": f"```{message}```" if code else message}).raise_for_status()
L149     except Exception as e:
L150         print(f"âš ï¸ Slacké€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
L151
L152 def _slack_debug(text: str, chunk=2800):
L153     url=os.getenv("SLACK_WEBHOOK_URL")
L154     if not url: print("âš ï¸ SLACK_WEBHOOK_URL æœªè¨­å®š"); return
L155     i=0
L156     while i<len(text):
L157         j=min(len(text), i+chunk); k=text.rfind("\n", i, j); j=k if k>i+100 else j
L158         blk={"type":"section","text":{"type":"mrkdwn","text":f"```{text[i:j]}```"}}
L159         try: requests.post(url, json={"blocks":[blk]}).raise_for_status()
L160         except Exception as e: print(f"âš ï¸ Slacké€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
L161         i=j
L162
L163 def _compact_debug(fb, sb, prevG, prevD, max_rows=140):
L164     # ---- åˆ—é¸æŠï¼šæ—¢å®šã¯æœ€å°åˆ—ã€DEBUG_ALL_COLS=True ã§å…¨åˆ—ã« ----
L165     want=["TR","EPS","REV","ROE","BETA_RAW","FCF","RS","TR_str","DIV_STREAK","DSC"]
L166     all_cols = _env_true("DEBUG_ALL_COLS", False)
L167     cols = list(fb.df_z.columns if all_cols else [c for c in want if c in fb.df_z.columns])
L168
L169     # ---- å·®åˆ†ï¼ˆå…¥æ›¿ï¼‰----
L170     Gp, Dp = set(prevG or []), set(prevD or [])
L171     g_new=[t for t in (sb.top_G or []) if t not in Gp]; g_out=[t for t in Gp if t not in (sb.top_G or [])]
L172     d_new=[t for t in (sb.top_D or []) if t not in Dp]; d_out=[t for t in Dp if t not in (sb.top_D or [])]
L173
L174     # ---- æ¬¡ç‚¹10ï¼ˆãƒ•ãƒ©ã‚°ã§æœ‰ç„¡åˆ‡æ›¿ï¼‰----
L175     show_near = _env_true("DEBUG_NEAR5", True)
L176     gs = getattr(fb,"g_score",None); ds = getattr(fb,"d_score_all",None)
L177     gs = gs.sort_values(ascending=False) if show_near and hasattr(gs,"sort_values") else None
L178     ds = ds.sort_values(ascending=False) if show_near and hasattr(ds,"sort_values") else None
L179     g_miss = ([t for t in gs.index if t not in (sb.top_G or [])][:10]) if gs is not None else []
L180     d_excl = set((sb.top_G or [])+(sb.top_D or []))
L181     d_miss = ([t for t in ds.index if t not in d_excl][:10]) if ds is not None else []
L182
L183     # ---- è¡Œé¸æŠï¼šæ—¢å®šã¯å…¥æ›¿+æ¡ç”¨+æ¬¡ç‚¹ã€DEBUG_ALL_ROWS=True ã§å…¨éŠ˜æŸ„ ----
L184     all_rows = _env_true("DEBUG_ALL_ROWS", False)
L185     focus = list(fb.df_z.index) if all_rows else sorted(set(g_new+g_out+d_new+d_out+(sb.top_G or [])+(sb.top_D or [])+g_miss+d_miss))
L186     focus = focus[:max_rows]
L187
L188     # ---- ãƒ˜ãƒƒãƒ€ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã‚’æ˜ç¤ºï¼‰----
L189     def _fmt_near(lbl, ser, lst):
L190         if ser is None: return f"{lbl}: off"
L191         parts=[]
L192         for t in lst:
L193             x=ser.get(t, float("nan"))
L194             parts.append(f"{t}:{x:.3f}" if pd.notna(x) else f"{t}:nan")
L195         return f"{lbl}: "+(", ".join(parts) if parts else "-")
L196     head=[f"G new/out: {len(g_new)}/{len(g_out)}  D new/out: {len(d_new)}/{len(d_out)}",
L197           _fmt_near("G near10", gs, g_miss),
L198           _fmt_near("D near10", ds, d_miss),
L199           f"Filters: G pre_mask=['trend_template'], D pre_filter={{'beta_max': {D_BETA_MAX}}}",
L200           f"Cols={'ALL' if all_cols else 'MIN'}  Rows={'ALL' if all_rows else 'SUBSET'}"]
L201
L202     # ---- ãƒ†ãƒ¼ãƒ–ãƒ« ----
L203     if fb.df_z.empty or not cols:
L204         tbl="(df_z or columns not available)"
L205     else:
L206         idx=[t for t in focus if t in fb.df_z.index]
L207         tbl=fb.df_z.loc[idx, cols].round(3).to_string(max_rows=None, max_cols=None)
L208
L209     # ---- æ¬ æãƒ­ã‚°ï¼ˆãƒ•ãƒ©ã‚°ã§æœ‰ç„¡åˆ‡æ›¿ï¼‰----
L210     miss_txt=""
L211     if _env_true("DEBUG_MISSING_LOGS", False):
L212         miss=getattr(fb,"missing_logs",None)
L213         if miss is not None and not miss.empty:
L214             miss_txt="\nMissing data (head)\n"+miss.head(10).to_string(index=False)
L215
L216     return "\n".join(head+["\nChanged/Selected (+ Near Miss)", tbl])+miss_txt
L217
L218 def _disjoint_keepG(top_G, top_D, poolD):
L219     """
L220     Gã«å«ã¾ã‚Œã‚‹éŠ˜æŸ„ã‚’Dã‹ã‚‰é™¤å»ã—ã€Dã¯poolDï¼ˆæ¬¡ç‚¹ï¼‰ã§è£œå……ã™ã‚‹ã€‚
L221     - å¼•æ•°:
L222         top_G: List[str]  â€¦ Gæœ€çµ‚12éŠ˜æŸ„
L223         top_D: List[str]  â€¦ Dæœ€çµ‚13éŠ˜æŸ„ï¼ˆé‡è¤‡ã‚’å«ã‚€å¯èƒ½æ€§ã‚ã‚Šï¼‰
L224         poolD: List[str]  â€¦ Då€™è£œã®é †ä½ãƒªã‚¹ãƒˆï¼ˆtop_Dã‚’å«ã‚€ä¸Šä½æ‹¡å¼µï¼‰
L225     - æˆ»ã‚Šå€¤: (top_G, top_D_disjoint)
L226     - æŒ™å‹•:
L227         1) Dã«Gé‡è¤‡ãŒã‚ã‚Œã°é †ã«ç½®æ›
L228         2) ç½®æ›å€™è£œã¯ poolD ã‹ã‚‰ã€æ—¢ä½¿ç”¨(GâˆªD)ã‚’é¿ã‘ã¦å‰ã‹ã‚‰æ¡ç”¨
L229         3) è£œå……åˆ†ãŒå°½ããŸå ´åˆã¯å…ƒã®éŠ˜æŸ„ã‚’æ®‹ã™ï¼ˆå®‰å…¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
L230     """
L231     used, D = set(top_G), list(top_D)
L232     i = 0
L233     for j, t in enumerate(D):
L234         if t in used:
L235             while i < len(poolD) and (poolD[i] in used or poolD[i] in D):
L236                 i += 1
L237             if i < len(poolD):
L238                 D[j] = poolD[i]; used.add(D[j]); i += 1
L239     return top_G, D
L240
L241
L242 # ===== Inputï¼šå¤–éƒ¨I/Oã¨å‰å‡¦ç†ï¼ˆCSV/APIãƒ»æ¬ æè£œå®Œï¼‰ =====
L243 class Input:
L244     def __init__(self, cand, exist, bench, price_max, finnhub_api_key=None):
L245         self.cand, self.exist, self.bench, self.price_max = cand, exist, bench, price_max
L246         self.api_key = finnhub_api_key or os.environ.get("FINNHUB_API_KEY")
L247
L248     # ---- ï¼ˆInputå°‚ç”¨ï¼‰EPSè£œå®Œãƒ»FCFç®—å‡ºç³» ----
L249     @staticmethod
L250     def impute_eps_ttm(df: pd.DataFrame, ttm_col: str="eps_ttm", q_col: str="eps_q_recent", out_col: str|None=None) -> pd.DataFrame:
L251         out_col = out_col or ttm_col; df = df.copy(); df["eps_imputed"] = False
L252         cand = df[q_col]*4; ok = df[ttm_col].isna() & cand.replace([np.inf,-np.inf], np.nan).notna()
L253         df.loc[ok, out_col], df.loc[ok,"eps_imputed"] = cand[ok], True; return df
L254
L255     _CF_ALIASES = {"cfo":["Operating Cash Flow","Total Cash From Operating Activities"], "capex":["Capital Expenditure","Capital Expenditures"]}
L256
L257     @staticmethod
L258     def _pick_row(df: pd.DataFrame, names: list[str]) -> pd.Series|None:
L259         if df is None or df.empty: return None
L260         idx_lower = {str(i).lower(): i for i in df.index}
L261         for name in names:
L262             key = name.lower()
L263             if key in idx_lower: return df.loc[idx_lower[key]]
L264         return None
L265
L266     @staticmethod
L267     def _sum_last_n(s: pd.Series|None, n: int) -> float|None:
L268         if s is None or s.empty: return None
L269         vals = s.dropna().astype(float); return None if vals.empty else vals.iloc[:n].sum()
L270
L271     @staticmethod
L272     def _latest(s: pd.Series|None) -> float|None:
L273         if s is None or s.empty: return None
L274         vals = s.dropna().astype(float); return vals.iloc[0] if not vals.empty else None
L275
L276     def fetch_cfo_capex_ttm_yf(self, tickers: list[str]) -> pd.DataFrame:
L277         from concurrent.futures import ThreadPoolExecutor, as_completed
L278         pick, sumn, latest, aliases = self._pick_row, self._sum_last_n, self._latest, self._CF_ALIASES
L279
L280         def one(t: str):
L281             try:
L282                 tk = yf.Ticker(t)  # â˜… ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¯æ¸¡ã•ãªã„ï¼ˆYFãŒcurl_cffiã§ç®¡ç†ï¼‰
L283                 qcf = tk.quarterly_cashflow
L284                 cfo_q, capex_q = pick(qcf, aliases["cfo"]), pick(qcf, aliases["capex"])
L285                 fcf_q = pick(qcf, ["Free Cash Flow","FreeCashFlow","Free cash flow"])
L286                 cfo, capex, fcf = sumn(cfo_q,4), sumn(capex_q,4), sumn(fcf_q,4)
L287                 if any(v is None for v in (cfo, capex, fcf)):
L288                     acf = tk.cashflow
L289                     if cfo   is None: cfo   = latest(pick(acf, aliases["cfo"]))
L290                     if capex is None: capex = latest(pick(acf, aliases["capex"]))
L291                     if fcf   is None: fcf   = latest(pick(acf, ["Free Cash Flow","FreeCashFlow","Free cash flow"]))
L292             except Exception as e:
L293                 print(f"[warn] yf financials error: {t}: {e}"); cfo=capex=fcf=None
L294             n=np.nan
L295             return {"ticker":t,
L296                     "cfo_ttm_yf":   n if cfo   is None else cfo,
L297                     "capex_ttm_yf": n if capex is None else capex,
L298                     "fcf_ttm_yf_direct": n if fcf is None else fcf}
L299
L300         rows, mw = [], int(os.getenv("FIN_THREADS","8"))
L301         with ThreadPoolExecutor(max_workers=mw) as ex:
L302             for f in as_completed(ex.submit(one,t) for t in tickers): rows.append(f.result())
L303         return pd.DataFrame(rows).set_index("ticker")
L304
L305     _FINN_CFO_KEYS = ["netCashProvidedByOperatingActivities","netCashFromOperatingActivities","cashFlowFromOperatingActivities","operatingCashFlow"]
L306     _FINN_CAPEX_KEYS = ["capitalExpenditure","capitalExpenditures","purchaseOfPPE","investmentsInPropertyPlantAndEquipment"]
L307
L308     @staticmethod
L309     def _first_key(d: dict, keys: list[str]):
L310         for k in keys:
L311             if k in d and d[k] is not None: return d[k]
L312         return None
L313
L314     @staticmethod
L315     def _finn_get(session: requests.Session, url: str, params: dict, retries: int=3, sleep_s: float=0.5):
L316         for i in range(retries):
L317             r = session.get(url, params=params, timeout=15)
L318             if r.status_code==429: time.sleep(min(2**i*sleep_s,4.0)); continue
L319             r.raise_for_status(); return r.json()
L320         r.raise_for_status()
L321
L322     def fetch_cfo_capex_ttm_finnhub(self, tickers: list[str], api_key: str|None=None) -> pd.DataFrame:
L323         api_key = api_key or os.getenv("FINNHUB_API_KEY")
L324         if not api_key: raise ValueError("Finnhub API key not provided. Set FINNHUB_API_KEY or pass api_key=")
L325         base, s, rows = "https://finnhub.io/api/v1", requests.Session(), []
L326         for sym in tickers:
L327             cfo_ttm = capex_ttm = None
L328             try:
L329                 j = self._finn_get(s, f"{base}/stock/cash-flow", {"symbol":sym,"frequency":"quarterly","limit":8,"token":api_key})
L330                 arr = j.get("cashFlow") or []; cfo_vals, capex_vals = [], []
L331                 for item in arr[:4]:
L332                     cfo_vals.append(self._first_key(item,self._FINN_CFO_KEYS)); capex_vals.append(self._first_key(item,self._FINN_CAPEX_KEYS))
L333                 if any(v is not None for v in cfo_vals): cfo_ttm = float(np.nansum([np.nan if v is None else float(v) for v in cfo_vals]))
L334                 if any(v is not None for v in capex_vals): capex_ttm = float(np.nansum([np.nan if v is None else float(v) for v in capex_vals]))
L335             except Exception: pass
L336             if cfo_ttm is None or capex_ttm is None:
L337                 try:
L338                     j = self._finn_get(s, f"{base}/stock/cash-flow", {"symbol":sym,"frequency":"annual","limit":1,"token":api_key})
L339                     arr = j.get("cashFlow") or []
L340                     if arr:
L341                         item0 = arr[0]
L342                         if cfo_ttm is None:
L343                             v = self._first_key(item0,self._FINN_CFO_KEYS)
L344                             if v is not None: cfo_ttm = float(v)
L345                         if capex_ttm is None:
L346                             v = self._first_key(item0,self._FINN_CAPEX_KEYS)
L347                             if v is not None: capex_ttm = float(v)
L348                 except Exception: pass
L349             rows.append({"ticker":sym,"cfo_ttm_fh":np.nan if cfo_ttm is None else cfo_ttm,"capex_ttm_fh":np.nan if capex_ttm is None else capex_ttm})
L350         return pd.DataFrame(rows).set_index("ticker")
L351
L352     def compute_fcf_with_fallback(self, tickers: list[str], finnhub_api_key: str|None=None) -> pd.DataFrame:
L353         yf_df = self.fetch_cfo_capex_ttm_yf(tickers)
L354         T.log("financials (yf) done")
L355         miss_mask = yf_df[["cfo_ttm_yf","capex_ttm_yf","fcf_ttm_yf_direct"]].isna().any(axis=1)
L356         need = yf_df.index[miss_mask].tolist(); print(f"[T] yf financials missing: {len(need)} {need[:10]}{'...' if len(need)>10 else ''}")
L357         if need:
L358             fh_df = self.fetch_cfo_capex_ttm_finnhub(need, api_key=finnhub_api_key)
L359             df = yf_df.join(fh_df, how="left")
L360             for col_yf, col_fh in [("cfo_ttm_yf","cfo_ttm_fh"),("capex_ttm_yf","capex_ttm_fh")]:
L361                 df[col_yf] = df[col_yf].fillna(df[col_fh])
L362             print("[T] financials (finnhub) done (fallback only)")
L363         else:
L364             df = yf_df.assign(cfo_ttm_fh=np.nan, capex_ttm_fh=np.nan)
L365             print("[T] financials (finnhub) skipped (no missing)")
L366         df["cfo_ttm"]  = df["cfo_ttm_yf"].where(df["cfo_ttm_yf"].notna(), df["cfo_ttm_fh"])
L367         df["capex_ttm"] = df["capex_ttm_yf"].where(df["capex_ttm_yf"].notna(), df["capex_ttm_fh"])
L368         cfo, capex = pd.to_numeric(df["cfo_ttm"], errors="coerce"), pd.to_numeric(df["capex_ttm"], errors="coerce").abs()
L369         fcf_calc = cfo - capex
L370         fcf_direct = pd.to_numeric(df.get("fcf_ttm_yf_direct"), errors="coerce")
L371         df["fcf_ttm"] = fcf_calc.where(fcf_calc.notna(), fcf_direct)
L372         df["cfo_source"]  = np.where(df["cfo_ttm_yf"].notna(),"yfinance",np.where(df["cfo_ttm_fh"].notna(),"finnhub",""))
L373         df["capex_source"] = np.where(df["capex_ttm_yf"].notna(),"yfinance",np.where(df["capex_ttm_fh"].notna(),"finnhub",""))
L374         df["fcf_imputed"] = df[["cfo_ttm","capex_ttm"]].isna().any(axis=1) & df["fcf_ttm"].notna()
L375         cols = ["cfo_ttm_yf","capex_ttm_yf","cfo_ttm_fh","capex_ttm_fh","cfo_ttm","capex_ttm","fcf_ttm","fcf_ttm_yf_direct","cfo_source","capex_source","fcf_imputed"]
L376         return df[cols].sort_index()
L377
L378     def _build_eps_df(self, tickers, tickers_bulk, info):
L379         eps_rows=[]
L380         for t in tickers:
L381             info_t, eps_ttm, eps_q = info[t], info[t].get("trailingEps", np.nan), np.nan
L382             try:
L383                 qearn, so = tickers_bulk.tickers[t].quarterly_earnings, info_t.get("sharesOutstanding")
L384                 if so and qearn is not None and not qearn.empty and "Earnings" in qearn.columns:
L385                     eps_ttm_q = qearn["Earnings"].head(4).sum()/so
L386                     if pd.notna(eps_ttm_q) and (pd.isna(eps_ttm) or (abs(eps_ttm)>0 and abs(eps_ttm/eps_ttm_q)>3)): eps_ttm = eps_ttm_q
L387                     eps_q = qearn["Earnings"].iloc[-1]/so
L388             except Exception: pass
L389             eps_rows.append({"ticker":t,"eps_ttm":eps_ttm,"eps_q_recent":eps_q})
L390         return self.impute_eps_ttm(pd.DataFrame(eps_rows).set_index("ticker"))
L391
L392     def prepare_data(self):
L393         """Fetch price and fundamental data for all tickers."""
L394         cand_info = yf.Tickers(" ".join(self.cand)); cand_prices = {}
L395         for t in self.cand:
L396             try: cand_prices[t] = cand_info.tickers[t].fast_info.get("lastPrice", np.inf)
L397             except Exception as e: print(f"{t}: price fetch failed ({e})"); cand_prices[t] = np.inf
L398         cand_f = [t for t,p in cand_prices.items() if p<=self.price_max]
L399         T.log("price cap filter done (CAND_PRICE_MAX)")
L400         tickers = sorted(set(self.exist + cand_f))
L401         T.log(f"universe prepared: unique={len(tickers)} bench={self.bench}")
L402         data = yf.download(tickers + [self.bench], period="600d", auto_adjust=True, progress=False)
L403         T.log("yf.download done")
L404         px, spx = data["Close"], data["Close"][self.bench]
L405         clip_days = int(os.getenv("PRICE_CLIP_DAYS", "0"))   # 0ãªã‚‰ç„¡åŠ¹ï¼ˆæ—¢å®šï¼‰
L406         if clip_days > 0:
L407             px  = px.tail(clip_days + 1)
L408             spx = spx.tail(clip_days + 1)
L409             print(f"[T] price window clipped by env: {len(px)} rows (PRICE_CLIP_DAYS={clip_days})")
L410         else:
L411             print(f"[T] price window clip skipped; rows={len(px)}")
L412         tickers_bulk, info = yf.Tickers(" ".join(tickers)), {}
L413         for t in tickers:
L414             try: info[t] = tickers_bulk.tickers[t].info
L415             except Exception as e: print(f"{t}: info fetch failed ({e})"); info[t] = {}
L416         eps_df = self._build_eps_df(tickers, tickers_bulk, info)
L417         fcf_df = self.compute_fcf_with_fallback(tickers, finnhub_api_key=self.api_key)
L418         T.log("eps/fcf prep done")
L419         returns = px[tickers].pct_change()
L420         T.log("price prep/returns done")
L421         return dict(cand=cand_f, tickers=tickers, data=data, px=px, spx=spx, tickers_bulk=tickers_bulk, info=info, eps_df=eps_df, fcf_df=fcf_df, returns=returns)
L422
L423
L424 # ===== Selectorï¼šç›¸é–¢ä½æ¸›ãƒ»é¸å®šï¼ˆã‚¹ã‚³ã‚¢ï¼†ãƒªã‚¿ãƒ¼ãƒ³ã ã‘èª­ã‚€ï¼‰ =====
L425 class Selector:
L426     # ---- DRRS helpersï¼ˆSelectorå°‚ç”¨ï¼‰ ----
L427     @staticmethod
L428     def _z_np(X: np.ndarray) -> np.ndarray:
L429         X = np.asarray(X, dtype=np.float32); m = np.nanmean(X, axis=0, keepdims=True); s = np.nanstd(X, axis=0, keepdims=True)+1e-9
L430         return (np.nan_to_num(X)-m)/s
L431
L432     @classmethod
L433     def residual_corr(cls, R: np.ndarray, n_pc: int=3, shrink: float=0.1) -> np.ndarray:
L434         Z = cls._z_np(R); U,S,_ = np.linalg.svd(Z, full_matrices=False); F = U[:,:n_pc]*S[:n_pc]; B = np.linalg.lstsq(F, Z, rcond=None)[0]
L435         E = Z - F@B; C = np.corrcoef(E, rowvar=False)
L436         off = C - np.diag(np.diag(C)); iu = np.triu_indices_from(off,1); avg_abs = np.nanmean(np.abs(off[iu])) if iu[0].size else 0.0
L437         shrink_eff = float(np.clip(shrink + 0.5*avg_abs, 0.1, 0.6)); N = C.shape[0]
L438         return (1.0 - shrink_eff)*C + shrink_eff*np.eye(N, dtype=C.dtype)
L439
L440     @classmethod
L441     def rrqr_like_det(cls, R: np.ndarray, score: np.ndarray, k: int, gamma: float=1.0):
L442         Z, w = cls._z_np(R), (score-score.min())/(np.ptp(score)+1e-12); X = Z*(1.0 + gamma*w)
L443         N, k = X.shape[1], max(0, min(k, X.shape[1]))
L444         if k==0: return []
L445         S, selected, Rres = [], np.zeros(N, dtype=bool), X.copy()
L446         for _ in range(k):
L447             norms = (Rres*Rres).sum(axis=0); cand = np.where(~selected)[0]
L448             j = sorted(cand, key=lambda c:(-norms[c], -w[c], c))[0]
L449             S.append(j); selected[j]=True; u = X[:,j:j+1]; u/=(np.linalg.norm(u)+1e-12); Rres = Rres - u @ (u.T @ Rres)
L450         return sorted(S)
L451
L452     @staticmethod
L453     def _obj(corrM: np.ndarray, score: np.ndarray, idx, lam: float) -> float:
L454         idx = list(idx); P = corrM[np.ix_(idx, idx)]; s = (score-score.mean())/(score.std()+1e-9)
L455         return float(s[idx].sum() - lam*((P.sum()-np.trace(P))/2.0))
L456
L457     @classmethod
L458     def swap_local_det(cls, corrM: np.ndarray, score: np.ndarray, idx, lam: float=0.6, max_pass: int=15):
L459         S, best, improved, passes = sorted(idx), cls._obj(corrM, score, idx, lam), True, 0
L460         while improved and passes<max_pass:
L461             improved, passes = False, passes+1
L462             for i,out in enumerate(list(S)):
L463                 for inn in range(len(score)):
L464                     if inn in S: continue
L465                     cand = sorted(S[:i]+[inn]+S[i+1:]); v = cls._obj(corrM, score, cand, lam)
L466                     if v>best+1e-10: S, best, improved = cand, v, True; break
L467                 if improved: break
L468         return S, best
L469
L470     @staticmethod
L471     def _obj_with_cross(C_within: np.ndarray, C_cross: np.ndarray|None, score: np.ndarray, idx, lam: float, mu: float) -> float:
L472         idx = list(idx); P = C_within[np.ix_(idx, idx)]; s = (score-score.mean())/(score.std()+1e-9)
L473         within = (P.sum()-np.trace(P))/2.0; cross = 0.0
L474         if C_cross is not None and C_cross.size>0: cross = C_cross[idx,:].sum()
L475         return float(s[idx].sum() - lam*within - mu*cross)
L476
L477     @classmethod
L478     def swap_local_det_cross(cls, C_within: np.ndarray, C_cross: np.ndarray|None, score: np.ndarray, idx, lam: float=0.6, mu: float=0.3, max_pass: int=15):
L479         S, best, improved, passes, N = sorted(idx), cls._obj_with_cross(C_within,C_cross,score,idx,lam,mu), True, 0, len(score)
L480         while improved and passes<max_pass:
L481             improved, passes = False, passes+1
L482             for i,out in enumerate(list(S)):
L483                 for inn in range(N):
L484                     if inn in S: continue
L485                     cand = sorted(S[:i]+[inn]+S[i+1:]); v = cls._obj_with_cross(C_within,C_cross,score,cand,lam,mu)
L486                     if v>best+1e-10: S, best, improved = cand, v, True; break
L487                 if improved: break
L488         return S, best
L489
L490     @staticmethod
L491     def avg_corr(C: np.ndarray, idx) -> float:
L492         k = len(idx); P = C[np.ix_(idx, idx)]
L493         return float((P.sum()-np.trace(P))/(k*(k-1)+1e-12))
L494
L495     @classmethod
L496     def select_bucket_drrs(cls, returns_df: pd.DataFrame, score_ser: pd.Series, pool_tickers: list[str], k: int, *, n_pc: int, gamma: float, lam: float, eta: float, lookback: int, prev_tickers: list[str]|None, shrink: float=0.10, g_fixed_tickers: list[str]|None=None, mu: float=0.0):
L497         g_fixed = [t for t in (g_fixed_tickers or []) if t in returns_df.columns]
L498         union = [t for t in pool_tickers if t in returns_df.columns]
L499         for t in g_fixed:
L500             if t not in union: union.append(t)
L501         Rdf_all = returns_df[union]; Rdf_all = Rdf_all.iloc[-lookback:] if len(Rdf_all)>=lookback else Rdf_all; Rdf_all = Rdf_all.dropna()
L502         pool_eff, g_eff = [t for t in pool_tickers if t in Rdf_all.columns], [t for t in g_fixed if t in Rdf_all.columns]
L503         if len(pool_eff)==0: return dict(idx=[], tickers=[], avg_res_corr=np.nan, sum_score=0.0, objective=-np.inf)
L504         score = score_ser.reindex(pool_eff).to_numpy(dtype=np.float32)
L505         C_all = cls.residual_corr(Rdf_all.to_numpy(), n_pc=n_pc, shrink=shrink)
L506         col_pos = {c:i for i,c in enumerate(Rdf_all.columns)}; pool_pos = [col_pos[t] for t in pool_eff]
L507         C_within, C_cross = C_all[np.ix_(pool_pos,pool_pos)], None
L508         if len(g_eff)>0 and mu>0.0:
L509             g_pos = [col_pos[t] for t in g_eff]; C_cross = C_all[np.ix_(pool_pos,g_pos)]
L510         R_pool = Rdf_all[pool_eff].to_numpy(); S0 = cls.rrqr_like_det(R_pool, score, k, gamma=gamma)
L511         S, Jn = (cls.swap_local_det_cross(C_within, C_cross, score, S0, lam=lam, mu=mu, max_pass=15) if C_cross is not None else cls.swap_local_det(C_within, score, S0, lam=lam, max_pass=15))
L512         if prev_tickers:
L513             prev_idx = [pool_eff.index(t) for t in prev_tickers if t in pool_eff]
L514             if len(prev_idx)==min(k,len(pool_eff)):
L515                 Jp = (cls._obj_with_cross(C_within,C_cross,score,prev_idx,lam,mu) if C_cross is not None else cls._obj(C_within,score,prev_idx,lam))
L516                 if Jn < Jp + eta: S, Jn = sorted(prev_idx), Jp
L517         selected_tickers = [pool_eff[i] for i in S]
L518         return dict(idx=S, tickers=selected_tickers, avg_res_corr=cls.avg_corr(C_within,S), sum_score=float(score[S].sum()), objective=float(Jn))
L519
L520     # ---- é¸å®šï¼ˆã‚¹ã‚³ã‚¢ Series / returns ã ã‘ã‚’å—ã‘ã‚‹ï¼‰----
L521     def select_buckets(self, returns_df: pd.DataFrame, g_score: pd.Series, d_score_all: pd.Series, cfg: PipelineConfig) -> SelectionBundle:
L522         init_G = g_score.nlargest(min(cfg.drrs.corrM, len(g_score))).index.tolist(); prevG = _load_prev(G_PREV_JSON)
L523         resG = self.select_bucket_drrs(returns_df=returns_df, score_ser=g_score, pool_tickers=init_G, k=N_G,
L524                                        n_pc=cfg.drrs.G.get("n_pc",3), gamma=cfg.drrs.G.get("gamma",1.2),
L525                                        lam=cfg.drrs.G.get("lam",0.68), eta=cfg.drrs.G.get("eta",0.8),
L526                                        lookback=cfg.drrs.G.get("lookback",252), prev_tickers=prevG,
L527                                        shrink=cfg.drrs.shrink, g_fixed_tickers=None, mu=0.0)
L528         top_G = resG["tickers"]
L529
L530         # df_z ã«ä¾å­˜ã›ãšã€ã‚¹ã‚³ã‚¢ã® index ã‹ã‚‰ D ãƒ—ãƒ¼ãƒ«ã‚’æ§‹æˆï¼ˆæ©Ÿèƒ½ã¯åŒç­‰ï¼‰
L531         d_score = d_score_all.drop(top_G, errors='ignore')
L532         D_pool_index = d_score.index
L533         init_D = d_score.loc[D_pool_index].nlargest(min(cfg.drrs.corrM, len(D_pool_index))).index.tolist(); prevD = _load_prev(D_PREV_JSON)
L534         mu = cfg.drrs.cross_mu_gd
L535         resD = self.select_bucket_drrs(returns_df=returns_df, score_ser=d_score_all, pool_tickers=init_D, k=N_D,
L536                                        n_pc=cfg.drrs.D.get("n_pc",4), gamma=cfg.drrs.D.get("gamma",0.8),
L537                                        lam=cfg.drrs.D.get("lam",0.85), eta=cfg.drrs.D.get("eta",0.5),
L538                                        lookback=cfg.drrs.D.get("lookback",504), prev_tickers=prevD,
L539                                        shrink=cfg.drrs.shrink, g_fixed_tickers=top_G, mu=mu)
L540         top_D = resD["tickers"]
L541
L542         _save_sel(G_PREV_JSON, top_G, resG["avg_res_corr"], resG["sum_score"], resG["objective"])
L543         _save_sel(D_PREV_JSON, top_D, resD["avg_res_corr"], resD["sum_score"], resD["objective"])
L544         return SelectionBundle(resG=resG, resD=resD, top_G=top_G, top_D=top_D, init_G=init_G, init_D=init_D)
L545
L546
L547 # ===== Outputï¼šå‡ºåŠ›æ•´å½¢ã¨é€ä¿¡ï¼ˆè¡¨ç¤ºãƒ»Slackï¼‰ =====
L548 class Output:
L549
L550     def __init__(self, debug=False):
L551         self.debug = debug
L552         self.miss_df = self.g_table = self.d_table = self.io_table = self.df_metrics_fmt = self.debug_table = None
L553         self.g_title = self.d_title = ""
L554         self.g_formatters = self.d_formatters = {}
L555         # ä½ã‚¹ã‚³ã‚¢ï¼ˆGSC+DSCï¼‰Top10 è¡¨ç¤º/é€ä¿¡ç”¨
L556         self.low10_table = None
L557
L558     # --- è¡¨ç¤ºï¼ˆå…ƒ display_results ã®ãƒ­ã‚¸ãƒƒã‚¯ãã®ã¾ã¾ï¼‰ ---
L559     def display_results(self, *, exist, bench, df_z, g_score, d_score_all,
L560                         init_G, init_D, top_G, top_D, **kwargs):
L561         pd.set_option('display.float_format','{:.3f}'.format)
L562         print("ğŸ“ˆ ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼åˆ†æ•£æœ€é©åŒ–ã®çµæœ")
L563         if self.miss_df is not None and not self.miss_df.empty:
L564             print("Missing Data:")
L565             print(self.miss_df.to_string(index=False))
L566
L567         # ---- è¡¨ç¤ºç”¨ï¼šChanges/Near-Miss ã®ã‚¹ã‚³ã‚¢æºã‚’â€œæœ€çµ‚é›†è¨ˆâ€ã«çµ±ä¸€ã™ã‚‹ãƒ—ãƒ­ã‚­ã‚· ----
L568         try:
L569             sc = getattr(self, "_sc", None)
L570             agg_G = getattr(sc, "_agg_G", None)
L571             agg_D = getattr(sc, "_agg_D", None)
L572         except Exception:
L573             sc = agg_G = agg_D = None
L574         class _SeriesProxy:
L575             __slots__ = ("primary", "fallback")
L576             def __init__(self, primary, fallback): self.primary, self.fallback = primary, fallback
L577             def get(self, key, default=None):
L578                 try:
L579                     v = self.primary.get(key) if hasattr(self.primary, "get") else None
L580                     if v is not None and not (isinstance(v, float) and v != v):
L581                         return v
L582                 except Exception:
L583                     pass
L584                 try:
L585                     return self.fallback.get(key) if hasattr(self.fallback, "get") else default
L586                 except Exception:
L587                     return default
L588         g_score = _SeriesProxy(agg_G, g_score)
L589         d_score_all = _SeriesProxy(agg_D, d_score_all)
L590         near_G = getattr(sc, "_near_G", []) if sc else []
L591         near_D = getattr(sc, "_near_D", []) if sc else []
L592
L593         extra_G = [t for t in init_G if t not in top_G][:5]; G_UNI = top_G + extra_G
L594         gsc_series = pd.Series({t: g_score.get(t) for t in G_UNI}, name='GSC')
L595         self.g_table = pd.concat([df_z.loc[G_UNI,['GRW','MOM','TRD','VOL']], gsc_series], axis=1)
L596         self.g_table.index = [t + ("â­ï¸" if t in top_G else "") for t in G_UNI]
L597         self.g_formatters = {col:"{:.2f}".format for col in ['GRW','MOM','TRD','VOL']}; self.g_formatters['GSC'] = "{:.3f}".format
L598         self.g_title = (f"[Gæ  / {N_G} / {_fmt_w(g_weights)} / corrM={corrM} / "
L599                         f"LB={DRRS_G['lookback']} nPC={DRRS_G['n_pc']} Î³={DRRS_G['gamma']} Î»={DRRS_G['lam']} Î·={DRRS_G['eta']} shrink={DRRS_SHRINK}]")
L600         if near_G:
L601             add = [t for t in near_G if t not in set(G_UNI)][:10]
L602             if len(add) < 10:
L603                 try:
L604                     aggG = getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False)
L605                     out_now = sorted(set(exist) - set(top_G + top_D))  # ä»Šå› OUT
L606                     used = set(G_UNI + add)
L607                     def _push(lst):
L608                         nonlocal add, used
L609                         for t in lst:
L610                             if len(add) == 10: break
L611                             if t in aggG.index and t not in used:
L612                                 add.append(t); used.add(t)
L613                     _push(out_now)           # â‘  ä»Šå› OUT ã‚’å„ªå…ˆ
L614                     _push(list(aggG.index))  # â‘¡ ã¾ã è¶³ã‚Šãªã‘ã‚Œã°ä¸Šä½ã§å……å¡«
L615                 except Exception:
L616                     pass
L617             if add:
L618                 near_tbl = pd.concat([df_z.loc[add,['GRW','MOM','TRD','VOL']], pd.Series({t: g_score.get(t) for t in add}, name='GSC')], axis=1)
L619                 self.g_table = pd.concat([self.g_table, near_tbl], axis=0)
L620         print(self.g_title); print(self.g_table.to_string(formatters=self.g_formatters))
L621
L622         extra_D = [t for t in init_D if t not in top_D][:5]; D_UNI = top_D + extra_D
L623         cols_D = ['QAL','YLD','VOL','TRD']; d_disp = pd.DataFrame(index=D_UNI)
L624         d_disp['QAL'], d_disp['YLD'], d_disp['VOL'], d_disp['TRD'] = df_z.loc[D_UNI,'D_QAL'], df_z.loc[D_UNI,'D_YLD'], df_z.loc[D_UNI,'D_VOL_RAW'], df_z.loc[D_UNI,'D_TRD']
L625         dsc_series = pd.Series({t: d_score_all.get(t) for t in D_UNI}, name='DSC')
L626         self.d_table = pd.concat([d_disp, dsc_series], axis=1); self.d_table.index = [t + ("â­ï¸" if t in top_D else "") for t in D_UNI]
L627         self.d_formatters = {col:"{:.2f}".format for col in cols_D}; self.d_formatters['DSC']="{:.3f}".format
L628         import scorer
L629         dw_eff = scorer.D_WEIGHTS_EFF
L630         self.d_title = (f"[Dæ  / {N_D} / {_fmt_w(dw_eff)} / corrM={corrM} / "
L631                         f"LB={DRRS_D['lookback']} nPC={DRRS_D['n_pc']} Î³={DRRS_D['gamma']} Î»={DRRS_D['lam']} Î¼={CROSS_MU_GD} Î·={DRRS_D['eta']} shrink={DRRS_SHRINK}]")
L632         if near_D:
L633             add = [t for t in near_D if t not in set(D_UNI)][:10]
L634             if add:
L635                 d_disp2 = pd.DataFrame(index=add)
L636                 d_disp2['QAL'], d_disp2['YLD'], d_disp2['VOL'], d_disp2['TRD'] = df_z.loc[add,'D_QAL'], df_z.loc[add,'D_YLD'], df_z.loc[add,'D_VOL_RAW'], df_z.loc[add,'D_TRD']
L637                 near_tbl = pd.concat([d_disp2, pd.Series({t: d_score_all.get(t) for t in add}, name='DSC')], axis=1)
L638                 self.d_table = pd.concat([self.d_table, near_tbl], axis=0)
L639         print(self.d_title); print(self.d_table.to_string(formatters=self.d_formatters))
L640
L641         # === Changesï¼ˆIN ã® GSC/DSC ã‚’è¡¨ç¤ºã€‚OUT ã¯éŠ˜æŸ„åã®ã¿ï¼‰ ===
L642         in_list = sorted(set(list(top_G)+list(top_D)) - set(exist))
L643         out_list = sorted(set(exist) - set(list(top_G)+list(top_D)))
L644
L645         self.io_table = pd.DataFrame({
L646             'IN': pd.Series(in_list),
L647             '/ OUT': pd.Series(out_list)
L648         })
L649         g_list = [f"{g_score.get(t):.3f}" if pd.notna(g_score.get(t)) else 'â€”' for t in out_list]
L650         d_list = [f"{d_score_all.get(t):.3f}" if pd.notna(d_score_all.get(t)) else 'â€”' for t in out_list]
L651         self.io_table['GSC'] = pd.Series(g_list)
L652         self.io_table['DSC'] = pd.Series(d_list)
L653
L654         print("Changes:")
L655         print(self.io_table.to_string(index=False))
L656
L657         all_tickers = list(set(exist + list(top_G) + list(top_D) + [bench])); prices = yf.download(all_tickers, period='1y', auto_adjust=True, progress=False)['Close']
L658         ret = prices.pct_change(); portfolios = {'CUR':exist,'NEW':list(top_G)+list(top_D)}; metrics={}
L659         for name,ticks in portfolios.items():
L660             pr = ret[ticks].mean(axis=1, skipna=True).dropna(); cum = (1+pr).cumprod()-1; n = len(pr)
L661             if n>=252: ann_ret, ann_vol = (1+cum.iloc[-1])**(252/n)-1, pr.std()*np.sqrt(252)
L662             else: ann_ret, ann_vol = cum.iloc[-1], pr.std()*np.sqrt(n)
L663             sharpe, drawdown = ann_ret/ann_vol, (cum - cum.cummax()).min()
L664             if len(ticks)>=2:
L665                 C_raw = ret[ticks].corr(); RAW_rho = C_raw.mask(np.eye(len(ticks), dtype=bool)).stack().mean()
L666                 R = ret[ticks].dropna().to_numpy(); C_resid = Selector.residual_corr(R, n_pc=3, shrink=DRRS_SHRINK)
L667                 RESID_rho = float((C_resid.sum()-np.trace(C_resid))/(C_resid.shape[0]*(C_resid.shape[0]-1)))
L668             else: RAW_rho = RESID_rho = np.nan
L669             divy = ttm_div_yield_portfolio(ticks); metrics[name] = {'RET':ann_ret,'VOL':ann_vol,'SHP':sharpe,'MDD':drawdown,'RAWÏ':RAW_rho,'RESIDÏ':RESID_rho,'DIVY':divy}
L670         df_metrics = pd.DataFrame(metrics).T; df_metrics_pct = df_metrics.copy(); self.df_metrics = df_metrics
L671         for col in ['RET','VOL','MDD','DIVY']: df_metrics_pct[col] = df_metrics_pct[col]*100
L672         cols_order = ['RET','VOL','SHP','MDD','RAWÏ','RESIDÏ','DIVY']; df_metrics_pct = df_metrics_pct.reindex(columns=cols_order)
L673         def _fmt_row(s):
L674             return pd.Series({'RET':f"{s['RET']:.1f}%",'VOL':f"{s['VOL']:.1f}%",'SHP':f"{s['SHP']:.1f}",'MDD':f"{s['MDD']:.1f}%",'RAWÏ':(f"{s['RAWÏ']:.2f}" if pd.notna(s['RAWÏ']) else "NaN"),'RESIDÏ':(f"{s['RESIDÏ']:.2f}" if pd.notna(s['RESIDÏ']) else "NaN"),'DIVY':f"{s['DIVY']:.1f}%"})
L675         self.df_metrics_fmt = df_metrics_pct.apply(_fmt_row, axis=1); print("Performance Comparison:"); print(self.df_metrics_fmt.to_string())
L676         if self.debug:
L677             self.debug_table = pd.concat([df_z[['TR','EPS','REV','ROE','BETA','DIV','FCF','RS','TR_str','DIV_STREAK']], g_score.rename('GSC'), d_score_all.rename('DSC')], axis=1).round(3)
L678             print("Debug Data:"); print(self.debug_table.to_string())
L679
L680         # === è¿½åŠ : GSC+DSC ãŒä½ã„é † TOP10 ===
L681         try:
L682             all_scores = pd.DataFrame({'GSC': df_z['GSC'], 'DSC': df_z['DSC']}).copy()
L683             all_scores['G_plus_D'] = all_scores['GSC'] + all_scores['DSC']
L684             all_scores = all_scores.dropna(subset=['G_plus_D'])
L685             self.low10_table = all_scores.sort_values('G_plus_D', ascending=True).head(10).round(3)
L686             print("Low Score Candidates (GSC+DSC bottom 10):")
L687             print(self.low10_table.to_string())
L688         except Exception as e:
L689             print(f"[warn] low-score ranking failed: {e}")
L690             self.low10_table = None
L691
L692     # --- Slacké€ä¿¡ï¼ˆå…ƒ notify_slack ã®ãƒ­ã‚¸ãƒƒã‚¯ãã®ã¾ã¾ï¼‰ ---
L693     def notify_slack(self):
L694         SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")
L695         if not SLACK_WEBHOOK_URL: raise ValueError("SLACK_WEBHOOK_URL not set (ç’°å¢ƒå¤‰æ•°ãŒæœªè¨­å®šã§ã™)")
L696         def _filter_suffix_from(spec: dict, group: str) -> str:
L697             g = spec.get(group, {})
L698             parts = [str(m) for m in g.get("pre_mask", [])]
L699             for k, v in (g.get("pre_filter", {}) or {}).items():
L700                 base, op = (k[:-4], "<") if k.endswith("_max") else ((k[:-4], ">") if k.endswith("_min") else (k, "="))
L701                 name = {"beta": "Î²"}.get(base, base)
L702                 try: val = f"{float(v):g}"
L703                 except: val = str(v)
L704                 parts.append(f"{name}{op}{val}")
L705             return "" if not parts else " / filter:" + " & ".join(parts)
L706         def _inject_filter_suffix(title: str, group: str) -> str:
L707             suf = _filter_suffix_from(FILTER_SPEC, group)
L708             return f"{title[:-1]}{suf}]" if suf and title.endswith("]") else (title + suf)
L709         def _blk(title, tbl, fmt=None, drop=()):
L710             if tbl is None or getattr(tbl,'empty',False): return f"{title}\n(é¸å®šãªã—)\n"
L711             if drop and hasattr(tbl,'columns'):
L712                 keep = [c for c in tbl.columns if c not in drop]
L713                 tbl, fmt = tbl[keep], {k:v for k,v in (fmt or {}).items() if k in keep}
L714             return f"{title}\n```{tbl.to_string(formatters=fmt)}```\n"
L715
L716         g_title = _inject_filter_suffix(self.g_title, "G")
L717         d_title = _inject_filter_suffix(self.d_title, "D")
L718         message  = "ğŸ“ˆ ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼åˆ†æ•£æœ€é©åŒ–ã®çµæœ\n"
L719         if self.miss_df is not None and not self.miss_df.empty:
L720             message += "Missing Data\n```" + self.miss_df.to_string(index=False) + "```\n"
L721         message += _blk(g_title, self.g_table, self.g_formatters, drop=("TRD",))
L722         message += _blk(d_title, self.d_table, self.d_formatters)
L723         message += "Changes\n" + ("(å¤‰æ›´ãªã—)\n" if self.io_table is None or getattr(self.io_table,'empty',False) else f"```{self.io_table.to_string(index=False)}```\n")
L724         message += "Performance Comparison:\n```" + self.df_metrics_fmt.to_string() + "```"
L725         if self.debug and self.debug_table is not None:
L726             message += "\nDebug Data\n```" + self.debug_table.to_string() + "```"
L727         payload = {"text": message}
L728         try:
L729             resp = requests.post(SLACK_WEBHOOK_URL, json=payload); resp.raise_for_status(); print("âœ… Slackï¼ˆWebhookï¼‰ã¸é€ä¿¡ã—ã¾ã—ãŸ")
L730         except Exception as e: print(f"âš ï¸ Slacké€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
L731
L732
L733 def _infer_g_universe(feature_df, selected12=None, near5=None):
L734     try:
L735         out = feature_df.index[feature_df['group'].astype(str).str.upper().eq('G')].tolist()
L736         if out: return out
L737     except Exception:
L738         pass
L739     base = set()
L740     for lst in (selected12 or []), (near5 or []):
L741         for x in (lst or []): base.add(x)
L742     return list(base) if base else list(feature_df.index)
L743
L744
L745 def _fmt_with_fire_mark(tickers, feature_df):
L746     out = []
L747     for t in tickers or []:
L748         try:
L749             br = bool(feature_df.at[t, "G_BREAKOUT_recent_5d"])
L750             pb = bool(feature_df.at[t, "G_PULLBACK_recent_5d"])
L751             out.append(f"{t}{' ğŸ”¥' if (br or pb) else ''}")
L752         except Exception:
L753             out.append(t)
L754     return out
L755
L756
L757 def _label_recent_event(t, feature_df):
L758     try:
L759         br = bool(feature_df.at[t, "G_BREAKOUT_recent_5d"]); dbr = str(feature_df.at[t, "G_BREAKOUT_last_date"]) if br else ""
L760         pb = bool(feature_df.at[t, "G_PULLBACK_recent_5d"]); dpb = str(feature_df.at[t, "G_PULLBACK_last_date"]) if pb else ""
L761         if   br and not pb: return f"{t}ï¼ˆãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®š {dbr}ï¼‰"
L762         elif pb and not br: return f"{t}ï¼ˆæŠ¼ã—ç›®åç™º {dpb}ï¼‰"
L763         elif br and pb:     return f"{t}ï¼ˆãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®š {dbr}ï¼æŠ¼ã—ç›®åç™º {dpb}ï¼‰"
L764     except Exception:
L765         pass
L766     return t
L767
L768
L769 # ===== ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å¯è¦–åŒ–ï¼šG/Då…±é€šãƒ•ãƒ­ãƒ¼ï¼ˆå‡ºåŠ›ã¯ä¸å¤‰ï¼‰ ==============================
L770
L771 def io_build_input_bundle() -> InputBundle:
L772     """
L773     æ—¢å­˜ã®ã€ãƒ‡ãƒ¼ã‚¿å–å¾—â†’å‰å‡¦ç†ã€ã‚’å®Ÿè¡Œã—ã€InputBundle ã‚’è¿”ã™ã€‚
L774     å‡¦ç†å†…å®¹ãƒ»åˆ—åãƒ»ä¸¸ã‚ãƒ»ä¾‹å¤–ãƒ»ãƒ­ã‚°æ–‡è¨€ã¯ç¾è¡Œã©ãŠã‚Šï¼ˆå¤‰æ›´ç¦æ­¢ï¼‰ã€‚
L775     """
L776     inp = Input(cand=cand, exist=exist, bench=bench,
L777                 price_max=CAND_PRICE_MAX, finnhub_api_key=FINNHUB_API_KEY)
L778     state = inp.prepare_data()
L779     return InputBundle(
L780         cand=state["cand"], tickers=state["tickers"], bench=bench,
L781         data=state["data"], px=state["px"], spx=state["spx"],
L782         tickers_bulk=state["tickers_bulk"], info=state["info"],
L783         eps_df=state["eps_df"], fcf_df=state["fcf_df"],
L784         returns=state["returns"]
L785     )
L786
L787 def run_group(sc: Scorer, group: str, inb: InputBundle, cfg: PipelineConfig,
L788               n_target: int, prev_json_path: str) -> tuple[list, float, float, float]:
L789     """
L790     G/Dã‚’åŒä¸€æ‰‹é †ã§å‡¦ç†ï¼šæ¡ç‚¹â†’ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼â†’é¸å®šï¼ˆç›¸é–¢ä½æ¸›è¾¼ã¿ï¼‰ã€‚
L791     æˆ»ã‚Šå€¤ï¼š(pick, avg_res_corr, sum_score, objective)
L792     JSONä¿å­˜ã¯æ—¢å­˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆã‚­ãƒ¼åãƒ»ä¸¸ã‚æ¡ãƒ»é †åºï¼‰ã‚’è¸è¥²ã€‚
L793     """
L794     sc.cfg = cfg
L795
L796     if hasattr(sc, "score_build_features"):
L797         feat = sc.score_build_features(inb)
L798         if not hasattr(sc, "_feat_logged"):
L799             T.log("features built (scorer)")
L800             sc._feat_logged = True
L801         agg = sc.score_aggregate(feat, group, cfg) if hasattr(sc, "score_aggregate") else feat
L802     else:
L803         fb = sc.aggregate_scores(inb, cfg)
L804         if not hasattr(sc, "_feat_logged"):
L805             T.log("features built (scorer)")
L806             sc._feat_logged = True
L807         sc._feat = fb
L808         agg = fb.g_score if group == "G" else fb.d_score_all
L809         if group == "D" and hasattr(fb, "df"):
L810             agg = agg[fb.df['BETA'] < D_BETA_MAX]
L811
L812     if hasattr(sc, "filter_candidates"):
L813         mask = sc.filter_candidates(inb, agg, group, cfg)
L814         agg = agg[mask]
L815
L816     selector = Selector()
L817     prev = _load_prev(prev_json_path)
L818     if hasattr(sc, "select_diversified"):
L819         pick, avg_r, sum_sc, obj = sc.select_diversified(
L820             agg, group, cfg, n_target,
L821             selector=selector, prev_tickers=prev,
L822             corrM=cfg.drrs.corrM, shrink=cfg.drrs.shrink,
L823             cross_mu=cfg.drrs.cross_mu_gd
L824         )
L825     else:
L826         if group == "G":
L827             init = agg.nlargest(min(cfg.drrs.corrM, len(agg))).index.tolist()
L828             res = selector.select_bucket_drrs(
L829                 returns_df=inb.returns, score_ser=agg, pool_tickers=init, k=n_target,
L830                 n_pc=cfg.drrs.G.get("n_pc", 3), gamma=cfg.drrs.G.get("gamma", 1.2),
L831                 lam=cfg.drrs.G.get("lam", 0.68), eta=cfg.drrs.G.get("eta", 0.8),
L832                 lookback=cfg.drrs.G.get("lookback", 252), prev_tickers=prev,
L833                 shrink=cfg.drrs.shrink, g_fixed_tickers=None, mu=0.0
L834             )
L835         else:
L836             init = agg.nlargest(min(cfg.drrs.corrM, len(agg))).index.tolist()
L837             g_fixed = getattr(sc, "_top_G", None)
L838             res = selector.select_bucket_drrs(
L839                 returns_df=inb.returns, score_ser=agg, pool_tickers=init, k=n_target,
L840                 n_pc=cfg.drrs.D.get("n_pc", 4), gamma=cfg.drrs.D.get("gamma", 0.8),
L841                 lam=cfg.drrs.D.get("lam", 0.85), eta=cfg.drrs.D.get("eta", 0.5),
L842                 lookback=cfg.drrs.D.get("lookback", 504), prev_tickers=prev,
L843                 shrink=cfg.drrs.shrink, g_fixed_tickers=g_fixed,
L844                 mu=cfg.drrs.cross_mu_gd
L845             )
L846         pick = res["tickers"]; avg_r = res["avg_res_corr"]
L847         sum_sc = res["sum_score"]; obj = res["objective"]
L848         if group == "D":
L849             _, pick = _disjoint_keepG(getattr(sc, "_top_G", []), pick, init)
L850             T.log("selection finalized (G/D)")
L851     # --- Near-Miss: æƒœã—ãã‚‚é¸ã°ã‚Œãªã‹ã£ãŸä¸Šä½10ã‚’ä¿æŒï¼ˆSlackè¡¨ç¤ºç”¨ï¼‰ ---
L852     # 5) Near-Miss ã¨æœ€çµ‚é›†è¨ˆSeriesã‚’ä¿æŒï¼ˆè¡¨ç¤ºå°‚ç”¨ã€‚è¨ˆç®—ã¸å½±éŸ¿ãªã—ï¼‰
L853     try:
L854         pool = agg.drop(index=[t for t in pick if t in agg.index], errors="ignore")
L855         near10 = list(pool.sort_values(ascending=False).head(10).index)
L856         setattr(sc, f"_near_{group}", near10)
L857         setattr(sc, f"_agg_{group}", agg)
L858     except Exception:
L859         pass
L860
L861     _save_sel(prev_json_path, pick, avg_r, sum_sc, obj)
L862     if group == "D":
L863         T.log("save done")
L864     if group == "G":
L865         sc._top_G = pick
L866     return pick, avg_r, sum_sc, obj
L867
L868 def run_pipeline() -> SelectionBundle:
L869     """
L870     G/Då…±é€šãƒ•ãƒ­ãƒ¼ã®å…¥å£ã€‚I/Oã¯ã“ã“ã ã‘ã§å®Ÿæ–½ã—ã€è¨ˆç®—ã¯Scorerã«å§”è­²ã€‚
L871     Slackæ–‡è¨€ãƒ»ä¸¸ã‚ãƒ»é †åºã¯æ—¢å­˜ã® Output ã‚’ç”¨ã„ã¦å¤‰æ›´ã—ãªã„ã€‚
L872     """
L873     inb = io_build_input_bundle()
L874     cfg = PipelineConfig(
L875         weights=WeightsConfig(g=g_weights, d=D_weights),
L876         drrs=DRRSParams(corrM=corrM, shrink=DRRS_SHRINK,
L877                          G=DRRS_G, D=DRRS_D, cross_mu_gd=CROSS_MU_GD),
L878         price_max=CAND_PRICE_MAX
L879     )
L880     sc = Scorer()
L881     top_G, avgG, sumG, objG = run_group(sc, "G", inb, cfg, N_G, G_PREV_JSON)
L882     poolG = list(getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False).index)
L883     alpha = Scorer.spx_to_alpha(inb.spx)
L884     sectors = {t: (inb.info.get(t, {}).get("sector") or "U") for t in poolG}
L885     scores = {t: Scorer.g_score.get(t, 0.0) for t in poolG}
L886     top_G = Scorer.pick_top_softcap(scores, sectors, N=N_G, cap=2, alpha=alpha, hard=5)
L887     sc._top_G = top_G
L888     try:
L889         aggG = getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False)
L890         sc._near_G = [t for t in aggG.index if t not in set(top_G)][:10]
L891     except Exception:
L892         pass
L893     base = sum(Scorer.g_score.get(t,0.0) for t in poolG[:N_G])
L894     effs = sum(Scorer.g_score.get(t,0.0) for t in top_G)
L895     print(f"[soft_cap2] score_cost={(base-effs)/max(1e-9,abs(base)):.2%}, alpha={alpha:.3f}")
L896     top_D, avgD, sumD, objD = run_group(sc, "D", inb, cfg, N_D, D_PREV_JSON)
L897     fb = getattr(sc, "_feat", None)
L898     near_G = getattr(sc, "_near_G", [])
L899     selected12 = list(top_G)
L900     df = fb.df if fb is not None else pd.DataFrame()
L901     guni = _infer_g_universe(df, selected12, near_G)
L902     try:
L903         fire_recent = [t for t in guni
L904                        if (str(df.at[t, "G_BREAKOUT_recent_5d"]) == "True") or
L905                           (str(df.at[t, "G_PULLBACK_recent_5d"]) == "True")]
L906     except Exception:
L907         fire_recent = []
L908     lines = [
L909         "ã€Gæ ãƒ¬ãƒãƒ¼ãƒˆï½œé€±æ¬¡ãƒ¢ãƒ‹ã‚¿ï¼ˆç›´è¿‘5å–¶æ¥­æ—¥ï¼‰ã€‘",
L910         "ã€å‡¡ä¾‹ã€‘ğŸ”¥=ç›´è¿‘5å–¶æ¥­æ—¥å†…ã«ã€Œãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®šã€ã¾ãŸã¯ã€ŒæŠ¼ã—ç›®åç™ºã€ã‚’æ¤œçŸ¥",
L911         f"é¸å®š12: {', '.join(_fmt_with_fire_mark(selected12, df))}" if selected12 else "é¸å®š12: ãªã—",
L912         f"æ¬¡ç‚¹10: {', '.join(_fmt_with_fire_mark(near_G, df))}" if near_G else "æ¬¡ç‚¹10: ãªã—",
L913     ]
L914     if fire_recent:
L915         fire_list = ", ".join([_label_recent_event(t, df) for t in fire_recent])
L916         lines.append(f"éå»5å–¶æ¥­æ—¥ã®æ¤œçŸ¥: {fire_list}")
L917     else:
L918         lines.append("éå»5å–¶æ¥­æ—¥ã®æ¤œçŸ¥: ãªã—")
L919     try:
L920         webhook = os.environ.get("SLACK_WEBHOOK_URL", "")
L921         if webhook:
L922             requests.post(webhook, json={"text": "\n".join(lines)}, timeout=10)
L923     except Exception:
L924         pass
L925
L926     out = Output(debug=debug_mode)
L927     # è¡¨ç¤ºå´ã‹ã‚‰é¸å®šæ™‚ã®é›†è¨ˆã¸ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†ã«ä¿æŒï¼ˆè¡¨ç¤ºå°‚ç”¨ãƒ»å‰¯ä½œç”¨ãªã—ï¼‰
L928     try: out._sc = sc
L929     except Exception: pass
L930     if hasattr(sc, "_feat"):
L931         try:
L932             out.miss_df = sc._feat.missing_logs
L933             out.display_results(
L934                 exist=exist, bench=bench, df_z=sc._feat.df_z,
L935                 g_score=sc._feat.g_score, d_score_all=sc._feat.d_score_all,
L936                 init_G=top_G, init_D=top_D, top_G=top_G, top_D=top_D
L937             )
L938         except Exception:
L939             pass
L940     out.notify_slack()
L941     sb = SelectionBundle(
L942         resG={"tickers": top_G, "avg_res_corr": avgG,
L943               "sum_score": sumG, "objective": objG},
L944         resD={"tickers": top_D, "avg_res_corr": avgD,
L945               "sum_score": sumD, "objective": objD},
L946         top_G=top_G, top_D=top_D, init_G=top_G, init_D=top_D
L947     )
L948
L949     # --- Low Score Candidates (GSC+DSC bottom 10) : send before debug dump ---
L950     try:
L951         _low_df = (
L952             pd.DataFrame({"GSC": fb.g_score, "DSC": fb.d_score_all})
L953               .assign(G_plus_D=lambda x: x["GSC"] + x["DSC"])
L954               .sort_values("G_plus_D")
L955               .head(10)
L956               .round(3)
L957         )
L958         _slack("Low Score Candidates (GSC+DSC bottom 10)\n"
L959                "```"
L960                + _low_df.to_string(index=True, index_names=False)
L961                + "\n```")
L962     except Exception as _e:
L963         _slack(f"Low Score Candidates: ä½œæˆå¤±æ•—: {_e}")
L964
L965     if debug_mode:
L966         prevG, prevD = _load_prev(G_PREV_JSON), _load_prev(D_PREV_JSON)
L967         try:
L968             _slack_debug(_compact_debug(fb, sb, prevG, prevD))
L969         except Exception as e:
L970             print(f"[debug skipped] {e}")
L971
L972     return sb
L973
L974 if __name__ == "__main__":
L975     run_pipeline()
```

## <scorer.py>
```text
L1 # scorer.py 
L2 # kawatest
L3 # =============================================================================
L4 # Scorer: ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼/æŒ‡æ¨™ã®ç”Ÿæˆã¨åˆæˆã‚¹ã‚³ã‚¢ç®—å‡ºã‚’æ‹…ã†ç´”ç²‹å±¤
L5 #
L6 # ã€ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã ã‘èª­ã‚ã°åˆ†ã‹ã‚‹ãƒã‚¤ãƒ³ãƒˆã€‘
L7 # - å…¥åŠ›(InputBundle)ã¯ã€Œä¾¡æ ¼/å‡ºæ¥é«˜/ãƒ™ãƒ³ãƒ/åŸºæœ¬æƒ…å ±/EPS/FCF/ãƒªã‚¿ãƒ¼ãƒ³ã€ã‚’å«ã‚€DTO
L8 # - å‡ºåŠ›(FeatureBundle)ã¯ã€Œrawç‰¹å¾´é‡ dfã€ã€Œæ¨™æº–åŒ– df_zã€ã€ŒG/D ã‚¹ã‚³ã‚¢ã€ã€Œæ¬ æãƒ­ã‚°ã€
L9 # - é‡ã¿ç­‰ã®ã‚³ãƒ³ãƒ•ã‚£ã‚°(PipelineConfig)ã¯ factor ã‹ã‚‰æ¸¡ã™ï¼ˆcfg å¿…é ˆï¼‰
L10 # - æ—§ã‚«ãƒ©ãƒ åã¯ Scorer å†…ã§è‡ªå‹•ãƒªãƒãƒ¼ãƒ ã—ã¦å—ã‘å…¥ã‚Œï¼ˆå¾Œæ–¹äº’æ›ï¼‰
L11 #   ä¾‹) eps_ttm -> EPS_TTM, eps_q_recent -> EPS_Q_LastQ, fcf_ttm -> FCF_TTM
L12 #
L13 # ã€I/Oå¥‘ç´„ï¼ˆScorerãŒå‚ç…§ã™ã‚‹InputBundleãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰ã€‘
L14 #   - cand: List[str]    â€¦ å€™è£œéŠ˜æŸ„ï¼ˆå˜ä½“å®Ÿè¡Œã§ã¯æœªä½¿ç”¨ï¼‰
L15 #   - tickers: List[str] â€¦ å¯¾è±¡éŠ˜æŸ„ãƒªã‚¹ãƒˆ
L16 #   - bench: str         â€¦ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼ï¼ˆä¾‹ '^GSPC'ï¼‰
L17 #   - data: pd.DataFrame â€¦ yfinance downloadçµæœ ('Close','Volume' ç­‰ã®éšå±¤åˆ—)
L18 #   - px: pd.DataFrame   â€¦ data['Close'] ç›¸å½“ï¼ˆçµ‚å€¤ï¼‰
L19 #   - spx: pd.Series     â€¦ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®çµ‚å€¤
L20 #   - tickers_bulk: object         â€¦ yfinance.Tickers
L21 #   - info: Dict[str, dict]        â€¦ yfinance info per ticker
L22 #   - eps_df: pd.DataFrame         â€¦ å¿…é ˆåˆ—: EPS_TTM, EPS_Q_LastQï¼ˆæ—§åã‚‚å¯ï¼‰
L23 #   - fcf_df: pd.DataFrame         â€¦ å¿…é ˆåˆ—: FCF_TTMï¼ˆæ—§åã‚‚å¯ï¼‰
L24 #   - returns: pd.DataFrame        â€¦ px[tickers].pct_change() ç›¸å½“
L25 #
L26 # â€»å…¥å‡ºåŠ›ã®å½¢å¼ãƒ»ä¾‹å¤–æ–‡è¨€ã¯æ—¢å­˜å®Ÿè£…ã‚’å¤‰ãˆã¾ã›ã‚“ï¼ˆå®‰å…¨ãªçŸ­ç¸®ã®ã¿ï¼‰
L27 # =============================================================================
L28
L29 import os
L30 import requests
L31 import numpy as np
L32 import pandas as pd
L33 import yfinance as yf
L34 from typing import Any, TYPE_CHECKING
L35 from scipy.stats import zscore
L36
L37 if TYPE_CHECKING:
L38     from factor import PipelineConfig  # type: ignore  # å®Ÿè¡Œæ™‚importãªã—ï¼ˆå¾ªç’°å›é¿ï¼‰
L39
L40 # ---- Dividend Helpers -------------------------------------------------------
L41 def _last_close(t, price_map=None):
L42     if price_map and (c := price_map.get(t)) is not None:
L43         return float(c)
L44     try:
L45         h = yf.Ticker(t).history(period="5d")["Close"].dropna()
L46         return float(h.iloc[-1]) if len(h) else np.nan
L47     except Exception:
L48         return np.nan
L49
L50 def _ttm_div_sum(t, lookback_days=400):
L51     try:
L52         div = yf.Ticker(t).dividends
L53         if div is None or len(div) == 0:
L54             return 0.0
L55         cutoff = pd.Timestamp.utcnow().tz_localize(None) - pd.Timedelta(days=lookback_days)
L56         ttm = float(div[div.index.tz_localize(None) >= cutoff].sum())
L57         return ttm if ttm > 0 else float(div.tail(4).sum())
L58     except Exception:
L59         return 0.0
L60
L61 def ttm_div_yield_portfolio(tickers, price_map=None):
L62     ys = []
L63     for t in tickers:
L64         c = _last_close(t, price_map)
L65         if not np.isfinite(c) or c <= 0:
L66             ys.append(0.0)
L67             continue
L68         s = _ttm_div_sum(t)
L69         ys.append(s / c if s > 0 else 0.0)
L70     return float(np.mean(ys)) if ys else 0.0
L71
L72 # ---- ç°¡æ˜“ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆå®‰å…¨ãªçŸ­ç¸®ã®ã¿ï¼‰ -----------------------------------
L73 def winsorize_s(s: pd.Series, p=0.02):
L74     if s is None or s.dropna().empty: return s
L75     lo, hi = np.nanpercentile(s.astype(float), [100*p, 100*(1-p)]); return s.clip(lo, hi)
L76
L77 def robust_z(s: pd.Series, p=0.02):
L78     s2 = winsorize_s(s, p); return np.nan_to_num(zscore(s2.fillna(s2.mean())))
L79
L80 def _safe_div(a, b):
L81     try:
L82         if b is None or float(b)==0 or pd.isna(b): return np.nan
L83         return float(a)/float(b)
L84     except Exception: return np.nan
L85
L86 def _safe_last(series: pd.Series, default=np.nan):
L87     try: return float(series.iloc[-1])
L88     except Exception: return default
L89
L90 D_WEIGHTS_EFF = None  # å‡ºåŠ›è¡¨ç¤ºäº’æ›ã®ãŸã‚
L91
L92 # ---- Scorer æœ¬ä½“ -------------------------------------------------------------
L93 class Scorer:
L94     """
L95     - factor.py ã‹ã‚‰ã¯ `aggregate_scores(ib, cfg)` ã‚’å‘¼ã¶ã ã‘ã§OKã€‚
L96     - cfg ã¯å¿…é ˆï¼ˆfactor.PipelineConfig ã‚’æ¸¡ã™ï¼‰ã€‚
L97     - æ—§ã‚«ãƒ©ãƒ åã‚’è‡ªå‹•ãƒªãƒãƒ¼ãƒ ã—ã¦æ–°ã‚¹ã‚­ãƒ¼ãƒã«å¸åã—ã¾ã™ã€‚
L98     """
L99
L100     # === å…ˆé ­ã§æ—§â†’æ–°ã‚«ãƒ©ãƒ åãƒãƒƒãƒ—ï¼ˆç§»è¡Œç”¨ï¼‰ ===
L101     EPS_RENAME = {"eps_ttm":"EPS_TTM", "eps_q_recent":"EPS_Q_LastQ"}
L102     FCF_RENAME = {"fcf_ttm":"FCF_TTM"}
L103
L104     # === ã‚¹ã‚­ãƒ¼ãƒç°¡æ˜“ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€ä½é™ï¼‰ ===
L105     @staticmethod
L106     def _validate_ib_for_scorer(ib: Any):
L107         must_attrs = ["tickers","bench","data","px","spx","tickers_bulk","info","eps_df","fcf_df","returns"]
L108         miss = [a for a in must_attrs if not hasattr(ib, a) or getattr(ib, a) is None]
L109         if miss: raise ValueError(f"InputBundle is missing required attributes for Scorer: {miss}")
L110
L111         # å¾Œæ–¹äº’æ›ã®ãŸã‚ã€ã¾ãš rename ã‚’è©¦ã¿ã‚‹
L112         if any(c in ib.eps_df.columns for c in Scorer.EPS_RENAME.keys()):
L113             ib.eps_df.rename(columns=Scorer.EPS_RENAME, inplace=True)
L114         if any(c in ib.fcf_df.columns for c in Scorer.FCF_RENAME.keys()):
L115             ib.fcf_df.rename(columns=Scorer.FCF_RENAME, inplace=True)
L116
L117         # å¿…é ˆåˆ—ã®å­˜åœ¨ç¢ºèª
L118         need_eps = {"EPS_TTM","EPS_Q_LastQ"}
L119         need_fcf = {"FCF_TTM"}
L120         if not need_eps.issubset(set(ib.eps_df.columns)):
L121             raise ValueError(f"eps_df must contain columns {need_eps} (accepts old names via auto-rename). Got: {list(ib.eps_df.columns)}")
L122         if not need_fcf.issubset(set(ib.fcf_df.columns)):
L123             raise ValueError(f"fcf_df must contain columns {need_fcf} (accepts old names via auto-rename). Got: {list(ib.fcf_df.columns)}")
L124
L125     # ----ï¼ˆScorerå°‚ç”¨ï¼‰ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ»æŒ‡æ¨™ç³» ----
L126     @staticmethod
L127     def trend(s: pd.Series):
L128         if len(s)<200: return np.nan
L129         sma50, sma150, sma200 = s.rolling(50).mean().iloc[-1], s.rolling(150).mean().iloc[-1], s.rolling(200).mean().iloc[-1]
L130         prev200, p = s.rolling(200).mean().iloc[-21], s.iloc[-1]
L131         lo_52 = s[-252:].min() if len(s)>=252 else s.min(); hi_52 = s[-252:].max() if len(s)>=252 else s.max()
L132         rng = (hi_52 - lo_52) if hi_52>lo_52 else np.nan
L133         clip = lambda x,lo,hi: (np.nan if pd.isna(x) else max(lo,min(hi,x)))
L134         a = clip(p/(s.rolling(50).mean().iloc[-1]) - 1, -0.5, 0.5)
L135         b = clip(sma50/sma150 - 1, -0.5, 0.5)
L136         c = clip(sma150/sma200 - 1, -0.5, 0.5)
L137         d = clip(sma200/prev200 - 1, -0.2, 0.2)
L138         e = clip((p - lo_52) / (rng if rng and rng>0 else np.nan) - 0.5, -0.5, 0.5)
L139         parts = [0.0 if pd.isna(x) else x for x in (a,b,c,d,e)]
L140         return 0.30*parts[0] + 0.20*parts[1] + 0.15*parts[2] + 0.15*parts[3] + 0.20*parts[4]
L141
L142     @staticmethod
L143     def rs(s, b):
L144         n, nb = len(s), len(b)
L145         if n<60 or nb<60: return np.nan
L146         L12 = 252 if n>=252 and nb>=252 else min(n,nb)-1; L1 = 22 if n>=22 and nb>=22 else max(5, min(n,nb)//3)
L147         r12, r1, br12, br1 = s.iloc[-1]/s.iloc[-L12]-1, s.iloc[-1]/s.iloc[-L1]-1, b.iloc[-1]/b.iloc[-L12]-1, b.iloc[-1]/b.iloc[-L1]-1
L148         return (r12 - br12)*0.7 + (r1 - br1)*0.3
L149
L150     @staticmethod
L151     def tr_str(s):
L152         if len(s)<50: return np.nan
L153         return s.iloc[-1]/s.rolling(50).mean().iloc[-1] - 1
L154
L155     @staticmethod
L156     def rs_line_slope(s: pd.Series, b: pd.Series, win: int) -> float:
L157         r = (s/b).dropna()
L158         if len(r)<win: return np.nan
L159         y, x = np.log(r.iloc[-win:]), np.arange(win, dtype=float)
L160         try: return float(np.polyfit(x, y, 1)[0])
L161         except Exception: return np.nan
L162
L163     @staticmethod
L164     def ev_fallback(info_t: dict, tk: yf.Ticker) -> float:
L165         ev = info_t.get('enterpriseValue', np.nan)
L166         if pd.notna(ev) and ev>0: return float(ev)
L167         mc, debt, cash = info_t.get('marketCap', np.nan), np.nan, np.nan
L168         try:
L169             bs = tk.quarterly_balance_sheet
L170             if bs is not None and not bs.empty:
L171                 c = bs.columns[0]
L172                 for k in ("Total Debt","Long Term Debt","Short Long Term Debt"):
L173                     if k in bs.index: debt = float(bs.loc[k,c]); break
L174                 for k in ("Cash And Cash Equivalents","Cash And Cash Equivalents And Short Term Investments","Cash"):
L175                     if k in bs.index: cash = float(bs.loc[k,c]); break
L176         except Exception: pass
L177         if pd.notna(mc): return float(mc + (0 if pd.isna(debt) else debt) - (0 if pd.isna(cash) else cash))
L178         return np.nan
L179
L180     @staticmethod
L181     def dividend_status(ticker: str) -> str:
L182         t = yf.Ticker(ticker)
L183         try:
L184             if not t.dividends.empty: return "has"
L185         except Exception: return "unknown"
L186         try:
L187             a = t.actions
L188             if a is not None and not a.empty and "Stock Splits" in a.columns and a["Stock Splits"].abs().sum()>0: return "none_confident"
L189         except Exception: pass
L190         try:
L191             fi = t.fast_info
L192             if any(getattr(fi,k,None) for k in ("last_dividend_date","dividend_rate","dividend_yield")): return "maybe_missing"
L193         except Exception: pass
L194         return "unknown"
L195
L196     @staticmethod
L197     def div_streak(t):
L198         try:
L199             divs = yf.Ticker(t).dividends.dropna(); ann = divs.groupby(divs.index.year).sum(); ann = ann[ann.index<pd.Timestamp.today().year]
L200             years, streak = sorted(ann.index), 0
L201             for i in range(len(years)-1,0,-1):
L202                 if ann[years[i]] > ann[years[i-1]]: streak += 1
L203                 else: break
L204             return streak
L205         except Exception: return 0
L206
L207     @staticmethod
L208     def fetch_finnhub_metrics(symbol):
L209         api_key = os.environ.get("FINNHUB_API_KEY")
L210         if not api_key: return {}
L211         url, params = "https://finnhub.io/api/v1/stock/metric", {"symbol":symbol,"metric":"all","token":api_key}
L212         try:
L213             r = requests.get(url, params=params, timeout=10); r.raise_for_status(); m = r.json().get("metric",{})
L214             return {'EPS':m.get('epsGrowthTTMYoy'),'REV':m.get('revenueGrowthTTMYoy'),'ROE':m.get('roeTTM'),'BETA':m.get('beta'),'DIV':m.get('dividendYieldIndicatedAnnual'),'FCF':(m.get('freeCashFlowTTM')/m.get('enterpriseValue')) if m.get('freeCashFlowTTM') and m.get('enterpriseValue') else None}
L215         except Exception: return {}
L216
L217     @staticmethod
L218     def calc_beta(series: pd.Series, market: pd.Series, lookback=252):
L219         r, m = series.pct_change().dropna(), market.pct_change().dropna()
L220         n = min(len(r), len(m), lookback)
L221         if n<60: return np.nan
L222         r, m = r.iloc[-n:], m.iloc[-n:]; cov, var = np.cov(r, m)[0,1], np.var(m)
L223         return np.nan if var==0 else cov/var
L224
L225     @staticmethod
L226     def spx_to_alpha(spx: pd.Series, bands=(0.03,0.10), w=(0.6,0.4),
L227                      span=5, q=(0.20,0.40), alphas=(0.05,0.08,0.10)) -> float:
L228         """
L229         S&P500æŒ‡æ•°ã®ã¿ã‹ã‚‰æ“¬ä¼¼breadthã‚’ä½œã‚Šã€å±¥æ­´åˆ†ä½ã§Î±ã‚’æ®µéšæ±ºå®šã€‚
L230         bands=(Â±3%, Â±10%), w=(50DMA,200DMA), åˆ†ä½q=(20%,40%), alphas=(ä½,ä¸­,é«˜)
L231         """
L232         ma50, ma200 = spx.rolling(50).mean(), spx.rolling(200).mean()
L233         b50  = ((spx/ma50 - 1) + bands[0])/(2*bands[0])
L234         b200 = ((spx/ma200 - 1) + bands[1])/(2*bands[1])
L235         hist = (w[0]*b50 + w[1]*b200).clip(0,1).ewm(span=span).mean()
L236         b = float(hist.iloc[-1])
L237         lo, mid = float(hist.quantile(q[0])), float(hist.quantile(q[1]))
L238         return alphas[0] if b < lo else alphas[1] if b < mid else alphas[2]
L239
L240     @staticmethod
L241     def soft_cap_effective_scores(scores: pd.Series|dict, sectors: dict, cap=2, alpha=0.08) -> pd.Series:
L242         """
L243         åŒä¸€ã‚»ã‚¯ã‚¿ãƒ¼capè¶…éï¼ˆ3æœ¬ç›®ä»¥é™ï¼‰ã« Î±Ã—æ®µéšæ¸›ç‚¹ã‚’èª²ã—ãŸâ€œæœ‰åŠ¹ã‚¹ã‚³ã‚¢â€Seriesã‚’è¿”ã™ã€‚
L244         æˆ»ã‚Šå€¤ã¯é™é †ã‚½ãƒ¼ãƒˆæ¸ˆã¿ã€‚
L245         """
L246         s = pd.Series(scores, dtype=float); order = s.sort_values(ascending=False).index
L247         cnt, pen = {}, {}
L248         for t in order:
L249             sec = sectors.get(t, "U")
L250             k = cnt.get(sec, 0) + 1
L251             pen[t] = alpha * max(0, k - cap)
L252             cnt[sec] = k
L253         return (s - pd.Series(pen)).sort_values(ascending=False)
L254
L255     @staticmethod
L256     def pick_top_softcap(scores: pd.Series|dict, sectors: dict, N: int, cap=2, alpha=0.08, hard: int|None=5) -> list[str]:
L257         """
L258         soft-capé©ç”¨å¾Œã®ä¸Šä½Nãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚’è¿”ã™ã€‚hard>0ãªã‚‰éå¸¸ç”¨ãƒãƒ¼ãƒ‰ä¸Šé™ã§åŒä¸€ã‚»ã‚¯ã‚¿ãƒ¼è¶…éã‚’é–“å¼•ãï¼ˆæ—¢å®š=5ï¼‰ã€‚
L259         """
L260         eff = Scorer.soft_cap_effective_scores(scores, sectors, cap, alpha)
L261         if not hard:
L262             return list(eff.head(N).index)
L263         pick, used = [], {}
L264         for t in eff.index:
L265             s = sectors.get(t, "U")
L266             if used.get(s, 0) < hard:
L267                 pick.append(t)
L268                 used[s] = used.get(s, 0) + 1
L269             if len(pick) == N:
L270                 break
L271         return pick
L272
L273     # ---- ã‚¹ã‚³ã‚¢é›†è¨ˆï¼ˆDTO/Configã‚’å—ã‘å–ã‚Šã€FeatureBundleã‚’è¿”ã™ï¼‰ ----
L274     def aggregate_scores(self, ib: Any, cfg):
L275         if cfg is None:
L276             raise ValueError("cfg is required; pass factor.PipelineConfig")
L277         self._validate_ib_for_scorer(ib)
L278
L279         px, spx, tickers = ib.px, ib.spx, ib.tickers
L280         tickers_bulk, info, eps_df, fcf_df = ib.tickers_bulk, ib.info, ib.eps_df, ib.fcf_df
L281         families = set(getattr(cfg.weights,'g',{})) | set(getattr(cfg.weights,'d',{}))
L282
L283         df, missing_logs = pd.DataFrame(index=tickers), []
L284         for t in tickers:
L285             d, s = info[t], px[t]; ev = self.ev_fallback(d, tickers_bulk.tickers[t])
L286             # --- åŸºæœ¬ç‰¹å¾´ ---
L287             df.loc[t,'TR']   = self.trend(s)
L288             df.loc[t,'EPS']  = eps_df.loc[t,'EPS_TTM'] if t in eps_df.index else np.nan
L289             df.loc[t,'REV']  = d.get('revenueGrowth',np.nan)
L290             df.loc[t,'ROE']  = d.get('returnOnEquity',np.nan)
L291             df.loc[t,'BETA'] = self.calc_beta(s, spx, lookback=252)
L292
L293             # --- é…å½“ï¼ˆæ¬ æè£œå®Œå«ã‚€ï¼‰ ---
L294             div = d.get('dividendYield') if d.get('dividendYield') is not None else d.get('trailingAnnualDividendYield')
L295             if div is None or pd.isna(div):
L296                 try:
L297                     divs = yf.Ticker(t).dividends
L298                     if divs is not None and not divs.empty:
L299                         last_close = s.iloc[-1]; div_1y = divs[divs.index >= (divs.index.max() - pd.Timedelta(days=365))].sum()
L300                         if last_close and last_close>0: div = float(div_1y/last_close)
L301                 except Exception: pass
L302             df.loc[t,'DIV'] = 0.0 if (div is None or pd.isna(div)) else float(div)
L303
L304             # --- FCF/EV ---
L305             fcf_val = fcf_df.loc[t,'FCF_TTM'] if t in fcf_df.index else np.nan
L306             df.loc[t,'FCF'] = (fcf_val/ev) if (pd.notna(fcf_val) and pd.notna(ev) and ev>0) else np.nan
L307
L308             # --- ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ãƒ»ãƒœãƒ©é–¢é€£ ---
L309             df.loc[t,'RS'], df.loc[t,'TR_str'] = self.rs(s, spx), self.tr_str(s)
L310             r, rm = s.pct_change().dropna(), spx.pct_change().dropna()
L311             n = int(min(len(r), len(rm)))
L312
L313             DOWNSIDE_DEV = np.nan
L314             if n>=60:
L315                 r6 = r.iloc[-min(len(r),126):]; neg = r6[r6<0]
L316                 if len(neg)>=10: DOWNSIDE_DEV = float(neg.std(ddof=0)*np.sqrt(252))
L317             df.loc[t,'DOWNSIDE_DEV'] = DOWNSIDE_DEV
L318
L319             MDD_1Y = np.nan
L320             try:
L321                 w = s.iloc[-min(len(s),252):].dropna()
L322                 if len(w)>=30:
L323                     roll_max = w.cummax(); MDD_1Y = float((w/roll_max - 1.0).min())
L324             except Exception: pass
L325             df.loc[t,'MDD_1Y'] = MDD_1Y
L326
L327             RESID_VOL = np.nan
L328             if n>=120:
L329                 rr, rrm = r.iloc[-n:].align(rm.iloc[-n:], join='inner')
L330                 if len(rr)==len(rrm) and len(rr)>=120 and rrm.var()>0:
L331                     beta = float(np.cov(rr, rrm)[0,1]/np.var(rrm)); resid = rr - beta*rrm
L332                     RESID_VOL = float(resid.std(ddof=0)*np.sqrt(252))
L333             df.loc[t,'RESID_VOL'] = RESID_VOL
L334
L335             DOWN_OUTPERF = np.nan
L336             if n>=60:
L337                 m, x = rm.iloc[-n:], r.iloc[-n:]; mask = m<0
L338                 if mask.sum()>=10:
L339                     mr, sr = float(m[mask].mean()), float(x[mask].mean())
L340                     DOWN_OUTPERF = (sr - mr)/abs(mr) if mr!=0 else np.nan
L341             df.loc[t,'DOWN_OUTPERF'] = DOWN_OUTPERF
L342
L343             # --- é•·æœŸç§»å‹•å¹³å‡/ä½ç½® ---
L344             sma200 = s.rolling(200).mean(); df.loc[t,'EXT_200'] = np.nan
L345             if pd.notna(sma200.iloc[-1]) and sma200.iloc[-1]!=0: df.loc[t,'EXT_200'] = abs(float(s.iloc[-1]/sma200.iloc[-1]-1.0))
L346
L347             # --- é…å½“ã®è©³ç´°ç³» ---
L348             DIV_TTM_PS=DIV_VAR5=DIV_YOY=DIV_FCF_COVER=np.nan
L349             try:
L350                 divs = yf.Ticker(t).dividends.dropna()
L351                 if not divs.empty:
L352                     last_close = s.iloc[-1]; div_1y = float(divs[divs.index >= (divs.index.max()-pd.Timedelta(days=365))].sum())
L353                     DIV_TTM_PS = div_1y if div_1y>0 else np.nan
L354                     ann = divs.groupby(divs.index.year).sum()
L355                     if len(ann)>=2 and ann.iloc[-2]!=0: DIV_YOY = float(ann.iloc[-1]/ann.iloc[-2]-1.0)
L356                     tail = ann.iloc[-5:] if len(ann)>=5 else ann
L357                     if len(tail)>=3 and tail.mean()!=0: DIV_VAR5 = float(tail.std(ddof=1)/abs(tail.mean()))
L358                 so = d.get('sharesOutstanding',None)
L359                 if so and pd.notna(DIV_TTM_PS) and pd.notna(fcf_val) and fcf_val!=0:
L360                     DIV_FCF_COVER = float((fcf_val)/(DIV_TTM_PS*float(so)))
L361             except Exception: pass
L362             df.loc[t,'DIV_TTM_PS'], df.loc[t,'DIV_VAR5'], df.loc[t,'DIV_YOY'], df.loc[t,'DIV_FCF_COVER'] = DIV_TTM_PS, DIV_VAR5, DIV_YOY, DIV_FCF_COVER
L363
L364             # --- è²¡å‹™å®‰å®šæ€§ ---
L365             df.loc[t,'DEBT2EQ'], df.loc[t,'CURR_RATIO'] = d.get('debtToEquity',np.nan), d.get('currentRatio',np.nan)
L366
L367             # --- EPS å¤‰å‹• ---
L368             EPS_VAR_8Q = np.nan
L369             try:
L370                 qe, so = tickers_bulk.tickers[t].quarterly_earnings, d.get('sharesOutstanding',None)
L371                 if qe is not None and not qe.empty and so:
L372                     eps_q = (qe['Earnings'].dropna().astype(float)/float(so)).replace([np.inf,-np.inf],np.nan)
L373                     if len(eps_q)>=4: EPS_VAR_8Q = float(eps_q.iloc[-min(8,len(eps_q)):].std(ddof=1))
L374             except Exception: pass
L375             df.loc[t,'EPS_VAR_8Q'] = EPS_VAR_8Q
L376
L377             # --- ã‚µã‚¤ã‚º/æµå‹•æ€§ ---
L378             df.loc[t,'MARKET_CAP'] = d.get('marketCap',np.nan); adv60 = np.nan
L379             try:
L380                 vol_series = ib.data['Volume'][t].dropna()
L381                 if len(vol_series)>=5 and len(s)==len(vol_series):
L382                     dv = (vol_series*s).rolling(60).mean(); adv60 = float(dv.iloc[-1])
L383             except Exception: pass
L384             df.loc[t,'ADV60_USD'] = adv60
L385
L386             # --- å£²ä¸Š/åˆ©ç›Šã®åŠ é€Ÿåº¦ç­‰ ---
L387             REV_Q_YOY=EPS_Q_YOY=REV_YOY_ACC=REV_YOY_VAR=np.nan
L388             REV_ANNUAL_STREAK = np.nan
L389             try:
L390                 qe, so = tickers_bulk.tickers[t].quarterly_earnings, d.get('sharesOutstanding',None)
L391                 if qe is not None and not qe.empty:
L392                     if 'Revenue' in qe.columns:
L393                         rev = qe['Revenue'].dropna().astype(float)
L394                         if len(rev)>=5: REV_Q_YOY = _safe_div(rev.iloc[-1]-rev.iloc[-5], rev.iloc[-5])
L395                         if len(rev)>=6:
L396                             yoy_now = _safe_div(rev.iloc[-1]-rev.iloc[-5], rev.iloc[-5]); yoy_prev = _safe_div(rev.iloc[-2]-rev.iloc[-6], rev.iloc[-6])
L397                             if pd.notna(yoy_now) and pd.notna(yoy_prev): REV_YOY_ACC = yoy_now - yoy_prev
L398                         yoy_list=[]
L399                         for k in range(1,5):
L400                             if len(rev)>=4+k:
L401                                 y = _safe_div(rev.iloc[-k]-rev.iloc[-(k+4)], rev.iloc[-(k+4)])
L402                                 if pd.notna(y): yoy_list.append(y)
L403                         if len(yoy_list)>=2: REV_YOY_VAR = float(np.std(yoy_list, ddof=1))
L404                         # NEW: å¹´æ¬¡ã®æŒç¶šæ€§ï¼ˆç›´è¿‘ã‹ã‚‰é¡ã£ã¦å‰å¹´æ¯”ãƒ—ãƒ©ã‚¹ãŒä½•å¹´é€£ç¶šã‹ã€å››åŠæœŸ4æœ¬æƒã†å®Œå…¨å¹´ã®ã¿ï¼‰
L405                         try:
L406                             g = rev.groupby(rev.index.year)
L407                             ann_sum, cnt = g.sum(), g.count()
L408                             ann_sum = ann_sum[cnt >= 4]
L409                             if len(ann_sum) >= 3:
L410                                 yoy = ann_sum.pct_change().dropna()
L411                                 streak = 0
L412                                 for v in yoy.iloc[::-1]:
L413                                     if pd.isna(v) or v <= 0:
L414                                         break
L415                                     streak += 1
L416                                 REV_ANNUAL_STREAK = float(streak)
L417                         except Exception:
L418                             pass
L419                     if 'Earnings' in qe.columns and so:
L420                         eps_series = (qe['Earnings'].dropna().astype(float)/float(so)).replace([np.inf,-np.inf],np.nan)
L421                         if len(eps_series)>=5 and pd.notna(eps_series.iloc[-5]) and eps_series.iloc[-5]!=0:
L422                             EPS_Q_YOY = _safe_div(eps_series.iloc[-1]-eps_series.iloc[-5], eps_series.iloc[-5])
L423             except Exception: pass
L424             df.loc[t,'REV_Q_YOY'], df.loc[t,'EPS_Q_YOY'], df.loc[t,'REV_YOY_ACC'], df.loc[t,'REV_YOY_VAR'] = REV_Q_YOY, EPS_Q_YOY, REV_YOY_ACC, REV_YOY_VAR
L425             df.loc[t,'REV_ANN_STREAK'] = REV_ANNUAL_STREAK
L426
L427             # --- Rule of 40 ã‚„å‘¨è¾º ---
L428             total_rev_ttm = d.get('totalRevenue',np.nan)
L429             FCF_MGN = _safe_div(fcf_val, total_rev_ttm)
L430             df.loc[t,'FCF_MGN'] = FCF_MGN
L431             rule40 = np.nan
L432             try:
L433                 r = df.loc[t,'REV']; rule40 = (r if pd.notna(r) else np.nan) + (FCF_MGN if pd.notna(FCF_MGN) else np.nan)
L434             except Exception: pass
L435             df.loc[t,'RULE40'] = rule40
L436
L437             # --- ãƒˆãƒ¬ãƒ³ãƒ‰è£œåŠ© ---
L438             sma50  = s.rolling(50).mean()
L439             sma150 = s.rolling(150).mean()
L440             sma200 = s.rolling(200).mean()
L441             p = _safe_last(s)
L442
L443             df.loc[t,'MA50_OVER_150'] = (
L444                 _safe_last(sma50)/_safe_last(sma150) - 1
L445                 if pd.notna(_safe_last(sma50)) and pd.notna(_safe_last(sma150)) and _safe_last(sma150)!=0 else np.nan
L446             )
L447             df.loc[t,'MA150_OVER_200'] = (
L448                 _safe_last(sma150)/_safe_last(sma200) - 1
L449                 if pd.notna(_safe_last(sma150)) and pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L450             )
L451
L452             lo52 = s[-252:].min() if len(s)>=252 else s.min()
L453             df.loc[t,'P_OVER_LOW52'] = (p/lo52 - 1) if (lo52 and lo52>0 and pd.notna(p)) else np.nan
L454
L455             df.loc[t,'MA200_SLOPE_1M'] = np.nan
L456             if len(sma200.dropna()) >= 21:
L457                 cur200 = _safe_last(sma200)
L458                 old2001 = float(sma200.iloc[-21])
L459                 if old2001:
L460                     df.loc[t,'MA200_SLOPE_1M'] = cur200/old2001 - 1
L461
L462             df.loc[t,'P_OVER_150'] = p/_safe_last(sma150)-1 if pd.notna(_safe_last(sma150)) and _safe_last(sma150)!=0 else np.nan
L463             df.loc[t,'P_OVER_200'] = p/_safe_last(sma200)-1 if pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L464             df.loc[t,'MA50_OVER_200'] = _safe_last(sma50)/_safe_last(sma200)-1 if pd.notna(_safe_last(sma50)) and pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L465             df.loc[t,'MA200_SLOPE_5M'] = np.nan
L466             if len(sma200.dropna())>=105:
L467                 cur200, old200 = _safe_last(sma200), float(sma200.iloc[-105])
L468                 if old200 and old200!=0: df.loc[t,'MA200_SLOPE_5M'] = cur200/old200 - 1
L469             # NEW: 200æ—¥ç·šãŒé€£ç¶šã§ä¸Šå‘ãã®ã€Œæ—¥æ•°ã€
L470             df.loc[t,'MA200_UP_STREAK_D'] = np.nan
L471             try:
L472                 s200 = sma200.dropna()
L473                 if len(s200) >= 2:
L474                     diff200 = s200.diff()
L475                     up = 0
L476                     for v in diff200.iloc[::-1]:
L477                         if pd.isna(v) or v <= 0:
L478                             break
L479                         up += 1
L480                     df.loc[t,'MA200_UP_STREAK_D'] = float(up)
L481             except Exception:
L482                 pass
L483             df.loc[t,'LOW52PCT25_EXCESS'] = np.nan if (lo52 is None or lo52<=0 or pd.isna(p)) else (p/(lo52*1.25)-1)
L484             hi52 = s[-252:].max() if len(s)>=252 else s.max(); df.loc[t,'NEAR_52W_HIGH'] = np.nan
L485             if hi52 and hi52>0 and pd.notna(p):
L486                 d_hi = (p/hi52)-1.0; df.loc[t,'NEAR_52W_HIGH'] = -abs(min(0.0, d_hi))
L487             df.loc[t,'RS_SLOPE_6W'] = self.rs_line_slope(s, ib.spx, 30)
L488             df.loc[t,'RS_SLOPE_13W'] = self.rs_line_slope(s, ib.spx, 65)
L489
L490             df.loc[t,'DIV_STREAK'] = self.div_streak(t)
L491
L492             # --- æ¬ æãƒ¡ãƒ¢ ---
L493             fin_cols = ['REV','ROE','BETA','DIV','FCF']
L494             need_finnhub = [col for col in fin_cols if pd.isna(df.loc[t,col])]
L495             if need_finnhub:
L496                 fin_data = self.fetch_finnhub_metrics(t)
L497                 for col in need_finnhub:
L498                     val = fin_data.get(col)
L499                     if val is not None and not pd.isna(val): df.loc[t,col] = val
L500             for col in fin_cols + ['EPS','RS','TR_str','DIV_STREAK']:
L501                 if pd.isna(df.loc[t,col]):
L502                     if col=='DIV':
L503                         status = self.dividend_status(t)
L504                         if status!='none_confident': missing_logs.append({'Ticker':t,'Column':col,'Status':status})
L505                     else:
L506                         missing_logs.append({'Ticker':t,'Column':col})
L507
L508         def _trend_template_pass(row, rs_alpha_thresh=0.10):
L509             c1 = (row.get('P_OVER_150', np.nan) > 0) and (row.get('P_OVER_200', np.nan) > 0)
L510             c2 = (row.get('MA150_OVER_200', np.nan) > 0)
L511             c3 = (row.get('MA200_SLOPE_1M', np.nan) > 0)
L512             c4 = (row.get('MA50_OVER_150', np.nan) > 0) and (row.get('MA50_OVER_200', np.nan) > 0)
L513             c5 = (row.get('TR_str', np.nan) > 0)
L514             c6 = (row.get('P_OVER_LOW52', np.nan) >= 0.30)
L515             c7 = (row.get('NEAR_52W_HIGH', np.nan) >= -0.25)
L516             c8 = (row.get('RS', np.nan) >= 0.10)
L517             return bool(c1 and c2 and c3 and c4 and c5 and c6 and c7 and c8)
L518
L519         if 'trend_template' not in df.columns: df['trend_template'] = df.apply(_trend_template_pass, axis=1).fillna(False)
L520         assert 'trend_template' in df.columns
L521
L522         # === ZåŒ–ã¨åˆæˆ ===
L523         for col in ['ROE','FCF','REV','EPS']: df[f'{col}_W'] = winsorize_s(df[col], 0.02)
L524
L525         df_z = pd.DataFrame(index=df.index)
L526         for col in ['EPS','REV','ROE','FCF','RS','TR_str','BETA','DIV','DIV_STREAK']: df_z[col] = robust_z(df[col])
L527         df_z['REV'], df_z['EPS'], df_z['TR'] = robust_z(df['REV_W']), robust_z(df['EPS_W']), robust_z(df['TR'])
L528         for col in ['P_OVER_150','P_OVER_200','MA50_OVER_200','MA200_SLOPE_5M','LOW52PCT25_EXCESS','NEAR_52W_HIGH','RS_SLOPE_6W','RS_SLOPE_13W','MA200_UP_STREAK_D']: df_z[col] = robust_z(df[col])
L529         for col in ['REV_Q_YOY','EPS_Q_YOY','REV_YOY_ACC','REV_YOY_VAR','FCF_MGN','RULE40','REV_ANN_STREAK']: df_z[col] = robust_z(df[col])
L530         for col in ['DOWNSIDE_DEV','MDD_1Y','RESID_VOL','DOWN_OUTPERF','EXT_200','DIV_TTM_PS','DIV_VAR5','DIV_YOY','DIV_FCF_COVER','DEBT2EQ','CURR_RATIO','EPS_VAR_8Q','MARKET_CAP','ADV60_USD']: df_z[col] = robust_z(df[col])
L531
L532         df_z['SIZE'], df_z['LIQ'] = robust_z(np.log1p(df['MARKET_CAP'])), robust_z(np.log1p(df['ADV60_USD']))
L533         df_z['QUALITY_F'] = robust_z(0.6*df['FCF_W'] + 0.4*df['ROE_W']).clip(-3.0,3.0)
L534         df_z['YIELD_F']   = 0.3*df_z['DIV'] + 0.7*df_z['DIV_STREAK']
L535         df_z['GROWTH_F']  = robust_z(
L536               0.30*df_z['REV']
L537             + 0.20*df_z['EPS_Q_YOY']
L538             + 0.15*df_z['REV_Q_YOY']
L539             + 0.15*df_z['REV_YOY_ACC']
L540             + 0.10*df_z['RULE40']
L541             + 0.10*df_z['FCF_MGN']
L542             + 0.10*df_z['REV_ANN_STREAK']
L543             - 0.05*df_z['REV_YOY_VAR']
L544         ).clip(-3.0,3.0)
L545         df_z['MOM_F'] = robust_z(
L546               0.40*df_z['RS']
L547             + 0.15*df_z['TR_str']
L548             + 0.15*df_z['RS_SLOPE_6W']
L549             + 0.15*df_z['RS_SLOPE_13W']
L550             + 0.10*df_z['MA200_SLOPE_5M']
L551             + 0.10*df_z['MA200_UP_STREAK_D']
L552         ).clip(-3.0,3.0)
L553         df_z['VOL'] = robust_z(df['BETA'])
L554         df_z.rename(columns={'GROWTH_F':'GRW','MOM_F':'MOM','QUALITY_F':'QAL','YIELD_F':'YLD'}, inplace=True)
L555         df_z['TRD'] = 0.0  # TRDã¯ã‚¹ã‚³ã‚¢å¯„ä¸ã‹ã‚‰å¤–ã—ã€ãƒ†ãƒ³ãƒ—ãƒ¬åˆ¤å®šã¯ãƒ•ã‚£ãƒ«ã‚¿ã§è¡Œã†ï¼ˆåˆ—ã¯è¡¨ç¤ºäº’æ›ã®ãŸã‚æ®‹ã™ï¼‰
L556         if 'BETA' not in df_z.columns: df_z['BETA'] = robust_z(df['BETA'])
L557
L558         df_z['D_VOL_RAW'] = robust_z(0.40*df_z['DOWNSIDE_DEV'] + 0.22*df_z['RESID_VOL'] + 0.18*df_z['MDD_1Y'] - 0.10*df_z['DOWN_OUTPERF'] - 0.05*df_z['EXT_200'] - 0.08*df_z['SIZE'] - 0.10*df_z['LIQ'] + 0.10*df_z['BETA'])
L559         df_z['D_QAL']     = robust_z(0.35*df_z['QAL'] + 0.20*df_z['FCF'] + 0.15*df_z['CURR_RATIO'] - 0.15*df_z['DEBT2EQ'] - 0.15*df_z['EPS_VAR_8Q'])
L560         df_z['D_YLD']     = robust_z(0.45*df_z['DIV'] + 0.25*df_z['DIV_STREAK'] + 0.20*df_z['DIV_FCF_COVER'] - 0.10*df_z['DIV_VAR5'])
L561         df_z['D_TRD']     = robust_z(0.40*df_z.get('MA200_SLOPE_5M',0) - 0.30*df_z.get('EXT_200',0) + 0.15*df_z.get('NEAR_52W_HIGH',0) + 0.15*df_z['TR'])
L562
L563         # --- é‡ã¿ã¯ cfg ã‚’å„ªå…ˆï¼ˆå¤–éƒ¨ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ï¼‰ ---
L564         # â‘  å…¨éŠ˜æŸ„ã§ G/D ã‚¹ã‚³ã‚¢ã‚’ç®—å‡ºï¼ˆunmaskedï¼‰
L565         g_score_all = df_z.mul(pd.Series(cfg.weights.g)).sum(axis=1)
L566
L567         d_comp = pd.concat({
L568             'QAL': df_z['D_QAL'],
L569             'YLD': df_z['D_YLD'],
L570             'VOL': df_z['D_VOL_RAW'],
L571             'TRD': df_z['D_TRD']
L572         }, axis=1)
L573         dw = pd.Series(cfg.weights.d, dtype=float).reindex(['QAL','YLD','VOL','TRD']).fillna(0.0)
L574         globals()['D_WEIGHTS_EFF'] = dw.copy()
L575         d_score_all = d_comp.mul(dw, axis=1).sum(axis=1)
L576
L577         # â‘¡ ãƒ†ãƒ³ãƒ—ãƒ¬åˆ¤å®šï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ãã®ã¾ã¾ï¼‰
L578         mask = df['trend_template']
L579         if not bool(mask.any()):
L580             mask = (
L581                 (df.get('P_OVER_LOW52', np.nan) >= 0.25) &
L582                 (df.get('NEAR_52W_HIGH', np.nan) >= -0.30) &
L583                 (df.get('RS', np.nan) >= 0.08) &
L584                 (df.get('MA200_SLOPE_1M', np.nan) > 0) &
L585                 (df.get('P_OVER_150', np.nan) > 0) & (df.get('P_OVER_200', np.nan) > 0) &
L586                 (df.get('MA150_OVER_200', np.nan) > 0) &
L587                 (df.get('MA50_OVER_150', np.nan) > 0) & (df.get('MA50_OVER_200', np.nan) > 0) &
L588                 (df.get('TR_str', np.nan) > 0)
L589             ).fillna(False)
L590             df['trend_template'] = mask
L591
L592         # â‘¢ æ¡ç”¨ç”¨ã¯ maskã€è¡¨ç¤º/åˆ†æç”¨ã¯åˆ—ã§å…¨éŠ˜æŸ„ä¿å­˜
L593         g_score = g_score_all.loc[mask]
L594         Scorer.g_score = g_score
L595         df_z['GSC'] = g_score_all
L596         df_z['DSC'] = d_score_all
L597
L598         try:
L599             df = _apply_growth_entry_flags(df, ib, self, win_breakout=5, win_pullback=5)
L600         except Exception:
L601             pass
L602
L603         from factor import FeatureBundle  # type: ignore  # å®Ÿè¡Œæ™‚importãªã—ï¼ˆå¾ªç’°å›é¿ï¼‰
L604         return FeatureBundle(
L605             df=df,
L606             df_z=df_z,
L607             g_score=g_score,
L608             d_score_all=d_score_all,
L609             missing_logs=pd.DataFrame(missing_logs)
L610         )
L611
L612
L613 def _apply_growth_entry_flags(feature_df, bundle, self_obj, win_breakout=5, win_pullback=5):
L614     """
L615     Gæ ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã«å¯¾ã—ã€ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®š/æŠ¼ã—ç›®åç™ºã®ã€Œç›´è¿‘Nå–¶æ¥­æ—¥å†…ã®ç™ºç«ã€ã‚’åˆ¤å®šã—ã€
L616     æ¬¡ã®åˆ—ã‚’ feature_df ã«è¿½åŠ ã™ã‚‹ï¼ˆindex=tickerï¼‰ã€‚
L617       - G_BREAKOUT_recent_5d : bool
L618       - G_BREAKOUT_last_date : str "YYYY-MM-DD"
L619       - G_PULLBACK_recent_5d : bool
L620       - G_PULLBACK_last_date : str "YYYY-MM-DD"
L621       - G_PIVOT_price        : float
L622     å¤±æ•—ã—ã¦ã‚‚ä¾‹å¤–ã¯æ¡ã‚Šæ½°ã—ã€æ—¢å­˜å‡¦ç†ã‚’é˜»å®³ã—ãªã„ã€‚
L623     """
L624     try:
L625         px   = bundle.px                      # çµ‚å€¤ DataFrame
L626         hi   = bundle.data['High']
L627         lo   = bundle.data['Low']
L628         vol  = bundle.data['Volume']
L629         bench= bundle.spx                     # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ Series
L630
L631         # Gãƒ¦ãƒ‹ãƒãƒ¼ã‚¹æ¨å®šï¼šself.g_universe å„ªå…ˆ â†’ feature_df['group']=='G' â†’ å…¨éŠ˜æŸ„
L632         g_universe = getattr(self_obj, "g_universe", None)
L633         if g_universe is None:
L634             try:
L635                 g_universe = feature_df.index[feature_df['group'].astype(str).str.upper().eq('G')].tolist()
L636             except Exception:
L637                 g_universe = list(feature_df.index)
L638         if not g_universe:
L639             return feature_df
L640
L641         # æŒ‡æ¨™
L642         ema21 = px[g_universe].ewm(span=21, adjust=False).mean()
L643         ma50  = px[g_universe].rolling(50).mean()
L644         ma150 = px[g_universe].rolling(150).mean()
L645         ma200 = px[g_universe].rolling(200).mean()
L646         atr20 = (hi[g_universe] - lo[g_universe]).rolling(20).mean()
L647         vol20 = vol[g_universe].rolling(20).mean()
L648         vol50 = vol[g_universe].rolling(50).mean()
L649
L650         # ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆåˆæ ¼
L651         trend_template_ok = (px[g_universe] > ma50) & (px[g_universe] > ma150) & (px[g_universe] > ma200) \
L652                             & (ma150 > ma200) & (ma200.diff() > 0)
L653
L654         # æ±ç”¨ãƒ”ãƒœãƒƒãƒˆï¼šç›´è¿‘65å–¶æ¥­æ—¥ã®é«˜å€¤ï¼ˆå½“æ—¥é™¤å¤–ï¼‰
L655         pivot_price = hi[g_universe].rolling(65).max().shift(1)
L656
L657         # ç›¸å¯¾åŠ›ï¼šå¹´å†…é«˜å€¤æ›´æ–°
L658         bench_aligned = bench.reindex(px.index).ffill()
L659         rs = px[g_universe].div(bench_aligned, axis=0)
L660         rs_high = rs.rolling(252).max().shift(1)
L661
L662         # ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆã€Œç™ºç”Ÿæ—¥ã€ï¼šæ¡ä»¶ç«‹ã¡ä¸ŠãŒã‚Š
L663         breakout_today = trend_template_ok & (px[g_universe] > pivot_price) \
L664                          & (vol[g_universe] >= 1.5 * vol50) & (rs > rs_high)
L665         breakout_event = breakout_today & ~breakout_today.shift(1).fillna(False)
L666
L667         # æŠ¼ã—ç›®åç™ºã€Œç™ºç”Ÿæ—¥ã€ï¼šEMA21å¸¯Ã—å‡ºæ¥é«˜ãƒ‰ãƒ©ã‚¤ã‚¢ãƒƒãƒ—Ã—å‰æ—¥é«˜å€¤è¶ŠãˆÃ—çµ‚å€¤EMA21ä¸Š
L668         near_ema21_band = px[g_universe].between(ema21 - atr20, ema21 + atr20)
L669         volume_dryup = (vol20 / vol50) <= 1.0
L670         pullback_bounce_confirmed = (px[g_universe] > hi[g_universe].shift(1)) & (px[g_universe] > ema21)
L671         pullback_today = trend_template_ok & near_ema21_band & volume_dryup & pullback_bounce_confirmed
L672         pullback_event = pullback_today & ~pullback_today.shift(1).fillna(False)
L673
L674         # ç›´è¿‘Nå–¶æ¥­æ—¥å†…ã®ç™ºç« / æœ€çµ‚ç™ºç”Ÿæ—¥
L675         rows = []
L676         for t in g_universe:
L677             def _recent_and_date(s, win):
L678                 sw = s[t].iloc[-win:]
L679                 if sw.any():
L680                     d = sw[sw].index[-1]
L681                     return True, d.strftime("%Y-%m-%d")
L682                 return False, ""
L683             br_recent, br_date = _recent_and_date(breakout_event, win_breakout)
L684             pb_recent, pb_date = _recent_and_date(pullback_event, win_pullback)
L685             rows.append((t, {
L686                 "G_BREAKOUT_recent_5d": br_recent,
L687                 "G_BREAKOUT_last_date": br_date,
L688                 "G_PULLBACK_recent_5d": pb_recent,
L689                 "G_PULLBACK_last_date": pb_date,
L690                 "G_PIVOT_price": float(pivot_price[t].iloc[-1]) if t in pivot_price.columns else float('nan'),
L691             }))
L692         flags = pd.DataFrame({k: v for k, v in rows}).T
L693
L694         # åˆ—ã‚’ä½œæˆãƒ»ä¸Šæ›¸ã
L695         cols = ["G_BREAKOUT_recent_5d","G_BREAKOUT_last_date","G_PULLBACK_recent_5d","G_PULLBACK_last_date","G_PIVOT_price"]
L696         for c in cols:
L697             if c not in feature_df.columns:
L698                 feature_df[c] = np.nan
L699         feature_df.loc[flags.index, flags.columns] = flags
L700
L701     except Exception:
L702         pass
L703     return feature_df
L704
L705
L706
```

## <drift.py>
```text
L1 import pandas as pd, yfinance as yf
L2 import numpy as np
L3 import requests
L4 import os
L5 import csv
L6 import time
L7 from pathlib import Path
L8
L9 # Debug flag
L10 debug_mode = False  # set to True for detailed output
L11
L12 # --- Finnhub settings & helper ---
L13 FINNHUB_API_KEY = os.environ.get("FINNHUB_API_KEY")
L14 if not FINNHUB_API_KEY:
L15     raise ValueError("FINNHUB_API_KEY not set (ç’°å¢ƒå¤‰æ•°ãŒæœªè¨­å®šã§ã™)")
L16
L17 RATE_LIMIT = 55  # requests per minute (free tier is 60)
L18 call_times = []
L19
L20
L21 def finnhub_get(endpoint, params):
L22     """Call Finnhub API with basic rate limiting."""
L23     now = time.time()
L24     cutoff = now - 60
L25     while call_times and call_times[0] < cutoff:
L26         call_times.pop(0)
L27     if len(call_times) >= RATE_LIMIT:
L28         sleep_time = 60 - (now - call_times[0])
L29         time.sleep(sleep_time)
L30     params = {**params, "token": FINNHUB_API_KEY}
L31     try:
L32         resp = requests.get(f"https://finnhub.io/api/v1/{endpoint}", params=params)
L33         resp.raise_for_status()
L34         data = resp.json()
L35     except requests.exceptions.JSONDecodeError as e:
L36         print(f"âš ï¸ Finnhub API JSON decode error: {e}")
L37         return {}
L38     except Exception as e:
L39         print(f"âš ï¸ Finnhub API error: {e}")
L40         return {}
L41     call_times.append(time.time())
L42     return data
L43
L44
L45 def fetch_price(symbol):
L46     try:
L47         data = finnhub_get("quote", {"symbol": symbol})
L48         price = data.get("c")
L49         return float(price) if price not in (None, 0) else float("nan")
L50     except Exception:
L51         return float("nan")
L52
L53
L54 def fetch_vix_ma5():
L55     """Retrieve VIX 5-day moving average via yfinance."""
L56     try:
L57         vix = (
L58             yf.download("^VIX", period="7d", interval="1d", progress=False, auto_adjust=False)["Close"]
L59             .dropna()
L60             .tail(5)
L61         )
L62         if len(vix) < 5:
L63             return float("nan")
L64         return vix.mean().item()
L65     except Exception:
L66         return float("nan")
L67
L68
L69 # === Minervini-like sell signals ===
L70 def _yf_df(sym, period="6mo"):
L71     """æ—¥è¶³/MA/å‡ºæ¥é«˜å¹³å‡ã‚’å–å¾—ã€‚æ¬ ææ™‚ã¯ Noneã€‚"""
L72     try:
L73         df = yf.download(sym, period=period, interval="1d", auto_adjust=False, progress=False)
L74         if df is None or df.empty:
L75             return None
L76         return df.dropna().assign(
L77             ma20=lambda d: d["Close"].rolling(20).mean(),
L78             ma50=lambda d: d["Close"].rolling(50).mean(),
L79             vol50=lambda d: d["Volume"].rolling(50).mean(),
L80         )
L81     except Exception:
L82         return None
L83
L84
L85 def _scalar(row, col):
L86     """Series/npã‚¹ã‚«ãƒ©â†’Pythonã‚¹ã‚«ãƒ©åŒ–ï¼ˆNaNã¯NaNã®ã¾ã¾ï¼‰"""
L87     try:
L88         v = row[col]
L89         if hasattr(v, "item"):
L90             try:
L91                 v = v.item()
L92             except Exception:
L93                 pass
L94         return v
L95     except Exception:
L96         return float("nan")
L97
L98
L99 def _is_strict_down(seq):
L100     """æ•°åˆ—ãŒå³å¯†ã«é€£ç¶šã§åˆ‡ã‚Šä¸‹ãŒã£ã¦ã„ã‚‹ã‹ï¼ˆlen>=4ã‚’æƒ³å®šï¼‰ã€‚NaNå«ã¿ã¯Falseã€‚"""
L101     try:
L102         xs = [float(x) for x in seq]
L103         if any(pd.isna(x) for x in xs) or len(xs) < 4:
L104             return False
L105         return all(b < a for a, b in zip(xs[:-1], xs[1:]))
L106     except Exception:
L107         return False
L108
L109
L110 def _signals_for_day(df, idx):
L111     """df.loc[idx] 1æ—¥åˆ†ã«å¯¾ã—ã‚·ã‚°ãƒŠãƒ«é…åˆ—ã‚’è¿”ã™ï¼ˆå€¤å‹•ã/å‡ºæ¥é«˜ãƒ™ãƒ¼ã‚¹ã®ã¿ï¼‰ã€‚"""
L112     try:
L113         sig = []
L114         d = df.loc[idx]
L115         close = _scalar(d, "Close")
L116         open_ = _scalar(d, "Open")
L117         ma20 = _scalar(d, "ma20")
L118         ma50 = _scalar(d, "ma50")
L119         vol = _scalar(d, "Volume")
L120         vol50 = _scalar(df.iloc[-1], "vol50")
L121         if any(pd.isna(x) for x in (close, open_, vol, vol50)):
L122             return sig
L123         if pd.notna(ma20) and close < ma20:
L124             sig.append("20DMAâ†“")
L125         if pd.notna(ma50) and close < ma50 and vol > 1.5 * vol50:
L126             sig.append("50DMAâ†“(å¤§å•†ã„)")
L127
L128         last4 = df.loc[:idx].tail(4)
L129         lows_desc = _is_strict_down(last4["Low"].tolist())
L130         last10 = df.loc[:idx].tail(10)
L131         reds = int((last10["Close"] < last10["Open"]).sum())
L132         if lows_desc or reds > 5:
L133             sig.append("é€£ç¶šå®‰å€¤/é™°ç·šå„ªå‹¢")
L134
L135         ups = int((last10["Close"] > last10["Open"]).sum())
L136         if ups >= 7:
L137             sig.append("ä¸Šã’åé‡(>70%)")
L138
L139         last15 = df.loc[:idx].tail(15)
L140         base0 = _scalar(last15.iloc[0], "Close") if len(last15) > 0 else float("nan")
L141         if pd.notna(base0) and base0 != 0 and (close / base0 - 1) >= 0.25:
L142             sig.append("+25%/15æ—¥å†…")
L143
L144         if len(df.loc[:idx]) >= 2:
L145             t1, t0 = df.loc[:idx].iloc[-2], df.loc[:idx].iloc[-1]
L146             t1_high = _scalar(t1, "High")
L147             t0_open = _scalar(t0, "Open")
L148             t0_close = _scalar(t0, "Close")
L149             if all(pd.notna(x) for x in (t1_high, t0_open, t0_close)):
L150                 if (t0_open > t1_high * 1.02) and (t0_close < t0_open):
L151                     sig.append("GUâ†’é™°ç·š")
L152         return sig
L153     except Exception:
L154         return []
L155
L156
L157 def scan_sell_signals(symbols, lookback_days=5):
L158     """
L159     ç›´è¿‘ lookback_days æ—¥ã®ã†ã¡ä¸€åº¦ã§ã‚‚ã‚·ã‚°ãƒŠãƒ«ãŒå‡ºãŸã‚‰ {sym: [(date,[signals]),...]} ã‚’è¿”ã™ã€‚
L160     æ—¥ä»˜ã¯ YYYY-MM-DDã€‚Slackã§åˆ—æŒ™ã™ã‚‹ã€‚
L161     """
L162     out = {}
L163     for s in symbols:
L164         df = _yf_df(s)
L165         if df is None or len(df) < 60:
L166             continue
L167         alerts = []
L168         for idx in df.tail(lookback_days).index:
L169             tags = _signals_for_day(df, idx)
L170             if tags:
L171                 alerts.append((idx.strftime("%Y-%m-%d"), tags))
L172         if alerts:
L173             out[s] = alerts
L174     return out
L175
L176
L177 def load_portfolio():
L178     tickers_path = Path(__file__).with_name("current_tickers.csv")
L179     with tickers_path.open() as f:
L180         reader = list(csv.reader(f))
L181     return [
L182         {"symbol": sym.strip().upper(), "shares": int(qty), "target_ratio": 1 / len(reader)}
L183         for sym, qty in reader
L184     ]
L185
L186
L187 def compute_threshold():
L188     vix_ma5 = fetch_vix_ma5()
L189     drift_threshold = 10 if vix_ma5 < 20 else 12 if vix_ma5 < 26 else float("inf")
L190     return vix_ma5, drift_threshold
L191
L192
L193 def build_dataframe(portfolio):
L194     for stock in portfolio:
L195         price = fetch_price(stock["symbol"])
L196         stock["price"] = price
L197         stock["value"] = price * stock["shares"]
L198
L199     df = pd.DataFrame(portfolio)
L200     total_value = df["value"].sum()
L201     df["current_ratio"] = df["value"] / total_value
L202     df["drift"] = df["current_ratio"] - df["target_ratio"]
L203     df["drift_abs"] = df["drift"].abs()
L204     total_drift_abs = df["drift_abs"].sum()
L205     df["adjusted_ratio"] = df["current_ratio"] - df["drift"] / 2
L206     df["adjustable"] = (
L207         (df["adjusted_ratio"] * total_value) >= df["price"]
L208     ) & df["price"].notna() & df["price"].gt(0)
L209     return df, total_value, total_drift_abs
L210
L211
L212 def simulate(df, total_value, total_drift_abs, drift_threshold):
L213     alert = drift_threshold != float("inf") and total_drift_abs * 100 > drift_threshold
L214     if alert:
L215         df["trade_shares"] = df.apply(
L216             lambda r: int(round(((r["adjusted_ratio"] * total_value) - r["value"]) / r["price"]))
L217             if r["adjustable"] and r["price"] > 0 else 0,
L218             axis=1,
L219         )
L220         df["new_shares"] = df["shares"] + df["trade_shares"]
L221         df["new_value"] = df["new_shares"] * df["price"]
L222         new_total_value = df["new_value"].sum()
L223         df["simulated_ratio"] = df["new_value"] / new_total_value
L224         df["simulated_drift_abs"] = (df["simulated_ratio"] - df["target_ratio"]).abs()
L225         simulated_total_drift_abs = df["simulated_drift_abs"].sum()
L226     else:
L227         df["trade_shares"] = np.nan
L228         df["new_shares"] = np.nan
L229         df["new_value"] = np.nan
L230         new_total_value = np.nan
L231         df["simulated_ratio"] = np.nan
L232         df["simulated_drift_abs"] = np.nan
L233         simulated_total_drift_abs = np.nan
L234     return df, alert, new_total_value, simulated_total_drift_abs
L235
L236
L237 def prepare_summary(df, total_drift_abs, alert):
L238     summary = {
L239         "symbol": "åˆè¨ˆ",
L240         "shares": df["shares"].sum(),
L241         "value": df["value"].sum(),
L242         "current_ratio": np.nan,
L243         "drift_abs": total_drift_abs,
L244     }
L245     if alert:
L246         summary["trade_shares"] = np.nan
L247     # Sort details by evaluation value descending before appending summary
L248     df = df.sort_values(by="value", ascending=False)
L249     df = pd.concat([df, pd.DataFrame([summary])], ignore_index=True)
L250     if alert:
L251         cols = ["symbol", "shares", "value", "current_ratio", "drift_abs", "trade_shares"]
L252         df_small = df[cols].copy()
L253         df_small.columns = ["sym", "qty", "val", "now", "|d|", "Î”qty"]
L254     else:
L255         cols = ["symbol", "shares", "value", "current_ratio", "drift_abs"]
L256         df_small = df[cols].copy()
L257         df_small.columns = ["sym", "qty", "val", "now", "|d|"]
L258     return df_small
L259
L260
L261 def currency(x):
L262     return f"${x:,.0f}" if pd.notnull(x) else ""
L263
L264
L265 def formatters_for(alert):
L266     formatters = {"val": currency, "now": "{:.2%}".format, "|d|": "{:.2%}".format}
L267     if alert:
L268         formatters["Î”qty"] = "{:.0f}".format
L269     return formatters
L270
L271
L272 def build_header(vix_ma5, drift_threshold, total_drift_abs, alert, simulated_total_drift_abs):
L273     header = (
L274         f"*ğŸ“ˆ VIX MA5:* {vix_ma5:.2f}\n"
L275         f"*ğŸ“Š ãƒ‰ãƒªãƒ•ãƒˆé–¾å€¤:* {'ğŸ”´(é«˜VIX)' if drift_threshold == float('inf') else str(drift_threshold)+'%'}\n"
L276         f"*ğŸ“‰ ç¾åœ¨ã®ãƒ‰ãƒªãƒ•ãƒˆåˆè¨ˆ:* {total_drift_abs * 100:.2f}%\n"
L277     )
L278     if alert:
L279         header += f"*ğŸ” åŠæˆ»ã—å¾Œãƒ‰ãƒªãƒ•ãƒˆåˆè¨ˆ(æƒ³å®š):* {simulated_total_drift_abs * 100:.2f}%\n"
L280         header += "ğŸš¨ *ã‚¢ãƒ©ãƒ¼ãƒˆ: ç™ºç”Ÿï¼ï¼ Î”qtyã®ãƒã‚¤ãƒŠã‚¹éŠ˜æŸ„ã‚’å£²å´ã€ä»»æ„ã®éŠ˜æŸ„ã‚’è²·ã„å¢—ã—ã¦ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚Šã¾ã—ã‚‡ã†ï¼*\n"
L281     else:
L282         header += "âœ… ã‚¢ãƒ©ãƒ¼ãƒˆãªã—\n"
L283     return header
L284
L285
L286 def send_slack(text):
L287     SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")
L288     if not SLACK_WEBHOOK_URL:
L289         raise ValueError("SLACK_WEBHOOK_URL not set (ç’°å¢ƒå¤‰æ•°ãŒæœªè¨­å®šã§ã™)")
L290     payload = {"text": text}
L291     try:
L292         resp = requests.post(SLACK_WEBHOOK_URL, json=payload)
L293         resp.raise_for_status()
L294         print("âœ… Slackï¼ˆWebhookï¼‰ã¸é€ä¿¡ã—ã¾ã—ãŸ")
L295     except Exception as e:
L296         print(f"âš ï¸ Slacké€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
L297
L298
L299 def send_debug(debug_text):
L300     SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")
L301     if not SLACK_WEBHOOK_URL:
L302         raise ValueError("SLACK_WEBHOOK_URL not set (ç’°å¢ƒå¤‰æ•°ãŒæœªè¨­å®šã§ã™)")
L303     debug_payload = {"text": "```" + debug_text + "```"}
L304     try:
L305         resp = requests.post(SLACK_WEBHOOK_URL, json=debug_payload)
L306         resp.raise_for_status()
L307         print("âœ… Debugæƒ…å ±ã‚’Slackã«é€ä¿¡ã—ã¾ã—ãŸ")
L308     except Exception as e:
L309         print(f"âš ï¸ Slacké€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
L310
L311
L312 def main():
L313     portfolio = load_portfolio()
L314     symbols = [r["symbol"] for r in portfolio]
L315     sell_alerts = scan_sell_signals(symbols, lookback_days=5)
L316     vix_ma5, drift_threshold = compute_threshold()
L317     df, total_value, total_drift_abs = build_dataframe(portfolio)
L318     df, alert, new_total_value, simulated_total_drift_abs = simulate(
L319         df, total_value, total_drift_abs, drift_threshold
L320     )
L321     df_small = prepare_summary(df, total_drift_abs, alert)
L322     if 'df_small' in locals() and isinstance(df_small, pd.DataFrame) and not df_small.empty:
L323         col_sym = "sym" if "sym" in df_small.columns else ("symbol" if "symbol" in df_small.columns else None)
L324         if col_sym:
L325             df_small.insert(0, "âš ", df_small[col_sym].apply(lambda x: "ğŸ”´" if x in sell_alerts else ""))
L326     formatters = formatters_for(alert)
L327     header = build_header(
L328         vix_ma5, drift_threshold, total_drift_abs, alert, simulated_total_drift_abs
L329     )
L330     if sell_alerts:
L331         def fmt_pair(date_tags):
L332             date, tags = date_tags
L333             return f"{date}:" + "ãƒ»".join(tags)
L334         listed = []
L335         for t, arr in sell_alerts.items():
L336             listed.append(f"*{t}*ï¼ˆ" + ", ".join(fmt_pair(x) for x in arr) + "ï¼‰")
L337         hits = ", ".join(listed)
L338         if "âœ… ã‚¢ãƒ©ãƒ¼ãƒˆãªã—" in header:
L339             header = header.replace(
L340                 "âœ… ã‚¢ãƒ©ãƒ¼ãƒˆãªã—",
L341                 f"âš ï¸ å£²ã‚Šã‚·ã‚°ãƒŠãƒ«ã‚ã‚Š: {len(sell_alerts)}éŠ˜æŸ„\nğŸŸ¥ {hits}",
L342             )
L343         else:
L344             header += f"\nğŸŸ¥ {hits}"
L345     table_text = df_small.to_string(formatters=formatters, index=False)
L346     send_slack(header + "\n```" + table_text + "```")
L347
L348     if debug_mode:
L349         debug_cols = [
L350             "symbol",
L351             "shares",
L352             "price",
L353             "value",
L354             "current_ratio",
L355             "drift",
L356             "drift_abs",
L357             "adjusted_ratio",
L358             "adjustable",
L359             "trade_shares",
L360             "new_shares",
L361             "new_value",
L362             "simulated_ratio",
L363             "simulated_drift_abs",
L364         ]
L365         debug_text = (
L366             "=== DEBUG: full dataframe ===\n"
L367             + df[debug_cols].to_string()
L368             + f"\n\ntotal_value={total_value}, new_total_value={new_total_value}\n"
L369             + f"total_drift_abs={total_drift_abs}, simulated_total_drift_abs={simulated_total_drift_abs}"
L370         )
L371         print("\n" + debug_text)
L372         send_debug(debug_text)
L373
L374
L375 if __name__ == "__main__":
L376     main()
L377
```

## <.github/workflows/weekly-report.yml>
```text
L1 name: Weekly Stock Report
L2
L3 on:
L4   push:
L5     branches: [ main ]
L6   schedule:
L7     - cron: '0 0 * * 6'  # UTC 00:00 â†’ JST 09:00ï¼ˆåœŸï¼‰
L8   workflow_dispatch:
L9
L10 jobs:
L11   build-and-report:
L12     runs-on: ubuntu-latest
L13
L14     steps:
L15       - name: Debug start
L16         run: echo 'ğŸš€ DEBUGstarted'
L17               
L18       - name: Checkout repository
L19         uses: actions/checkout@v3
L20
L21       - name: Setup Python
L22         uses: actions/setup-python@v5
L23         with:
L24           python-version: '3.x'                # ï¼ˆå¿…è¦æœ€å°é™ã®ã¾ã¾ã€‚å›ºå®šã—ãŸã‘ã‚Œã° '3.13'ï¼‰
L25           cache: 'pip'                         # â˜… pipã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æœ‰åŠ¹åŒ–
L26           cache-dependency-path: requirements.txt  # â˜… ä¾å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã«
L27
L28       - name: Install dependencies
L29         run: pip install -r requirements.txt
L30
L31       - name: Prepare results directory
L32         run: mkdir -p results
L33
L34       - name: Cache previous results
L35         uses: actions/cache@v3
L36         with:
L37           path: results
L38           key: results-${{ github.run_id }}
L39           restore-keys: |
L40             results-
L41
L42       - name: Run factor.py
L43         env:
L44           SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
L45           FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
L46           FIN_THREADS: "8"
L47         run: python factor.py
```

## <.github/workflows/daily-report.yml>
```text
L1 name: Daily Stock Report
L2
L3 on:
L4   push:
L5     branches: [ main ]
L6   schedule:
L7     - cron: '30 23 * * 2-6'  # UTC 23:30 â†’ JST 08:30ï¼ˆç«ã€œåœŸï¼‰
L8   workflow_dispatch:
L9
L10 jobs:
L11   build-and-report:
L12     runs-on: ubuntu-latest
L13
L14     steps:
L15       - name: Debug start
L16         run: echo 'ğŸš€ DEBUGstarted'
L17               
L18       - name: Checkout repository
L19         uses: actions/checkout@v3
L20
L21       - name: Setup Python
L22         uses: actions/setup-python@v4
L23         with:
L24           python-version: '3.x'
L25
L26       - name: Install dependencies
L27         run: pip install -r requirements.txt
L28
L29       - name: Prepare results directory
L30         run: mkdir -p results
L31
L32       - name: Cache previous results
L33         uses: actions/cache@v3
L34         with:
L35           path: results
L36           key: results-${{ github.run_id }}
L37           restore-keys: |
L38             results-
L39
L40       - name: Run drift.py
L41         env:
L42           SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
L43           FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
L44         run: python drift.py
```

## <documents/README.md>
```text
L1 # é‹ç”¨ãƒ«ãƒ¼ãƒ«
L2
L3 ## åŸºæœ¬æ§‹æˆ
L4 - 25éŠ˜æŸ„ã‚’å‡ç­‰é…åˆ†ï¼ˆç¾é‡‘ã‚’é™¤ã1éŠ˜æŸ„ã‚ãŸã‚Š4%ï¼‰
L5 - moomooè¨¼åˆ¸ã§é‹ç”¨
L6
L7 ## Barbell Growth-Defenseæ–¹é‡
L8 - Growthæ 12éŠ˜æŸ„ï¼šé«˜æˆé•·ã§ä¹–é›¢æºã¨ãªã‚‹æ”»ã‚ã®éŠ˜æŸ„
L9 - Defenseæ 13éŠ˜æŸ„ï¼šä½ãƒœãƒ©ã§å®‰å®šæˆé•·ã—é…å½“ã‚’å¢—ã‚„ã™å®ˆã‚Šã®éŠ˜æŸ„
L10 - ã€ŒçŒ›çƒˆã«ä¼¸ã³ã‚‹æ”»ã‚ Ã— ç€å®Ÿã«ç¨¼ãç›¾ã€ã®çµ„åˆã›ã§ä¹–é›¢â†’åŠæˆ»ã—ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ã‚’ç‹™ã†
L11
L12 ## ç¾é‡‘æ¯”ç‡ï¼ˆVIX 5æ—¥ç§»å‹•å¹³å‡ã§åˆ¤å®šï¼‰
L13 - VIX MA5 < 20: 5%
L14 - 20 â‰¤ VIX MA5 < 26: 7.5%
L15 - VIX MA5 â‰¥ 27: 12%ï¼ˆé«˜VIXç·Šæ€¥ãƒ¢ãƒ¼ãƒ‰ï¼‰
L16
L17 ## ãƒ‰ãƒªãƒ•ãƒˆé–¾å€¤
L18 - VIX MA5 < 20: 10%
L19 - 20 â‰¤ VIX MA5 < 26: 12%
L20 - VIX MA5 â‰¥ 27: é«˜VIXç·Šæ€¥ãƒ¢ãƒ¼ãƒ‰ã¸ç§»è¡Œ
L21
L22 ## é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã®é‹ç”¨
L23 - æ¯å–¶æ¥­æ—¥ã€â‘ 90æ—¥çµŒé or â‘¡ãƒ‰ãƒªãƒ•ãƒˆãŒé–¾å€¤è¶…éã§åŠæˆ»ã—
L24 - åŠæˆ»ã—ï¼šä¹–é›¢ã®50%ã‚’ä¸­å¤®ã¸å¯„ã›ã€ç¾é‡‘æ¯”ç‡ã‚’ä¸Šè¡¨ã©ãŠã‚Šã«èª¿æ•´
L25 - å…¨éŠ˜æŸ„ã®ãƒˆãƒ¬ãƒ¼ãƒªãƒ³ã‚°ã‚¹ãƒˆãƒƒãƒ—(TS)ã‚’å†è¨­å®š
L26 - ãƒ‰ãƒªãƒ•ãƒˆï¼Î£|ç¾åœ¨æ¯”ç‡âˆ’4%|ï¼ˆç«¯æ•°åˆ‡ã‚Šæ¨ã¦ï¼‰
L27
L28 ## é«˜VIXç·Šæ€¥ãƒ¢ãƒ¼ãƒ‰ï¼ˆMA5 > 27ã§ç™ºå‹•ï¼‰
L29 1. å…¨25éŠ˜æŸ„ã‚’å„4%ã¸å…¨æˆ»ã—
L30 2. ç¾é‡‘æ¯”ç‡12%ã¸å¼•ä¸Šã’
L31 3. å…¨éŠ˜æŸ„ã®TSã‚’å†è¨­å®šã—ä»¥é™ã®å£²è²·ã¨ãƒ‰ãƒªãƒ•ãƒˆè¨ˆç®—ã‚’åœæ­¢
L32
L33 ## é«˜VIXç·Šæ€¥ãƒ¢ãƒ¼ãƒ‰ã®è§£é™¤
L34 - MA5 < 23 ã¾ãŸã¯30å–¶æ¥­æ—¥çµŒéã§è§£é™¤
L35 - ç·Šæ€¥ãƒ¢ãƒ¼ãƒ‰ä¸­ã«TSç™ºå‹•ã§æ¸›å°‘ã—ãŸéŠ˜æŸ„ã‚’è£œå……ã—25éŠ˜æŸ„Ã—4%ã«ãƒªãƒãƒ©ãƒ³ã‚¹
L36 - é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã®æ—¥æ¬¡ãƒã‚§ãƒƒã‚¯ã‚’å†é–‹
L37
L38 ## æ®µéšçš„ãƒˆãƒ¬ãƒ¼ãƒªãƒ³ã‚°ã‚¹ãƒˆãƒƒãƒ—
L39 - Growth: åŸºæœ¬25%
L40 - Defense: åŸºæœ¬20%
L41 - å«ã¿ç›ŠãŒ40/60/80%ã«é”ã—ãŸã‚‰TSã‚’3/5/8ãƒã‚¤ãƒ³ãƒˆãšã¤å¼•ãä¸Šã’
L42 - TSç™ºå‹•ã§æ¸›å°‘ã—ãŸéŠ˜æŸ„ã¯ç¿Œæ—¥ä»¥é™ã«è£œå……ï¼ˆç·Šæ€¥ãƒ¢ãƒ¼ãƒ‰ä¸­ã¯è£œå……ã—ãªã„ï¼‰
L43
L44 ## å…¥æ›¿éŠ˜æŸ„é¸å®š
L45 - Oxfordã‚­ãƒ£ãƒ”ã‚¿ãƒ«ï¼ã‚¤ãƒ³ã‚«ãƒ ã€Alpha Investorã€Motley Fool Stock Advisorã€moomooã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ç­‰ã‚’å‚è€ƒã«chatGPTã§æ¤œè¨
L46 - å¹´é–“NISAæ ã¯Growthç¾¤ã®ä¸­ã‹ã‚‰ä½ãƒœãƒ©éŠ˜æŸ„ã‚’é¸å®šã—åˆ©ç”¨ã€‚é•·æœŸä¿æŒã«ã¯ã“ã ã‚ã‚‰ãªã„ã€‚
L47
L48 ## å®Ÿè¡Œã‚¿ã‚¤ãƒŸãƒ³ã‚°
L49 - åˆ¤å®šï¼šç±³å›½å¸‚å ´çµ‚å€¤ç›´å¾Œ
L50 - åŸ·è¡Œï¼šç¿Œå–¶æ¥­æ—¥ã®ç±³å›½å¯„ä»˜ãæˆè¡Œ
L51
L52 ## VIXæ—©è¦‹è¡¨
L53 | VIX MA5 | ãƒ‰ãƒªãƒ•ãƒˆé–¾å€¤ | ç¾é‡‘æ¯”ç‡ | ãƒ¢ãƒ¼ãƒ‰ |
L54 |--------|--------------|---------|-------|
L55 | <20    | 10           | 5%      | é€šå¸¸ |
L56 | 20â€“26  | 12           | 7.5%    | é€šå¸¸ |
L57 | â‰¥27    | â€“            | 12%     | é«˜VIXç·Šæ€¥ |
```

## <documents/drift_design.md>
```text
L1 # drift.py è©³ç´°è¨­è¨ˆæ›¸
L2
L3 ## æ¦‚è¦
L4 - 25éŠ˜æŸ„ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®ãƒ‰ãƒªãƒ•ãƒˆã‚’æ—¥æ¬¡ç›£è¦–ã—ã€é–¾å€¤è¶…éæ™‚ã«åŠæˆ»ã—æ¡ˆã‚’Slacké€šçŸ¥ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚
L5 - Finnhubã¨yfinanceã‹ã‚‰ä¾¡æ ¼ãƒ»VIXæƒ…å ±ã‚’å–å¾—ã—ã€ç¾æ³æ¯”ç‡ã¨èª¿æ•´æ¡ˆã‚’è¨ˆç®—ã€‚
L6
L7 ## å®šæ•°ãƒ»è¨­å®š
L8 - `FINNHUB_API_KEY` / `SLACK_WEBHOOK_URL` ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã€‚
L9 - ç„¡æ–™æ ã‚’è€ƒæ…®ã—ãŸAPIãƒ¬ãƒ¼ãƒˆåˆ¶é™: `RATE_LIMIT = 55`ã€‚
L10 - ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ç”¨ãƒ•ãƒ©ã‚° `debug_mode`ã€‚
L11
L12 ## ä¸»ãªé–¢æ•°
L13 ### finnhub_get
L14 - åŸºæœ¬çš„ãªãƒ¬ãƒ¼ãƒˆåˆ¶é™ä»˜ãã§Finnhub APIã‚’å‘¼ã³å‡ºã—ã€JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¾æ›¸ã§è¿”ã™ã€‚
L15
L16 ### fetch_price
L17 - `quote` ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã§æ ªä¾¡ã‚’å–å¾—ã—ã€å¤±æ•—æ™‚ã¯ `NaN` ã‚’è¿”ã™ã€‚
L18
L19 ### fetch_vix_ma5
L20 - yfinanceã§VIXçµ‚å€¤ã‚’å–å¾—ã—ã€ç›´è¿‘5å–¶æ¥­æ—¥ã®ç§»å‹•å¹³å‡ã‚’ç®—å‡ºã€‚
L21
L22 ### load_portfolio
L23 - `current_tickers.csv` ã‹ã‚‰éŠ˜æŸ„ã¨ä¿æœ‰æ ªæ•°ã‚’èª­ã¿è¾¼ã¿ã€ç›®æ¨™æ¯”ç‡4%ã‚’ä»˜ä¸ã—ãŸãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã€‚
L24
L25 ### compute_threshold
L26 - VIX MA5ã«å¿œã˜ã¦ãƒ‰ãƒªãƒ•ãƒˆé–¾å€¤ã‚’10%/12%/é«˜VIXãƒ¢ãƒ¼ãƒ‰(âˆ)ã«è¨­å®šã€‚
L27
L28 ### build_dataframe
L29 - å„éŠ˜æŸ„ã®è©•ä¾¡é¡ã‚„ç¾åœ¨æ¯”ç‡ã€ãƒ‰ãƒªãƒ•ãƒˆã€åŠæˆ»ã—å¾Œæ¯”ç‡(`adjusted_ratio`)ã‚’è¨ˆç®—ã—DataFrameåŒ–ã€‚
L30
L31 ### simulate
L32 - ãƒ‰ãƒªãƒ•ãƒˆåˆè¨ˆãŒé–¾å€¤ã‚’è¶…ãˆãŸå ´åˆã€åŠæˆ»ã—å¾Œã®å£²è²·æ ªæ•°ã¨æ–°æ¯”ç‡ã‚’è©¦ç®—ã—ã€ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆå¾Œãƒ‰ãƒªãƒ•ãƒˆã‚’è¿”ã™ã€‚
L33
L34 ### prepare_summary
L35 - è©•ä¾¡é¡é †ã«ä¸¦ã¹æ›¿ãˆãŸå¾Œã€åˆè¨ˆè¡Œã‚’ä»˜ä¸ã—ã¦Slackè¡¨ç¤ºç”¨ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆã€‚
L36
L37 ### formatters_for / currency
L38 - é€šè²¨ãƒ»æ¯”ç‡ãƒ»æ ªæ•°ã®è¡¨ç¤ºãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’å®šç¾©ã€‚
L39
L40 ### build_header
L41 - VIXãƒ»é–¾å€¤ãƒ»ãƒ‰ãƒªãƒ•ãƒˆå€¤ãŠã‚ˆã³ã‚¢ãƒ©ãƒ¼ãƒˆæœ‰ç„¡ã‚’Slackãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”¨ãƒ˜ãƒƒãƒ€ã«æ•´å½¢ã€‚
L42
L43 ### send_slack / send_debug
L44 - é€šå¸¸é€šçŸ¥ãŠã‚ˆã³ãƒ‡ãƒãƒƒã‚°è©³ç´°ã‚’Slack Webhookã¸é€ä¿¡ã€‚
L45
L46 ### main
L47 - ä¸Šè¨˜é–¢æ•°ã‚’é †ã«å‘¼ã³å‡ºã—ã€æ—¥æ¬¡ãƒ‰ãƒªãƒ•ãƒˆãƒã‚§ãƒƒã‚¯ã®ä¸€é€£å‡¦ç†ã‚’å®Ÿè¡Œã€‚
L48
L49 ## å®Ÿè¡Œãƒ•ãƒ­ãƒ¼
L50 1. `load_portfolio` ã§ç¾ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã‚’èª­ã¿è¾¼ã‚€ã€‚
L51 2. `compute_threshold` ã§VIX MA5ã¨ãƒ‰ãƒªãƒ•ãƒˆé–¾å€¤ã‚’æ±ºå®šã€‚
L52 3. `build_dataframe` ã§ç¾åœ¨æ¯”ç‡ã¨ãƒ‰ãƒªãƒ•ãƒˆã‚’è¨ˆç®—ã€‚
L53 4. `simulate` ã§é–¾å€¤è¶…éæ™‚ã®åŠæˆ»ã—æ¡ˆã‚’è©¦ç®—ã€‚
L54 5. `prepare_summary` ã¨ `build_header` ã§é€šçŸ¥æœ¬æ–‡ã¨ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ§‹ç¯‰ã€‚
L55 6. `send_slack` ã§çµæœã‚’é€ä¿¡ã€‚`debug_mode` ãŒTrueãªã‚‰ `send_debug` ã‚‚ä½µç”¨ã€‚
```

## <documents/factor_design.md>
```text
L1 # factor.py è©³ç´°è¨­è¨ˆæ›¸
L2
L3 ## æ¦‚è¦
L4 - æ—¢å­˜ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®éŠ˜æŸ„ã¨æ¤œè¨ä¸­ã®éŠ˜æŸ„ç¾¤ã‚’åŒæ™‚ã«æ‰±ã†éŠ˜æŸ„é¸å®šãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€‚
L5 - ä¾¡æ ¼ãƒ»è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šè¾¼ã¿ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã¨DRRSé¸å®šã‚’è¡Œã†ã“ã¨ã§ã€ä»¥ä¸‹ã®ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆã‚’å¾—ã‚‹ã€‚
L6   - æ¡ç”¨éŠ˜æŸ„ã¨æƒœã—ãã‚‚æ¼ã‚ŒãŸéŠ˜æŸ„ã®ã‚¹ã‚³ã‚¢ä¸€è¦§
L7   - IN/OUTã®ãƒ†ã‚£ãƒƒã‚«ãƒ¼ãƒªã‚¹ãƒˆã¨OUTå´ã®ä½ã‚¹ã‚³ã‚¢éŠ˜æŸ„
L8   - æ–°æ—§ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®æ¯”è¼ƒè¡¨
L9   - æ¤œè¨ä¸­éŠ˜æŸ„ã®ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆæ•´ç†ç”¨ï¼‰
L10
L11 ## å…¨ä½“ãƒ•ãƒ­ãƒ¼
L12 1. **Input** â€“ `current_tickers.csv`ã¨`candidate_tickers.csv`ã‚’èª­ã¿è¾¼ã¿ã€yfinanceã‚„Finnhubã®APIã‹ã‚‰ä¾¡æ ¼ãƒ»è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã¦`InputBundle`ã‚’æ•´å‚™ã€‚
L13 2. **Score Calculation** â€“ ScorerãŒç‰¹å¾´é‡ã‚’è¨ˆç®—ã—å› å­ã‚¹ã‚³ã‚¢ã‚’åˆæˆã—ã¦`FeatureBundle`ã‚’ç”Ÿæˆã€‚
L14 3. **Correlation Reduction & Selection** â€“ SelectorãŒDRRSãƒ­ã‚¸ãƒƒã‚¯ã§ç›¸é–¢ã‚’æŠ‘ãˆã¤ã¤G/DéŠ˜æŸ„ã‚’é¸å®šã—`SelectionBundle`ã‚’å¾—ã‚‹ã€‚
L15 4. **Output** â€“ æ¡ç”¨çµæœã¨å‘¨è¾ºæƒ…å ±ã‚’è¡¨ãƒ»Slacké€šçŸ¥ã¨ã—ã¦å‡ºåŠ›ã€‚
L16
L17 ```mermaid
L18 flowchart LR
L19   A[Input\nAPI & å‰å‡¦ç†] --> B[Score Calculation\nç‰¹å¾´é‡ãƒ»å› å­åˆæˆ]
L20   B --> C[Correlation Reduction\nDRRSé¸å®š]
L21   C --> D[Output\nSlacké€šçŸ¥]
L22 ```
L23
L24 ## å®šæ•°ãƒ»è¨­å®š
L25 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L26 | --- | --- | --- |
L27 | `exist` / `cand` | ç¾è¡Œãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã¨æ¤œè¨ä¸­éŠ˜æŸ„ã®ãƒ†ã‚£ãƒƒã‚«ãƒ¼ãƒªã‚¹ãƒˆ | ã‚¹ã‚³ã‚¢å¯¾è±¡ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã®æ§‹æˆã€å€™è£œæ•´ç† |
L28 | `bench` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼ | ç›¸å¯¾å¼·ã•ãƒ»Î²ç®—å‡ºã€ãƒãƒ¼ãƒˆæ¯”è¼ƒ |
L29 | `CAND_PRICE_MAX` | å€™è£œéŠ˜æŸ„ã®è¨±å®¹ä¾¡æ ¼ä¸Šé™ | é«˜é¡éŠ˜æŸ„ã®äº‹å‰é™¤å¤– |
L30 | `N_G` / `N_D` | G/Dæ¡ç”¨æ ã®ä»¶æ•° | æœ€çµ‚çš„ã«é¸ã¶éŠ˜æŸ„æ•°ã®åˆ¶ç´„ |
L31 | `g_weights` / `D_weights` | å„å› å­ã®é‡ã¿dict | G/Dã‚¹ã‚³ã‚¢åˆæˆ |
L32 | `D_BETA_MAX` | Dãƒã‚±ãƒƒãƒˆã®è¨±å®¹Î²ä¸Šé™ | é«˜Î²éŠ˜æŸ„ã®é™¤å¤–ãƒ•ã‚£ãƒ«ã‚¿ |
L33 | `FILTER_SPEC` | G/Dã”ã¨ã®å‰å‡¦ç†ãƒ•ã‚£ãƒ«ã‚¿ | ãƒˆãƒ¬ãƒ³ãƒ‰ãƒã‚¹ã‚¯ã‚„Î²ä¸Šé™è¨­å®š |
L34 | `corrM` | DRRSåˆæœŸãƒ—ãƒ¼ãƒ«ã®æœ€å¤§ä»¶æ•° | ç›¸é–¢è¡Œåˆ—ã‚µã‚¤ã‚ºåˆ¶å¾¡ |
L35 | `DRRS_G` / `DRRS_D` | DRRSãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿dict | ãƒã‚±ãƒƒãƒˆåˆ¥ã®ç›¸é–¢ä½æ¸›è¨­å®š |
L36 | `DRRS_SHRINK` | æ®‹å·®ç›¸é–¢ã®å¯¾è§’ã‚·ãƒ¥ãƒªãƒ³ã‚¯ç‡ | `residual_corr`ã®å®‰å®šåŒ– |
L37 | `CROSS_MU_GD` | G-Dé–“ã‚¯ãƒ­ã‚¹ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£Î¼ | 2ãƒã‚±ãƒƒãƒˆåŒæ™‚æœ€é©åŒ–ã§ç›¸é–¢æŠ‘åˆ¶ |
L38 | `RESULTS_DIR` | é¸å®šçµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª | `_save_sel`/`_load_prev`ã®å…¥å‡ºåŠ› |
L39
L40 é¸å®šçµæœã¯`results/`é…ä¸‹ã«JSONã¨ã—ã¦ä¿å­˜ã—ã€æ¬¡å›å®Ÿè¡Œæ™‚ã«`_load_prev`ã§èª­ã¿è¾¼ã‚“ã§é¸å®šæ¡ä»¶ã«åæ˜ ã€‚
L41
L42 ## DTO/Config
L43 å„ã‚¹ãƒ†ãƒƒãƒ—é–“ã§å—ã‘æ¸¡ã™ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¨è¨­å®šå€¤ã€‚å¤‰æ•°ã®æ„å‘³åˆã„ã¨åˆ©ç”¨ç®‡æ‰€ã‚’ä»¥ä¸‹ã«ç¤ºã™ã€‚
L44
L45 ### InputBundleï¼ˆInput â†’ Scorerï¼‰
L46 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L47 | --- | --- | --- |
L48 | `cand` | å€™è£œéŠ˜æŸ„ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã®ãƒªã‚¹ãƒˆ | OUTãƒ†ãƒ¼ãƒ–ãƒ«ã‚„ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°å¯¾è±¡ã®æ¯é›†å›£ |
L49 | `tickers` | ç¾è¡Œ+å€™è£œã‚’åˆã‚ã›ãŸãƒ†ã‚£ãƒƒã‚«ãƒ¼ä¸€è¦§ | ä¾¡æ ¼ãƒ»å‡ºæ¥é«˜ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€ãƒªã‚¿ãƒ¼ãƒ³è¨ˆç®— |
L50 | `bench` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼ | ç›¸å¯¾å¼·ã•ãƒ»Î²ç®—å‡ºã€ãƒãƒ¼ãƒˆæ¯”è¼ƒ |
L51 | `data` | yfinanceã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœï¼ˆéšå±¤åˆ—ï¼‰ | `px`/`spx`/ãƒªã‚¿ãƒ¼ãƒ³ç­‰ã®åŸºç¤ãƒ‡ãƒ¼ã‚¿ |
L52 | `px` | `data['Close']`ã ã‘ã‚’æŠœãå‡ºã—ãŸä¾¡æ ¼ç³»åˆ— | æŒ‡æ¨™è¨ˆç®—ãƒ»ãƒªã‚¿ãƒ¼ãƒ³ç”Ÿæˆ |
L53 | `spx` | `data['Close'][bench]` ã®Series | `rs`ã‚„`calc_beta`ã®åŸºæº–æŒ‡æ•° |
L54 | `tickers_bulk` | `yf.Tickers`ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ | `info`ç­‰ã®ä¸€æ‹¬å–å¾— |
L55 | `info` | ãƒ†ã‚£ãƒƒã‚«ãƒ¼åˆ¥ã®yfinanceæƒ…å ±dict | ã‚»ã‚¯ã‚¿ãƒ¼åˆ¤å®šã‚„EPSè£œå®Œ |
L56 | `eps_df` | EPS TTM/ç›´è¿‘EPSç­‰ã‚’ã¾ã¨ã‚ãŸè¡¨ | æˆé•·æŒ‡æ¨™ã®ç®—å‡º |
L57 | `fcf_df` | CFOãƒ»CapExãƒ»FCF TTMã¨æƒ…å ±æºãƒ•ãƒ©ã‚° | FCF/EVã‚„é…å½“ã‚«ãƒãƒ¬ãƒƒã‚¸ |
L58 | `returns` | `px.pct_change()`ã®ãƒªã‚¿ãƒ¼ãƒ³è¡¨ | ç›¸é–¢è¡Œåˆ—ãƒ»DRRSè¨ˆç®— |
L59
L60 ### FeatureBundleï¼ˆScorer â†’ Selectorï¼‰
L61 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L62 | --- | --- | --- |
L63 | `df` | è¨ˆç®—æ¸ˆã¿æŒ‡æ¨™ã®ç”Ÿå€¤ãƒ†ãƒ¼ãƒ–ãƒ« | ãƒ‡ãƒãƒƒã‚°ãƒ»å‡ºåŠ›è¡¨ç¤º |
L64 | `df_z` | ã‚¦ã‚£ãƒ³ã‚¶ãƒ¼å¾ŒZã‚¹ã‚³ã‚¢åŒ–ã—ãŸæŒ‡æ¨™è¡¨ | å› å­ã‚¹ã‚³ã‚¢åˆæˆã€é¸å®šåŸºæº– |
L65 | `g_score` | Gãƒã‚±ãƒƒãƒˆç·åˆã‚¹ã‚³ã‚¢ | Gé¸å®šã€IN/OUTæ¯”è¼ƒ |
L66 | `d_score_all` | Dãƒã‚±ãƒƒãƒˆç·åˆã‚¹ã‚³ã‚¢ï¼ˆå…¨éŠ˜æŸ„ï¼‰ | Dé¸å®šã€ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚° |
L67 | `missing_logs` | æ¬ ææŒ‡æ¨™ã¨è£œå®ŒçŠ¶æ³ã®ãƒ­ã‚° | ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ |
L68
L69 ### SelectionBundleï¼ˆSelector â†’ Outputï¼‰
L70 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L71 | --- | --- | --- |
L72 | `resG` | Gé¸å®šçµæœã®è©³ç´°dictï¼ˆ`tickers`ã€ç›®çš„å€¤ç­‰ï¼‰ | çµæœä¿å­˜ãƒ»å¹³å‡ç›¸é–¢ãªã©ã®æŒ‡æ¨™è¡¨ç¤º |
L73 | `resD` | Dé¸å®šçµæœã®è©³ç´°dict | åŒä¸Š |
L74 | `top_G` | æœ€çµ‚æ¡ç”¨Gãƒ†ã‚£ãƒƒã‚«ãƒ¼ | æ–°ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæ§‹ç¯‰ |
L75 | `top_D` | æœ€çµ‚æ¡ç”¨Dãƒ†ã‚£ãƒƒã‚«ãƒ¼ | åŒä¸Š |
L76 | `init_G` | DRRSå‰ã®GåˆæœŸå€™è£œ | æƒœã—ãã‚‚å¤–ã‚ŒãŸéŠ˜æŸ„è¡¨ç¤º |
L77 | `init_D` | DRRSå‰ã®DåˆæœŸå€™è£œ | åŒä¸Š |
L78
L79 ### WeightsConfig
L80 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L81 | --- | --- | --- |
L82 | `g` | Gå› å­ï¼ˆGRW/MOM/VOLï¼‰ã®é‡ã¿dict | `g_score`åˆæˆ |
L83 | `d` | Då› å­ï¼ˆD_QAL/D_YLD/D_VOL_RAW/D_TRDï¼‰ã®é‡ã¿dict | `d_score_all`åˆæˆ |
L84
L85 ### DRRSParams
L86 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L87 | --- | --- | --- |
L88 | `corrM` | DRRSåˆæœŸãƒ—ãƒ¼ãƒ«ã®æœ€å¤§ä»¶æ•° | ç›¸é–¢è¡Œåˆ—ã‚µã‚¤ã‚ºåˆ¶å¾¡ |
L89 | `shrink` | æ®‹å·®ç›¸é–¢ã®ã‚·ãƒ¥ãƒªãƒ³ã‚¯ç‡ | `residual_corr`ã®å¯¾è§’å¼·èª¿ |
L90 | `G` | Gãƒã‚±ãƒƒãƒˆç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿dictï¼ˆ`lookback`ç­‰ï¼‰ | `select_bucket_drrs`è¨­å®š |
L91 | `D` | Dãƒã‚±ãƒƒãƒˆç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿dict | åŒä¸Š |
L92 | `cross_mu_gd` | G-Dã‚¯ãƒ­ã‚¹ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£ä¿‚æ•°Î¼ | `select_buckets`ã®ç›®çš„é–¢æ•° |
L93
L94 ### PipelineConfig
L95 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L96 | --- | --- | --- |
L97 | `weights` | `WeightsConfig`ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ | ã‚¹ã‚³ã‚¢åˆæˆã®é‡ã¿å‚ç…§ |
L98 | `drrs` | `DRRSParams`ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ | é¸å®šã‚¹ãƒ†ãƒƒãƒ—ã®è¨­å®šå€¤ |
L99 | `price_max` | å€™è£œéŠ˜æŸ„ã®è¨±å®¹ä¾¡æ ¼ä¸Šé™ | Inputæ®µéšã§ã®ãƒ•ã‚£ãƒ«ã‚¿ |
L100
L101 ## å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
L102 - `winsorize_s` / `robust_z` : å¤–ã‚Œå€¤å‡¦ç†ã¨Zã‚¹ã‚³ã‚¢åŒ–ã€‚
L103 - `_safe_div` / `_safe_last` : ä¾‹å¤–ã‚’æ½°ã—ãŸåˆ†å‰²ãƒ»æœ«å°¾å–å¾—ã€‚
L104 - `_load_prev` / `_save_sel` : é¸å®šçµæœã®èª­ã¿æ›¸ãã€‚
L105
L106 ## ã‚¯ãƒ©ã‚¹è¨­è¨ˆ
L107 ### Step1: Input
L108 `current_tickers.csv`ã®ç¾è¡ŒéŠ˜æŸ„ã¨`candidate_tickers.csv`ã®æ¤œè¨ä¸­éŠ˜æŸ„ã‚’èµ·ç‚¹ã«ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„ã™ã‚‹ã€‚å¤–éƒ¨I/Oã¨å‰å‡¦ç†ã‚’æ‹…å½“ã—ã€`prepare_data`ã§`InputBundle`ã‚’ç”Ÿæˆã€‚ä¾¡æ ¼ãƒ»è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã¯**yfinanceã‚’å„ªå…ˆã—ã€æ¬ æãŒã‚ã‚‹æŒ‡æ¨™ã®ã¿Finnhub APIã§è£œå®Œ**ã™ã‚‹ã€‚
L109 ä¸»ãªãƒ¡ã‚½ãƒƒãƒ‰:
L110 - `impute_eps_ttm` : å››åŠæœŸEPSÃ—4ã§TTMã‚’æ¨å®šã—æ¬ ææ™‚ã®ã¿å·®ã—æ›¿ãˆã€‚
L111 - `fetch_cfo_capex_ttm_yf` : yfinanceã®å››åŠæœŸ/å¹´æ¬¡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼ã‹ã‚‰CFOãƒ»CapExãƒ»FCF TTMã‚’ç®—å‡ºã€‚
L112 - `fetch_cfo_capex_ttm_finnhub` : yfinanceã§æ¬ ã‘ãŸéŠ˜æŸ„ã®ã¿Finnhub APIã§è£œå®Œã€‚
L113 - `compute_fcf_with_fallback` : yfinanceå€¤ã‚’åŸºæº–ã«Finnhubå€¤ã§ç©´åŸ‹ã‚ã—ã€CFO/CapEx/FCFã¨æƒ…å ±æºãƒ•ãƒ©ã‚°ã‚’è¿”ã™ã€‚
L114 - `_build_eps_df` : `info`ã‚„`quarterly_earnings`ã‹ã‚‰EPS TTMã¨ç›´è¿‘EPSã‚’è¨ˆç®—ã—ã€`impute_eps_ttm`ã§è£œå®Œã€‚
L115 - `prepare_data` :
L116     0. CSVã‹ã‚‰ç¾è¡ŒéŠ˜æŸ„ã¨å€™è£œéŠ˜æŸ„ã®ãƒ†ã‚£ãƒƒã‚«ãƒ¼ä¸€è¦§ã‚’èª­ã¿è¾¼ã‚€ã€‚
L117     1. å€™è£œéŠ˜æŸ„ã®ç¾åœ¨å€¤ã‚’å–å¾—ã—ä¾¡æ ¼ä¸Šé™ã§ãƒ•ã‚£ãƒ«ã‚¿ã€‚
L118     2. æ—¢å­˜+å€™è£œã‹ã‚‰å¯¾è±¡ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚’æ±ºå®šã—ã€ä¾¡æ ¼ãƒ»å‡ºæ¥é«˜ã‚’ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆyfinanceï¼‰ã€‚
L119     3. yfinanceå€¤ã‚’åŸºã«EPS/FCFãƒ†ãƒ¼ãƒ–ãƒ«ã‚„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç³»åˆ—ã€ãƒªã‚¿ãƒ¼ãƒ³ã‚’æ§‹ç¯‰ã—ã€æ¬ æã‚»ãƒ«ã¯Finnhubå‘¼ã³å‡ºã—ã§ç©´åŸ‹ã‚ã€‚
L120     4. ä¸Šè¨˜ã‚’`InputBundle`ã«æ ¼ç´ã—ã¦è¿”ã™ã€‚
L121
L122 ### Step2: Score Calculation (Scorer)
L123 ç‰¹å¾´é‡è¨ˆç®—ã¨ã‚¹ã‚³ã‚¢åˆæˆã‚’æ‹…å½“ã—ã€`FeatureBundle`ã‚’è¿”ã™ã€‚
L124
L125 #### è£œåŠ©é–¢æ•°
L126 - `trend(s)` : 50/150/200æ—¥ç§»å‹•å¹³å‡ã‚„52é€±ãƒ¬ãƒ³ã‚¸ã‹ã‚‰-0.5ã€œ0.5ã§æ§‹æˆã•ã‚ŒãŸãƒˆãƒ¬ãƒ³ãƒ‰æŒ‡æ¨™ã€‚
L127 - `rs(s,b)` / `tr_str(s)` / `rs_line_slope(s,b,win)` : ç›¸å¯¾å¼·ã•ã‚„çŸ­æœŸãƒˆãƒ¬ãƒ³ãƒ‰ã€RSå›å¸°å‚¾ãã‚’ç®—å‡ºã€‚
L128 - `ev_fallback` : `enterpriseValue`æ¬ ææ™‚ã«è² å‚µãƒ»ç¾é‡‘ã‹ã‚‰EVã‚’æ¨å®šã€‚
L129 - `dividend_status` / `div_streak` : é…å½“æœªè¨­å®šçŠ¶æ³ã®åˆ¤å®šã¨å¢—é…å¹´æ•°ã‚«ã‚¦ãƒ³ãƒˆã€‚
L130 - `fetch_finnhub_metrics` : Finnhub APIã‹ã‚‰EPSæˆé•·ãƒ»ROEãƒ»Î²ãªã©ä¸è¶³æŒ‡æ¨™ã‚’å–å¾—ã€‚
L131 - `calc_beta` : ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ã®å…±åˆ†æ•£ã‹ã‚‰Î²ã‚’ç®—å‡ºã€‚
L132 - `spx_to_alpha` : S&P500ã®ä½ç½®æƒ…å ±ã‹ã‚‰DRRSã§ç”¨ã„ã‚‹Î±ã‚’æ¨å®šã€‚
L133 - `soft_cap_effective_scores` / `pick_top_softcap` : ã‚»ã‚¯ã‚¿ãƒ¼ã‚½ãƒ•ãƒˆã‚­ãƒ£ãƒƒãƒ—ä»˜ãã‚¹ã‚³ã‚¢èª¿æ•´ã¨ä¸Šä½æŠ½å‡ºã€‚
L134
L135 **è£œåŠ©é–¢æ•°ã¨ç”ŸæˆæŒ‡æ¨™**
L136
L137 | è£œåŠ©é–¢æ•° | ç”ŸæˆæŒ‡æ¨™ | ç•¥ç§° |
L138 | --- | --- | --- |
L139 | `trend` | ãƒˆãƒ¬ãƒ³ãƒ‰ç·åˆå€¤ | `TR` |
L140 | `rs` | ç›¸å¯¾å¼·ã• | `RS` |
L141 | `tr_str` | ä¾¡æ ¼ã¨50æ—¥ç·šã®ä¹–é›¢ | `TR_str` |
L142 | `rs_line_slope` | RSç·šã®å›å¸°å‚¾ã | `RS_SLOPE_*` |
L143 | `calc_beta` | Î² | `BETA` |
L144 | `div_streak` | é€£ç¶šå¢—é…å¹´æ•° | `DIV_STREAK` |
L145
L146 #### `aggregate_scores` è©³ç´°
L147 1. å„éŠ˜æŸ„ã®ä¾¡æ ¼ç³»åˆ—ã‚„`info`ã‚’åŸºã«ä»¥ä¸‹ã‚’ç®—å‡ºã€‚
L148    - **ãƒˆãƒ¬ãƒ³ãƒ‰/ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ **: `TR`ã€`RS`ã€`TR_str`ã€å¤šæ§˜ãªç§»å‹•å¹³å‡æ¯”ã€`RS_SLOPE_*`ãªã©ã€‚
L149    - **ãƒªã‚¹ã‚¯**: `BETA`ã€`DOWNSIDE_DEV`ã€`MDD_1Y`ã€`RESID_VOL`ã€`DOWN_OUTPERF`ã€`EXT_200`ç­‰ã€‚
L150    - **é…å½“**: `DIV`ã€`DIV_TTM_PS`ã€`DIV_VAR5`ã€`DIV_YOY`ã€`DIV_FCF_COVER`ã€`DIV_STREAK`ã€‚
L151    - **è²¡å‹™ãƒ»æˆé•·**: `EPS`ã€`REV`ã€`ROE`ã€`FCF/EV`ã€`REV_Q_YOY`ã€`EPS_Q_YOY`ã€`REV_YOY_ACC`ã€`REV_YOY_VAR`ã€`REV_ANN_STREAK`ã€`RULE40`ã€`FCF_MGN` ç­‰ã€‚
L152    - **å®‰å®šæ€§/ã‚µã‚¤ã‚º**: `DEBT2EQ`ã€`CURR_RATIO`ã€`MARKET_CAP`ã€`ADV60_USD`ã€`EPS_VAR_8Q`ãªã©ã€‚
L153 2. æŒ‡æ¨™æ¬ æã¯Finnhub APIç­‰ã§è£œå®Œã—ã€æœªå–å¾—é …ç›®ã‚’`missing_logs`ã«è¨˜éŒ²ã€‚
L154 3. `winsorize_s`â†’`robust_z`ã§æ¨™æº–åŒ–ã—`df_z`ã¸ä¿å­˜ã€‚ã‚µã‚¤ã‚ºãƒ»æµå‹•æ€§ã¯å¯¾æ•°å¤‰æ›ã€‚
L155 4. æ­£è¦åŒ–æ¸ˆæŒ‡æ¨™ã‹ã‚‰å› å­ã‚¹ã‚³ã‚¢ã‚’åˆæˆã€‚
L156    - å„å› å­ã®æ§‹æˆã¨é‡ã¿ã¯ä»¥ä¸‹ã®é€šã‚Šã€‚
L157      - **GRW**: 0.30Ã—`REV` + 0.20Ã—`EPS_Q_YOY` + 0.15Ã—`REV_Q_YOY` + 0.15Ã—`REV_YOY_ACC` + 0.10Ã—`RULE40` + 0.10Ã—`FCF_MGN` + 0.10Ã—`REV_ANN_STREAK` âˆ’ 0.05Ã—`REV_YOY_VAR`ã€‚
L158      - **MOM**: 0.40Ã—`RS` + 0.15Ã—`TR_str` + 0.15Ã—`RS_SLOPE_6W` + 0.15Ã—`RS_SLOPE_13W` + 0.10Ã—`MA200_SLOPE_5M` + 0.10Ã—`MA200_UP_STREAK_D`ã€‚
L159      - **VOL**: `BETA`å˜ä½“ã‚’ä½¿ç”¨ã€‚
L160      - **QAL**: 0.60Ã—`FCF_W` + 0.40Ã—`ROE_W`ã§ä½œæˆã€‚
L161      - **YLD**: 0.30Ã—`DIV` + 0.70Ã—`DIV_STREAK`ã€‚
L162      - **D_QAL**: 0.35Ã—`QAL` + 0.20Ã—`FCF` + 0.15Ã—`CURR_RATIO` âˆ’ 0.15Ã—`DEBT2EQ` âˆ’ 0.15Ã—`EPS_VAR_8Q`ã€‚
L163      - **D_YLD**: 0.45Ã—`DIV` + 0.25Ã—`DIV_STREAK` + 0.20Ã—`DIV_FCF_COVER` âˆ’ 0.10Ã—`DIV_VAR5`ã€‚
L164      - **D_VOL_RAW**: 0.40Ã—`DOWNSIDE_DEV` + 0.22Ã—`RESID_VOL` + 0.18Ã—`MDD_1Y` âˆ’ 0.10Ã—`DOWN_OUTPERF` âˆ’ 0.05Ã—`EXT_200` âˆ’ 0.08Ã—`SIZE` âˆ’ 0.10Ã—`LIQ` + 0.10Ã—`BETA`ã€‚
L165      - **D_TRD**: 0.40Ã—`MA200_SLOPE_5M` âˆ’ 0.30Ã—`EXT_200` + 0.15Ã—`NEAR_52W_HIGH` + 0.15Ã—`TR`ã€‚
L166     - ä¸»ãªæŒ‡æ¨™ã®ç•¥ç§°ã¨æ„å‘³:
L167
L168       | ç•¥ç§° | è£œåŠ©é–¢æ•° | æ¦‚è¦ |
L169       | --- | --- | --- |
L170       | TR | `trend` | 50/150/200æ—¥ç§»å‹•å¹³å‡ã¨52é€±ãƒ¬ãƒ³ã‚¸ã‚’çµ„ã¿åˆã‚ã›ãŸãƒˆãƒ¬ãƒ³ãƒ‰ç·åˆå€¤ |
L171       | RS | `rs` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«å¯¾ã™ã‚‹ç›¸å¯¾å¼·ã•ï¼ˆ12M/1Mãƒªã‚¿ãƒ¼ãƒ³å·®ï¼‰ |
L172       | TR_str | `tr_str` | ä¾¡æ ¼ã¨50æ—¥ç§»å‹•å¹³å‡ã®ä¹–é›¢ |
L173       | RS_SLOPE_6W | `rs_line_slope` | ç›¸å¯¾å¼·ã•ç·šã®6é€±å›å¸°å‚¾ã |
L174       | RS_SLOPE_13W | `rs_line_slope` | ç›¸å¯¾å¼·ã•ç·šã®13é€±å›å¸°å‚¾ã |
L175       | MA200_SLOPE_5M | - | 200æ—¥ç§»å‹•å¹³å‡ã®5ã‹æœˆé¨°è½ç‡ |
L176       | MA200_UP_STREAK_D | - | 200æ—¥ç·šãŒé€£ç¶šã§ä¸Šå‘ã„ãŸæ—¥æ•° |
L177       | BETA | `calc_beta` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«å¯¾ã™ã‚‹Î² |
L178       | DOWNSIDE_DEV | - | ä¸‹æ–¹ãƒªã‚¿ãƒ¼ãƒ³ã®ã¿ã®å¹´ç‡åŒ–æ¨™æº–åå·® |
L179       | RESID_VOL | - | Î²ã§èª¿æ•´ã—ãŸæ®‹å·®ãƒªã‚¿ãƒ¼ãƒ³ã®å¹´ç‡åŒ–æ¨™æº–åå·® |
L180       | MDD_1Y | - | éå»1å¹´ã®æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³ |
L181       | DOWN_OUTPERF | - | å¸‚å ´ä¸‹è½æ—¥ã«å¯¾ã™ã‚‹å¹³å‡è¶…éãƒªã‚¿ãƒ¼ãƒ³ |
L182       | EXT_200 | - | 200æ—¥ç§»å‹•å¹³å‡ã‹ã‚‰ã®çµ¶å¯¾ä¹–é›¢ç‡ |
L183       | NEAR_52W_HIGH | - | 52é€±é«˜å€¤ã¾ã§ã®ä¸‹æ–¹è·é›¢ï¼ˆ0=é«˜å€¤ï¼‰ |
L184       | FCF_W | - | ã‚¦ã‚£ãƒ³ã‚¶ãƒ¼å‡¦ç†å¾Œã®FCF/EV |
L185       | ROE_W | - | ã‚¦ã‚£ãƒ³ã‚¶ãƒ¼å‡¦ç†å¾Œã®ROE |
L186       | FCF | - | FCF/EV |
L187       | QAL | - | FCF_Wã¨ROE_Wã‚’çµ„ã¿åˆã‚ã›ãŸå“è³ªã‚¹ã‚³ã‚¢ |
L188       | CURR_RATIO | - | æµå‹•æ¯”ç‡ |
L189       | DEBT2EQ | - | è² å‚µè³‡æœ¬å€ç‡ |
L190       | EPS_VAR_8Q | - | EPSã®8å››åŠæœŸæ¨™æº–åå·® |
L191       | DIV | - | å¹´ç‡æ›ç®—é…å½“åˆ©å›ã‚Š |
L192       | DIV_STREAK | `div_streak` | é€£ç¶šå¢—é…å¹´æ•° |
L193       | DIV_FCF_COVER | - | é…å½“ã®FCFã‚«ãƒãƒ¬ãƒƒã‚¸ |
L194       | DIV_VAR5 | - | 5å¹´é…å½“å¤‰å‹•ç‡ |
L195       | DIV_TTM_PS | - | 1æ ªå½“ãŸã‚ŠTTMé…å½“ |
L196       | DIV_YOY | - | å‰å¹´æ¯”é…å½“æˆé•·ç‡ |
L197       | REV | - | å£²ä¸Šæˆé•·ç‡TTM |
L198       | EPS_Q_YOY | - | å››åŠæœŸEPSã®å‰å¹´åŒæœŸæ¯” |
L199       | REV_Q_YOY | - | å››åŠæœŸå£²ä¸Šã®å‰å¹´åŒæœŸæ¯” |
L200       | REV_YOY_ACC | - | å£²ä¸Šæˆé•·ç‡ã®åŠ é€Ÿåˆ† |
L201       | RULE40 | - | å£²ä¸Šæˆé•·ç‡ã¨FCFãƒãƒ¼ã‚¸ãƒ³ã®åˆè¨ˆ |
L202       | FCF_MGN | - | FCFãƒãƒ¼ã‚¸ãƒ³ |
L203       | REV_ANN_STREAK | - | å¹´æ¬¡å£²ä¸Šæˆé•·ã®é€£ç¶šå¹´æ•° |
L204       | REV_YOY_VAR | - | å¹´æ¬¡å£²ä¸Šæˆé•·ç‡ã®å¤‰å‹•æ€§ |
L205       | SIZE | - | æ™‚ä¾¡ç·é¡ã®å¯¾æ•°å€¤ |
L206       | LIQ | - | 60æ—¥å¹³å‡å‡ºæ¥é«˜ãƒ‰ãƒ«ã®å¯¾æ•°å€¤ |
L207    - Gãƒã‚±ãƒƒãƒˆ: `GRW`ã€`MOM`ã€`VOL`ã‚’`cfg.weights.g`ï¼ˆ0.40/0.45/-0.15ï¼‰ã§åŠ é‡ã—`g_score`ã‚’å¾—ã‚‹ã€‚
L208    - Dãƒã‚±ãƒƒãƒˆ: `D_QAL`ã€`D_YLD`ã€`D_VOL_RAW`ã€`D_TRD`ã‚’`cfg.weights.d`ï¼ˆ0.15/0.15/-0.45/0.25ï¼‰ã§åŠ é‡ã—`d_score_all`ã‚’ç®—å‡ºã€‚
L209    - ã‚»ã‚¯ã‚¿ãƒ¼capã«ã‚ˆã‚‹`soft_cap_effective_scores`ã‚’é©ç”¨ã—ã€Gæ¡ç”¨éŠ˜æŸ„ã«ã¯ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨ã€‚
L210 5. `_apply_growth_entry_flags`ã§ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆ/æŠ¼ã—ç›®ç™ºç«çŠ¶æ³ã‚’ä»˜åŠ ã—ã€`FeatureBundle`ã‚’è¿”ã™ã€‚
L211
L212 ### Step3: Correlation Reduction & Selection (Selector)
L213 DRRSã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ç›¸é–¢ã‚’æŠ‘ãˆãŸéŠ˜æŸ„é¸å®šã‚’è¡Œã„ã€`SelectionBundle`ã‚’è¿”ã™ã€‚`results/`ã«ä¿å­˜ã•ã‚ŒãŸå‰å›é¸å®šï¼ˆ`G_selection.json` / `D_selection.json`ï¼‰ã‚’`_load_prev`ã§èª­ã¿è¾¼ã¿ã€ç›®çš„å€¤ãŒå¤§ããæ‚ªåŒ–ã—ãªã„é™ã‚Šç¶­æŒã™ã‚‹ã€‚æ–°ã—ã„æ¡ç”¨é›†åˆã¯`_save_sel`ã§JSONã«æ›¸ãå‡ºã—æ¬¡å›ä»¥é™ã®å…¥åŠ›ã«å‚™ãˆã‚‹ã€‚
L214 ä¸»ãªãƒ¡ã‚½ãƒƒãƒ‰:
L215 - `residual_corr` : åç›Šç‡è¡Œåˆ—ã‚’Zã‚¹ã‚³ã‚¢åŒ–ã—ã€ä¸Šä½ä¸»æˆåˆ†ã‚’é™¤å»ã—ãŸæ®‹å·®ã‹ã‚‰ç›¸é–¢è¡Œåˆ—ã‚’æ±‚ã‚ã€å¹³å‡ç›¸é–¢ã«å¿œã˜ã¦ã‚·ãƒ¥ãƒªãƒ³ã‚¯ã€‚
L216 - `rrqr_like_det` : ã‚¹ã‚³ã‚¢ã‚’é‡ã¿ä»˜ã‘ã—ãŸQRåˆ†è§£é¢¨ã®æ‰‹é †ã§åˆæœŸå€™è£œã‚’kä»¶æŠ½å‡ºã—ã€ã‚¹ã‚³ã‚¢ã®é«˜ã„éç›¸é–¢ãªé›†åˆã‚’å¾—ã‚‹ã€‚
L217 - `swap_local_det` / `swap_local_det_cross` : `sum(score) - Î»*within_corr - Î¼*cross_corr`ã‚’ç›®çš„é–¢æ•°ã¨ã—ã¦ã€å…¥ã‚Œæ›¿ãˆæ¢ç´¢ã§å±€æ‰€çš„ã«æœ€é©åŒ–ã€‚
L218 - `select_bucket_drrs` : ãƒ—ãƒ¼ãƒ«éŠ˜æŸ„ã¨ã‚¹ã‚³ã‚¢ã‹ã‚‰æ®‹å·®ç›¸é–¢ã‚’è¨ˆç®—ã—ã€ä¸Šè¨˜2æ®µéš(åˆæœŸé¸æŠâ†’å…¥ã‚Œæ›¿ãˆ)ã§kéŠ˜æŸ„ã‚’æ±ºå®šã€‚éå»æ¡ç”¨éŠ˜æŸ„ã¨ã®æ¯”è¼ƒã§ç›®çš„å€¤ãŒåŠ£åŒ–ã—ãªã‘ã‚Œã°ç¶­æŒã™ã‚‹ã€‚
L219 - `select_buckets` : Gãƒã‚±ãƒƒãƒˆã‚’é¸å®šå¾Œã€ãã®çµæœã‚’é™¤ã„ãŸå€™è£œã‹ã‚‰Dãƒã‚±ãƒƒãƒˆã‚’é¸ã¶ã€‚Dé¸å®šæ™‚ã¯Gã¨ã®ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£Î¼ã‚’ä»˜ä¸ã—ã€ä¸¡ãƒã‚±ãƒƒãƒˆã®åˆ†æ•£ã‚’åˆ¶å¾¡ã™ã‚‹ã€‚
L220
L221 #### ç›¸é–¢ä½æ¸›ãƒ­ã‚¸ãƒƒã‚¯è©³ç´°
L222 1. **æ®‹å·®ç›¸é–¢è¡Œåˆ—ã®æ§‹ç¯‰ (`residual_corr`)**
L223    - ãƒªã‚¿ãƒ¼ãƒ³è¡Œåˆ—`R`ã‚’Zã‚¹ã‚³ã‚¢åŒ–ã€‚
L224    - SVDã§ä¸Šä½`n_pc`ä¸»æˆåˆ†`F`ã‚’æ±‚ã‚ã€æœ€å°äºŒä¹—ã§ä¿‚æ•°`B`ã‚’ç®—å‡ºã—æ®‹å·®`E = Z - F@B`ã‚’å¾—ã‚‹ã€‚
L225    - `E`ã®ç›¸é–¢è¡Œåˆ—`C`ã‚’è¨ˆç®—ã—ã€å¹³å‡çµ¶å¯¾ç›¸é–¢ã«å¿œã˜ã¦ã‚·ãƒ¥ãƒªãƒ³ã‚¯é‡`shrink_eff`ã‚’è£œæ­£ã—ã¦å¯¾è§’ã‚’å¼·èª¿ã€‚
L226 2. **åˆæœŸå€™è£œã®æŠ½å‡º (`rrqr_like_det`)**
L227    - ã‚¹ã‚³ã‚¢ã‚’0-1æ­£è¦åŒ–ã—ãŸé‡ã¿`w`ã¨ã—ã€`Z*(1+Î³w)`ã§åˆ—ãƒãƒ«ãƒ ã‚’å¼·èª¿ã€‚
L228    - æ®‹å·®ãƒãƒ«ãƒ æœ€å¤§ã®åˆ—ã‚’é€æ¬¡é¸ã³ã€QRãƒ©ã‚¤ã‚¯ãªãƒ‡ãƒ•ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã£ã¦éç›¸é–¢ã‹ã¤é«˜ã‚¹ã‚³ã‚¢ãª`k`éŠ˜æŸ„é›†åˆ`S0`ã‚’å¾—ã‚‹ã€‚
L229 3. **å±€æ‰€æ¢ç´¢ (`swap_local_det` / `swap_local_det_cross`)**
L230    - ç›®çš„é–¢æ•°`Î£z_score âˆ’ Î»Â·within_corr âˆ’ Î¼Â·cross_corr`ã‚’æœ€å¤§åŒ–ã€‚
L231    - é¸æŠé›†åˆã®å„éŠ˜æŸ„ã‚’ä»–å€™è£œã¨å…¥ã‚Œæ›¿ãˆã€æ”¹å–„ãŒãªããªã‚‹ã¾ã§ã¾ãŸã¯`max_pass`å›ã¾ã§æ¢ç´¢ã€‚
L232    - `swap_local_det_cross`ã¯Gãƒã‚±ãƒƒãƒˆã¨ã®ã‚¯ãƒ­ã‚¹ç›¸é–¢è¡Œåˆ—`C_cross`ã‚’ä½¿ç”¨ã—ã€ãƒšãƒŠãƒ«ãƒ†ã‚£`Î¼`ã‚’ä»˜ä¸ã€‚
L233 4. **éå»æ¡ç”¨ã®ç¶­æŒã¨ã‚¯ãƒ­ã‚¹ãƒšãƒŠãƒ«ãƒ†ã‚£ (`select_bucket_drrs` / `select_buckets`)**
L234    - å±€æ‰€æ¢ç´¢çµæœ`S`ã¨éå»é›†åˆ`P`ã®ç›®çš„å€¤ã‚’æ¯”è¼ƒã—ã€`S`ãŒ`P`ã‚ˆã‚Š`Î·`æœªæº€ã®æ”¹å–„ãªã‚‰`P`ã‚’ç¶­æŒã€‚
L235    - `select_buckets`ã§ã¯Gã‚’å…ˆã«æ±ºå®šã—ã€Dé¸å®šæ™‚ã«Gã¨ã®ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£`Î¼`ã‚’åŠ ãˆã¦ã‚¯ãƒ­ã‚¹åˆ†æ•£ã‚’æŠ‘åˆ¶ã€‚
L236
L237 ### Step4: Output
L238 é¸å®šçµæœã‚’å¯è¦–åŒ–ã—å…±æœ‰ã™ã‚‹å·¥ç¨‹ã€‚ä»¥ä¸‹ã®å†…å®¹ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«åŒ–ã—ã¦æ¨™æº–å‡ºåŠ›ã¨Slackã¸é€ã‚‹ã€‚
L239 - æ¡ç”¨éŠ˜æŸ„ã¨æƒœã—ãã‚‚é¸å¤–ã¨ãªã£ãŸéŠ˜æŸ„ã®ã‚¹ã‚³ã‚¢ä¸€è¦§
L240 - IN/OUTãƒªã‚¹ãƒˆã¨OUTéŠ˜æŸ„ã®ã‚¹ã‚³ã‚¢ï¼ˆä½å¾—ç‚¹éŠ˜æŸ„ã‚’ç¢ºèªã—ã‚„ã™ãï¼‰
L241 - æ–°æ—§ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®æ¯”è¼ƒè¡¨ï¼ˆçµ„å…¥ã‚Œãƒ»é™¤å¤–ã€ã‚¹ã‚³ã‚¢å¤‰åŒ–ï¼‰
L242 - æ¤œè¨ä¸­éŠ˜æŸ„ã®ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°
L243
L244 ä¸»ãªãƒ¡ã‚½ãƒƒãƒ‰:
L245 - `display_results` : ä¸Šè¨˜ãƒ†ãƒ¼ãƒ–ãƒ«ã«åŠ ãˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã‚„åˆ†æ•£åŒ–æŒ‡æ¨™ã‚’è¡¨ç¤ºã€‚
L246 - `notify_slack` : Slack Webhookã¸åŒå†…å®¹ã‚’é€ä¿¡ã€‚
L247 - è£œåŠ©:`_avg_offdiag`ã€`_resid_avg_rho`ã€`_raw_avg_rho`ã€`_cross_block_raw_rho`ã€‚
L248
L249 ## ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
L250 1. `PipelineConfig`ã‚’æ§‹ç¯‰ã€‚
L251 2. **Step1** `Input.prepare_data`ã§`InputBundle`ã‚’ç”Ÿæˆã€‚
L252 3. **Step2** `Scorer.aggregate_scores`ã§`FeatureBundle`ã‚’å–å¾—ã€‚
L253 4. **Step3** `Selector.select_buckets`ã§`SelectionBundle`ã‚’ç®—å‡ºã€‚
L254 5. **Step4** `Output.display_results`ã¨`notify_slack`ã§çµæœã‚’å‡ºåŠ›ã€‚
```

## <current_tickers.csv>
```text
L1 ENB,40
L2 ABBV,9
L3 KO,25
L4 SO,19
L5 VZ,41
L6 HD,4
L7 FUTU,9
L8 AU,32
L9 PLTR,11
L10 BAP,7
L11 CRDO,15
L12 CALM,15
L13 RIGL,42
L14 IDR,64
L15 MCD,6
L16 JNJ,10
L17 TIGR,145
L18 DUK,14
L19 ELVA,290
L20 DRD,95
L21 ASM,380
L22 MNST,28
L23 BBIO,36
L24 TBPH,130
L25 WMT,18
```

## <candidate_tickers.csv>
```text
L1 AAPL
L2 ABT
L3 ACAD
L4 ACGL
L5 ADSK
L6 AEE
L7 AEM
L8 AGI
L9 AGX
L10 AJX
L11 ALAB
L12 ALKS
L13 ALRS
L14 AMCR
L15 AMG
L16 AMGN
L17 AMSC
L18 AMZN
L19 ANET
L20 ANIP
L21 APH
L22 APP
L23 ARQT
L24 AS
L25 ASLE
L26 ASML
L27 ASND
L28 ATAT
L29 ATGE
L30 AUPH
L31 AVGO
L32 AXON
L33 BABA
L34 BAP
L35 BBVA
L36 BGNE
L37 BK
L38 BLBD
L39 BMO
L40 BMY
L41 BR
L42 BRKB
L43 BROS
L44 BSX
L45 BWA
L46 BWAY
L47 BWMN
L48 BWXT
L49 BZ
L50 CASY
L51 CBOE
L52 CCB
L53 CCEP
L54 CCJ
L55 CECO
L56 CELH
L57 CI
L58 CIEN
L59 CL
L60 CLS
L61 CMCL
L62 CMCSA
L63 CME
L64 CMI
L65 CMPO
L66 COF
L67 COMM
L68 CPNG
L69 CPRT
L70 CTAS
L71 CTSH
L72 CW
L73 CYBR
L74 DASH
L75 DAVE
L76 DB
L77 DDOG
L78 DGICA
L79 DIS
L80 DKNG
L81 DOCS
L82 DXPE
L83 EA
L84 EGO
L85 EME
L86 EPRT
L87 EQX
L88 ERJ
L89 EVBN
L90 EVRG
L91 FEIM
L92 FIG
L93 FIX
L94 FNV
L95 FOXA
L96 FSI
L97 FSM
L98 FSS
L99 GD
L100 GE
L101 GENI
L102 GFI
L103 GH
L104 GILD
L105 GILT
L106 GIS
L107 GLW
L108 GMAB
L109 GNL
L110 GOOG
L111 GOOGL
L112 GRAB
L113 GROY
L114 GS
L115 HALO
L116 HEI
L117 HG
L118 HL
L119 HON
L120 HOOD
L121 HWM
L122 IAG
L123 IBKR
L124 IBM
L125 ICE
L126 IDCC
L127 IDXX
L128 IIIN
L129 IONQ
L130 IREN
L131 JD
L132 JPM
L133 KGC
L134 KHC
L135 KLAC
L136 KNSA
L137 KRMN
L138 KTOS
L139 LEN
L140 LEU
L141 LGCY
L142 LH
L143 LIF
L144 LITE
L145 LKNC
L146 LNC
L147 LRCX
L148 LTM
L149 LVMUY
L150 LYG
L151 MDLZ
L152 MEDP
L153 MELI
L154 META
L155 MIRM
L156 MMC
L157 MS
L158 MSFT
L159 MU
L160 MVBF
L161 NARI
L162 NBIS
L163 NCSM
L164 NEM
L165 NET
L166 NEU
L167 NLY
L168 NNI
L169 NRIM
L170 NTR
L171 NTRS
L172 NVDA
L173 NXST
L174 NYAX
L175 NYT
L176 OLO
L177 ONC
L178 OVBC
L179 PAAS
L180 PAC
L181 PAHC
L182 PAYX
L183 PDD
L184 PEP
L185 PG
L186 PGNY
L187 PHIN
L188 PHM
L189 PHR
L190 PINS
L191 PODD
L192 PPIH
L193 PRCH
L194 PRIM
L195 PRM
L196 PSIX
L197 PSTL
L198 PTGX
L199 QURE
L200 R
L201 RACE
L202 RBB
L203 RBRK
L204 RCL
L205 RDDT
L206 REGN
L207 RERE
L208 RGLD
L209 RITM
L210 RKLB
L211 RMD
L212 ROAD
L213 ROKU
L214 RSI
L215 RTX
L216 RYTM
L217 SAMG
L218 SAP
L219 SBUX
L220 SCHW
L221 SD
L222 SE
L223 SKYW
L224 SPOT
L225 STE
L226 STRL
L227 SWAV
L228 SYK
L229 SYY
L230 TARS
L231 TATT
L232 TBBK
L233 TFPM
L234 TJX
L235 TK
L236 TME
L237 TMUS
L238 TOL
L239 TOST
L240 TREE
L241 TSM
L242 TTC
L243 UBER
L244 UI
L245 UNP
L246 VEEV
L247 VITL
L248 VRSK
L249 VTS
L250 WAY
L251 WGS
L252 WHG
L253 WLDN
L254 WM
L255 WMB
L256 WWD
L257 XEL
L258 XPEV
L259 YOU
L260 YUM
L261 ZM
L262 ZTS
```

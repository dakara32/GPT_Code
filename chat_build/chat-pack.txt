# === Chat Paste Pack (single file) ===
# Repo: dakara32/GPT_Code @ main
# Files: factor.py, scorer.py, drift.py, .github/workflows/weekly-report.yml, .github/workflows/daily-report.yml, documents/README.md, documents/drift_design.md, documents/factor_design.md, current_tickers.csv, candidate_tickers.csv
# 使い方: 下記を丸ごとコピーしてチャットに貼り付け（iPhoneはGitHubアプリの「Copy file contents」が便利）。
---

## <factor.py>
```text
L1 """
L2 ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
L3 ┃ ROLE of factor.py                                     ┃
L4 ┃  - Orchestration ONLY（外部I/O・SSOT・Slack出力）     ┃
L5 ┃  - 計算ロジック（採点/フィルタ/相関低減）は scorer.py ┃
L6 ┃  - ここでロジックを実装/変更しない                   ┃
L7 ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
L8 """
L9 # === NOTE: 機能・入出力・ログ文言・例外挙動は不変。安全な短縮（import統合/複数代入/内包表記/メソッドチェーン/一行化/空行圧縮など）のみ適用 ===
L10 import yfinance as yf, pandas as pd, numpy as np, os, requests, time, json
L11 from scipy.stats import zscore
L12 from dataclasses import dataclass
L13 from typing import Dict, List
L14 from scorer import Scorer, ttm_div_yield_portfolio
L15 import os
L16 import requests
L17 from time import perf_counter
L18
L19
L20 class T:
L21     t = perf_counter()
L22
L23     @staticmethod
L24     def log(tag: str):
L25         now = perf_counter()
L26         print(f"[T] {tag}: {now - T.t:.2f}s")
L27         T.t = now
L28
L29
L30 T.log("start")
L31
L32 # ===== ユニバースと定数（冒頭に固定） =====
L33 exist, cand = [pd.read_csv(f, header=None)[0].tolist() for f in ("current_tickers.csv","candidate_tickers.csv")]
L34 T.log(f"csv loaded: exist={len(exist)} cand={len(cand)}")
L35 CAND_PRICE_MAX, bench = 450, '^GSPC'  # 価格上限・ベンチマーク
L36 N_G, N_D = 12, 13  # G/D枠サイズ
L37 g_weights = {'GRW':0.40,'MOM':0.45,'VOL':-0.15}
L38 D_BETA_MAX = float(os.environ.get("D_BETA_MAX", "0.8"))
L39 FILTER_SPEC = {"G":{"pre_mask":["trend_template"]},"D":{"pre_filter":{"beta_max":D_BETA_MAX}}}
L40 D_weights = {'QAL':0.15,'YLD':0.15,'VOL':-0.45,'TRD':0.25}
L41 def _fmt_w(w): return " ".join(f"{k}{int(v*100)}" for k,v in w.items())
L42
L43 # DRRS 初期プール・各種パラメータ
L44 corrM = 45
L45 DRRS_G, DRRS_D = dict(lookback=252,n_pc=3,gamma=1.2,lam=0.68,eta=0.8), dict(lookback=504,n_pc=4,gamma=0.8,lam=0.85,eta=0.5)
L46 DRRS_SHRINK = 0.10  # 残差相関の対角シュリンク（基礎）
L47
L48 # クロス相関ペナルティ（未定義なら設定）
L49 try: CROSS_MU_GD
L50 except NameError: CROSS_MU_GD = 0.40  # 推奨 0.35–0.45（lam=0.85想定）
L51
L52 # 出力関連
L53 RESULTS_DIR, G_PREV_JSON, D_PREV_JSON = "results", os.path.join("results","G_selection.json"), os.path.join("results","D_selection.json")
L54 os.makedirs(RESULTS_DIR, exist_ok=True)
L55
L56 # その他
L57 debug_mode, FINNHUB_API_KEY = False, os.environ.get("FINNHUB_API_KEY")
L58
L59
L60 # ===== 共有DTO（クラス間I/O契約）＋ Config =====
L61 @dataclass(frozen=True)
L62 class InputBundle:
L63     # Input → Scorer で受け渡す素材（I/O禁止の生データ）
L64     cand: List[str]
L65     tickers: List[str]
L66     bench: str
L67     data: pd.DataFrame              # yfinance download結果（'Close','Volume'等の階層列）
L68     px: pd.DataFrame                # data['Close']
L69     spx: pd.Series                  # data['Close'][bench]
L70     tickers_bulk: object            # yfinance.Tickers
L71     info: Dict[str, dict]           # yfinance info per ticker
L72     eps_df: pd.DataFrame            # ['eps_ttm','eps_q_recent',...]
L73     fcf_df: pd.DataFrame            # ['fcf_ttm', ...]
L74     returns: pd.DataFrame           # px[tickers].pct_change()
L75
L76 @dataclass(frozen=True)
L77 class FeatureBundle:
L78     df: pd.DataFrame
L79     df_z: pd.DataFrame
L80     g_score: pd.Series
L81     d_score_all: pd.Series
L82     missing_logs: pd.DataFrame
L83
L84 @dataclass(frozen=True)
L85 class SelectionBundle:
L86     resG: dict
L87     resD: dict
L88     top_G: List[str]
L89     top_D: List[str]
L90     init_G: List[str]
L91     init_D: List[str]
L92
L93 @dataclass(frozen=True)
L94 class WeightsConfig:
L95     g: Dict[str,float]
L96     d: Dict[str,float]
L97
L98 @dataclass(frozen=True)
L99 class DRRSParams:
L100     corrM: int
L101     shrink: float
L102     G: Dict[str,float]   # lookback, n_pc, gamma, lam, eta
L103     D: Dict[str,float]
L104     cross_mu_gd: float
L105
L106 @dataclass(frozen=True)
L107 class PipelineConfig:
L108     weights: WeightsConfig
L109     drrs: DRRSParams
L110     price_max: float
L111
L112
L113 # ===== 共通ユーティリティ（複数クラスで使用） =====
L114 def winsorize_s(s: pd.Series, p=0.02):
L115     if s is None or s.dropna().empty: return s
L116     lo, hi = np.nanpercentile(s.astype(float), [100*p, 100*(1-p)]); return s.clip(lo, hi)
L117
L118 def robust_z(s: pd.Series, p=0.02):
L119     s2 = winsorize_s(s, p); return np.nan_to_num(zscore(s2.fillna(s2.mean())))
L120
L121 def _safe_div(a, b):
L122     try:
L123         if b is None or float(b)==0 or pd.isna(b): return np.nan
L124         return float(a)/float(b)
L125     except Exception: return np.nan
L126
L127 def _safe_last(series: pd.Series, default=np.nan):
L128     try: return float(series.iloc[-1])
L129     except Exception: return default
L130
L131 def _load_prev(path: str):
L132     try: return json.load(open(path)).get("tickers")
L133     except Exception: return None
L134
L135 def _save_sel(path: str, tickers: list[str], avg_r: float, sum_score: float, objective: float):
L136     with open(path,"w") as f:
L137         json.dump({"tickers":tickers,"avg_res_corr":round(avg_r,6),"sum_score":round(sum_score,6),"objective":round(objective,6)}, f, indent=2)
L138
L139 def _env_true(name: str, default=False):
L140     v = os.getenv(name)
L141     return default if v is None else v.strip().lower() == "true"
L142
L143 def _slack(message, code=False):
L144     url = os.getenv("SLACK_WEBHOOK_URL")
L145     if not url:
L146         print("⚠️ SLACK_WEBHOOK_URL 未設定"); return
L147     try:
L148         requests.post(url, json={"text": f"```{message}```" if code else message}).raise_for_status()
L149     except Exception as e:
L150         print(f"⚠️ Slack通知エラー: {e}")
L151
L152 def _slack_debug(text: str, chunk=2800):
L153     url=os.getenv("SLACK_WEBHOOK_URL")
L154     if not url: print("⚠️ SLACK_WEBHOOK_URL 未設定"); return
L155     i=0
L156     while i<len(text):
L157         j=min(len(text), i+chunk); k=text.rfind("\n", i, j); j=k if k>i+100 else j
L158         blk={"type":"section","text":{"type":"mrkdwn","text":f"```{text[i:j]}```"}}
L159         try: requests.post(url, json={"blocks":[blk]}).raise_for_status()
L160         except Exception as e: print(f"⚠️ Slack通知エラー: {e}")
L161         i=j
L162
L163 def _compact_debug(fb, sb, prevG, prevD, max_rows=140):
L164     # ---- 列選択：既定は最小列、DEBUG_ALL_COLS=True で全列に ----
L165     want=["TR","EPS","REV","ROE","BETA_RAW","FCF","RS","TR_str","DIV_STREAK","DSC"]
L166     all_cols = _env_true("DEBUG_ALL_COLS", False)
L167     cols = list(fb.df_z.columns if all_cols else [c for c in want if c in fb.df_z.columns])
L168
L169     # ---- 差分（入替）----
L170     Gp, Dp = set(prevG or []), set(prevD or [])
L171     g_new=[t for t in (sb.top_G or []) if t not in Gp]; g_out=[t for t in Gp if t not in (sb.top_G or [])]
L172     d_new=[t for t in (sb.top_D or []) if t not in Dp]; d_out=[t for t in Dp if t not in (sb.top_D or [])]
L173
L174     # ---- 次点10（フラグで有無切替）----
L175     show_near = _env_true("DEBUG_NEAR5", True)
L176     gs = getattr(fb,"g_score",None); ds = getattr(fb,"d_score_all",None)
L177     gs = gs.sort_values(ascending=False) if show_near and hasattr(gs,"sort_values") else None
L178     ds = ds.sort_values(ascending=False) if show_near and hasattr(ds,"sort_values") else None
L179     g_miss = ([t for t in gs.index if t not in (sb.top_G or [])][:10]) if gs is not None else []
L180     d_excl = set((sb.top_G or [])+(sb.top_D or []))
L181     d_miss = ([t for t in ds.index if t not in d_excl][:10]) if ds is not None else []
L182
L183     # ---- 行選択：既定は入替+採用+次点、DEBUG_ALL_ROWS=True で全銘柄 ----
L184     all_rows = _env_true("DEBUG_ALL_ROWS", False)
L185     focus = list(fb.df_z.index) if all_rows else sorted(set(g_new+g_out+d_new+d_out+(sb.top_G or [])+(sb.top_D or [])+g_miss+d_miss))
L186     focus = focus[:max_rows]
L187
L188     # ---- ヘッダ（フィルター条件を明示）----
L189     def _fmt_near(lbl, ser, lst):
L190         if ser is None: return f"{lbl}: off"
L191         parts=[]
L192         for t in lst:
L193             x=ser.get(t, float("nan"))
L194             parts.append(f"{t}:{x:.3f}" if pd.notna(x) else f"{t}:nan")
L195         return f"{lbl}: "+(", ".join(parts) if parts else "-")
L196     head=[f"G new/out: {len(g_new)}/{len(g_out)}  D new/out: {len(d_new)}/{len(d_out)}",
L197           _fmt_near("G near10", gs, g_miss),
L198           _fmt_near("D near10", ds, d_miss),
L199           f"Filters: G pre_mask=['trend_template'], D pre_filter={{'beta_max': {D_BETA_MAX}}}",
L200           f"Cols={'ALL' if all_cols else 'MIN'}  Rows={'ALL' if all_rows else 'SUBSET'}"]
L201
L202     # ---- テーブル ----
L203     if fb.df_z.empty or not cols:
L204         tbl="(df_z or columns not available)"
L205     else:
L206         idx=[t for t in focus if t in fb.df_z.index]
L207         tbl=fb.df_z.loc[idx, cols].round(3).to_string(max_rows=None, max_cols=None)
L208
L209     # ---- 欠損ログ（フラグで有無切替）----
L210     miss_txt=""
L211     if _env_true("DEBUG_MISSING_LOGS", False):
L212         miss=getattr(fb,"missing_logs",None)
L213         if miss is not None and not miss.empty:
L214             miss_txt="\nMissing data (head)\n"+miss.head(10).to_string(index=False)
L215
L216     return "\n".join(head+["\nChanged/Selected (+ Near Miss)", tbl])+miss_txt
L217
L218 def _disjoint_keepG(top_G, top_D, poolD):
L219     """
L220     Gに含まれる銘柄をDから除去し、DはpoolD（次点）で補充する。
L221     - 引数:
L222         top_G: List[str]  … G最終12銘柄
L223         top_D: List[str]  … D最終13銘柄（重複を含む可能性あり）
L224         poolD: List[str]  … D候補の順位リスト（top_Dを含む上位拡張）
L225     - 戻り値: (top_G, top_D_disjoint)
L226     - 挙動:
L227         1) DにG重複があれば順に置換
L228         2) 置換候補は poolD から、既使用(G∪D)を避けて前から採用
L229         3) 補充分が尽きた場合は元の銘柄を残す（安全フォールバック）
L230     """
L231     used, D = set(top_G), list(top_D)
L232     i = 0
L233     for j, t in enumerate(D):
L234         if t in used:
L235             while i < len(poolD) and (poolD[i] in used or poolD[i] in D):
L236                 i += 1
L237             if i < len(poolD):
L238                 D[j] = poolD[i]; used.add(D[j]); i += 1
L239     return top_G, D
L240
L241
L242 # ===== Input：外部I/Oと前処理（CSV/API・欠損補完） =====
L243 class Input:
L244     def __init__(self, cand, exist, bench, price_max, finnhub_api_key=None):
L245         self.cand, self.exist, self.bench, self.price_max = cand, exist, bench, price_max
L246         self.api_key = finnhub_api_key or os.environ.get("FINNHUB_API_KEY")
L247
L248     # ---- （Input専用）EPS補完・FCF算出系 ----
L249     @staticmethod
L250     def impute_eps_ttm(df: pd.DataFrame, ttm_col: str="eps_ttm", q_col: str="eps_q_recent", out_col: str|None=None) -> pd.DataFrame:
L251         out_col = out_col or ttm_col; df = df.copy(); df["eps_imputed"] = False
L252         cand = df[q_col]*4; ok = df[ttm_col].isna() & cand.replace([np.inf,-np.inf], np.nan).notna()
L253         df.loc[ok, out_col], df.loc[ok,"eps_imputed"] = cand[ok], True; return df
L254
L255     _CF_ALIASES = {"cfo":["Operating Cash Flow","Total Cash From Operating Activities"], "capex":["Capital Expenditure","Capital Expenditures"]}
L256
L257     @staticmethod
L258     def _pick_row(df: pd.DataFrame, names: list[str]) -> pd.Series|None:
L259         if df is None or df.empty: return None
L260         idx_lower = {str(i).lower(): i for i in df.index}
L261         for name in names:
L262             key = name.lower()
L263             if key in idx_lower: return df.loc[idx_lower[key]]
L264         return None
L265
L266     @staticmethod
L267     def _sum_last_n(s: pd.Series|None, n: int) -> float|None:
L268         if s is None or s.empty: return None
L269         vals = s.dropna().astype(float); return None if vals.empty else vals.iloc[:n].sum()
L270
L271     @staticmethod
L272     def _latest(s: pd.Series|None) -> float|None:
L273         if s is None or s.empty: return None
L274         vals = s.dropna().astype(float); return vals.iloc[0] if not vals.empty else None
L275
L276     def fetch_cfo_capex_ttm_yf(self, tickers: list[str]) -> pd.DataFrame:
L277         from concurrent.futures import ThreadPoolExecutor, as_completed
L278         pick, sumn, latest, aliases = self._pick_row, self._sum_last_n, self._latest, self._CF_ALIASES
L279
L280         def one(t: str):
L281             try:
L282                 tk = yf.Ticker(t)  # ★ セッションは渡さない（YFがcurl_cffiで管理）
L283                 qcf = tk.quarterly_cashflow
L284                 cfo_q, capex_q = pick(qcf, aliases["cfo"]), pick(qcf, aliases["capex"])
L285                 fcf_q = pick(qcf, ["Free Cash Flow","FreeCashFlow","Free cash flow"])
L286                 cfo, capex, fcf = sumn(cfo_q,4), sumn(capex_q,4), sumn(fcf_q,4)
L287                 if any(v is None for v in (cfo, capex, fcf)):
L288                     acf = tk.cashflow
L289                     if cfo   is None: cfo   = latest(pick(acf, aliases["cfo"]))
L290                     if capex is None: capex = latest(pick(acf, aliases["capex"]))
L291                     if fcf   is None: fcf   = latest(pick(acf, ["Free Cash Flow","FreeCashFlow","Free cash flow"]))
L292             except Exception as e:
L293                 print(f"[warn] yf financials error: {t}: {e}"); cfo=capex=fcf=None
L294             n=np.nan
L295             return {"ticker":t,
L296                     "cfo_ttm_yf":   n if cfo   is None else cfo,
L297                     "capex_ttm_yf": n if capex is None else capex,
L298                     "fcf_ttm_yf_direct": n if fcf is None else fcf}
L299
L300         rows, mw = [], int(os.getenv("FIN_THREADS","8"))
L301         with ThreadPoolExecutor(max_workers=mw) as ex:
L302             for f in as_completed(ex.submit(one,t) for t in tickers): rows.append(f.result())
L303         return pd.DataFrame(rows).set_index("ticker")
L304
L305     _FINN_CFO_KEYS = ["netCashProvidedByOperatingActivities","netCashFromOperatingActivities","cashFlowFromOperatingActivities","operatingCashFlow"]
L306     _FINN_CAPEX_KEYS = ["capitalExpenditure","capitalExpenditures","purchaseOfPPE","investmentsInPropertyPlantAndEquipment"]
L307
L308     @staticmethod
L309     def _first_key(d: dict, keys: list[str]):
L310         for k in keys:
L311             if k in d and d[k] is not None: return d[k]
L312         return None
L313
L314     @staticmethod
L315     def _finn_get(session: requests.Session, url: str, params: dict, retries: int=3, sleep_s: float=0.5):
L316         for i in range(retries):
L317             r = session.get(url, params=params, timeout=15)
L318             if r.status_code==429: time.sleep(min(2**i*sleep_s,4.0)); continue
L319             r.raise_for_status(); return r.json()
L320         r.raise_for_status()
L321
L322     def fetch_cfo_capex_ttm_finnhub(self, tickers: list[str], api_key: str|None=None) -> pd.DataFrame:
L323         api_key = api_key or os.getenv("FINNHUB_API_KEY")
L324         if not api_key: raise ValueError("Finnhub API key not provided. Set FINNHUB_API_KEY or pass api_key=")
L325         base, s, rows = "https://finnhub.io/api/v1", requests.Session(), []
L326         for sym in tickers:
L327             cfo_ttm = capex_ttm = None
L328             try:
L329                 j = self._finn_get(s, f"{base}/stock/cash-flow", {"symbol":sym,"frequency":"quarterly","limit":8,"token":api_key})
L330                 arr = j.get("cashFlow") or []; cfo_vals, capex_vals = [], []
L331                 for item in arr[:4]:
L332                     cfo_vals.append(self._first_key(item,self._FINN_CFO_KEYS)); capex_vals.append(self._first_key(item,self._FINN_CAPEX_KEYS))
L333                 if any(v is not None for v in cfo_vals): cfo_ttm = float(np.nansum([np.nan if v is None else float(v) for v in cfo_vals]))
L334                 if any(v is not None for v in capex_vals): capex_ttm = float(np.nansum([np.nan if v is None else float(v) for v in capex_vals]))
L335             except Exception: pass
L336             if cfo_ttm is None or capex_ttm is None:
L337                 try:
L338                     j = self._finn_get(s, f"{base}/stock/cash-flow", {"symbol":sym,"frequency":"annual","limit":1,"token":api_key})
L339                     arr = j.get("cashFlow") or []
L340                     if arr:
L341                         item0 = arr[0]
L342                         if cfo_ttm is None:
L343                             v = self._first_key(item0,self._FINN_CFO_KEYS)
L344                             if v is not None: cfo_ttm = float(v)
L345                         if capex_ttm is None:
L346                             v = self._first_key(item0,self._FINN_CAPEX_KEYS)
L347                             if v is not None: capex_ttm = float(v)
L348                 except Exception: pass
L349             rows.append({"ticker":sym,"cfo_ttm_fh":np.nan if cfo_ttm is None else cfo_ttm,"capex_ttm_fh":np.nan if capex_ttm is None else capex_ttm})
L350         return pd.DataFrame(rows).set_index("ticker")
L351
L352     def compute_fcf_with_fallback(self, tickers: list[str], finnhub_api_key: str|None=None) -> pd.DataFrame:
L353         yf_df = self.fetch_cfo_capex_ttm_yf(tickers)
L354         T.log("financials (yf) done")
L355         miss_mask = yf_df[["cfo_ttm_yf","capex_ttm_yf","fcf_ttm_yf_direct"]].isna().any(axis=1)
L356         need = yf_df.index[miss_mask].tolist(); print(f"[T] yf financials missing: {len(need)} {need[:10]}{'...' if len(need)>10 else ''}")
L357         if need:
L358             fh_df = self.fetch_cfo_capex_ttm_finnhub(need, api_key=finnhub_api_key)
L359             df = yf_df.join(fh_df, how="left")
L360             for col_yf, col_fh in [("cfo_ttm_yf","cfo_ttm_fh"),("capex_ttm_yf","capex_ttm_fh")]:
L361                 df[col_yf] = df[col_yf].fillna(df[col_fh])
L362             print("[T] financials (finnhub) done (fallback only)")
L363         else:
L364             df = yf_df.assign(cfo_ttm_fh=np.nan, capex_ttm_fh=np.nan)
L365             print("[T] financials (finnhub) skipped (no missing)")
L366         df["cfo_ttm"]  = df["cfo_ttm_yf"].where(df["cfo_ttm_yf"].notna(), df["cfo_ttm_fh"])
L367         df["capex_ttm"] = df["capex_ttm_yf"].where(df["capex_ttm_yf"].notna(), df["capex_ttm_fh"])
L368         cfo, capex = pd.to_numeric(df["cfo_ttm"], errors="coerce"), pd.to_numeric(df["capex_ttm"], errors="coerce").abs()
L369         fcf_calc = cfo - capex
L370         fcf_direct = pd.to_numeric(df.get("fcf_ttm_yf_direct"), errors="coerce")
L371         df["fcf_ttm"] = fcf_calc.where(fcf_calc.notna(), fcf_direct)
L372         df["cfo_source"]  = np.where(df["cfo_ttm_yf"].notna(),"yfinance",np.where(df["cfo_ttm_fh"].notna(),"finnhub",""))
L373         df["capex_source"] = np.where(df["capex_ttm_yf"].notna(),"yfinance",np.where(df["capex_ttm_fh"].notna(),"finnhub",""))
L374         df["fcf_imputed"] = df[["cfo_ttm","capex_ttm"]].isna().any(axis=1) & df["fcf_ttm"].notna()
L375         cols = ["cfo_ttm_yf","capex_ttm_yf","cfo_ttm_fh","capex_ttm_fh","cfo_ttm","capex_ttm","fcf_ttm","fcf_ttm_yf_direct","cfo_source","capex_source","fcf_imputed"]
L376         return df[cols].sort_index()
L377
L378     def _build_eps_df(self, tickers, tickers_bulk, info):
L379         eps_rows=[]
L380         for t in tickers:
L381             info_t, eps_ttm, eps_q = info[t], info[t].get("trailingEps", np.nan), np.nan
L382             try:
L383                 qearn, so = tickers_bulk.tickers[t].quarterly_earnings, info_t.get("sharesOutstanding")
L384                 if so and qearn is not None and not qearn.empty and "Earnings" in qearn.columns:
L385                     eps_ttm_q = qearn["Earnings"].head(4).sum()/so
L386                     if pd.notna(eps_ttm_q) and (pd.isna(eps_ttm) or (abs(eps_ttm)>0 and abs(eps_ttm/eps_ttm_q)>3)): eps_ttm = eps_ttm_q
L387                     eps_q = qearn["Earnings"].iloc[-1]/so
L388             except Exception: pass
L389             eps_rows.append({"ticker":t,"eps_ttm":eps_ttm,"eps_q_recent":eps_q})
L390         return self.impute_eps_ttm(pd.DataFrame(eps_rows).set_index("ticker"))
L391
L392     def prepare_data(self):
L393         """Fetch price and fundamental data for all tickers."""
L394         cand_info = yf.Tickers(" ".join(self.cand)); cand_prices = {}
L395         for t in self.cand:
L396             try: cand_prices[t] = cand_info.tickers[t].fast_info.get("lastPrice", np.inf)
L397             except Exception as e: print(f"{t}: price fetch failed ({e})"); cand_prices[t] = np.inf
L398         cand_f = [t for t,p in cand_prices.items() if p<=self.price_max]
L399         T.log("price cap filter done (CAND_PRICE_MAX)")
L400         tickers = sorted(set(self.exist + cand_f))
L401         T.log(f"universe prepared: unique={len(tickers)} bench={self.bench}")
L402         data = yf.download(tickers + [self.bench], period="600d", auto_adjust=True, progress=False)
L403         T.log("yf.download done")
L404         px, spx = data["Close"], data["Close"][self.bench]
L405         clip_days = int(os.getenv("PRICE_CLIP_DAYS", "0"))   # 0なら無効（既定）
L406         if clip_days > 0:
L407             px  = px.tail(clip_days + 1)
L408             spx = spx.tail(clip_days + 1)
L409             print(f"[T] price window clipped by env: {len(px)} rows (PRICE_CLIP_DAYS={clip_days})")
L410         else:
L411             print(f"[T] price window clip skipped; rows={len(px)}")
L412         tickers_bulk, info = yf.Tickers(" ".join(tickers)), {}
L413         for t in tickers:
L414             try: info[t] = tickers_bulk.tickers[t].info
L415             except Exception as e: print(f"{t}: info fetch failed ({e})"); info[t] = {}
L416         eps_df = self._build_eps_df(tickers, tickers_bulk, info)
L417         fcf_df = self.compute_fcf_with_fallback(tickers, finnhub_api_key=self.api_key)
L418         T.log("eps/fcf prep done")
L419         returns = px[tickers].pct_change()
L420         T.log("price prep/returns done")
L421         return dict(cand=cand_f, tickers=tickers, data=data, px=px, spx=spx, tickers_bulk=tickers_bulk, info=info, eps_df=eps_df, fcf_df=fcf_df, returns=returns)
L422
L423
L424 # ===== Selector：相関低減・選定（スコア＆リターンだけ読む） =====
L425 class Selector:
L426     # ---- DRRS helpers（Selector専用） ----
L427     @staticmethod
L428     def _z_np(X: np.ndarray) -> np.ndarray:
L429         X = np.asarray(X, dtype=np.float32); m = np.nanmean(X, axis=0, keepdims=True); s = np.nanstd(X, axis=0, keepdims=True)+1e-9
L430         return (np.nan_to_num(X)-m)/s
L431
L432     @classmethod
L433     def residual_corr(cls, R: np.ndarray, n_pc: int=3, shrink: float=0.1) -> np.ndarray:
L434         Z = cls._z_np(R); U,S,_ = np.linalg.svd(Z, full_matrices=False); F = U[:,:n_pc]*S[:n_pc]; B = np.linalg.lstsq(F, Z, rcond=None)[0]
L435         E = Z - F@B; C = np.corrcoef(E, rowvar=False)
L436         off = C - np.diag(np.diag(C)); iu = np.triu_indices_from(off,1); avg_abs = np.nanmean(np.abs(off[iu])) if iu[0].size else 0.0
L437         shrink_eff = float(np.clip(shrink + 0.5*avg_abs, 0.1, 0.6)); N = C.shape[0]
L438         return (1.0 - shrink_eff)*C + shrink_eff*np.eye(N, dtype=C.dtype)
L439
L440     @classmethod
L441     def rrqr_like_det(cls, R: np.ndarray, score: np.ndarray, k: int, gamma: float=1.0):
L442         Z, w = cls._z_np(R), (score-score.min())/(np.ptp(score)+1e-12); X = Z*(1.0 + gamma*w)
L443         N, k = X.shape[1], max(0, min(k, X.shape[1]))
L444         if k==0: return []
L445         S, selected, Rres = [], np.zeros(N, dtype=bool), X.copy()
L446         for _ in range(k):
L447             norms = (Rres*Rres).sum(axis=0); cand = np.where(~selected)[0]
L448             j = sorted(cand, key=lambda c:(-norms[c], -w[c], c))[0]
L449             S.append(j); selected[j]=True; u = X[:,j:j+1]; u/=(np.linalg.norm(u)+1e-12); Rres = Rres - u @ (u.T @ Rres)
L450         return sorted(S)
L451
L452     @staticmethod
L453     def _obj(corrM: np.ndarray, score: np.ndarray, idx, lam: float) -> float:
L454         idx = list(idx); P = corrM[np.ix_(idx, idx)]; s = (score-score.mean())/(score.std()+1e-9)
L455         return float(s[idx].sum() - lam*((P.sum()-np.trace(P))/2.0))
L456
L457     @classmethod
L458     def swap_local_det(cls, corrM: np.ndarray, score: np.ndarray, idx, lam: float=0.6, max_pass: int=15):
L459         S, best, improved, passes = sorted(idx), cls._obj(corrM, score, idx, lam), True, 0
L460         while improved and passes<max_pass:
L461             improved, passes = False, passes+1
L462             for i,out in enumerate(list(S)):
L463                 for inn in range(len(score)):
L464                     if inn in S: continue
L465                     cand = sorted(S[:i]+[inn]+S[i+1:]); v = cls._obj(corrM, score, cand, lam)
L466                     if v>best+1e-10: S, best, improved = cand, v, True; break
L467                 if improved: break
L468         return S, best
L469
L470     @staticmethod
L471     def _obj_with_cross(C_within: np.ndarray, C_cross: np.ndarray|None, score: np.ndarray, idx, lam: float, mu: float) -> float:
L472         idx = list(idx); P = C_within[np.ix_(idx, idx)]; s = (score-score.mean())/(score.std()+1e-9)
L473         within = (P.sum()-np.trace(P))/2.0; cross = 0.0
L474         if C_cross is not None and C_cross.size>0: cross = C_cross[idx,:].sum()
L475         return float(s[idx].sum() - lam*within - mu*cross)
L476
L477     @classmethod
L478     def swap_local_det_cross(cls, C_within: np.ndarray, C_cross: np.ndarray|None, score: np.ndarray, idx, lam: float=0.6, mu: float=0.3, max_pass: int=15):
L479         S, best, improved, passes, N = sorted(idx), cls._obj_with_cross(C_within,C_cross,score,idx,lam,mu), True, 0, len(score)
L480         while improved and passes<max_pass:
L481             improved, passes = False, passes+1
L482             for i,out in enumerate(list(S)):
L483                 for inn in range(N):
L484                     if inn in S: continue
L485                     cand = sorted(S[:i]+[inn]+S[i+1:]); v = cls._obj_with_cross(C_within,C_cross,score,cand,lam,mu)
L486                     if v>best+1e-10: S, best, improved = cand, v, True; break
L487                 if improved: break
L488         return S, best
L489
L490     @staticmethod
L491     def avg_corr(C: np.ndarray, idx) -> float:
L492         k = len(idx); P = C[np.ix_(idx, idx)]
L493         return float((P.sum()-np.trace(P))/(k*(k-1)+1e-12))
L494
L495     @classmethod
L496     def select_bucket_drrs(cls, returns_df: pd.DataFrame, score_ser: pd.Series, pool_tickers: list[str], k: int, *, n_pc: int, gamma: float, lam: float, eta: float, lookback: int, prev_tickers: list[str]|None, shrink: float=0.10, g_fixed_tickers: list[str]|None=None, mu: float=0.0):
L497         g_fixed = [t for t in (g_fixed_tickers or []) if t in returns_df.columns]
L498         union = [t for t in pool_tickers if t in returns_df.columns]
L499         for t in g_fixed:
L500             if t not in union: union.append(t)
L501         Rdf_all = returns_df[union]; Rdf_all = Rdf_all.iloc[-lookback:] if len(Rdf_all)>=lookback else Rdf_all; Rdf_all = Rdf_all.dropna()
L502         pool_eff, g_eff = [t for t in pool_tickers if t in Rdf_all.columns], [t for t in g_fixed if t in Rdf_all.columns]
L503         if len(pool_eff)==0: return dict(idx=[], tickers=[], avg_res_corr=np.nan, sum_score=0.0, objective=-np.inf)
L504         score = score_ser.reindex(pool_eff).to_numpy(dtype=np.float32)
L505         C_all = cls.residual_corr(Rdf_all.to_numpy(), n_pc=n_pc, shrink=shrink)
L506         col_pos = {c:i for i,c in enumerate(Rdf_all.columns)}; pool_pos = [col_pos[t] for t in pool_eff]
L507         C_within, C_cross = C_all[np.ix_(pool_pos,pool_pos)], None
L508         if len(g_eff)>0 and mu>0.0:
L509             g_pos = [col_pos[t] for t in g_eff]; C_cross = C_all[np.ix_(pool_pos,g_pos)]
L510         R_pool = Rdf_all[pool_eff].to_numpy(); S0 = cls.rrqr_like_det(R_pool, score, k, gamma=gamma)
L511         S, Jn = (cls.swap_local_det_cross(C_within, C_cross, score, S0, lam=lam, mu=mu, max_pass=15) if C_cross is not None else cls.swap_local_det(C_within, score, S0, lam=lam, max_pass=15))
L512         if prev_tickers:
L513             prev_idx = [pool_eff.index(t) for t in prev_tickers if t in pool_eff]
L514             if len(prev_idx)==min(k,len(pool_eff)):
L515                 Jp = (cls._obj_with_cross(C_within,C_cross,score,prev_idx,lam,mu) if C_cross is not None else cls._obj(C_within,score,prev_idx,lam))
L516                 if Jn < Jp + eta: S, Jn = sorted(prev_idx), Jp
L517         selected_tickers = [pool_eff[i] for i in S]
L518         return dict(idx=S, tickers=selected_tickers, avg_res_corr=cls.avg_corr(C_within,S), sum_score=float(score[S].sum()), objective=float(Jn))
L519
L520     # ---- 選定（スコア Series / returns だけを受ける）----
L521     def select_buckets(self, returns_df: pd.DataFrame, g_score: pd.Series, d_score_all: pd.Series, cfg: PipelineConfig) -> SelectionBundle:
L522         init_G = g_score.nlargest(min(cfg.drrs.corrM, len(g_score))).index.tolist(); prevG = _load_prev(G_PREV_JSON)
L523         resG = self.select_bucket_drrs(returns_df=returns_df, score_ser=g_score, pool_tickers=init_G, k=N_G,
L524                                        n_pc=cfg.drrs.G.get("n_pc",3), gamma=cfg.drrs.G.get("gamma",1.2),
L525                                        lam=cfg.drrs.G.get("lam",0.68), eta=cfg.drrs.G.get("eta",0.8),
L526                                        lookback=cfg.drrs.G.get("lookback",252), prev_tickers=prevG,
L527                                        shrink=cfg.drrs.shrink, g_fixed_tickers=None, mu=0.0)
L528         top_G = resG["tickers"]
L529
L530         # df_z に依存せず、スコアの index から D プールを構成（機能は同等）
L531         d_score = d_score_all.drop(top_G, errors='ignore')
L532         D_pool_index = d_score.index
L533         init_D = d_score.loc[D_pool_index].nlargest(min(cfg.drrs.corrM, len(D_pool_index))).index.tolist(); prevD = _load_prev(D_PREV_JSON)
L534         mu = cfg.drrs.cross_mu_gd
L535         resD = self.select_bucket_drrs(returns_df=returns_df, score_ser=d_score_all, pool_tickers=init_D, k=N_D,
L536                                        n_pc=cfg.drrs.D.get("n_pc",4), gamma=cfg.drrs.D.get("gamma",0.8),
L537                                        lam=cfg.drrs.D.get("lam",0.85), eta=cfg.drrs.D.get("eta",0.5),
L538                                        lookback=cfg.drrs.D.get("lookback",504), prev_tickers=prevD,
L539                                        shrink=cfg.drrs.shrink, g_fixed_tickers=top_G, mu=mu)
L540         top_D = resD["tickers"]
L541
L542         _save_sel(G_PREV_JSON, top_G, resG["avg_res_corr"], resG["sum_score"], resG["objective"])
L543         _save_sel(D_PREV_JSON, top_D, resD["avg_res_corr"], resD["sum_score"], resD["objective"])
L544         return SelectionBundle(resG=resG, resD=resD, top_G=top_G, top_D=top_D, init_G=init_G, init_D=init_D)
L545
L546
L547 # ===== Output：出力整形と送信（表示・Slack） =====
L548 class Output:
L549
L550     def __init__(self, debug=False):
L551         self.debug = debug
L552         self.miss_df = self.g_table = self.d_table = self.io_table = self.df_metrics_fmt = self.debug_table = None
L553         self.g_title = self.d_title = ""
L554         self.g_formatters = self.d_formatters = {}
L555         # 低スコア（GSC+DSC）Top10 表示/送信用
L556         self.low10_table = None
L557
L558     # --- 表示（元 display_results のロジックそのまま） ---
L559     def display_results(self, *, exist, bench, df_z, g_score, d_score_all,
L560                         init_G, init_D, top_G, top_D, **kwargs):
L561         pd.set_option('display.float_format','{:.3f}'.format)
L562         print("📈 ファクター分散最適化の結果")
L563         if self.miss_df is not None and not self.miss_df.empty:
L564             print("Missing Data:")
L565             print(self.miss_df.to_string(index=False))
L566
L567         # ---- 表示用：Changes/Near-Miss のスコア源を“最終集計”に統一するプロキシ ----
L568         try:
L569             sc = getattr(self, "_sc", None)
L570             agg_G = getattr(sc, "_agg_G", None)
L571             agg_D = getattr(sc, "_agg_D", None)
L572         except Exception:
L573             sc = agg_G = agg_D = None
L574         class _SeriesProxy:
L575             __slots__ = ("primary", "fallback")
L576             def __init__(self, primary, fallback): self.primary, self.fallback = primary, fallback
L577             def get(self, key, default=None):
L578                 try:
L579                     v = self.primary.get(key) if hasattr(self.primary, "get") else None
L580                     if v is not None and not (isinstance(v, float) and v != v):
L581                         return v
L582                 except Exception:
L583                     pass
L584                 try:
L585                     return self.fallback.get(key) if hasattr(self.fallback, "get") else default
L586                 except Exception:
L587                     return default
L588         g_score = _SeriesProxy(agg_G, g_score)
L589         d_score_all = _SeriesProxy(agg_D, d_score_all)
L590         near_G = getattr(sc, "_near_G", []) if sc else []
L591         near_D = getattr(sc, "_near_D", []) if sc else []
L592
L593         extra_G = [t for t in init_G if t not in top_G][:5]; G_UNI = top_G + extra_G
L594         gsc_series = pd.Series({t: g_score.get(t) for t in G_UNI}, name='GSC')
L595         self.g_table = pd.concat([df_z.loc[G_UNI,['GRW','MOM','TRD','VOL']], gsc_series], axis=1)
L596         self.g_table.index = [t + ("⭐️" if t in top_G else "") for t in G_UNI]
L597         self.g_formatters = {col:"{:.2f}".format for col in ['GRW','MOM','TRD','VOL']}; self.g_formatters['GSC'] = "{:.3f}".format
L598         self.g_title = (f"[G枠 / {N_G} / {_fmt_w(g_weights)} / corrM={corrM} / "
L599                         f"LB={DRRS_G['lookback']} nPC={DRRS_G['n_pc']} γ={DRRS_G['gamma']} λ={DRRS_G['lam']} η={DRRS_G['eta']} shrink={DRRS_SHRINK}]")
L600         if near_G:
L601             add = [t for t in near_G if t not in set(G_UNI)][:10]
L602             if len(add) < 10:
L603                 try:
L604                     aggG = getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False)
L605                     out_now = sorted(set(exist) - set(top_G + top_D))  # 今回 OUT
L606                     used = set(G_UNI + add)
L607                     def _push(lst):
L608                         nonlocal add, used
L609                         for t in lst:
L610                             if len(add) == 10: break
L611                             if t in aggG.index and t not in used:
L612                                 add.append(t); used.add(t)
L613                     _push(out_now)           # ① 今回 OUT を優先
L614                     _push(list(aggG.index))  # ② まだ足りなければ上位で充填
L615                 except Exception:
L616                     pass
L617             if add:
L618                 near_tbl = pd.concat([df_z.loc[add,['GRW','MOM','TRD','VOL']], pd.Series({t: g_score.get(t) for t in add}, name='GSC')], axis=1)
L619                 self.g_table = pd.concat([self.g_table, near_tbl], axis=0)
L620         print(self.g_title); print(self.g_table.to_string(formatters=self.g_formatters))
L621
L622         extra_D = [t for t in init_D if t not in top_D][:5]; D_UNI = top_D + extra_D
L623         cols_D = ['QAL','YLD','VOL','TRD']; d_disp = pd.DataFrame(index=D_UNI)
L624         d_disp['QAL'], d_disp['YLD'], d_disp['VOL'], d_disp['TRD'] = df_z.loc[D_UNI,'D_QAL'], df_z.loc[D_UNI,'D_YLD'], df_z.loc[D_UNI,'D_VOL_RAW'], df_z.loc[D_UNI,'D_TRD']
L625         dsc_series = pd.Series({t: d_score_all.get(t) for t in D_UNI}, name='DSC')
L626         self.d_table = pd.concat([d_disp, dsc_series], axis=1); self.d_table.index = [t + ("⭐️" if t in top_D else "") for t in D_UNI]
L627         self.d_formatters = {col:"{:.2f}".format for col in cols_D}; self.d_formatters['DSC']="{:.3f}".format
L628         import scorer
L629         dw_eff = scorer.D_WEIGHTS_EFF
L630         self.d_title = (f"[D枠 / {N_D} / {_fmt_w(dw_eff)} / corrM={corrM} / "
L631                         f"LB={DRRS_D['lookback']} nPC={DRRS_D['n_pc']} γ={DRRS_D['gamma']} λ={DRRS_D['lam']} μ={CROSS_MU_GD} η={DRRS_D['eta']} shrink={DRRS_SHRINK}]")
L632         if near_D:
L633             add = [t for t in near_D if t not in set(D_UNI)][:10]
L634             if add:
L635                 d_disp2 = pd.DataFrame(index=add)
L636                 d_disp2['QAL'], d_disp2['YLD'], d_disp2['VOL'], d_disp2['TRD'] = df_z.loc[add,'D_QAL'], df_z.loc[add,'D_YLD'], df_z.loc[add,'D_VOL_RAW'], df_z.loc[add,'D_TRD']
L637                 near_tbl = pd.concat([d_disp2, pd.Series({t: d_score_all.get(t) for t in add}, name='DSC')], axis=1)
L638                 self.d_table = pd.concat([self.d_table, near_tbl], axis=0)
L639         print(self.d_title); print(self.d_table.to_string(formatters=self.d_formatters))
L640
L641         # === Changes（IN の GSC/DSC を表示。OUT は銘柄名のみ） ===
L642         in_list = sorted(set(list(top_G)+list(top_D)) - set(exist))
L643         out_list = sorted(set(exist) - set(list(top_G)+list(top_D)))
L644
L645         self.io_table = pd.DataFrame({
L646             'IN': pd.Series(in_list),
L647             '/ OUT': pd.Series(out_list)
L648         })
L649         g_list = [f"{g_score.get(t):.3f}" if pd.notna(g_score.get(t)) else '—' for t in out_list]
L650         d_list = [f"{d_score_all.get(t):.3f}" if pd.notna(d_score_all.get(t)) else '—' for t in out_list]
L651         self.io_table['GSC'] = pd.Series(g_list)
L652         self.io_table['DSC'] = pd.Series(d_list)
L653
L654         print("Changes:")
L655         print(self.io_table.to_string(index=False))
L656
L657         all_tickers = list(set(exist + list(top_G) + list(top_D) + [bench])); prices = yf.download(all_tickers, period='1y', auto_adjust=True, progress=False)['Close']
L658         ret = prices.pct_change(); portfolios = {'CUR':exist,'NEW':list(top_G)+list(top_D)}; metrics={}
L659         for name,ticks in portfolios.items():
L660             pr = ret[ticks].mean(axis=1, skipna=True).dropna(); cum = (1+pr).cumprod()-1; n = len(pr)
L661             if n>=252: ann_ret, ann_vol = (1+cum.iloc[-1])**(252/n)-1, pr.std()*np.sqrt(252)
L662             else: ann_ret, ann_vol = cum.iloc[-1], pr.std()*np.sqrt(n)
L663             sharpe, drawdown = ann_ret/ann_vol, (cum - cum.cummax()).min()
L664             if len(ticks)>=2:
L665                 C_raw = ret[ticks].corr(); RAW_rho = C_raw.mask(np.eye(len(ticks), dtype=bool)).stack().mean()
L666                 R = ret[ticks].dropna().to_numpy(); C_resid = Selector.residual_corr(R, n_pc=3, shrink=DRRS_SHRINK)
L667                 RESID_rho = float((C_resid.sum()-np.trace(C_resid))/(C_resid.shape[0]*(C_resid.shape[0]-1)))
L668             else: RAW_rho = RESID_rho = np.nan
L669             divy = ttm_div_yield_portfolio(ticks); metrics[name] = {'RET':ann_ret,'VOL':ann_vol,'SHP':sharpe,'MDD':drawdown,'RAWρ':RAW_rho,'RESIDρ':RESID_rho,'DIVY':divy}
L670         df_metrics = pd.DataFrame(metrics).T; df_metrics_pct = df_metrics.copy(); self.df_metrics = df_metrics
L671         for col in ['RET','VOL','MDD','DIVY']: df_metrics_pct[col] = df_metrics_pct[col]*100
L672         cols_order = ['RET','VOL','SHP','MDD','RAWρ','RESIDρ','DIVY']; df_metrics_pct = df_metrics_pct.reindex(columns=cols_order)
L673         def _fmt_row(s):
L674             return pd.Series({'RET':f"{s['RET']:.1f}%",'VOL':f"{s['VOL']:.1f}%",'SHP':f"{s['SHP']:.1f}",'MDD':f"{s['MDD']:.1f}%",'RAWρ':(f"{s['RAWρ']:.2f}" if pd.notna(s['RAWρ']) else "NaN"),'RESIDρ':(f"{s['RESIDρ']:.2f}" if pd.notna(s['RESIDρ']) else "NaN"),'DIVY':f"{s['DIVY']:.1f}%"})
L675         self.df_metrics_fmt = df_metrics_pct.apply(_fmt_row, axis=1); print("Performance Comparison:"); print(self.df_metrics_fmt.to_string())
L676         if self.debug:
L677             self.debug_table = pd.concat([df_z[['TR','EPS','REV','ROE','BETA','DIV','FCF','RS','TR_str','DIV_STREAK']], g_score.rename('GSC'), d_score_all.rename('DSC')], axis=1).round(3)
L678             print("Debug Data:"); print(self.debug_table.to_string())
L679
L680         # === 追加: GSC+DSC が低い順 TOP10 ===
L681         try:
L682             all_scores = pd.DataFrame({'GSC': df_z['GSC'], 'DSC': df_z['DSC']}).copy()
L683             all_scores['G_plus_D'] = all_scores['GSC'] + all_scores['DSC']
L684             all_scores = all_scores.dropna(subset=['G_plus_D'])
L685             self.low10_table = all_scores.sort_values('G_plus_D', ascending=True).head(10).round(3)
L686             print("Low Score Candidates (GSC+DSC bottom 10):")
L687             print(self.low10_table.to_string())
L688         except Exception as e:
L689             print(f"[warn] low-score ranking failed: {e}")
L690             self.low10_table = None
L691
L692     # --- Slack送信（元 notify_slack のロジックそのまま） ---
L693     def notify_slack(self):
L694         SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")
L695         if not SLACK_WEBHOOK_URL: raise ValueError("SLACK_WEBHOOK_URL not set (環境変数が未設定です)")
L696         def _filter_suffix_from(spec: dict, group: str) -> str:
L697             g = spec.get(group, {})
L698             parts = [str(m) for m in g.get("pre_mask", [])]
L699             for k, v in (g.get("pre_filter", {}) or {}).items():
L700                 base, op = (k[:-4], "<") if k.endswith("_max") else ((k[:-4], ">") if k.endswith("_min") else (k, "="))
L701                 name = {"beta": "β"}.get(base, base)
L702                 try: val = f"{float(v):g}"
L703                 except: val = str(v)
L704                 parts.append(f"{name}{op}{val}")
L705             return "" if not parts else " / filter:" + " & ".join(parts)
L706         def _inject_filter_suffix(title: str, group: str) -> str:
L707             suf = _filter_suffix_from(FILTER_SPEC, group)
L708             return f"{title[:-1]}{suf}]" if suf and title.endswith("]") else (title + suf)
L709         def _blk(title, tbl, fmt=None, drop=()):
L710             if tbl is None or getattr(tbl,'empty',False): return f"{title}\n(選定なし)\n"
L711             if drop and hasattr(tbl,'columns'):
L712                 keep = [c for c in tbl.columns if c not in drop]
L713                 tbl, fmt = tbl[keep], {k:v for k,v in (fmt or {}).items() if k in keep}
L714             return f"{title}\n```{tbl.to_string(formatters=fmt)}```\n"
L715
L716         g_title = _inject_filter_suffix(self.g_title, "G")
L717         d_title = _inject_filter_suffix(self.d_title, "D")
L718         message  = "📈 ファクター分散最適化の結果\n"
L719         if self.miss_df is not None and not self.miss_df.empty:
L720             message += "Missing Data\n```" + self.miss_df.to_string(index=False) + "```\n"
L721         message += _blk(g_title, self.g_table, self.g_formatters, drop=("TRD",))
L722         message += _blk(d_title, self.d_table, self.d_formatters)
L723         message += "Changes\n" + ("(変更なし)\n" if self.io_table is None or getattr(self.io_table,'empty',False) else f"```{self.io_table.to_string(index=False)}```\n")
L724         message += "Performance Comparison:\n```" + self.df_metrics_fmt.to_string() + "```"
L725         if self.debug and self.debug_table is not None:
L726             message += "\nDebug Data\n```" + self.debug_table.to_string() + "```"
L727         payload = {"text": message}
L728         try:
L729             resp = requests.post(SLACK_WEBHOOK_URL, json=payload); resp.raise_for_status(); print("✅ Slack（Webhook）へ送信しました")
L730         except Exception as e: print(f"⚠️ Slack通知エラー: {e}")
L731
L732
L733 def _infer_g_universe(feature_df, selected12=None, near5=None):
L734     try:
L735         out = feature_df.index[feature_df['group'].astype(str).str.upper().eq('G')].tolist()
L736         if out: return out
L737     except Exception:
L738         pass
L739     base = set()
L740     for lst in (selected12 or []), (near5 or []):
L741         for x in (lst or []): base.add(x)
L742     return list(base) if base else list(feature_df.index)
L743
L744
L745 def _fmt_with_fire_mark(tickers, feature_df):
L746     out = []
L747     for t in tickers or []:
L748         try:
L749             br = bool(feature_df.at[t, "G_BREAKOUT_recent_5d"])
L750             pb = bool(feature_df.at[t, "G_PULLBACK_recent_5d"])
L751             out.append(f"{t}{' 🔥' if (br or pb) else ''}")
L752         except Exception:
L753             out.append(t)
L754     return out
L755
L756
L757 def _label_recent_event(t, feature_df):
L758     try:
L759         br = bool(feature_df.at[t, "G_BREAKOUT_recent_5d"]); dbr = str(feature_df.at[t, "G_BREAKOUT_last_date"]) if br else ""
L760         pb = bool(feature_df.at[t, "G_PULLBACK_recent_5d"]); dpb = str(feature_df.at[t, "G_PULLBACK_last_date"]) if pb else ""
L761         if   br and not pb: return f"{t}（ブレイクアウト確定 {dbr}）"
L762         elif pb and not br: return f"{t}（押し目反発 {dpb}）"
L763         elif br and pb:     return f"{t}（ブレイクアウト確定 {dbr}／押し目反発 {dpb}）"
L764     except Exception:
L765         pass
L766     return t
L767
L768
L769 # ===== パイプライン可視化：G/D共通フロー（出力は不変） ==============================
L770
L771 def io_build_input_bundle() -> InputBundle:
L772     """
L773     既存の『データ取得→前処理』を実行し、InputBundle を返す。
L774     処理内容・列名・丸め・例外・ログ文言は現行どおり（変更禁止）。
L775     """
L776     inp = Input(cand=cand, exist=exist, bench=bench,
L777                 price_max=CAND_PRICE_MAX, finnhub_api_key=FINNHUB_API_KEY)
L778     state = inp.prepare_data()
L779     return InputBundle(
L780         cand=state["cand"], tickers=state["tickers"], bench=bench,
L781         data=state["data"], px=state["px"], spx=state["spx"],
L782         tickers_bulk=state["tickers_bulk"], info=state["info"],
L783         eps_df=state["eps_df"], fcf_df=state["fcf_df"],
L784         returns=state["returns"]
L785     )
L786
L787 def run_group(sc: Scorer, group: str, inb: InputBundle, cfg: PipelineConfig,
L788               n_target: int, prev_json_path: str) -> tuple[list, float, float, float]:
L789     """
L790     G/Dを同一手順で処理：採点→フィルター→選定（相関低減込み）。
L791     戻り値：(pick, avg_res_corr, sum_score, objective)
L792     JSON保存は既存フォーマット（キー名・丸め桁・順序）を踏襲。
L793     """
L794     sc.cfg = cfg
L795
L796     if hasattr(sc, "score_build_features"):
L797         feat = sc.score_build_features(inb)
L798         if not hasattr(sc, "_feat_logged"):
L799             T.log("features built (scorer)")
L800             sc._feat_logged = True
L801         agg = sc.score_aggregate(feat, group, cfg) if hasattr(sc, "score_aggregate") else feat
L802     else:
L803         fb = sc.aggregate_scores(inb, cfg)
L804         if not hasattr(sc, "_feat_logged"):
L805             T.log("features built (scorer)")
L806             sc._feat_logged = True
L807         sc._feat = fb
L808         agg = fb.g_score if group == "G" else fb.d_score_all
L809         if group == "D" and hasattr(fb, "df"):
L810             agg = agg[fb.df['BETA'] < D_BETA_MAX]
L811
L812     if hasattr(sc, "filter_candidates"):
L813         mask = sc.filter_candidates(inb, agg, group, cfg)
L814         agg = agg[mask]
L815
L816     selector = Selector()
L817     prev = _load_prev(prev_json_path)
L818     if hasattr(sc, "select_diversified"):
L819         pick, avg_r, sum_sc, obj = sc.select_diversified(
L820             agg, group, cfg, n_target,
L821             selector=selector, prev_tickers=prev,
L822             corrM=cfg.drrs.corrM, shrink=cfg.drrs.shrink,
L823             cross_mu=cfg.drrs.cross_mu_gd
L824         )
L825     else:
L826         if group == "G":
L827             init = agg.nlargest(min(cfg.drrs.corrM, len(agg))).index.tolist()
L828             res = selector.select_bucket_drrs(
L829                 returns_df=inb.returns, score_ser=agg, pool_tickers=init, k=n_target,
L830                 n_pc=cfg.drrs.G.get("n_pc", 3), gamma=cfg.drrs.G.get("gamma", 1.2),
L831                 lam=cfg.drrs.G.get("lam", 0.68), eta=cfg.drrs.G.get("eta", 0.8),
L832                 lookback=cfg.drrs.G.get("lookback", 252), prev_tickers=prev,
L833                 shrink=cfg.drrs.shrink, g_fixed_tickers=None, mu=0.0
L834             )
L835         else:
L836             init = agg.nlargest(min(cfg.drrs.corrM, len(agg))).index.tolist()
L837             g_fixed = getattr(sc, "_top_G", None)
L838             res = selector.select_bucket_drrs(
L839                 returns_df=inb.returns, score_ser=agg, pool_tickers=init, k=n_target,
L840                 n_pc=cfg.drrs.D.get("n_pc", 4), gamma=cfg.drrs.D.get("gamma", 0.8),
L841                 lam=cfg.drrs.D.get("lam", 0.85), eta=cfg.drrs.D.get("eta", 0.5),
L842                 lookback=cfg.drrs.D.get("lookback", 504), prev_tickers=prev,
L843                 shrink=cfg.drrs.shrink, g_fixed_tickers=g_fixed,
L844                 mu=cfg.drrs.cross_mu_gd
L845             )
L846         pick = res["tickers"]; avg_r = res["avg_res_corr"]
L847         sum_sc = res["sum_score"]; obj = res["objective"]
L848         if group == "D":
L849             _, pick = _disjoint_keepG(getattr(sc, "_top_G", []), pick, init)
L850             T.log("selection finalized (G/D)")
L851     # --- Near-Miss: 惜しくも選ばれなかった上位10を保持（Slack表示用） ---
L852     # 5) Near-Miss と最終集計Seriesを保持（表示専用。計算へ影響なし）
L853     try:
L854         pool = agg.drop(index=[t for t in pick if t in agg.index], errors="ignore")
L855         near10 = list(pool.sort_values(ascending=False).head(10).index)
L856         setattr(sc, f"_near_{group}", near10)
L857         setattr(sc, f"_agg_{group}", agg)
L858     except Exception:
L859         pass
L860
L861     _save_sel(prev_json_path, pick, avg_r, sum_sc, obj)
L862     if group == "D":
L863         T.log("save done")
L864     if group == "G":
L865         sc._top_G = pick
L866     return pick, avg_r, sum_sc, obj
L867
L868 def run_pipeline() -> SelectionBundle:
L869     """
L870     G/D共通フローの入口。I/Oはここだけで実施し、計算はScorerに委譲。
L871     Slack文言・丸め・順序は既存の Output を用いて変更しない。
L872     """
L873     inb = io_build_input_bundle()
L874     cfg = PipelineConfig(
L875         weights=WeightsConfig(g=g_weights, d=D_weights),
L876         drrs=DRRSParams(corrM=corrM, shrink=DRRS_SHRINK,
L877                          G=DRRS_G, D=DRRS_D, cross_mu_gd=CROSS_MU_GD),
L878         price_max=CAND_PRICE_MAX
L879     )
L880     sc = Scorer()
L881     top_G, avgG, sumG, objG = run_group(sc, "G", inb, cfg, N_G, G_PREV_JSON)
L882     poolG = list(getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False).index)
L883     alpha = Scorer.spx_to_alpha(inb.spx)
L884     sectors = {t: (inb.info.get(t, {}).get("sector") or "U") for t in poolG}
L885     scores = {t: Scorer.g_score.get(t, 0.0) for t in poolG}
L886     top_G = Scorer.pick_top_softcap(scores, sectors, N=N_G, cap=2, alpha=alpha, hard=5)
L887     sc._top_G = top_G
L888     try:
L889         aggG = getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False)
L890         sc._near_G = [t for t in aggG.index if t not in set(top_G)][:10]
L891     except Exception:
L892         pass
L893     base = sum(Scorer.g_score.get(t,0.0) for t in poolG[:N_G])
L894     effs = sum(Scorer.g_score.get(t,0.0) for t in top_G)
L895     print(f"[soft_cap2] score_cost={(base-effs)/max(1e-9,abs(base)):.2%}, alpha={alpha:.3f}")
L896     top_D, avgD, sumD, objD = run_group(sc, "D", inb, cfg, N_D, D_PREV_JSON)
L897     fb = getattr(sc, "_feat", None)
L898     near_G = getattr(sc, "_near_G", [])
L899     selected12 = list(top_G)
L900     df = fb.df if fb is not None else pd.DataFrame()
L901     guni = _infer_g_universe(df, selected12, near_G)
L902     try:
L903         fire_recent = [t for t in guni
L904                        if (str(df.at[t, "G_BREAKOUT_recent_5d"]) == "True") or
L905                           (str(df.at[t, "G_PULLBACK_recent_5d"]) == "True")]
L906     except Exception:
L907         fire_recent = []
L908     lines = [
L909         "【G枠レポート｜週次モニタ（直近5営業日）】",
L910         "【凡例】🔥=直近5営業日内に「ブレイクアウト確定」または「押し目反発」を検知",
L911         f"選定12: {', '.join(_fmt_with_fire_mark(selected12, df))}" if selected12 else "選定12: なし",
L912         f"次点10: {', '.join(_fmt_with_fire_mark(near_G, df))}" if near_G else "次点10: なし",
L913     ]
L914     if fire_recent:
L915         fire_list = ", ".join([_label_recent_event(t, df) for t in fire_recent])
L916         lines.append(f"過去5営業日の検知: {fire_list}")
L917     else:
L918         lines.append("過去5営業日の検知: なし")
L919     try:
L920         webhook = os.environ.get("SLACK_WEBHOOK_URL", "")
L921         if webhook:
L922             requests.post(webhook, json={"text": "\n".join(lines)}, timeout=10)
L923     except Exception:
L924         pass
L925
L926     out = Output(debug=debug_mode)
L927     # 表示側から選定時の集計へアクセスできるように保持（表示専用・副作用なし）
L928     try: out._sc = sc
L929     except Exception: pass
L930     if hasattr(sc, "_feat"):
L931         try:
L932             out.miss_df = sc._feat.missing_logs
L933             out.display_results(
L934                 exist=exist, bench=bench, df_z=sc._feat.df_z,
L935                 g_score=sc._feat.g_score, d_score_all=sc._feat.d_score_all,
L936                 init_G=top_G, init_D=top_D, top_G=top_G, top_D=top_D
L937             )
L938         except Exception:
L939             pass
L940     out.notify_slack()
L941     sb = SelectionBundle(
L942         resG={"tickers": top_G, "avg_res_corr": avgG,
L943               "sum_score": sumG, "objective": objG},
L944         resD={"tickers": top_D, "avg_res_corr": avgD,
L945               "sum_score": sumD, "objective": objD},
L946         top_G=top_G, top_D=top_D, init_G=top_G, init_D=top_D
L947     )
L948
L949     # --- Low Score Candidates (GSC+DSC bottom 10) : send before debug dump ---
L950     try:
L951         _low_df = (
L952             pd.DataFrame({"GSC": fb.g_score, "DSC": fb.d_score_all})
L953               .assign(G_plus_D=lambda x: x["GSC"] + x["DSC"])
L954               .sort_values("G_plus_D")
L955               .head(10)
L956               .round(3)
L957         )
L958         _slack("Low Score Candidates (GSC+DSC bottom 10)\n"
L959                "```"
L960                + _low_df.to_string(index=True, index_names=False)
L961                + "\n```")
L962     except Exception as _e:
L963         _slack(f"Low Score Candidates: 作成失敗: {_e}")
L964
L965     if debug_mode:
L966         prevG, prevD = _load_prev(G_PREV_JSON), _load_prev(D_PREV_JSON)
L967         try:
L968             _slack_debug(_compact_debug(fb, sb, prevG, prevD))
L969         except Exception as e:
L970             print(f"[debug skipped] {e}")
L971
L972     return sb
L973
L974 if __name__ == "__main__":
L975     run_pipeline()
```

## <scorer.py>
```text
L1 # scorer.py 
L2 # kawatest
L3 # =============================================================================
L4 # Scorer: ファクター/指標の生成と合成スコア算出を担う純粋層
L5 #
L6 # 【このファイルだけ読めば分かるポイント】
L7 # - 入力(InputBundle)は「価格/出来高/ベンチ/基本情報/EPS/FCF/リターン」を含むDTO
L8 # - 出力(FeatureBundle)は「raw特徴量 df」「標準化 df_z」「G/D スコア」「欠損ログ」
L9 # - 重み等のコンフィグ(PipelineConfig)は factor から渡す（cfg 必須）
L10 # - 旧カラム名は Scorer 内で自動リネームして受け入れ（後方互換）
L11 #   例) eps_ttm -> EPS_TTM, eps_q_recent -> EPS_Q_LastQ, fcf_ttm -> FCF_TTM
L12 #
L13 # 【I/O契約（Scorerが参照するInputBundleフィールド）】
L14 #   - cand: List[str]    … 候補銘柄（単体実行では未使用）
L15 #   - tickers: List[str] … 対象銘柄リスト
L16 #   - bench: str         … ベンチマークティッカー（例 '^GSPC'）
L17 #   - data: pd.DataFrame … yfinance download結果 ('Close','Volume' 等の階層列)
L18 #   - px: pd.DataFrame   … data['Close'] 相当（終値）
L19 #   - spx: pd.Series     … ベンチマークの終値
L20 #   - tickers_bulk: object         … yfinance.Tickers
L21 #   - info: Dict[str, dict]        … yfinance info per ticker
L22 #   - eps_df: pd.DataFrame         … 必須列: EPS_TTM, EPS_Q_LastQ（旧名も可）
L23 #   - fcf_df: pd.DataFrame         … 必須列: FCF_TTM（旧名も可）
L24 #   - returns: pd.DataFrame        … px[tickers].pct_change() 相当
L25 #
L26 # ※入出力の形式・例外文言は既存実装を変えません（安全な短縮のみ）
L27 # =============================================================================
L28
L29 import os
L30 import requests
L31 import numpy as np
L32 import pandas as pd
L33 import yfinance as yf
L34 from typing import Any, TYPE_CHECKING
L35 from scipy.stats import zscore
L36
L37 if TYPE_CHECKING:
L38     from factor import PipelineConfig  # type: ignore  # 実行時importなし（循環回避）
L39
L40 # ---- Dividend Helpers -------------------------------------------------------
L41 def _last_close(t, price_map=None):
L42     if price_map and (c := price_map.get(t)) is not None:
L43         return float(c)
L44     try:
L45         h = yf.Ticker(t).history(period="5d")["Close"].dropna()
L46         return float(h.iloc[-1]) if len(h) else np.nan
L47     except Exception:
L48         return np.nan
L49
L50 def _ttm_div_sum(t, lookback_days=400):
L51     try:
L52         div = yf.Ticker(t).dividends
L53         if div is None or len(div) == 0:
L54             return 0.0
L55         cutoff = pd.Timestamp.utcnow().tz_localize(None) - pd.Timedelta(days=lookback_days)
L56         ttm = float(div[div.index.tz_localize(None) >= cutoff].sum())
L57         return ttm if ttm > 0 else float(div.tail(4).sum())
L58     except Exception:
L59         return 0.0
L60
L61 def ttm_div_yield_portfolio(tickers, price_map=None):
L62     ys = []
L63     for t in tickers:
L64         c = _last_close(t, price_map)
L65         if not np.isfinite(c) or c <= 0:
L66             ys.append(0.0)
L67             continue
L68         s = _ttm_div_sum(t)
L69         ys.append(s / c if s > 0 else 0.0)
L70     return float(np.mean(ys)) if ys else 0.0
L71
L72 # ---- 簡易ユーティリティ（安全な短縮のみ） -----------------------------------
L73 def winsorize_s(s: pd.Series, p=0.02):
L74     if s is None or s.dropna().empty: return s
L75     lo, hi = np.nanpercentile(s.astype(float), [100*p, 100*(1-p)]); return s.clip(lo, hi)
L76
L77 def robust_z(s: pd.Series, p=0.02):
L78     s2 = winsorize_s(s, p); return np.nan_to_num(zscore(s2.fillna(s2.mean())))
L79
L80 def _safe_div(a, b):
L81     try:
L82         if b is None or float(b)==0 or pd.isna(b): return np.nan
L83         return float(a)/float(b)
L84     except Exception: return np.nan
L85
L86 def _safe_last(series: pd.Series, default=np.nan):
L87     try: return float(series.iloc[-1])
L88     except Exception: return default
L89
L90 D_WEIGHTS_EFF = None  # 出力表示互換のため
L91
L92 # ---- Scorer 本体 -------------------------------------------------------------
L93 class Scorer:
L94     """
L95     - factor.py からは `aggregate_scores(ib, cfg)` を呼ぶだけでOK。
L96     - cfg は必須（factor.PipelineConfig を渡す）。
L97     - 旧カラム名を自動リネームして新スキーマに吸収します。
L98     """
L99
L100     # === 先頭で旧→新カラム名マップ（移行用） ===
L101     EPS_RENAME = {"eps_ttm":"EPS_TTM", "eps_q_recent":"EPS_Q_LastQ"}
L102     FCF_RENAME = {"fcf_ttm":"FCF_TTM"}
L103
L104     # === スキーマ簡易チェック（最低限） ===
L105     @staticmethod
L106     def _validate_ib_for_scorer(ib: Any):
L107         must_attrs = ["tickers","bench","data","px","spx","tickers_bulk","info","eps_df","fcf_df","returns"]
L108         miss = [a for a in must_attrs if not hasattr(ib, a) or getattr(ib, a) is None]
L109         if miss: raise ValueError(f"InputBundle is missing required attributes for Scorer: {miss}")
L110
L111         # 後方互換のため、まず rename を試みる
L112         if any(c in ib.eps_df.columns for c in Scorer.EPS_RENAME.keys()):
L113             ib.eps_df.rename(columns=Scorer.EPS_RENAME, inplace=True)
L114         if any(c in ib.fcf_df.columns for c in Scorer.FCF_RENAME.keys()):
L115             ib.fcf_df.rename(columns=Scorer.FCF_RENAME, inplace=True)
L116
L117         # 必須列の存在確認
L118         need_eps = {"EPS_TTM","EPS_Q_LastQ"}
L119         need_fcf = {"FCF_TTM"}
L120         if not need_eps.issubset(set(ib.eps_df.columns)):
L121             raise ValueError(f"eps_df must contain columns {need_eps} (accepts old names via auto-rename). Got: {list(ib.eps_df.columns)}")
L122         if not need_fcf.issubset(set(ib.fcf_df.columns)):
L123             raise ValueError(f"fcf_df must contain columns {need_fcf} (accepts old names via auto-rename). Got: {list(ib.fcf_df.columns)}")
L124
L125     # ----（Scorer専用）テクニカル・指標系 ----
L126     @staticmethod
L127     def trend(s: pd.Series):
L128         if len(s)<200: return np.nan
L129         sma50, sma150, sma200 = s.rolling(50).mean().iloc[-1], s.rolling(150).mean().iloc[-1], s.rolling(200).mean().iloc[-1]
L130         prev200, p = s.rolling(200).mean().iloc[-21], s.iloc[-1]
L131         lo_52 = s[-252:].min() if len(s)>=252 else s.min(); hi_52 = s[-252:].max() if len(s)>=252 else s.max()
L132         rng = (hi_52 - lo_52) if hi_52>lo_52 else np.nan
L133         clip = lambda x,lo,hi: (np.nan if pd.isna(x) else max(lo,min(hi,x)))
L134         a = clip(p/(s.rolling(50).mean().iloc[-1]) - 1, -0.5, 0.5)
L135         b = clip(sma50/sma150 - 1, -0.5, 0.5)
L136         c = clip(sma150/sma200 - 1, -0.5, 0.5)
L137         d = clip(sma200/prev200 - 1, -0.2, 0.2)
L138         e = clip((p - lo_52) / (rng if rng and rng>0 else np.nan) - 0.5, -0.5, 0.5)
L139         parts = [0.0 if pd.isna(x) else x for x in (a,b,c,d,e)]
L140         return 0.30*parts[0] + 0.20*parts[1] + 0.15*parts[2] + 0.15*parts[3] + 0.20*parts[4]
L141
L142     @staticmethod
L143     def rs(s, b):
L144         n, nb = len(s), len(b)
L145         if n<60 or nb<60: return np.nan
L146         L12 = 252 if n>=252 and nb>=252 else min(n,nb)-1; L1 = 22 if n>=22 and nb>=22 else max(5, min(n,nb)//3)
L147         r12, r1, br12, br1 = s.iloc[-1]/s.iloc[-L12]-1, s.iloc[-1]/s.iloc[-L1]-1, b.iloc[-1]/b.iloc[-L12]-1, b.iloc[-1]/b.iloc[-L1]-1
L148         return (r12 - br12)*0.7 + (r1 - br1)*0.3
L149
L150     @staticmethod
L151     def tr_str(s):
L152         if len(s)<50: return np.nan
L153         return s.iloc[-1]/s.rolling(50).mean().iloc[-1] - 1
L154
L155     @staticmethod
L156     def rs_line_slope(s: pd.Series, b: pd.Series, win: int) -> float:
L157         r = (s/b).dropna()
L158         if len(r)<win: return np.nan
L159         y, x = np.log(r.iloc[-win:]), np.arange(win, dtype=float)
L160         try: return float(np.polyfit(x, y, 1)[0])
L161         except Exception: return np.nan
L162
L163     @staticmethod
L164     def ev_fallback(info_t: dict, tk: yf.Ticker) -> float:
L165         ev = info_t.get('enterpriseValue', np.nan)
L166         if pd.notna(ev) and ev>0: return float(ev)
L167         mc, debt, cash = info_t.get('marketCap', np.nan), np.nan, np.nan
L168         try:
L169             bs = tk.quarterly_balance_sheet
L170             if bs is not None and not bs.empty:
L171                 c = bs.columns[0]
L172                 for k in ("Total Debt","Long Term Debt","Short Long Term Debt"):
L173                     if k in bs.index: debt = float(bs.loc[k,c]); break
L174                 for k in ("Cash And Cash Equivalents","Cash And Cash Equivalents And Short Term Investments","Cash"):
L175                     if k in bs.index: cash = float(bs.loc[k,c]); break
L176         except Exception: pass
L177         if pd.notna(mc): return float(mc + (0 if pd.isna(debt) else debt) - (0 if pd.isna(cash) else cash))
L178         return np.nan
L179
L180     @staticmethod
L181     def dividend_status(ticker: str) -> str:
L182         t = yf.Ticker(ticker)
L183         try:
L184             if not t.dividends.empty: return "has"
L185         except Exception: return "unknown"
L186         try:
L187             a = t.actions
L188             if a is not None and not a.empty and "Stock Splits" in a.columns and a["Stock Splits"].abs().sum()>0: return "none_confident"
L189         except Exception: pass
L190         try:
L191             fi = t.fast_info
L192             if any(getattr(fi,k,None) for k in ("last_dividend_date","dividend_rate","dividend_yield")): return "maybe_missing"
L193         except Exception: pass
L194         return "unknown"
L195
L196     @staticmethod
L197     def div_streak(t):
L198         try:
L199             divs = yf.Ticker(t).dividends.dropna(); ann = divs.groupby(divs.index.year).sum(); ann = ann[ann.index<pd.Timestamp.today().year]
L200             years, streak = sorted(ann.index), 0
L201             for i in range(len(years)-1,0,-1):
L202                 if ann[years[i]] > ann[years[i-1]]: streak += 1
L203                 else: break
L204             return streak
L205         except Exception: return 0
L206
L207     @staticmethod
L208     def fetch_finnhub_metrics(symbol):
L209         api_key = os.environ.get("FINNHUB_API_KEY")
L210         if not api_key: return {}
L211         url, params = "https://finnhub.io/api/v1/stock/metric", {"symbol":symbol,"metric":"all","token":api_key}
L212         try:
L213             r = requests.get(url, params=params, timeout=10); r.raise_for_status(); m = r.json().get("metric",{})
L214             return {'EPS':m.get('epsGrowthTTMYoy'),'REV':m.get('revenueGrowthTTMYoy'),'ROE':m.get('roeTTM'),'BETA':m.get('beta'),'DIV':m.get('dividendYieldIndicatedAnnual'),'FCF':(m.get('freeCashFlowTTM')/m.get('enterpriseValue')) if m.get('freeCashFlowTTM') and m.get('enterpriseValue') else None}
L215         except Exception: return {}
L216
L217     @staticmethod
L218     def calc_beta(series: pd.Series, market: pd.Series, lookback=252):
L219         r, m = series.pct_change().dropna(), market.pct_change().dropna()
L220         n = min(len(r), len(m), lookback)
L221         if n<60: return np.nan
L222         r, m = r.iloc[-n:], m.iloc[-n:]; cov, var = np.cov(r, m)[0,1], np.var(m)
L223         return np.nan if var==0 else cov/var
L224
L225     @staticmethod
L226     def spx_to_alpha(spx: pd.Series, bands=(0.03,0.10), w=(0.6,0.4),
L227                      span=5, q=(0.20,0.40), alphas=(0.05,0.08,0.10)) -> float:
L228         """
L229         S&P500指数のみから擬似breadthを作り、履歴分位でαを段階決定。
L230         bands=(±3%, ±10%), w=(50DMA,200DMA), 分位q=(20%,40%), alphas=(低,中,高)
L231         """
L232         ma50, ma200 = spx.rolling(50).mean(), spx.rolling(200).mean()
L233         b50  = ((spx/ma50 - 1) + bands[0])/(2*bands[0])
L234         b200 = ((spx/ma200 - 1) + bands[1])/(2*bands[1])
L235         hist = (w[0]*b50 + w[1]*b200).clip(0,1).ewm(span=span).mean()
L236         b = float(hist.iloc[-1])
L237         lo, mid = float(hist.quantile(q[0])), float(hist.quantile(q[1]))
L238         return alphas[0] if b < lo else alphas[1] if b < mid else alphas[2]
L239
L240     @staticmethod
L241     def soft_cap_effective_scores(scores: pd.Series|dict, sectors: dict, cap=2, alpha=0.08) -> pd.Series:
L242         """
L243         同一セクターcap超過（3本目以降）に α×段階減点を課した“有効スコア”Seriesを返す。
L244         戻り値は降順ソート済み。
L245         """
L246         s = pd.Series(scores, dtype=float); order = s.sort_values(ascending=False).index
L247         cnt, pen = {}, {}
L248         for t in order:
L249             sec = sectors.get(t, "U")
L250             k = cnt.get(sec, 0) + 1
L251             pen[t] = alpha * max(0, k - cap)
L252             cnt[sec] = k
L253         return (s - pd.Series(pen)).sort_values(ascending=False)
L254
L255     @staticmethod
L256     def pick_top_softcap(scores: pd.Series|dict, sectors: dict, N: int, cap=2, alpha=0.08, hard: int|None=5) -> list[str]:
L257         """
L258         soft-cap適用後の上位Nティッカーを返す。hard>0なら非常用ハード上限で同一セクター超過を間引く（既定=5）。
L259         """
L260         eff = Scorer.soft_cap_effective_scores(scores, sectors, cap, alpha)
L261         if not hard:
L262             return list(eff.head(N).index)
L263         pick, used = [], {}
L264         for t in eff.index:
L265             s = sectors.get(t, "U")
L266             if used.get(s, 0) < hard:
L267                 pick.append(t)
L268                 used[s] = used.get(s, 0) + 1
L269             if len(pick) == N:
L270                 break
L271         return pick
L272
L273     # ---- スコア集計（DTO/Configを受け取り、FeatureBundleを返す） ----
L274     def aggregate_scores(self, ib: Any, cfg):
L275         if cfg is None:
L276             raise ValueError("cfg is required; pass factor.PipelineConfig")
L277         self._validate_ib_for_scorer(ib)
L278
L279         px, spx, tickers = ib.px, ib.spx, ib.tickers
L280         tickers_bulk, info, eps_df, fcf_df = ib.tickers_bulk, ib.info, ib.eps_df, ib.fcf_df
L281         families = set(getattr(cfg.weights,'g',{})) | set(getattr(cfg.weights,'d',{}))
L282
L283         df, missing_logs = pd.DataFrame(index=tickers), []
L284         for t in tickers:
L285             d, s = info[t], px[t]; ev = self.ev_fallback(d, tickers_bulk.tickers[t])
L286             # --- 基本特徴 ---
L287             df.loc[t,'TR']   = self.trend(s)
L288             df.loc[t,'EPS']  = eps_df.loc[t,'EPS_TTM'] if t in eps_df.index else np.nan
L289             df.loc[t,'REV']  = d.get('revenueGrowth',np.nan)
L290             df.loc[t,'ROE']  = d.get('returnOnEquity',np.nan)
L291             df.loc[t,'BETA'] = self.calc_beta(s, spx, lookback=252)
L292
L293             # --- 配当（欠損補完含む） ---
L294             div = d.get('dividendYield') if d.get('dividendYield') is not None else d.get('trailingAnnualDividendYield')
L295             if div is None or pd.isna(div):
L296                 try:
L297                     divs = yf.Ticker(t).dividends
L298                     if divs is not None and not divs.empty:
L299                         last_close = s.iloc[-1]; div_1y = divs[divs.index >= (divs.index.max() - pd.Timedelta(days=365))].sum()
L300                         if last_close and last_close>0: div = float(div_1y/last_close)
L301                 except Exception: pass
L302             df.loc[t,'DIV'] = 0.0 if (div is None or pd.isna(div)) else float(div)
L303
L304             # --- FCF/EV ---
L305             fcf_val = fcf_df.loc[t,'FCF_TTM'] if t in fcf_df.index else np.nan
L306             df.loc[t,'FCF'] = (fcf_val/ev) if (pd.notna(fcf_val) and pd.notna(ev) and ev>0) else np.nan
L307
L308             # --- モメンタム・ボラ関連 ---
L309             df.loc[t,'RS'], df.loc[t,'TR_str'] = self.rs(s, spx), self.tr_str(s)
L310             r, rm = s.pct_change().dropna(), spx.pct_change().dropna()
L311             n = int(min(len(r), len(rm)))
L312
L313             DOWNSIDE_DEV = np.nan
L314             if n>=60:
L315                 r6 = r.iloc[-min(len(r),126):]; neg = r6[r6<0]
L316                 if len(neg)>=10: DOWNSIDE_DEV = float(neg.std(ddof=0)*np.sqrt(252))
L317             df.loc[t,'DOWNSIDE_DEV'] = DOWNSIDE_DEV
L318
L319             MDD_1Y = np.nan
L320             try:
L321                 w = s.iloc[-min(len(s),252):].dropna()
L322                 if len(w)>=30:
L323                     roll_max = w.cummax(); MDD_1Y = float((w/roll_max - 1.0).min())
L324             except Exception: pass
L325             df.loc[t,'MDD_1Y'] = MDD_1Y
L326
L327             RESID_VOL = np.nan
L328             if n>=120:
L329                 rr, rrm = r.iloc[-n:].align(rm.iloc[-n:], join='inner')
L330                 if len(rr)==len(rrm) and len(rr)>=120 and rrm.var()>0:
L331                     beta = float(np.cov(rr, rrm)[0,1]/np.var(rrm)); resid = rr - beta*rrm
L332                     RESID_VOL = float(resid.std(ddof=0)*np.sqrt(252))
L333             df.loc[t,'RESID_VOL'] = RESID_VOL
L334
L335             DOWN_OUTPERF = np.nan
L336             if n>=60:
L337                 m, x = rm.iloc[-n:], r.iloc[-n:]; mask = m<0
L338                 if mask.sum()>=10:
L339                     mr, sr = float(m[mask].mean()), float(x[mask].mean())
L340                     DOWN_OUTPERF = (sr - mr)/abs(mr) if mr!=0 else np.nan
L341             df.loc[t,'DOWN_OUTPERF'] = DOWN_OUTPERF
L342
L343             # --- 長期移動平均/位置 ---
L344             sma200 = s.rolling(200).mean(); df.loc[t,'EXT_200'] = np.nan
L345             if pd.notna(sma200.iloc[-1]) and sma200.iloc[-1]!=0: df.loc[t,'EXT_200'] = abs(float(s.iloc[-1]/sma200.iloc[-1]-1.0))
L346
L347             # --- 配当の詳細系 ---
L348             DIV_TTM_PS=DIV_VAR5=DIV_YOY=DIV_FCF_COVER=np.nan
L349             try:
L350                 divs = yf.Ticker(t).dividends.dropna()
L351                 if not divs.empty:
L352                     last_close = s.iloc[-1]; div_1y = float(divs[divs.index >= (divs.index.max()-pd.Timedelta(days=365))].sum())
L353                     DIV_TTM_PS = div_1y if div_1y>0 else np.nan
L354                     ann = divs.groupby(divs.index.year).sum()
L355                     if len(ann)>=2 and ann.iloc[-2]!=0: DIV_YOY = float(ann.iloc[-1]/ann.iloc[-2]-1.0)
L356                     tail = ann.iloc[-5:] if len(ann)>=5 else ann
L357                     if len(tail)>=3 and tail.mean()!=0: DIV_VAR5 = float(tail.std(ddof=1)/abs(tail.mean()))
L358                 so = d.get('sharesOutstanding',None)
L359                 if so and pd.notna(DIV_TTM_PS) and pd.notna(fcf_val) and fcf_val!=0:
L360                     DIV_FCF_COVER = float((fcf_val)/(DIV_TTM_PS*float(so)))
L361             except Exception: pass
L362             df.loc[t,'DIV_TTM_PS'], df.loc[t,'DIV_VAR5'], df.loc[t,'DIV_YOY'], df.loc[t,'DIV_FCF_COVER'] = DIV_TTM_PS, DIV_VAR5, DIV_YOY, DIV_FCF_COVER
L363
L364             # --- 財務安定性 ---
L365             df.loc[t,'DEBT2EQ'], df.loc[t,'CURR_RATIO'] = d.get('debtToEquity',np.nan), d.get('currentRatio',np.nan)
L366
L367             # --- EPS 変動 ---
L368             EPS_VAR_8Q = np.nan
L369             try:
L370                 qe, so = tickers_bulk.tickers[t].quarterly_earnings, d.get('sharesOutstanding',None)
L371                 if qe is not None and not qe.empty and so:
L372                     eps_q = (qe['Earnings'].dropna().astype(float)/float(so)).replace([np.inf,-np.inf],np.nan)
L373                     if len(eps_q)>=4: EPS_VAR_8Q = float(eps_q.iloc[-min(8,len(eps_q)):].std(ddof=1))
L374             except Exception: pass
L375             df.loc[t,'EPS_VAR_8Q'] = EPS_VAR_8Q
L376
L377             # --- サイズ/流動性 ---
L378             df.loc[t,'MARKET_CAP'] = d.get('marketCap',np.nan); adv60 = np.nan
L379             try:
L380                 vol_series = ib.data['Volume'][t].dropna()
L381                 if len(vol_series)>=5 and len(s)==len(vol_series):
L382                     dv = (vol_series*s).rolling(60).mean(); adv60 = float(dv.iloc[-1])
L383             except Exception: pass
L384             df.loc[t,'ADV60_USD'] = adv60
L385
L386             # --- 売上/利益の加速度等 ---
L387             REV_Q_YOY=EPS_Q_YOY=REV_YOY_ACC=REV_YOY_VAR=np.nan
L388             REV_ANNUAL_STREAK = np.nan
L389             try:
L390                 qe, so = tickers_bulk.tickers[t].quarterly_earnings, d.get('sharesOutstanding',None)
L391                 if qe is not None and not qe.empty:
L392                     if 'Revenue' in qe.columns:
L393                         rev = qe['Revenue'].dropna().astype(float)
L394                         if len(rev)>=5: REV_Q_YOY = _safe_div(rev.iloc[-1]-rev.iloc[-5], rev.iloc[-5])
L395                         if len(rev)>=6:
L396                             yoy_now = _safe_div(rev.iloc[-1]-rev.iloc[-5], rev.iloc[-5]); yoy_prev = _safe_div(rev.iloc[-2]-rev.iloc[-6], rev.iloc[-6])
L397                             if pd.notna(yoy_now) and pd.notna(yoy_prev): REV_YOY_ACC = yoy_now - yoy_prev
L398                         yoy_list=[]
L399                         for k in range(1,5):
L400                             if len(rev)>=4+k:
L401                                 y = _safe_div(rev.iloc[-k]-rev.iloc[-(k+4)], rev.iloc[-(k+4)])
L402                                 if pd.notna(y): yoy_list.append(y)
L403                         if len(yoy_list)>=2: REV_YOY_VAR = float(np.std(yoy_list, ddof=1))
L404                         # NEW: 年次の持続性（直近から遡って前年比プラスが何年連続か、四半期4本揃う完全年のみ）
L405                         try:
L406                             g = rev.groupby(rev.index.year)
L407                             ann_sum, cnt = g.sum(), g.count()
L408                             ann_sum = ann_sum[cnt >= 4]
L409                             if len(ann_sum) >= 3:
L410                                 yoy = ann_sum.pct_change().dropna()
L411                                 streak = 0
L412                                 for v in yoy.iloc[::-1]:
L413                                     if pd.isna(v) or v <= 0:
L414                                         break
L415                                     streak += 1
L416                                 REV_ANNUAL_STREAK = float(streak)
L417                         except Exception:
L418                             pass
L419                     if 'Earnings' in qe.columns and so:
L420                         eps_series = (qe['Earnings'].dropna().astype(float)/float(so)).replace([np.inf,-np.inf],np.nan)
L421                         if len(eps_series)>=5 and pd.notna(eps_series.iloc[-5]) and eps_series.iloc[-5]!=0:
L422                             EPS_Q_YOY = _safe_div(eps_series.iloc[-1]-eps_series.iloc[-5], eps_series.iloc[-5])
L423             except Exception: pass
L424             df.loc[t,'REV_Q_YOY'], df.loc[t,'EPS_Q_YOY'], df.loc[t,'REV_YOY_ACC'], df.loc[t,'REV_YOY_VAR'] = REV_Q_YOY, EPS_Q_YOY, REV_YOY_ACC, REV_YOY_VAR
L425             df.loc[t,'REV_ANN_STREAK'] = REV_ANNUAL_STREAK
L426
L427             # --- Rule of 40 や周辺 ---
L428             total_rev_ttm = d.get('totalRevenue',np.nan)
L429             FCF_MGN = _safe_div(fcf_val, total_rev_ttm)
L430             df.loc[t,'FCF_MGN'] = FCF_MGN
L431             rule40 = np.nan
L432             try:
L433                 r = df.loc[t,'REV']; rule40 = (r if pd.notna(r) else np.nan) + (FCF_MGN if pd.notna(FCF_MGN) else np.nan)
L434             except Exception: pass
L435             df.loc[t,'RULE40'] = rule40
L436
L437             # --- トレンド補助 ---
L438             sma50  = s.rolling(50).mean()
L439             sma150 = s.rolling(150).mean()
L440             sma200 = s.rolling(200).mean()
L441             p = _safe_last(s)
L442
L443             df.loc[t,'MA50_OVER_150'] = (
L444                 _safe_last(sma50)/_safe_last(sma150) - 1
L445                 if pd.notna(_safe_last(sma50)) and pd.notna(_safe_last(sma150)) and _safe_last(sma150)!=0 else np.nan
L446             )
L447             df.loc[t,'MA150_OVER_200'] = (
L448                 _safe_last(sma150)/_safe_last(sma200) - 1
L449                 if pd.notna(_safe_last(sma150)) and pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L450             )
L451
L452             lo52 = s[-252:].min() if len(s)>=252 else s.min()
L453             df.loc[t,'P_OVER_LOW52'] = (p/lo52 - 1) if (lo52 and lo52>0 and pd.notna(p)) else np.nan
L454
L455             df.loc[t,'MA200_SLOPE_1M'] = np.nan
L456             if len(sma200.dropna()) >= 21:
L457                 cur200 = _safe_last(sma200)
L458                 old2001 = float(sma200.iloc[-21])
L459                 if old2001:
L460                     df.loc[t,'MA200_SLOPE_1M'] = cur200/old2001 - 1
L461
L462             df.loc[t,'P_OVER_150'] = p/_safe_last(sma150)-1 if pd.notna(_safe_last(sma150)) and _safe_last(sma150)!=0 else np.nan
L463             df.loc[t,'P_OVER_200'] = p/_safe_last(sma200)-1 if pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L464             df.loc[t,'MA50_OVER_200'] = _safe_last(sma50)/_safe_last(sma200)-1 if pd.notna(_safe_last(sma50)) and pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L465             df.loc[t,'MA200_SLOPE_5M'] = np.nan
L466             if len(sma200.dropna())>=105:
L467                 cur200, old200 = _safe_last(sma200), float(sma200.iloc[-105])
L468                 if old200 and old200!=0: df.loc[t,'MA200_SLOPE_5M'] = cur200/old200 - 1
L469             # NEW: 200日線が連続で上向きの「日数」
L470             df.loc[t,'MA200_UP_STREAK_D'] = np.nan
L471             try:
L472                 s200 = sma200.dropna()
L473                 if len(s200) >= 2:
L474                     diff200 = s200.diff()
L475                     up = 0
L476                     for v in diff200.iloc[::-1]:
L477                         if pd.isna(v) or v <= 0:
L478                             break
L479                         up += 1
L480                     df.loc[t,'MA200_UP_STREAK_D'] = float(up)
L481             except Exception:
L482                 pass
L483             df.loc[t,'LOW52PCT25_EXCESS'] = np.nan if (lo52 is None or lo52<=0 or pd.isna(p)) else (p/(lo52*1.25)-1)
L484             hi52 = s[-252:].max() if len(s)>=252 else s.max(); df.loc[t,'NEAR_52W_HIGH'] = np.nan
L485             if hi52 and hi52>0 and pd.notna(p):
L486                 d_hi = (p/hi52)-1.0; df.loc[t,'NEAR_52W_HIGH'] = -abs(min(0.0, d_hi))
L487             df.loc[t,'RS_SLOPE_6W'] = self.rs_line_slope(s, ib.spx, 30)
L488             df.loc[t,'RS_SLOPE_13W'] = self.rs_line_slope(s, ib.spx, 65)
L489
L490             df.loc[t,'DIV_STREAK'] = self.div_streak(t)
L491
L492             # --- 欠損メモ ---
L493             fin_cols = ['REV','ROE','BETA','DIV','FCF']
L494             need_finnhub = [col for col in fin_cols if pd.isna(df.loc[t,col])]
L495             if need_finnhub:
L496                 fin_data = self.fetch_finnhub_metrics(t)
L497                 for col in need_finnhub:
L498                     val = fin_data.get(col)
L499                     if val is not None and not pd.isna(val): df.loc[t,col] = val
L500             for col in fin_cols + ['EPS','RS','TR_str','DIV_STREAK']:
L501                 if pd.isna(df.loc[t,col]):
L502                     if col=='DIV':
L503                         status = self.dividend_status(t)
L504                         if status!='none_confident': missing_logs.append({'Ticker':t,'Column':col,'Status':status})
L505                     else:
L506                         missing_logs.append({'Ticker':t,'Column':col})
L507
L508         def _trend_template_pass(row, rs_alpha_thresh=0.10):
L509             c1 = (row.get('P_OVER_150', np.nan) > 0) and (row.get('P_OVER_200', np.nan) > 0)
L510             c2 = (row.get('MA150_OVER_200', np.nan) > 0)
L511             c3 = (row.get('MA200_SLOPE_1M', np.nan) > 0)
L512             c4 = (row.get('MA50_OVER_150', np.nan) > 0) and (row.get('MA50_OVER_200', np.nan) > 0)
L513             c5 = (row.get('TR_str', np.nan) > 0)
L514             c6 = (row.get('P_OVER_LOW52', np.nan) >= 0.30)
L515             c7 = (row.get('NEAR_52W_HIGH', np.nan) >= -0.25)
L516             c8 = (row.get('RS', np.nan) >= 0.10)
L517             return bool(c1 and c2 and c3 and c4 and c5 and c6 and c7 and c8)
L518
L519         if 'trend_template' not in df.columns: df['trend_template'] = df.apply(_trend_template_pass, axis=1).fillna(False)
L520         assert 'trend_template' in df.columns
L521
L522         # === Z化と合成 ===
L523         for col in ['ROE','FCF','REV','EPS']: df[f'{col}_W'] = winsorize_s(df[col], 0.02)
L524
L525         df_z = pd.DataFrame(index=df.index)
L526         for col in ['EPS','REV','ROE','FCF','RS','TR_str','BETA','DIV','DIV_STREAK']: df_z[col] = robust_z(df[col])
L527         df_z['REV'], df_z['EPS'], df_z['TR'] = robust_z(df['REV_W']), robust_z(df['EPS_W']), robust_z(df['TR'])
L528         for col in ['P_OVER_150','P_OVER_200','MA50_OVER_200','MA200_SLOPE_5M','LOW52PCT25_EXCESS','NEAR_52W_HIGH','RS_SLOPE_6W','RS_SLOPE_13W','MA200_UP_STREAK_D']: df_z[col] = robust_z(df[col])
L529         for col in ['REV_Q_YOY','EPS_Q_YOY','REV_YOY_ACC','REV_YOY_VAR','FCF_MGN','RULE40','REV_ANN_STREAK']: df_z[col] = robust_z(df[col])
L530         for col in ['DOWNSIDE_DEV','MDD_1Y','RESID_VOL','DOWN_OUTPERF','EXT_200','DIV_TTM_PS','DIV_VAR5','DIV_YOY','DIV_FCF_COVER','DEBT2EQ','CURR_RATIO','EPS_VAR_8Q','MARKET_CAP','ADV60_USD']: df_z[col] = robust_z(df[col])
L531
L532         df_z['SIZE'], df_z['LIQ'] = robust_z(np.log1p(df['MARKET_CAP'])), robust_z(np.log1p(df['ADV60_USD']))
L533         df_z['QUALITY_F'] = robust_z(0.6*df['FCF_W'] + 0.4*df['ROE_W']).clip(-3.0,3.0)
L534         df_z['YIELD_F']   = 0.3*df_z['DIV'] + 0.7*df_z['DIV_STREAK']
L535         df_z['GROWTH_F']  = robust_z(
L536               0.30*df_z['REV']
L537             + 0.20*df_z['EPS_Q_YOY']
L538             + 0.15*df_z['REV_Q_YOY']
L539             + 0.15*df_z['REV_YOY_ACC']
L540             + 0.10*df_z['RULE40']
L541             + 0.10*df_z['FCF_MGN']
L542             + 0.10*df_z['REV_ANN_STREAK']
L543             - 0.05*df_z['REV_YOY_VAR']
L544         ).clip(-3.0,3.0)
L545         df_z['MOM_F'] = robust_z(
L546               0.40*df_z['RS']
L547             + 0.15*df_z['TR_str']
L548             + 0.15*df_z['RS_SLOPE_6W']
L549             + 0.15*df_z['RS_SLOPE_13W']
L550             + 0.10*df_z['MA200_SLOPE_5M']
L551             + 0.10*df_z['MA200_UP_STREAK_D']
L552         ).clip(-3.0,3.0)
L553         df_z['VOL'] = robust_z(df['BETA'])
L554         df_z.rename(columns={'GROWTH_F':'GRW','MOM_F':'MOM','QUALITY_F':'QAL','YIELD_F':'YLD'}, inplace=True)
L555         df_z['TRD'] = 0.0  # TRDはスコア寄与から外し、テンプレ判定はフィルタで行う（列は表示互換のため残す）
L556         if 'BETA' not in df_z.columns: df_z['BETA'] = robust_z(df['BETA'])
L557
L558         df_z['D_VOL_RAW'] = robust_z(0.40*df_z['DOWNSIDE_DEV'] + 0.22*df_z['RESID_VOL'] + 0.18*df_z['MDD_1Y'] - 0.10*df_z['DOWN_OUTPERF'] - 0.05*df_z['EXT_200'] - 0.08*df_z['SIZE'] - 0.10*df_z['LIQ'] + 0.10*df_z['BETA'])
L559         df_z['D_QAL']     = robust_z(0.35*df_z['QAL'] + 0.20*df_z['FCF'] + 0.15*df_z['CURR_RATIO'] - 0.15*df_z['DEBT2EQ'] - 0.15*df_z['EPS_VAR_8Q'])
L560         df_z['D_YLD']     = robust_z(0.45*df_z['DIV'] + 0.25*df_z['DIV_STREAK'] + 0.20*df_z['DIV_FCF_COVER'] - 0.10*df_z['DIV_VAR5'])
L561         df_z['D_TRD']     = robust_z(0.40*df_z.get('MA200_SLOPE_5M',0) - 0.30*df_z.get('EXT_200',0) + 0.15*df_z.get('NEAR_52W_HIGH',0) + 0.15*df_z['TR'])
L562
L563         # --- 重みは cfg を優先（外部があればそれを使用） ---
L564         # ① 全銘柄で G/D スコアを算出（unmasked）
L565         g_score_all = df_z.mul(pd.Series(cfg.weights.g)).sum(axis=1)
L566
L567         d_comp = pd.concat({
L568             'QAL': df_z['D_QAL'],
L569             'YLD': df_z['D_YLD'],
L570             'VOL': df_z['D_VOL_RAW'],
L571             'TRD': df_z['D_TRD']
L572         }, axis=1)
L573         dw = pd.Series(cfg.weights.d, dtype=float).reindex(['QAL','YLD','VOL','TRD']).fillna(0.0)
L574         globals()['D_WEIGHTS_EFF'] = dw.copy()
L575         d_score_all = d_comp.mul(dw, axis=1).sum(axis=1)
L576
L577         # ② テンプレ判定（既存ロジックそのまま）
L578         mask = df['trend_template']
L579         if not bool(mask.any()):
L580             mask = (
L581                 (df.get('P_OVER_LOW52', np.nan) >= 0.25) &
L582                 (df.get('NEAR_52W_HIGH', np.nan) >= -0.30) &
L583                 (df.get('RS', np.nan) >= 0.08) &
L584                 (df.get('MA200_SLOPE_1M', np.nan) > 0) &
L585                 (df.get('P_OVER_150', np.nan) > 0) & (df.get('P_OVER_200', np.nan) > 0) &
L586                 (df.get('MA150_OVER_200', np.nan) > 0) &
L587                 (df.get('MA50_OVER_150', np.nan) > 0) & (df.get('MA50_OVER_200', np.nan) > 0) &
L588                 (df.get('TR_str', np.nan) > 0)
L589             ).fillna(False)
L590             df['trend_template'] = mask
L591
L592         # ③ 採用用は mask、表示/分析用は列で全銘柄保存
L593         g_score = g_score_all.loc[mask]
L594         Scorer.g_score = g_score
L595         df_z['GSC'] = g_score_all
L596         df_z['DSC'] = d_score_all
L597
L598         try:
L599             df = _apply_growth_entry_flags(df, ib, self, win_breakout=5, win_pullback=5)
L600         except Exception:
L601             pass
L602
L603         from factor import FeatureBundle  # type: ignore  # 実行時importなし（循環回避）
L604         return FeatureBundle(
L605             df=df,
L606             df_z=df_z,
L607             g_score=g_score,
L608             d_score_all=d_score_all,
L609             missing_logs=pd.DataFrame(missing_logs)
L610         )
L611
L612
L613 def _apply_growth_entry_flags(feature_df, bundle, self_obj, win_breakout=5, win_pullback=5):
L614     """
L615     G枠ユニバースに対し、ブレイクアウト確定/押し目反発の「直近N営業日内の発火」を判定し、
L616     次の列を feature_df に追加する（index=ticker）。
L617       - G_BREAKOUT_recent_5d : bool
L618       - G_BREAKOUT_last_date : str "YYYY-MM-DD"
L619       - G_PULLBACK_recent_5d : bool
L620       - G_PULLBACK_last_date : str "YYYY-MM-DD"
L621       - G_PIVOT_price        : float
L622     失敗しても例外は握り潰し、既存処理を阻害しない。
L623     """
L624     try:
L625         px   = bundle.px                      # 終値 DataFrame
L626         hi   = bundle.data['High']
L627         lo   = bundle.data['Low']
L628         vol  = bundle.data['Volume']
L629         bench= bundle.spx                     # ベンチマーク Series
L630
L631         # Gユニバース推定：self.g_universe 優先 → feature_df['group']=='G' → 全銘柄
L632         g_universe = getattr(self_obj, "g_universe", None)
L633         if g_universe is None:
L634             try:
L635                 g_universe = feature_df.index[feature_df['group'].astype(str).str.upper().eq('G')].tolist()
L636             except Exception:
L637                 g_universe = list(feature_df.index)
L638         if not g_universe:
L639             return feature_df
L640
L641         # 指標
L642         ema21 = px[g_universe].ewm(span=21, adjust=False).mean()
L643         ma50  = px[g_universe].rolling(50).mean()
L644         ma150 = px[g_universe].rolling(150).mean()
L645         ma200 = px[g_universe].rolling(200).mean()
L646         atr20 = (hi[g_universe] - lo[g_universe]).rolling(20).mean()
L647         vol20 = vol[g_universe].rolling(20).mean()
L648         vol50 = vol[g_universe].rolling(50).mean()
L649
L650         # トレンドテンプレート合格
L651         trend_template_ok = (px[g_universe] > ma50) & (px[g_universe] > ma150) & (px[g_universe] > ma200) \
L652                             & (ma150 > ma200) & (ma200.diff() > 0)
L653
L654         # 汎用ピボット：直近65営業日の高値（当日除外）
L655         pivot_price = hi[g_universe].rolling(65).max().shift(1)
L656
L657         # 相対力：年内高値更新
L658         bench_aligned = bench.reindex(px.index).ffill()
L659         rs = px[g_universe].div(bench_aligned, axis=0)
L660         rs_high = rs.rolling(252).max().shift(1)
L661
L662         # ブレイクアウト「発生日」：条件立ち上がり
L663         breakout_today = trend_template_ok & (px[g_universe] > pivot_price) \
L664                          & (vol[g_universe] >= 1.5 * vol50) & (rs > rs_high)
L665         breakout_event = breakout_today & ~breakout_today.shift(1).fillna(False)
L666
L667         # 押し目反発「発生日」：EMA21帯×出来高ドライアップ×前日高値越え×終値EMA21上
L668         near_ema21_band = px[g_universe].between(ema21 - atr20, ema21 + atr20)
L669         volume_dryup = (vol20 / vol50) <= 1.0
L670         pullback_bounce_confirmed = (px[g_universe] > hi[g_universe].shift(1)) & (px[g_universe] > ema21)
L671         pullback_today = trend_template_ok & near_ema21_band & volume_dryup & pullback_bounce_confirmed
L672         pullback_event = pullback_today & ~pullback_today.shift(1).fillna(False)
L673
L674         # 直近N営業日内の発火 / 最終発生日
L675         rows = []
L676         for t in g_universe:
L677             def _recent_and_date(s, win):
L678                 sw = s[t].iloc[-win:]
L679                 if sw.any():
L680                     d = sw[sw].index[-1]
L681                     return True, d.strftime("%Y-%m-%d")
L682                 return False, ""
L683             br_recent, br_date = _recent_and_date(breakout_event, win_breakout)
L684             pb_recent, pb_date = _recent_and_date(pullback_event, win_pullback)
L685             rows.append((t, {
L686                 "G_BREAKOUT_recent_5d": br_recent,
L687                 "G_BREAKOUT_last_date": br_date,
L688                 "G_PULLBACK_recent_5d": pb_recent,
L689                 "G_PULLBACK_last_date": pb_date,
L690                 "G_PIVOT_price": float(pivot_price[t].iloc[-1]) if t in pivot_price.columns else float('nan'),
L691             }))
L692         flags = pd.DataFrame({k: v for k, v in rows}).T
L693
L694         # 列を作成・上書き
L695         cols = ["G_BREAKOUT_recent_5d","G_BREAKOUT_last_date","G_PULLBACK_recent_5d","G_PULLBACK_last_date","G_PIVOT_price"]
L696         for c in cols:
L697             if c not in feature_df.columns:
L698                 feature_df[c] = np.nan
L699         feature_df.loc[flags.index, flags.columns] = flags
L700
L701     except Exception:
L702         pass
L703     return feature_df
L704
L705
L706
```

## <drift.py>
```text
L1 import pandas as pd, yfinance as yf
L2 import numpy as np
L3 import requests
L4 import os
L5 import csv
L6 import time
L7 from pathlib import Path
L8
L9 # Debug flag
L10 debug_mode = False  # set to True for detailed output
L11
L12 # --- Finnhub settings & helper ---
L13 FINNHUB_API_KEY = os.environ.get("FINNHUB_API_KEY")
L14 if not FINNHUB_API_KEY:
L15     raise ValueError("FINNHUB_API_KEY not set (環境変数が未設定です)")
L16
L17 RATE_LIMIT = 55  # requests per minute (free tier is 60)
L18 call_times = []
L19
L20
L21 def finnhub_get(endpoint, params):
L22     """Call Finnhub API with basic rate limiting."""
L23     now = time.time()
L24     cutoff = now - 60
L25     while call_times and call_times[0] < cutoff:
L26         call_times.pop(0)
L27     if len(call_times) >= RATE_LIMIT:
L28         sleep_time = 60 - (now - call_times[0])
L29         time.sleep(sleep_time)
L30     params = {**params, "token": FINNHUB_API_KEY}
L31     try:
L32         resp = requests.get(f"https://finnhub.io/api/v1/{endpoint}", params=params)
L33         resp.raise_for_status()
L34         data = resp.json()
L35     except requests.exceptions.JSONDecodeError as e:
L36         print(f"⚠️ Finnhub API JSON decode error: {e}")
L37         return {}
L38     except Exception as e:
L39         print(f"⚠️ Finnhub API error: {e}")
L40         return {}
L41     call_times.append(time.time())
L42     return data
L43
L44
L45 def fetch_price(symbol):
L46     try:
L47         data = finnhub_get("quote", {"symbol": symbol})
L48         price = data.get("c")
L49         return float(price) if price not in (None, 0) else float("nan")
L50     except Exception:
L51         return float("nan")
L52
L53
L54 def fetch_vix_ma5():
L55     """Retrieve VIX 5-day moving average via yfinance."""
L56     try:
L57         vix = (
L58             yf.download("^VIX", period="7d", interval="1d", progress=False, auto_adjust=False)["Close"]
L59             .dropna()
L60             .tail(5)
L61         )
L62         if len(vix) < 5:
L63             return float("nan")
L64         return vix.mean().item()
L65     except Exception:
L66         return float("nan")
L67
L68
L69 # === Minervini-like sell signals ===
L70 def _yf_df(sym, period="6mo"):
L71     """日足/MA/出来高平均を取得。欠損時は None。"""
L72     try:
L73         df = yf.download(sym, period=period, interval="1d", auto_adjust=False, progress=False)
L74         if df is None or df.empty:
L75             return None
L76         return df.dropna().assign(
L77             ma20=lambda d: d["Close"].rolling(20).mean(),
L78             ma50=lambda d: d["Close"].rolling(50).mean(),
L79             vol50=lambda d: d["Volume"].rolling(50).mean(),
L80         )
L81     except Exception:
L82         return None
L83
L84
L85 def _scalar(row, col):
L86     """Series/npスカラ→Pythonスカラ化（NaNはNaNのまま）"""
L87     try:
L88         v = row[col]
L89         if hasattr(v, "item"):
L90             try:
L91                 v = v.item()
L92             except Exception:
L93                 pass
L94         return v
L95     except Exception:
L96         return float("nan")
L97
L98
L99 def _is_strict_down(seq):
L100     """数列が厳密に連続で切り下がっているか（len>=4を想定）。NaN含みはFalse。"""
L101     try:
L102         xs = [float(x) for x in seq]
L103         if any(pd.isna(x) for x in xs) or len(xs) < 4:
L104             return False
L105         return all(b < a for a, b in zip(xs[:-1], xs[1:]))
L106     except Exception:
L107         return False
L108
L109
L110 def _signals_for_day(df, idx):
L111     """df.loc[idx] 1日分に対しシグナル配列を返す（値動き/出来高ベースのみ）。"""
L112     try:
L113         sig = []
L114         d = df.loc[idx]
L115         close = _scalar(d, "Close")
L116         open_ = _scalar(d, "Open")
L117         ma20 = _scalar(d, "ma20")
L118         ma50 = _scalar(d, "ma50")
L119         vol = _scalar(d, "Volume")
L120         vol50 = _scalar(df.iloc[-1], "vol50")
L121         if any(pd.isna(x) for x in (close, open_, vol, vol50)):
L122             return sig
L123         if pd.notna(ma20) and close < ma20:
L124             sig.append("20DMA↓")
L125         if pd.notna(ma50) and close < ma50 and vol > 1.5 * vol50:
L126             sig.append("50DMA↓(大商い)")
L127
L128         last4 = df.loc[:idx].tail(4)
L129         lows_desc = _is_strict_down(last4["Low"].tolist())
L130         last10 = df.loc[:idx].tail(10)
L131         reds = int((last10["Close"] < last10["Open"]).sum())
L132         if lows_desc or reds > 5:
L133             sig.append("連続安値/陰線優勢")
L134
L135         ups = int((last10["Close"] > last10["Open"]).sum())
L136         if ups >= 7:
L137             sig.append("上げ偏重(>70%)")
L138
L139         last15 = df.loc[:idx].tail(15)
L140         base0 = _scalar(last15.iloc[0], "Close") if len(last15) > 0 else float("nan")
L141         if pd.notna(base0) and base0 != 0 and (close / base0 - 1) >= 0.25:
L142             sig.append("+25%/15日内")
L143
L144         if len(df.loc[:idx]) >= 2:
L145             t1, t0 = df.loc[:idx].iloc[-2], df.loc[:idx].iloc[-1]
L146             t1_high = _scalar(t1, "High")
L147             t0_open = _scalar(t0, "Open")
L148             t0_close = _scalar(t0, "Close")
L149             if all(pd.notna(x) for x in (t1_high, t0_open, t0_close)):
L150                 if (t0_open > t1_high * 1.02) and (t0_close < t0_open):
L151                     sig.append("GU→陰線")
L152         return sig
L153     except Exception:
L154         return []
L155
L156
L157 def scan_sell_signals(symbols, lookback_days=5):
L158     """
L159     直近 lookback_days 日のうち一度でもシグナルが出たら {sym: [(date,[signals]),...]} を返す。
L160     日付は YYYY-MM-DD。Slackで列挙する。
L161     """
L162     out = {}
L163     for s in symbols:
L164         df = _yf_df(s)
L165         if df is None or len(df) < 60:
L166             continue
L167         alerts = []
L168         for idx in df.tail(lookback_days).index:
L169             tags = _signals_for_day(df, idx)
L170             if tags:
L171                 alerts.append((idx.strftime("%Y-%m-%d"), tags))
L172         if alerts:
L173             out[s] = alerts
L174     return out
L175
L176
L177 def load_portfolio():
L178     tickers_path = Path(__file__).with_name("current_tickers.csv")
L179     with tickers_path.open() as f:
L180         reader = list(csv.reader(f))
L181     return [
L182         {"symbol": sym.strip().upper(), "shares": int(qty), "target_ratio": 1 / len(reader)}
L183         for sym, qty in reader
L184     ]
L185
L186
L187 def compute_threshold():
L188     vix_ma5 = fetch_vix_ma5()
L189     drift_threshold = 10 if vix_ma5 < 20 else 12 if vix_ma5 < 26 else float("inf")
L190     return vix_ma5, drift_threshold
L191
L192
L193 def build_dataframe(portfolio):
L194     for stock in portfolio:
L195         price = fetch_price(stock["symbol"])
L196         stock["price"] = price
L197         stock["value"] = price * stock["shares"]
L198
L199     df = pd.DataFrame(portfolio)
L200     total_value = df["value"].sum()
L201     df["current_ratio"] = df["value"] / total_value
L202     df["drift"] = df["current_ratio"] - df["target_ratio"]
L203     df["drift_abs"] = df["drift"].abs()
L204     total_drift_abs = df["drift_abs"].sum()
L205     df["adjusted_ratio"] = df["current_ratio"] - df["drift"] / 2
L206     df["adjustable"] = (
L207         (df["adjusted_ratio"] * total_value) >= df["price"]
L208     ) & df["price"].notna() & df["price"].gt(0)
L209     return df, total_value, total_drift_abs
L210
L211
L212 def simulate(df, total_value, total_drift_abs, drift_threshold):
L213     alert = drift_threshold != float("inf") and total_drift_abs * 100 > drift_threshold
L214     if alert:
L215         df["trade_shares"] = df.apply(
L216             lambda r: int(round(((r["adjusted_ratio"] * total_value) - r["value"]) / r["price"]))
L217             if r["adjustable"] and r["price"] > 0 else 0,
L218             axis=1,
L219         )
L220         df["new_shares"] = df["shares"] + df["trade_shares"]
L221         df["new_value"] = df["new_shares"] * df["price"]
L222         new_total_value = df["new_value"].sum()
L223         df["simulated_ratio"] = df["new_value"] / new_total_value
L224         df["simulated_drift_abs"] = (df["simulated_ratio"] - df["target_ratio"]).abs()
L225         simulated_total_drift_abs = df["simulated_drift_abs"].sum()
L226     else:
L227         df["trade_shares"] = np.nan
L228         df["new_shares"] = np.nan
L229         df["new_value"] = np.nan
L230         new_total_value = np.nan
L231         df["simulated_ratio"] = np.nan
L232         df["simulated_drift_abs"] = np.nan
L233         simulated_total_drift_abs = np.nan
L234     return df, alert, new_total_value, simulated_total_drift_abs
L235
L236
L237 def prepare_summary(df, total_drift_abs, alert):
L238     summary = {
L239         "symbol": "合計",
L240         "shares": df["shares"].sum(),
L241         "value": df["value"].sum(),
L242         "current_ratio": np.nan,
L243         "drift_abs": total_drift_abs,
L244     }
L245     if alert:
L246         summary["trade_shares"] = np.nan
L247     # Sort details by evaluation value descending before appending summary
L248     df = df.sort_values(by="value", ascending=False)
L249     df = pd.concat([df, pd.DataFrame([summary])], ignore_index=True)
L250     if alert:
L251         cols = ["symbol", "shares", "value", "current_ratio", "drift_abs", "trade_shares"]
L252         df_small = df[cols].copy()
L253         df_small.columns = ["sym", "qty", "val", "now", "|d|", "Δqty"]
L254     else:
L255         cols = ["symbol", "shares", "value", "current_ratio", "drift_abs"]
L256         df_small = df[cols].copy()
L257         df_small.columns = ["sym", "qty", "val", "now", "|d|"]
L258     return df_small
L259
L260
L261 def currency(x):
L262     return f"${x:,.0f}" if pd.notnull(x) else ""
L263
L264
L265 def formatters_for(alert):
L266     formatters = {"val": currency, "now": "{:.2%}".format, "|d|": "{:.2%}".format}
L267     if alert:
L268         formatters["Δqty"] = "{:.0f}".format
L269     return formatters
L270
L271
L272 def build_header(vix_ma5, drift_threshold, total_drift_abs, alert, simulated_total_drift_abs):
L273     header = (
L274         f"*📈 VIX MA5:* {vix_ma5:.2f}\n"
L275         f"*📊 ドリフト閾値:* {'🔴(高VIX)' if drift_threshold == float('inf') else str(drift_threshold)+'%'}\n"
L276         f"*📉 現在のドリフト合計:* {total_drift_abs * 100:.2f}%\n"
L277     )
L278     if alert:
L279         header += f"*🔁 半戻し後ドリフト合計(想定):* {simulated_total_drift_abs * 100:.2f}%\n"
L280         header += "🚨 *アラート: 発生！！ Δqtyのマイナス銘柄を売却、任意の銘柄を買い増してバランスを取りましょう！*\n"
L281     else:
L282         header += "✅ アラートなし\n"
L283     return header
L284
L285
L286 def send_slack(text):
L287     SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")
L288     if not SLACK_WEBHOOK_URL:
L289         raise ValueError("SLACK_WEBHOOK_URL not set (環境変数が未設定です)")
L290     payload = {"text": text}
L291     try:
L292         resp = requests.post(SLACK_WEBHOOK_URL, json=payload)
L293         resp.raise_for_status()
L294         print("✅ Slack（Webhook）へ送信しました")
L295     except Exception as e:
L296         print(f"⚠️ Slack通知エラー: {e}")
L297
L298
L299 def send_debug(debug_text):
L300     SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")
L301     if not SLACK_WEBHOOK_URL:
L302         raise ValueError("SLACK_WEBHOOK_URL not set (環境変数が未設定です)")
L303     debug_payload = {"text": "```" + debug_text + "```"}
L304     try:
L305         resp = requests.post(SLACK_WEBHOOK_URL, json=debug_payload)
L306         resp.raise_for_status()
L307         print("✅ Debug情報をSlackに送信しました")
L308     except Exception as e:
L309         print(f"⚠️ Slack通知エラー: {e}")
L310
L311
L312 def main():
L313     portfolio = load_portfolio()
L314     symbols = [r["symbol"] for r in portfolio]
L315     sell_alerts = scan_sell_signals(symbols, lookback_days=5)
L316     vix_ma5, drift_threshold = compute_threshold()
L317     df, total_value, total_drift_abs = build_dataframe(portfolio)
L318     df, alert, new_total_value, simulated_total_drift_abs = simulate(
L319         df, total_value, total_drift_abs, drift_threshold
L320     )
L321     df_small = prepare_summary(df, total_drift_abs, alert)
L322     if 'df_small' in locals() and isinstance(df_small, pd.DataFrame) and not df_small.empty:
L323         col_sym = "sym" if "sym" in df_small.columns else ("symbol" if "symbol" in df_small.columns else None)
L324         if col_sym:
L325             df_small.insert(0, "⚠", df_small[col_sym].apply(lambda x: "🔴" if x in sell_alerts else ""))
L326     formatters = formatters_for(alert)
L327     header = build_header(
L328         vix_ma5, drift_threshold, total_drift_abs, alert, simulated_total_drift_abs
L329     )
L330     if sell_alerts:
L331         def fmt_pair(date_tags):
L332             date, tags = date_tags
L333             return f"{date}:" + "・".join(tags)
L334         listed = []
L335         for t, arr in sell_alerts.items():
L336             listed.append(f"*{t}*（" + ", ".join(fmt_pair(x) for x in arr) + "）")
L337         hits = ", ".join(listed)
L338         if "✅ アラートなし" in header:
L339             header = header.replace(
L340                 "✅ アラートなし",
L341                 f"⚠️ 売りシグナルあり: {len(sell_alerts)}銘柄\n🟥 {hits}",
L342             )
L343         else:
L344             header += f"\n🟥 {hits}"
L345     table_text = df_small.to_string(formatters=formatters, index=False)
L346     send_slack(header + "\n```" + table_text + "```")
L347
L348     if debug_mode:
L349         debug_cols = [
L350             "symbol",
L351             "shares",
L352             "price",
L353             "value",
L354             "current_ratio",
L355             "drift",
L356             "drift_abs",
L357             "adjusted_ratio",
L358             "adjustable",
L359             "trade_shares",
L360             "new_shares",
L361             "new_value",
L362             "simulated_ratio",
L363             "simulated_drift_abs",
L364         ]
L365         debug_text = (
L366             "=== DEBUG: full dataframe ===\n"
L367             + df[debug_cols].to_string()
L368             + f"\n\ntotal_value={total_value}, new_total_value={new_total_value}\n"
L369             + f"total_drift_abs={total_drift_abs}, simulated_total_drift_abs={simulated_total_drift_abs}"
L370         )
L371         print("\n" + debug_text)
L372         send_debug(debug_text)
L373
L374
L375 if __name__ == "__main__":
L376     main()
L377
```

## <.github/workflows/weekly-report.yml>
```text
L1 name: Weekly Stock Report
L2
L3 on:
L4   push:
L5     branches: [ main ]
L6   schedule:
L7     - cron: '0 0 * * 6'  # UTC 00:00 → JST 09:00（土）
L8   workflow_dispatch:
L9
L10 jobs:
L11   build-and-report:
L12     runs-on: ubuntu-latest
L13
L14     steps:
L15       - name: Debug start
L16         run: echo '🚀 DEBUGstarted'
L17               
L18       - name: Checkout repository
L19         uses: actions/checkout@v3
L20
L21       - name: Setup Python
L22         uses: actions/setup-python@v5
L23         with:
L24           python-version: '3.x'                # （必要最小限のまま。固定したければ '3.13'）
L25           cache: 'pip'                         # ★ pipキャッシュを有効化
L26           cache-dependency-path: requirements.txt  # ★ 依存ファイルをキャッシュキーに
L27
L28       - name: Install dependencies
L29         run: pip install -r requirements.txt
L30
L31       - name: Prepare results directory
L32         run: mkdir -p results
L33
L34       - name: Cache previous results
L35         uses: actions/cache@v3
L36         with:
L37           path: results
L38           key: results-${{ github.run_id }}
L39           restore-keys: |
L40             results-
L41
L42       - name: Run factor.py
L43         env:
L44           SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
L45           FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
L46           FIN_THREADS: "8"
L47         run: python factor.py
```

## <.github/workflows/daily-report.yml>
```text
L1 name: Daily Stock Report
L2
L3 on:
L4   push:
L5     branches: [ main ]
L6   schedule:
L7     - cron: '30 23 * * 2-6'  # UTC 23:30 → JST 08:30（火〜土）
L8   workflow_dispatch:
L9
L10 jobs:
L11   build-and-report:
L12     runs-on: ubuntu-latest
L13
L14     steps:
L15       - name: Debug start
L16         run: echo '🚀 DEBUGstarted'
L17               
L18       - name: Checkout repository
L19         uses: actions/checkout@v3
L20
L21       - name: Setup Python
L22         uses: actions/setup-python@v4
L23         with:
L24           python-version: '3.x'
L25
L26       - name: Install dependencies
L27         run: pip install -r requirements.txt
L28
L29       - name: Prepare results directory
L30         run: mkdir -p results
L31
L32       - name: Cache previous results
L33         uses: actions/cache@v3
L34         with:
L35           path: results
L36           key: results-${{ github.run_id }}
L37           restore-keys: |
L38             results-
L39
L40       - name: Run drift.py
L41         env:
L42           SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
L43           FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
L44         run: python drift.py
```

## <documents/README.md>
```text
L1 # 運用ルール
L2
L3 ## 基本構成
L4 - 25銘柄を均等配分（現金を除き1銘柄あたり4%）
L5 - moomoo証券で運用
L6
L7 ## Barbell Growth-Defense方針
L8 - Growth枠12銘柄：高成長で乖離源となる攻めの銘柄
L9 - Defense枠13銘柄：低ボラで安定成長し配当を増やす守りの銘柄
L10 - 「猛烈に伸びる攻め × 着実に稼ぐ盾」の組合せで乖離→半戻しプレミアムを狙う
L11
L12 ## 現金比率（VIX 5日移動平均で判定）
L13 - VIX MA5 < 20: 5%
L14 - 20 ≤ VIX MA5 < 26: 7.5%
L15 - VIX MA5 ≥ 27: 12%（高VIX緊急モード）
L16
L17 ## ドリフト閾値
L18 - VIX MA5 < 20: 10%
L19 - 20 ≤ VIX MA5 < 26: 12%
L20 - VIX MA5 ≥ 27: 高VIX緊急モードへ移行
L21
L22 ## 通常モードの運用
L23 - 毎営業日、①90日経過 or ②ドリフトが閾値超過で半戻し
L24 - 半戻し：乖離の50%を中央へ寄せ、現金比率を上表どおりに調整
L25 - 全銘柄のトレーリングストップ(TS)を再設定
L26 - ドリフト＝Σ|現在比率−4%|（端数切り捨て）
L27
L28 ## 高VIX緊急モード（MA5 > 27で発動）
L29 1. 全25銘柄を各4%へ全戻し
L30 2. 現金比率12%へ引上げ
L31 3. 全銘柄のTSを再設定し以降の売買とドリフト計算を停止
L32
L33 ## 高VIX緊急モードの解除
L34 - MA5 < 23 または30営業日経過で解除
L35 - 緊急モード中にTS発動で減少した銘柄を補充し25銘柄×4%にリバランス
L36 - 通常モードの日次チェックを再開
L37
L38 ## 段階的トレーリングストップ
L39 - Growth: 基本25%
L40 - Defense: 基本20%
L41 - 含み益が40/60/80%に達したらTSを3/5/8ポイントずつ引き上げ
L42 - TS発動で減少した銘柄は翌日以降に補充（緊急モード中は補充しない）
L43
L44 ## 入替銘柄選定
L45 - Oxfordキャピタル／インカム、Alpha Investor、Motley Fool Stock Advisor、moomooスクリーニング等を参考にchatGPTで検討
L46 - 年間NISA枠はGrowth群の中から低ボラ銘柄を選定し利用。長期保持にはこだわらない。
L47
L48 ## 実行タイミング
L49 - 判定：米国市場終値直後
L50 - 執行：翌営業日の米国寄付き成行
L51
L52 ## VIX早見表
L53 | VIX MA5 | ドリフト閾値 | 現金比率 | モード |
L54 |--------|--------------|---------|-------|
L55 | <20    | 10           | 5%      | 通常 |
L56 | 20–26  | 12           | 7.5%    | 通常 |
L57 | ≥27    | –            | 12%     | 高VIX緊急 |
```

## <documents/drift_design.md>
```text
L1 # drift.py 詳細設計書
L2
L3 ## 概要
L4 - 25銘柄ポートフォリオのドリフトを日次監視し、閾値超過時に半戻し案をSlack通知するスクリプト。
L5 - Finnhubとyfinanceから価格・VIX情報を取得し、現況比率と調整案を計算。
L6
L7 ## 定数・設定
L8 - `FINNHUB_API_KEY` / `SLACK_WEBHOOK_URL` を環境変数から取得。
L9 - 無料枠を考慮したAPIレート制限: `RATE_LIMIT = 55`。
L10 - デバッグ出力用フラグ `debug_mode`。
L11
L12 ## 主な関数
L13 ### finnhub_get
L14 - 基本的なレート制限付きでFinnhub APIを呼び出し、JSONレスポンスを辞書で返す。
L15
L16 ### fetch_price
L17 - `quote` エンドポイントで株価を取得し、失敗時は `NaN` を返す。
L18
L19 ### fetch_vix_ma5
L20 - yfinanceでVIX終値を取得し、直近5営業日の移動平均を算出。
L21
L22 ### load_portfolio
L23 - `current_tickers.csv` から銘柄と保有株数を読み込み、目標比率4%を付与したリストを生成。
L24
L25 ### compute_threshold
L26 - VIX MA5に応じてドリフト閾値を10%/12%/高VIXモード(∞)に設定。
L27
L28 ### build_dataframe
L29 - 各銘柄の評価額や現在比率、ドリフト、半戻し後比率(`adjusted_ratio`)を計算しDataFrame化。
L30
L31 ### simulate
L32 - ドリフト合計が閾値を超えた場合、半戻し後の売買株数と新比率を試算し、シミュレート後ドリフトを返す。
L33
L34 ### prepare_summary
L35 - 評価額順に並べ替えた後、合計行を付与してSlack表示用テーブルを作成。
L36
L37 ### formatters_for / currency
L38 - 通貨・比率・株数の表示フォーマットを定義。
L39
L40 ### build_header
L41 - VIX・閾値・ドリフト値およびアラート有無をSlackメッセージ用ヘッダに整形。
L42
L43 ### send_slack / send_debug
L44 - 通常通知およびデバッグ詳細をSlack Webhookへ送信。
L45
L46 ### main
L47 - 上記関数を順に呼び出し、日次ドリフトチェックの一連処理を実行。
L48
L49 ## 実行フロー
L50 1. `load_portfolio` で現ポートフォリオを読み込む。
L51 2. `compute_threshold` でVIX MA5とドリフト閾値を決定。
L52 3. `build_dataframe` で現在比率とドリフトを計算。
L53 4. `simulate` で閾値超過時の半戻し案を試算。
L54 5. `prepare_summary` と `build_header` で通知本文とテーブルを構築。
L55 6. `send_slack` で結果を送信。`debug_mode` がTrueなら `send_debug` も併用。
```

## <documents/factor_design.md>
```text
L1 # factor.py 詳細設計書
L2
L3 ## 概要
L4 - 既存ポートフォリオの銘柄と検討中の銘柄群を同時に扱う銘柄選定パイプライン。
L5 - 価格・財務データを取り込みスコアリングとDRRS選定を行うことで、以下のアウトプットを得る。
L6   - 採用銘柄と惜しくも漏れた銘柄のスコア一覧
L7   - IN/OUTのティッカーリストとOUT側の低スコア銘柄
L8   - 新旧ポートフォリオの比較表
L9   - 検討中銘柄の低スコアランキング（整理用）
L10
L11 ## 全体フロー
L12 1. **Input** – `current_tickers.csv`と`candidate_tickers.csv`を読み込み、yfinanceやFinnhubのAPIから価格・財務データを収集して`InputBundle`を整備。
L13 2. **Score Calculation** – Scorerが特徴量を計算し因子スコアを合成して`FeatureBundle`を生成。
L14 3. **Correlation Reduction & Selection** – SelectorがDRRSロジックで相関を抑えつつG/D銘柄を選定し`SelectionBundle`を得る。
L15 4. **Output** – 採用結果と周辺情報を表・Slack通知として出力。
L16
L17 ```mermaid
L18 flowchart LR
L19   A[Input\nAPI & 前処理] --> B[Score Calculation\n特徴量・因子合成]
L20   B --> C[Correlation Reduction\nDRRS選定]
L21   C --> D[Output\nSlack通知]
L22 ```
L23
L24 ## 定数・設定
L25 | 変数 | 内容 | 主な用途 |
L26 | --- | --- | --- |
L27 | `exist` / `cand` | 現行ポートフォリオと検討中銘柄のティッカーリスト | スコア対象ユニバースの構成、候補整理 |
L28 | `bench` | ベンチマークティッカー | 相対強さ・β算出、ポート比較 |
L29 | `CAND_PRICE_MAX` | 候補銘柄の許容価格上限 | 高額銘柄の事前除外 |
L30 | `N_G` / `N_D` | G/D採用枠の件数 | 最終的に選ぶ銘柄数の制約 |
L31 | `g_weights` / `D_weights` | 各因子の重みdict | G/Dスコア合成 |
L32 | `D_BETA_MAX` | Dバケットの許容β上限 | 高β銘柄の除外フィルタ |
L33 | `FILTER_SPEC` | G/Dごとの前処理フィルタ | トレンドマスクやβ上限設定 |
L34 | `corrM` | DRRS初期プールの最大件数 | 相関行列サイズ制御 |
L35 | `DRRS_G` / `DRRS_D` | DRRSパラメータdict | バケット別の相関低減設定 |
L36 | `DRRS_SHRINK` | 残差相関の対角シュリンク率 | `residual_corr`の安定化 |
L37 | `CROSS_MU_GD` | G-D間クロス相関ペナルティμ | 2バケット同時最適化で相関抑制 |
L38 | `RESULTS_DIR` | 選定結果保存ディレクトリ | `_save_sel`/`_load_prev`の入出力 |
L39
L40 選定結果は`results/`配下にJSONとして保存し、次回実行時に`_load_prev`で読み込んで選定条件に反映。
L41
L42 ## DTO/Config
L43 各ステップ間で受け渡すデータ構造と設定値。変数の意味合いと利用箇所を以下に示す。
L44
L45 ### InputBundle（Input → Scorer）
L46 | 変数 | 内容 | 主な用途 |
L47 | --- | --- | --- |
L48 | `cand` | 候補銘柄ティッカーのリスト | OUTテーブルや低スコアランキング対象の母集団 |
L49 | `tickers` | 現行+候補を合わせたティッカー一覧 | 価格・出来高ダウンロード、リターン計算 |
L50 | `bench` | ベンチマークティッカー | 相対強さ・β算出、ポート比較 |
L51 | `data` | yfinanceのダウンロード結果（階層列） | `px`/`spx`/リターン等の基礎データ |
L52 | `px` | `data['Close']`だけを抜き出した価格系列 | 指標計算・リターン生成 |
L53 | `spx` | `data['Close'][bench]` のSeries | `rs`や`calc_beta`の基準指数 |
L54 | `tickers_bulk` | `yf.Tickers`オブジェクト | `info`等の一括取得 |
L55 | `info` | ティッカー別のyfinance情報dict | セクター判定やEPS補完 |
L56 | `eps_df` | EPS TTM/直近EPS等をまとめた表 | 成長指標の算出 |
L57 | `fcf_df` | CFO・CapEx・FCF TTMと情報源フラグ | FCF/EVや配当カバレッジ |
L58 | `returns` | `px.pct_change()`のリターン表 | 相関行列・DRRS計算 |
L59
L60 ### FeatureBundle（Scorer → Selector）
L61 | 変数 | 内容 | 主な用途 |
L62 | --- | --- | --- |
L63 | `df` | 計算済み指標の生値テーブル | デバッグ・出力表示 |
L64 | `df_z` | ウィンザー後Zスコア化した指標表 | 因子スコア合成、選定基準 |
L65 | `g_score` | Gバケット総合スコア | G選定、IN/OUT比較 |
L66 | `d_score_all` | Dバケット総合スコア（全銘柄） | D選定、低スコアランキング |
L67 | `missing_logs` | 欠損指標と補完状況のログ | データ品質チェック |
L68
L69 ### SelectionBundle（Selector → Output）
L70 | 変数 | 内容 | 主な用途 |
L71 | --- | --- | --- |
L72 | `resG` | G選定結果の詳細dict（`tickers`、目的値等） | 結果保存・平均相関などの指標表示 |
L73 | `resD` | D選定結果の詳細dict | 同上 |
L74 | `top_G` | 最終採用Gティッカー | 新ポートフォリオ構築 |
L75 | `top_D` | 最終採用Dティッカー | 同上 |
L76 | `init_G` | DRRS前のG初期候補 | 惜しくも外れた銘柄表示 |
L77 | `init_D` | DRRS前のD初期候補 | 同上 |
L78
L79 ### WeightsConfig
L80 | 変数 | 内容 | 主な用途 |
L81 | --- | --- | --- |
L82 | `g` | G因子（GRW/MOM/VOL）の重みdict | `g_score`合成 |
L83 | `d` | D因子（D_QAL/D_YLD/D_VOL_RAW/D_TRD）の重みdict | `d_score_all`合成 |
L84
L85 ### DRRSParams
L86 | 変数 | 内容 | 主な用途 |
L87 | --- | --- | --- |
L88 | `corrM` | DRRS初期プールの最大件数 | 相関行列サイズ制御 |
L89 | `shrink` | 残差相関のシュリンク率 | `residual_corr`の対角強調 |
L90 | `G` | Gバケット用パラメータdict（`lookback`等） | `select_bucket_drrs`設定 |
L91 | `D` | Dバケット用パラメータdict | 同上 |
L92 | `cross_mu_gd` | G-Dクロス相関ペナルティ係数μ | `select_buckets`の目的関数 |
L93
L94 ### PipelineConfig
L95 | 変数 | 内容 | 主な用途 |
L96 | --- | --- | --- |
L97 | `weights` | `WeightsConfig`のインスタンス | スコア合成の重み参照 |
L98 | `drrs` | `DRRSParams`のインスタンス | 選定ステップの設定値 |
L99 | `price_max` | 候補銘柄の許容価格上限 | Input段階でのフィルタ |
L100
L101 ## 共通ユーティリティ
L102 - `winsorize_s` / `robust_z` : 外れ値処理とZスコア化。
L103 - `_safe_div` / `_safe_last` : 例外を潰した分割・末尾取得。
L104 - `_load_prev` / `_save_sel` : 選定結果の読み書き。
L105
L106 ## クラス設計
L107 ### Step1: Input
L108 `current_tickers.csv`の現行銘柄と`candidate_tickers.csv`の検討中銘柄を起点にデータを集約する。外部I/Oと前処理を担当し、`prepare_data`で`InputBundle`を生成。価格・財務データの取得は**yfinanceを優先し、欠損がある指標のみFinnhub APIで補完**する。
L109 主なメソッド:
L110 - `impute_eps_ttm` : 四半期EPS×4でTTMを推定し欠損時のみ差し替え。
L111 - `fetch_cfo_capex_ttm_yf` : yfinanceの四半期/年次キャッシュフローからCFO・CapEx・FCF TTMを算出。
L112 - `fetch_cfo_capex_ttm_finnhub` : yfinanceで欠けた銘柄のみFinnhub APIで補完。
L113 - `compute_fcf_with_fallback` : yfinance値を基準にFinnhub値で穴埋めし、CFO/CapEx/FCFと情報源フラグを返す。
L114 - `_build_eps_df` : `info`や`quarterly_earnings`からEPS TTMと直近EPSを計算し、`impute_eps_ttm`で補完。
L115 - `prepare_data` :
L116     0. CSVから現行銘柄と候補銘柄のティッカー一覧を読み込む。
L117     1. 候補銘柄の現在値を取得し価格上限でフィルタ。
L118     2. 既存+候補から対象ティッカーを決定し、価格・出来高を一括ダウンロード（yfinance）。
L119     3. yfinance値を基にEPS/FCFテーブルやベンチマーク系列、リターンを構築し、欠損セルはFinnhub呼び出しで穴埋め。
L120     4. 上記を`InputBundle`に格納して返す。
L121
L122 ### Step2: Score Calculation (Scorer)
L123 特徴量計算とスコア合成を担当し、`FeatureBundle`を返す。
L124
L125 #### 補助関数
L126 - `trend(s)` : 50/150/200日移動平均や52週レンジから-0.5〜0.5で構成されたトレンド指標。
L127 - `rs(s,b)` / `tr_str(s)` / `rs_line_slope(s,b,win)` : 相対強さや短期トレンド、RS回帰傾きを算出。
L128 - `ev_fallback` : `enterpriseValue`欠損時に負債・現金からEVを推定。
L129 - `dividend_status` / `div_streak` : 配当未設定状況の判定と増配年数カウント。
L130 - `fetch_finnhub_metrics` : Finnhub APIからEPS成長・ROE・βなど不足指標を取得。
L131 - `calc_beta` : ベンチマークとの共分散からβを算出。
L132 - `spx_to_alpha` : S&P500の位置情報からDRRSで用いるαを推定。
L133 - `soft_cap_effective_scores` / `pick_top_softcap` : セクターソフトキャップ付きスコア調整と上位抽出。
L134
L135 **補助関数と生成指標**
L136
L137 | 補助関数 | 生成指標 | 略称 |
L138 | --- | --- | --- |
L139 | `trend` | トレンド総合値 | `TR` |
L140 | `rs` | 相対強さ | `RS` |
L141 | `tr_str` | 価格と50日線の乖離 | `TR_str` |
L142 | `rs_line_slope` | RS線の回帰傾き | `RS_SLOPE_*` |
L143 | `calc_beta` | β | `BETA` |
L144 | `div_streak` | 連続増配年数 | `DIV_STREAK` |
L145
L146 #### `aggregate_scores` 詳細
L147 1. 各銘柄の価格系列や`info`を基に以下を算出。
L148    - **トレンド/モメンタム**: `TR`、`RS`、`TR_str`、多様な移動平均比、`RS_SLOPE_*`など。
L149    - **リスク**: `BETA`、`DOWNSIDE_DEV`、`MDD_1Y`、`RESID_VOL`、`DOWN_OUTPERF`、`EXT_200`等。
L150    - **配当**: `DIV`、`DIV_TTM_PS`、`DIV_VAR5`、`DIV_YOY`、`DIV_FCF_COVER`、`DIV_STREAK`。
L151    - **財務・成長**: `EPS`、`REV`、`ROE`、`FCF/EV`、`REV_Q_YOY`、`EPS_Q_YOY`、`REV_YOY_ACC`、`REV_YOY_VAR`、`REV_ANN_STREAK`、`RULE40`、`FCF_MGN` 等。
L152    - **安定性/サイズ**: `DEBT2EQ`、`CURR_RATIO`、`MARKET_CAP`、`ADV60_USD`、`EPS_VAR_8Q`など。
L153 2. 指標欠損はFinnhub API等で補完し、未取得項目を`missing_logs`に記録。
L154 3. `winsorize_s`→`robust_z`で標準化し`df_z`へ保存。サイズ・流動性は対数変換。
L155 4. 正規化済指標から因子スコアを合成。
L156    - 各因子の構成と重みは以下の通り。
L157      - **GRW**: 0.30×`REV` + 0.20×`EPS_Q_YOY` + 0.15×`REV_Q_YOY` + 0.15×`REV_YOY_ACC` + 0.10×`RULE40` + 0.10×`FCF_MGN` + 0.10×`REV_ANN_STREAK` − 0.05×`REV_YOY_VAR`。
L158      - **MOM**: 0.40×`RS` + 0.15×`TR_str` + 0.15×`RS_SLOPE_6W` + 0.15×`RS_SLOPE_13W` + 0.10×`MA200_SLOPE_5M` + 0.10×`MA200_UP_STREAK_D`。
L159      - **VOL**: `BETA`単体を使用。
L160      - **QAL**: 0.60×`FCF_W` + 0.40×`ROE_W`で作成。
L161      - **YLD**: 0.30×`DIV` + 0.70×`DIV_STREAK`。
L162      - **D_QAL**: 0.35×`QAL` + 0.20×`FCF` + 0.15×`CURR_RATIO` − 0.15×`DEBT2EQ` − 0.15×`EPS_VAR_8Q`。
L163      - **D_YLD**: 0.45×`DIV` + 0.25×`DIV_STREAK` + 0.20×`DIV_FCF_COVER` − 0.10×`DIV_VAR5`。
L164      - **D_VOL_RAW**: 0.40×`DOWNSIDE_DEV` + 0.22×`RESID_VOL` + 0.18×`MDD_1Y` − 0.10×`DOWN_OUTPERF` − 0.05×`EXT_200` − 0.08×`SIZE` − 0.10×`LIQ` + 0.10×`BETA`。
L165      - **D_TRD**: 0.40×`MA200_SLOPE_5M` − 0.30×`EXT_200` + 0.15×`NEAR_52W_HIGH` + 0.15×`TR`。
L166     - 主な指標の略称と意味:
L167
L168       | 略称 | 補助関数 | 概要 |
L169       | --- | --- | --- |
L170       | TR | `trend` | 50/150/200日移動平均と52週レンジを組み合わせたトレンド総合値 |
L171       | RS | `rs` | ベンチマークに対する相対強さ（12M/1Mリターン差） |
L172       | TR_str | `tr_str` | 価格と50日移動平均の乖離 |
L173       | RS_SLOPE_6W | `rs_line_slope` | 相対強さ線の6週回帰傾き |
L174       | RS_SLOPE_13W | `rs_line_slope` | 相対強さ線の13週回帰傾き |
L175       | MA200_SLOPE_5M | - | 200日移動平均の5か月騰落率 |
L176       | MA200_UP_STREAK_D | - | 200日線が連続で上向いた日数 |
L177       | BETA | `calc_beta` | ベンチマークに対するβ |
L178       | DOWNSIDE_DEV | - | 下方リターンのみの年率化標準偏差 |
L179       | RESID_VOL | - | βで調整した残差リターンの年率化標準偏差 |
L180       | MDD_1Y | - | 過去1年の最大ドローダウン |
L181       | DOWN_OUTPERF | - | 市場下落日に対する平均超過リターン |
L182       | EXT_200 | - | 200日移動平均からの絶対乖離率 |
L183       | NEAR_52W_HIGH | - | 52週高値までの下方距離（0=高値） |
L184       | FCF_W | - | ウィンザー処理後のFCF/EV |
L185       | ROE_W | - | ウィンザー処理後のROE |
L186       | FCF | - | FCF/EV |
L187       | QAL | - | FCF_WとROE_Wを組み合わせた品質スコア |
L188       | CURR_RATIO | - | 流動比率 |
L189       | DEBT2EQ | - | 負債資本倍率 |
L190       | EPS_VAR_8Q | - | EPSの8四半期標準偏差 |
L191       | DIV | - | 年率換算配当利回り |
L192       | DIV_STREAK | `div_streak` | 連続増配年数 |
L193       | DIV_FCF_COVER | - | 配当のFCFカバレッジ |
L194       | DIV_VAR5 | - | 5年配当変動率 |
L195       | DIV_TTM_PS | - | 1株当たりTTM配当 |
L196       | DIV_YOY | - | 前年比配当成長率 |
L197       | REV | - | 売上成長率TTM |
L198       | EPS_Q_YOY | - | 四半期EPSの前年同期比 |
L199       | REV_Q_YOY | - | 四半期売上の前年同期比 |
L200       | REV_YOY_ACC | - | 売上成長率の加速分 |
L201       | RULE40 | - | 売上成長率とFCFマージンの合計 |
L202       | FCF_MGN | - | FCFマージン |
L203       | REV_ANN_STREAK | - | 年次売上成長の連続年数 |
L204       | REV_YOY_VAR | - | 年次売上成長率の変動性 |
L205       | SIZE | - | 時価総額の対数値 |
L206       | LIQ | - | 60日平均出来高ドルの対数値 |
L207    - Gバケット: `GRW`、`MOM`、`VOL`を`cfg.weights.g`（0.40/0.45/-0.15）で加重し`g_score`を得る。
L208    - Dバケット: `D_QAL`、`D_YLD`、`D_VOL_RAW`、`D_TRD`を`cfg.weights.d`（0.15/0.15/-0.45/0.25）で加重し`d_score_all`を算出。
L209    - セクターcapによる`soft_cap_effective_scores`を適用し、G採用銘柄にはトレンドテンプレートフィルタを適用。
L210 5. `_apply_growth_entry_flags`でブレイクアウト/押し目発火状況を付加し、`FeatureBundle`を返す。
L211
L212 ### Step3: Correlation Reduction & Selection (Selector)
L213 DRRSアルゴリズムで相関を抑えた銘柄選定を行い、`SelectionBundle`を返す。`results/`に保存された前回選定（`G_selection.json` / `D_selection.json`）を`_load_prev`で読み込み、目的値が大きく悪化しない限り維持する。新しい採用集合は`_save_sel`でJSONに書き出し次回以降の入力に備える。
L214 主なメソッド:
L215 - `residual_corr` : 収益率行列をZスコア化し、上位主成分を除去した残差から相関行列を求め、平均相関に応じてシュリンク。
L216 - `rrqr_like_det` : スコアを重み付けしたQR分解風の手順で初期候補をk件抽出し、スコアの高い非相関な集合を得る。
L217 - `swap_local_det` / `swap_local_det_cross` : `sum(score) - λ*within_corr - μ*cross_corr`を目的関数として、入れ替え探索で局所的に最適化。
L218 - `select_bucket_drrs` : プール銘柄とスコアから残差相関を計算し、上記2段階(初期選択→入れ替え)でk銘柄を決定。過去採用銘柄との比較で目的値が劣化しなければ維持する。
L219 - `select_buckets` : Gバケットを選定後、その結果を除いた候補からDバケットを選ぶ。D選定時はGとの相関ペナルティμを付与し、両バケットの分散を制御する。
L220
L221 #### 相関低減ロジック詳細
L222 1. **残差相関行列の構築 (`residual_corr`)**
L223    - リターン行列`R`をZスコア化。
L224    - SVDで上位`n_pc`主成分`F`を求め、最小二乗で係数`B`を算出し残差`E = Z - F@B`を得る。
L225    - `E`の相関行列`C`を計算し、平均絶対相関に応じてシュリンク量`shrink_eff`を補正して対角を強調。
L226 2. **初期候補の抽出 (`rrqr_like_det`)**
L227    - スコアを0-1正規化した重み`w`とし、`Z*(1+γw)`で列ノルムを強調。
L228    - 残差ノルム最大の列を逐次選び、QRライクなデフレーションを行って非相関かつ高スコアな`k`銘柄集合`S0`を得る。
L229 3. **局所探索 (`swap_local_det` / `swap_local_det_cross`)**
L230    - 目的関数`Σz_score − λ·within_corr − μ·cross_corr`を最大化。
L231    - 選択集合の各銘柄を他候補と入れ替え、改善がなくなるまでまたは`max_pass`回まで探索。
L232    - `swap_local_det_cross`はGバケットとのクロス相関行列`C_cross`を使用し、ペナルティ`μ`を付与。
L233 4. **過去採用の維持とクロスペナルティ (`select_bucket_drrs` / `select_buckets`)**
L234    - 局所探索結果`S`と過去集合`P`の目的値を比較し、`S`が`P`より`η`未満の改善なら`P`を維持。
L235    - `select_buckets`ではGを先に決定し、D選定時にGとの相関ペナルティ`μ`を加えてクロス分散を抑制。
L236
L237 ### Step4: Output
L238 選定結果を可視化し共有する工程。以下の内容をテーブル化して標準出力とSlackへ送る。
L239 - 採用銘柄と惜しくも選外となった銘柄のスコア一覧
L240 - IN/OUTリストとOUT銘柄のスコア（低得点銘柄を確認しやすく）
L241 - 新旧ポートフォリオの比較表（組入れ・除外、スコア変化）
L242 - 検討中銘柄の低スコアランキング
L243
L244 主なメソッド:
L245 - `display_results` : 上記テーブルに加えパフォーマンス指標や分散化指標を表示。
L246 - `notify_slack` : Slack Webhookへ同内容を送信。
L247 - 補助:`_avg_offdiag`、`_resid_avg_rho`、`_raw_avg_rho`、`_cross_block_raw_rho`。
L248
L249 ## エントリポイント
L250 1. `PipelineConfig`を構築。
L251 2. **Step1** `Input.prepare_data`で`InputBundle`を生成。
L252 3. **Step2** `Scorer.aggregate_scores`で`FeatureBundle`を取得。
L253 4. **Step3** `Selector.select_buckets`で`SelectionBundle`を算出。
L254 5. **Step4** `Output.display_results`と`notify_slack`で結果を出力。
```

## <current_tickers.csv>
```text
L1 ENB,40
L2 ABBV,9
L3 KO,25
L4 SO,19
L5 VZ,41
L6 HD,4
L7 FUTU,9
L8 AU,32
L9 PLTR,11
L10 BAP,7
L11 CRDO,15
L12 CALM,15
L13 RIGL,42
L14 IDR,64
L15 MCD,6
L16 JNJ,10
L17 TIGR,145
L18 DUK,14
L19 ELVA,290
L20 DRD,95
L21 ASM,380
L22 MNST,28
L23 BBIO,36
L24 TBPH,130
L25 WMT,18
```

## <candidate_tickers.csv>
```text
L1 AAPL
L2 ABT
L3 ACAD
L4 ACGL
L5 ADSK
L6 AEE
L7 AEM
L8 AGI
L9 AGX
L10 AJX
L11 ALAB
L12 ALKS
L13 ALRS
L14 AMCR
L15 AMG
L16 AMGN
L17 AMSC
L18 AMZN
L19 ANET
L20 ANIP
L21 APH
L22 APP
L23 ARQT
L24 AS
L25 ASLE
L26 ASML
L27 ASND
L28 ATAT
L29 ATGE
L30 AUPH
L31 AVGO
L32 AXON
L33 BABA
L34 BAP
L35 BBVA
L36 BGNE
L37 BK
L38 BLBD
L39 BMO
L40 BMY
L41 BR
L42 BRKB
L43 BROS
L44 BSX
L45 BWA
L46 BWAY
L47 BWMN
L48 BWXT
L49 BZ
L50 CASY
L51 CBOE
L52 CCB
L53 CCEP
L54 CCJ
L55 CECO
L56 CELH
L57 CI
L58 CIEN
L59 CL
L60 CLS
L61 CMCL
L62 CMCSA
L63 CME
L64 CMI
L65 CMPO
L66 COF
L67 COMM
L68 CPNG
L69 CPRT
L70 CTAS
L71 CTSH
L72 CW
L73 CYBR
L74 DASH
L75 DAVE
L76 DB
L77 DDOG
L78 DGICA
L79 DIS
L80 DKNG
L81 DOCS
L82 DXPE
L83 EA
L84 EGO
L85 EME
L86 EPRT
L87 EQX
L88 ERJ
L89 EVBN
L90 EVRG
L91 FEIM
L92 FIG
L93 FIX
L94 FNV
L95 FOXA
L96 FSI
L97 FSM
L98 FSS
L99 GD
L100 GE
L101 GENI
L102 GFI
L103 GH
L104 GILD
L105 GILT
L106 GIS
L107 GLW
L108 GMAB
L109 GNL
L110 GOOG
L111 GOOGL
L112 GRAB
L113 GROY
L114 GS
L115 HALO
L116 HEI
L117 HG
L118 HL
L119 HON
L120 HOOD
L121 HWM
L122 IAG
L123 IBKR
L124 IBM
L125 ICE
L126 IDCC
L127 IDXX
L128 IIIN
L129 IONQ
L130 IREN
L131 JD
L132 JPM
L133 KGC
L134 KHC
L135 KLAC
L136 KNSA
L137 KRMN
L138 KTOS
L139 LEN
L140 LEU
L141 LGCY
L142 LH
L143 LIF
L144 LITE
L145 LKNC
L146 LNC
L147 LRCX
L148 LTM
L149 LVMUY
L150 LYG
L151 MDLZ
L152 MEDP
L153 MELI
L154 META
L155 MIRM
L156 MMC
L157 MS
L158 MSFT
L159 MU
L160 MVBF
L161 NARI
L162 NBIS
L163 NCSM
L164 NEM
L165 NET
L166 NEU
L167 NLY
L168 NNI
L169 NRIM
L170 NTR
L171 NTRS
L172 NVDA
L173 NXST
L174 NYAX
L175 NYT
L176 OLO
L177 ONC
L178 OVBC
L179 PAAS
L180 PAC
L181 PAHC
L182 PAYX
L183 PDD
L184 PEP
L185 PG
L186 PGNY
L187 PHIN
L188 PHM
L189 PHR
L190 PINS
L191 PODD
L192 PPIH
L193 PRCH
L194 PRIM
L195 PRM
L196 PSIX
L197 PSTL
L198 PTGX
L199 QURE
L200 R
L201 RACE
L202 RBB
L203 RBRK
L204 RCL
L205 RDDT
L206 REGN
L207 RERE
L208 RGLD
L209 RITM
L210 RKLB
L211 RMD
L212 ROAD
L213 ROKU
L214 RSI
L215 RTX
L216 RYTM
L217 SAMG
L218 SAP
L219 SBUX
L220 SCHW
L221 SD
L222 SE
L223 SKYW
L224 SPOT
L225 STE
L226 STRL
L227 SWAV
L228 SYK
L229 SYY
L230 TARS
L231 TATT
L232 TBBK
L233 TFPM
L234 TJX
L235 TK
L236 TME
L237 TMUS
L238 TOL
L239 TOST
L240 TREE
L241 TSM
L242 TTC
L243 UBER
L244 UI
L245 UNP
L246 VEEV
L247 VITL
L248 VRSK
L249 VTS
L250 WAY
L251 WGS
L252 WHG
L253 WLDN
L254 WM
L255 WMB
L256 WWD
L257 XEL
L258 XPEV
L259 YOU
L260 YUM
L261 ZM
L262 ZTS
```

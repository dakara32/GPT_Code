```text
# === Chat Paste Pack ===
# Repo: dakara32/GPT_Code @ main
# Files: drift.py, .github/workflows/daily-report.yml, documents/README.md, documents/drift_design.md
# ä½¿ã„æ–¹: ä¸‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’é †ã«è²¼ã‚Œã°ã“ã®ãƒãƒ£ãƒƒãƒˆã§å…¨ä½“æŠŠæ¡ã§ãã¾ã™ã€‚
# æ³¨è¨˜: å„ãƒ•ã‚¡ã‚¤ãƒ«ã¯å€‹åˆ¥ã« L1.. ã§è¡Œç•ªå·ä»˜ä¸ã€‚
---

## <drift.py>
```text
L1 import pandas as pd, yfinance as yf
L2 import numpy as np
L3 import requests
L4 import os
L5 import csv
L6 import json
L7 import time
L8 from pathlib import Path
L9
L10 # Debug flag
L11 debug_mode = False  # set to True for detailed output
L12
L13 # --- Finnhub settings & helper ---
L14 FINNHUB_API_KEY = os.environ.get("FINNHUB_API_KEY")
L15 if not FINNHUB_API_KEY:
L16     raise ValueError("FINNHUB_API_KEY not set (ç’°å¢ƒå¤‰æ•°ãŒæœªè¨­å®šã§ã™)")
L17
L18 RATE_LIMIT = 55  # requests per minute (free tier is 60)
L19 call_times = []
L20
L21
L22 def finnhub_get(endpoint, params):
L23     """Call Finnhub API with basic rate limiting."""
L24     now = time.time()
L25     cutoff = now - 60
L26     while call_times and call_times[0] < cutoff:
L27         call_times.pop(0)
L28     if len(call_times) >= RATE_LIMIT:
L29         sleep_time = 60 - (now - call_times[0])
L30         time.sleep(sleep_time)
L31     params = {**params, "token": FINNHUB_API_KEY}
L32     try:
L33         resp = requests.get(f"https://finnhub.io/api/v1/{endpoint}", params=params)
L34         resp.raise_for_status()
L35         data = resp.json()
L36     except requests.exceptions.JSONDecodeError as e:
L37         print(f"âš ï¸ Finnhub API JSON decode error: {e}")
L38         return {}
L39     except Exception as e:
L40         print(f"âš ï¸ Finnhub API error: {e}")
L41         return {}
L42     call_times.append(time.time())
L43     return data
L44
L45
L46 def fetch_price(symbol):
L47     try:
L48         data = finnhub_get("quote", {"symbol": symbol})
L49         price = data.get("c")
L50         return float(price) if price not in (None, 0) else float("nan")
L51     except Exception:
L52         return float("nan")
L53
L54
L55 def fetch_vix_ma5():
L56     """Retrieve VIX 5-day moving average via yfinance."""
L57     try:
L58         vix = (
L59             yf.download("^VIX", period="7d", interval="1d", progress=False, auto_adjust=False)["Close"]
L60             .dropna()
L61             .tail(5)
L62         )
L63         if len(vix) < 5:
L64             return float("nan")
L65         return vix.mean().item()
L66     except Exception:
L67         return float("nan")
L68
L69
L70 # --- BEGIN: breadth port ---
L71 RESULTS_DIR = "results"
L72 os.makedirs(RESULTS_DIR, exist_ok=True)
L73
L74
L75 def _breadth_state_file():
L76     return os.path.join(RESULTS_DIR, "breadth_state.json")
L77
L78
L79 def load_mode(default="NORMAL"):
L80     try:
L81         with open(_breadth_state_file(), "r") as f:
L82             m = json.load(f).get("mode", default)
L83         return m if m in ("EMERG", "CAUTION", "NORMAL") else default
L84     except Exception:
L85         return default
L86
L87
L88 def save_mode(mode: str):
L89     try:
L90         with open(_breadth_state_file(), "w") as f:
L91             json.dump({"mode": mode}, f)
L92     except Exception:
L93         pass
L94
L95
L96 def _read_universe_for_breadth():
L97     """current + candidateï¼ˆå­˜åœ¨ã™ã‚Œã°ï¼‰ã‚’åˆç®—ã—ã€ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
L98     cur = []
L99     try:
L100         with Path(__file__).with_name("current_tickers.csv").open() as f:
L101             cur = [r[0].strip().upper() for r in csv.reader(f) if r]
L102     except Exception:
L103         pass
L104     cand = []
L105     cand_path = Path(__file__).with_name("candidate_tickers.csv")
L106     if cand_path.exists():
L107         try:
L108             with cand_path.open() as f:
L109                 cand = [r[0].strip().upper() for r in csv.reader(f) if r]
L110         except Exception:
L111             pass
L112     # ç©ºã‚„é‡è¤‡ã‚’é™¤å»
L113     uni = sorted({t for t in (cur + cand) if t and t != "^GSPC"})
L114     return uni
L115
L116
L117 def trend_template_breadth_series(px: pd.DataFrame, spx: pd.Series, win_days: int | None = None) -> pd.Series:
L118     """
L119     scorer.py / Scorer.trend_template_breadth_series ã‚’ç§»æ¤ã€‚
L120     å„å–¶æ¥­æ—¥ã® trend_template åˆæ ¼â€œæœ¬æ•°â€=C ã‚’è¿”ã™ï¼ˆint Seriesï¼‰ã€‚
L121     """
L122     if px is None or px.empty:
L123         return pd.Series(dtype=int)
L124     px = px.dropna(how="all", axis=1)
L125     if win_days and win_days > 0:
L126         px = px.tail(win_days)
L127     if px.empty:
L128         return pd.Series(dtype=int)
L129     spx = spx.reindex(px.index).ffill()
L130
L131     ma50 = px.rolling(50).mean()
L132     ma150 = px.rolling(150).mean()
L133     ma200 = px.rolling(200).mean()
L134
L135     tt = (px > ma150)
L136     tt &= (px > ma200)
L137     tt &= (ma150 > ma200)
L138     tt &= (ma200 - ma200.shift(21) > 0)
L139     tt &= (ma50 > ma150)
L140     tt &= (ma50 > ma200)
L141     tt &= (px > ma50)
L142
L143     lo252 = px.rolling(252).min()
L144     hi252 = px.rolling(252).max()
L145     tt &= (px.divide(lo252).sub(1.0) >= 0.30)  # P_OVER_LOW52 >= 0.30
L146     tt &= (px >= (0.75 * hi252))  # NEAR_52W_HIGH >= -0.25
L147
L148     r12 = px.divide(px.shift(252)).sub(1.0)
L149     br12 = spx.divide(spx.shift(252)).sub(1.0)
L150     r1 = px.divide(px.shift(22)).sub(1.0)
L151     br1 = spx.divide(spx.shift(22)).sub(1.0)
L152     rs = 0.7 * (r12.sub(br12, axis=0)) + 0.3 * (r1.sub(br1, axis=0))
L153     tt &= (rs >= 0.10)
L154
L155     return tt.fillna(False).sum(axis=1).astype(int)
L156
L157
L158 def build_breadth_lead_lines() -> tuple[list[str], str]:
L159     """
L160     æ—§ factor._build_breadth_lead_lines ã¨åŒä¸€ãƒ­ã‚¸ãƒƒã‚¯ã€‚
L161     ãƒ˜ãƒƒãƒ€ã®å„è¡Œ(list[str])ã¨æ±ºå®šãƒ¢ãƒ¼ãƒ‰("EMERG"/"CAUTION"/"NORMAL")ã‚’è¿”ã™ã€‚
L162     """
L163     bench = "^GSPC"
L164     win = int(os.getenv("BREADTH_CALIB_WIN_DAYS", "600"))
L165     warmup = int(os.getenv("BREADTH_WARMUP_DAYS", "252"))
L166     use_calib = (
L167         os.getenv("BREADTH_USE_CALIB", "true").strip().lower() == "true"
L168     )
L169
L170     tickers = _read_universe_for_breadth()
L171     if not tickers:
L172         raise RuntimeError("breadth: universe empty")
L173
L174     data = yf.download(tickers + [bench], period=f"{win}d", auto_adjust=True, progress=False)
L175     px, spx = data["Close"][tickers], data["Close"][bench]
L176
L177     C_ts = trend_template_breadth_series(px, spx, win_days=win)
L178     if C_ts.empty:
L179         raise RuntimeError("breadth series empty")
L180     base = C_ts.iloc[warmup:] if len(C_ts) > warmup else C_ts
L181     C_full = int(C_ts.iloc[-1])
L182
L183     # åˆ†ä½
L184     q05 = int(
L185         np.nan_to_num(
L186             base.quantile(float(os.getenv("BREADTH_Q_EMERG_IN", "0.05"))),
L187             nan=0.0,
L188         )
L189     )
L190     q20 = int(
L191         np.nan_to_num(
L192             base.quantile(float(os.getenv("BREADTH_Q_EMERG_OUT", "0.20"))),
L193             nan=0.0,
L194         )
L195     )
L196     q60 = int(
L197         np.nan_to_num(
L198             base.quantile(float(os.getenv("BREADTH_Q_WARN_OUT", "0.60"))),
L199             nan=0.0,
L200         )
L201     )
L202
L203     # è‡ªå‹•/æ‰‹å‹•ã®ã—ãã„å€¤
L204     N_G = 12
L205     th_in_rec = max(N_G, q05)
L206     th_out_rec = max(int(np.ceil(1.5 * N_G)), q20)
L207     th_norm_rec = max(3 * N_G, q60)
L208     if use_calib:
L209         th_in, th_out, th_norm, th_src = (
L210             th_in_rec,
L211             th_out_rec,
L212             th_norm_rec,
L213             "è‡ªå‹•",
L214         )
L215     else:
L216         th_in = int(os.getenv("GTT_EMERG_IN", str(N_G)))
L217         th_out = int(os.getenv("GTT_EMERG_OUT", str(int(1.5 * N_G))))
L218         th_norm = int(os.getenv("GTT_CAUTION_OUT", str(3 * N_G)))
L219         th_src = "æ‰‹å‹•"
L220
L221     # ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹
L222     prev = load_mode("NORMAL")
L223     if prev == "EMERG":
L224         mode = (
L225             "EMERG"
L226             if (C_full < th_out)
L227             else ("CAUTION" if (C_full < th_norm) else "NORMAL")
L228         )
L229     elif prev == "CAUTION":
L230         mode = "CAUTION" if (C_full < th_norm) else "NORMAL"
L231     else:
L232         mode = (
L233             "EMERG"
L234             if (C_full < th_in)
L235             else ("CAUTION" if (C_full < th_norm) else "NORMAL")
L236         )
L237     save_mode(mode)
L238
L239     _MODE_JA = {"EMERG": "ç·Šæ€¥", "CAUTION": "è­¦æˆ’", "NORMAL": "é€šå¸¸"}
L240     _MODE_EMOJI = {"EMERG": "ğŸš¨", "CAUTION": "âš ï¸", "NORMAL": "ğŸŸ¢"}
L241     mode_ja, emoji = _MODE_JA.get(mode, mode), _MODE_EMOJI.get(mode, "â„¹ï¸")
L242     eff_days = len(base)
L243
L244     lead_lines = [
L245         f"{emoji} *ç¾åœ¨ãƒ¢ãƒ¼ãƒ‰: {mode_ja}*",
L246         f"ãƒ†ãƒ³ãƒ—ãƒ¬åˆæ ¼æœ¬æ•°: *{C_full}æœ¬*",
L247         f"ã—ãã„å€¤ï¼ˆ{th_src}ï¼‰",
L248         f"  ãƒ»ç·Šæ€¥å…¥ã‚Š: <{th_in}æœ¬",
L249         f"  ãƒ»ç·Šæ€¥è§£é™¤: â‰¥{th_out}æœ¬",
L250         f"  ãƒ»é€šå¸¸å¾©å¸°: â‰¥{th_norm}æœ¬",
L251         f"å‚è€ƒæŒ‡æ¨™ï¼ˆéå»~{win}å–¶æ¥­æ—¥, æœ‰åŠ¹={eff_days}æ—¥ï¼‰",
L252         f"  ãƒ»ä¸‹ä½5%: {q05}æœ¬",
L253         f"  ãƒ»ä¸‹ä½20%: {q20}æœ¬",
L254         f"  ãƒ»60%åˆ†ä½: {q60}æœ¬",
L255     ]
L256     return lead_lines, mode
L257
L258
L259 def build_breadth_header_block() -> str:
L260     """Slack å…ˆé ­ã«å·®ã—è¾¼ã‚€ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯æ–‡å­—åˆ—ã‚’è¿”ã™ã€‚å¤±æ•—æ™‚ã¯ç©ºæ–‡å­—ã€‚"""
L261     try:
L262         lines, _mode = build_breadth_lead_lines()
L263         return "```" + "\n".join(lines) + "```"
L264     except Exception:
L265         return ""
L266
L267
L268 # --- END: breadth port ---
L269
L270 # === Minervini-like sell signals ===
L271 def _yf_df(sym, period="6mo"):
L272     """æ—¥è¶³/MA/å‡ºæ¥é«˜å¹³å‡ã‚’å–å¾—ã€‚æ¬ ææ™‚ã¯ Noneã€‚"""
L273     try:
L274         df = yf.download(sym, period=period, interval="1d", auto_adjust=False, progress=False)
L275         if df is None or df.empty:
L276             return None
L277         return df.dropna().assign(
L278             ma20=lambda d: d["Close"].rolling(20).mean(),
L279             ma50=lambda d: d["Close"].rolling(50).mean(),
L280             vol50=lambda d: d["Volume"].rolling(50).mean(),
L281         )
L282     except Exception:
L283         return None
L284
L285
L286 def _scalar(row, col):
L287     """Series/npã‚¹ã‚«ãƒ©â†’Pythonã‚¹ã‚«ãƒ©åŒ–ï¼ˆNaNã¯NaNã®ã¾ã¾ï¼‰"""
L288     try:
L289         v = row[col]
L290         if hasattr(v, "item"):
L291             try:
L292                 v = v.item()
L293             except Exception:
L294                 pass
L295         return v
L296     except Exception:
L297         return float("nan")
L298
L299
L300 def _is_strict_down(seq):
L301     """æ•°åˆ—ãŒå³å¯†ã«é€£ç¶šã§åˆ‡ã‚Šä¸‹ãŒã£ã¦ã„ã‚‹ã‹ï¼ˆlen>=4ã‚’æƒ³å®šï¼‰ã€‚NaNå«ã¿ã¯Falseã€‚"""
L302     try:
L303         xs = [float(x) for x in seq]
L304         if any(pd.isna(x) for x in xs) or len(xs) < 4:
L305             return False
L306         return all(b < a for a, b in zip(xs[:-1], xs[1:]))
L307     except Exception:
L308         return False
L309
L310
L311 def _signals_for_day(df, idx):
L312     """df.loc[idx] 1æ—¥åˆ†ã«å¯¾ã—ã‚·ã‚°ãƒŠãƒ«é…åˆ—ã‚’è¿”ã™ï¼ˆå€¤å‹•ã/å‡ºæ¥é«˜ãƒ™ãƒ¼ã‚¹ã®ã¿ï¼‰ã€‚"""
L313     try:
L314         sig = []
L315         d = df.loc[idx]
L316         close = _scalar(d, "Close")
L317         open_ = _scalar(d, "Open")
L318         ma20 = _scalar(d, "ma20")
L319         ma50 = _scalar(d, "ma50")
L320         vol = _scalar(d, "Volume")
L321         vol50 = _scalar(df.iloc[-1], "vol50")
L322         if any(pd.isna(x) for x in (close, open_, vol, vol50)):
L323             return sig
L324         if pd.notna(ma20) and close < ma20:
L325             sig.append("20DMAâ†“")
L326         if pd.notna(ma50) and close < ma50 and vol > 1.5 * vol50:
L327             sig.append("50DMAâ†“(å¤§å•†ã„)")
L328
L329         last4 = df.loc[:idx].tail(4)
L330         lows_desc = _is_strict_down(last4["Low"].tolist())
L331         last10 = df.loc[:idx].tail(10)
L332         reds = int((last10["Close"] < last10["Open"]).sum())
L333         if lows_desc or reds > 5:
L334             sig.append("é€£ç¶šå®‰å€¤/é™°ç·šå„ªå‹¢")
L335
L336         ups = int((last10["Close"] > last10["Open"]).sum())
L337         if ups >= 7:
L338             sig.append("ä¸Šã’åé‡(>70%)")
L339
L340         last15 = df.loc[:idx].tail(15)
L341         base0 = _scalar(last15.iloc[0], "Close") if len(last15) > 0 else float("nan")
L342         if pd.notna(base0) and base0 != 0 and (close / base0 - 1) >= 0.25:
L343             sig.append("+25%/15æ—¥å†…")
L344
L345         if len(df.loc[:idx]) >= 2:
L346     
```
# === Chat Paste Pack ===
# Repo: dakara32/GPT_Code @ main
# Files: config.py, factor.py, scorer.py, .github/workflows/weekly-report.yml, documents/README.md, documents/factor_design.md
# 作成日時: 2025-09-19 18:54:43 (JST)
# 使い方: 下のチャンクを順に貼ればこのチャットで全体把握できます。
# 注記: 各ファイルは個別に L1.. で行番号付与。
---

## <config.py>
```text
L1 # 共通設定（factor / drift から参照）
L2 from dataclasses import dataclass
L3
L4 TOTAL_TARGETS = 20
L5
L6 # 基準のバケット数（NORMAL）
L7 COUNTS_BASE = {"G": 12, "D": 8}
L8
L9 # モード別の推奨バケット数
L10 COUNTS_BY_MODE = {
L11     "NORMAL": {"G": 12, "D": 8},
L12     "CAUTION": {"G": 10, "D": 8},
L13     "EMERG": {"G": 8,  "D": 8},
L14 }
L15
L16 # モード別のドリフト閾値（%）
L17 DRIFT_THRESHOLD_BY_MODE = {"NORMAL": 12, "CAUTION": 14, "EMERG": float("inf")}
L18
L19 # モード別のTS（基本幅, 小数=割合）
L20 TS_BASE_BY_MODE = {"NORMAL": 0.15, "CAUTION": 0.13, "EMERG": 0.10}
L21 # 利益到達(+30/+60/+100%)時の段階タイト化（ポイント差）
L22 TS_STEP_DELTAS_PT = (3, 6, 8)
L23
L24 # Breadthの校正は N_G に連動（緊急解除=ceil(1.5*N_G), 通常復帰=3*N_G）
L25 N_G = COUNTS_BASE["G"]
L26 N_D = COUNTS_BASE["D"]
L27
```

## <factor.py>
```text
L1 '''ROLE: Orchestration ONLY（外部I/O・SSOT・Slack出力）, 計算は scorer.py'''
L2 # === NOTE: 機能・入出力・ログ文言・例外挙動は不変。安全な短縮（import統合/複数代入/内包表記/メソッドチェーン/一行化/空行圧縮など）のみ適用 ===
L3 BONUS_COEFF = 0.55  # 推奨: 攻め=0.45 / 中庸=0.55 / 守り=0.65
L4 SWAP_DELTA_Z = 0.15   # 僅差判定: σの15%。(緩め=0.10 / 標準=0.15 / 固め=0.20)
L5 SWAP_KEEP_BUFFER = 3  # n_target+この順位以内の現行は保持。(粘り弱=2 / 標準=3 / 粘り強=4〜5)
L6 import os, time, requests
L7 import logging
L8 from time import perf_counter
L9 from dataclasses import dataclass
L10 from typing import Any, Dict, List, Tuple
L11 from concurrent.futures import ThreadPoolExecutor
L12 import numpy as np
L13 import pandas as pd
L14 import yfinance as yf
L15 from scipy.stats import zscore  # used via scorer
L16 from scorer import Scorer, ttm_div_yield_portfolio, _log
L17 import config
L18
L19 # その他
L20 debug_mode, FINNHUB_API_KEY = True, os.environ.get("FINNHUB_API_KEY")
L21
L22 logger = logging.getLogger(__name__)
L23 if debug_mode:
L24     logging.basicConfig(level=logging.INFO, force=True)
L25 else:
L26     logging.basicConfig(level=logging.WARNING, force=True)
L27
L28 class T:
L29     t = perf_counter()
L30     log = staticmethod(lambda tag: (lambda now=perf_counter(): (print(f"[T] {tag}: {now - T.t:.2f}s"), setattr(T, "t", now))[-1])())
L31
L32 T.log("start")
L33
L34 # === ユニバースと定数（冒頭に固定） ===
L35 exist, cand = [pd.read_csv(f, header=None)[0].tolist() for f in ("current_tickers.csv","candidate_tickers.csv")]
L36 T.log(f"csv loaded: exist={len(exist)} cand={len(cand)}")
L37 CAND_PRICE_MAX, bench = 450, '^GSPC'  # 価格上限・ベンチマーク
L38 N_G, N_D = config.N_G, config.N_D  # G/D枠サイズ（NORMAL基準: G12/D8）
L39 g_weights = {'GROWTH_F':0.35,'MOM':0.55,'VOL':-0.10}
L40 D_BETA_MAX = float(os.environ.get("D_BETA_MAX", "0.8"))
L41 FILTER_SPEC = {"G":{"pre_mask":["trend_template"]},"D":{"pre_filter":{"beta_max":D_BETA_MAX}}}
L42 D_weights = {'QAL':0.1,'YLD':0.3,'VOL':-0.5,'TRD':0.1}
L43 _fmt_w = lambda w: " ".join(f"{k}{int(v*100)}" for k, v in w.items())
L44
L45 # DRRS 初期プール・各種パラメータ
L46 corrM = 45
L47 DRRS_G, DRRS_D = dict(lookback=252,n_pc=3,gamma=1.2,lam=0.68,eta=0.8), dict(lookback=504,n_pc=4,gamma=0.8,lam=0.85,eta=0.5)
L48 DRRS_SHRINK = 0.10  # 残差相関の対角シュリンク（基礎）
L49
L50 # クロス相関ペナルティ（未定義なら設定）
L51 try: CROSS_MU_GD
L52 except NameError: CROSS_MU_GD = 0.40  # 推奨 0.35–0.45（lam=0.85想定）
L53
L54 # 出力関連
L55 RESULTS_DIR = "results"
L56 os.makedirs(RESULTS_DIR, exist_ok=True)
L57
L58 # === 共有DTO（クラス間I/O契約）＋ Config ===
L59 @dataclass(frozen=True)
L60 class InputBundle:
L61     # Input → Scorer で受け渡す素材（I/O禁止の生データ）
L62     cand: List[str]
L63     tickers: List[str]
L64     bench: str
L65     data: pd.DataFrame              # yfinance download結果（'Close','Volume'等の階層列）
L66     px: pd.DataFrame                # data['Close']
L67     spx: pd.Series                  # data['Close'][bench]
L68     tickers_bulk: object            # yfinance.Tickers
L69     info: Dict[str, dict]           # yfinance info per ticker
L70     eps_df: pd.DataFrame            # ['eps_ttm','eps_q_recent',...]
L71     fcf_df: pd.DataFrame            # ['fcf_ttm', ...]
L72     returns: pd.DataFrame           # px[tickers].pct_change()
L73
L74 @dataclass(frozen=True)
L75 class FeatureBundle:
L76     df: pd.DataFrame
L77     df_z: pd.DataFrame
L78     g_score: pd.Series
L79     d_score_all: pd.Series
L80     missing_logs: pd.DataFrame
L81     df_full: pd.DataFrame | None = None
L82     df_full_z: pd.DataFrame | None = None
L83     scaler: Any | None = None
L84
L85 @dataclass(frozen=True)
L86 class SelectionBundle:
L87     resG: dict
L88     resD: dict
L89     top_G: List[str]
L90     top_D: List[str]
L91     init_G: List[str]
L92     init_D: List[str]
L93
L94 @dataclass(frozen=True)
L95 class WeightsConfig:
L96     g: Dict[str,float]
L97     d: Dict[str,float]
L98
L99 @dataclass(frozen=True)
L100 class DRRSParams:
L101     corrM: int
L102     shrink: float
L103     G: Dict[str,float]   # lookback, n_pc, gamma, lam, eta
L104     D: Dict[str,float]
L105     cross_mu_gd: float
L106
L107 @dataclass(frozen=True)
L108 class PipelineConfig:
L109     weights: WeightsConfig
L110     drrs: DRRSParams
L111     price_max: float
L112     debug_mode: bool = False
L113
L114 # === 共通ユーティリティ（複数クラスで使用） ===
L115 # (unused local utils removed – use scorer.py versions if needed)
L116
L117 _env_true = lambda name, default=False: (os.getenv(name) or str(default)).strip().lower() == "true"
L118
L119 def _post_slack(payload: dict):
L120     url = os.getenv("SLACK_WEBHOOK_URL")
L121     if not url: print("⚠️ SLACK_WEBHOOK_URL 未設定"); return
L122     try:
L123         requests.post(url, json=payload).raise_for_status()
L124     except Exception as e:
L125         print(f"⚠️ Slack通知エラー: {e}")
L126
L127 def _slack_send_text_chunks(url: str, text: str, chunk: int = 2800) -> None:
L128     """Slackへテキストを分割送信（コードブロック形式）。"""
L129
L130     def _post_text(payload: str) -> None:
L131         try:
L132             resp = requests.post(url, json={"text": payload})
L133             print(f"[DBG] debug_post status={getattr(resp, 'status_code', None)} size={len(payload)}")
L134             if resp is not None:
L135                 resp.raise_for_status()
L136         except Exception as e:
L137             print(f"[ERR] debug_post_failed: {e}")
L138
L139     body = str(text or "").strip()
L140     if not body:
L141         print("[DBG] skip debug send: empty body")
L142         return
L143
L144     lines = body.splitlines()
L145     block: list[str] = []
L146     block_len = 0
L147
L148     def _flush() -> None:
L149         nonlocal block, block_len
L150         if not block:
L151             return
L152         payload = "```" + "\n".join(block) + "```"
L153         _post_text(payload)
L154         block, block_len = [], 0
L155
L156     for raw in lines:
L157         line = raw or ""
L158         while len(line) > chunk:
L159             head, line = line[:chunk], line[chunk:]
L160             _flush()
L161             _post_text("```" + head + "```")
L162         add_len = len(line) if not block else len(line) + 1
L163         if block and block_len + add_len > chunk:
L164             _flush()
L165             add_len = len(line)
L166         block.append(line)
L167         block_len += add_len
L168     _flush()
L169
L170 def _disjoint_keepG(top_G, top_D, poolD):
L171     """G重複をDから除去し、poolDで順次補充（枯渇時は元銘柄維持）。"""
L172     used, D, i = set(top_G), list(top_D), 0
L173     for j, t in enumerate(D):
L174         if t in used:
L175             while i < len(poolD) and (poolD[i] in used or poolD[i] in D):
L176                 i += 1
L177             if i < len(poolD):
L178                 D[j] = poolD[i]; used.add(D[j]); i += 1
L179     return top_G, D
L180
L181
L182 def _sticky_keep_current(agg: pd.Series, pick: list[str], incumbents: list[str],
L183                          n_target: int, delta_z: float, keep_buffer: int) -> list[str]:
L184     import pandas as pd, numpy as np
L185     sel = list(pick)
L186     if not sel: return sel
L187     ranked_sel = agg.reindex(sel).sort_values(ascending=False)
L188     kth = ranked_sel.iloc[min(len(sel), n_target)-1]
L189     sigma = float(agg.std()) if pd.notna(agg.std()) else 0.0
L190     thresh = kth - delta_z * sigma
L191     ranked_all = agg.sort_values(ascending=False)
L192     cand = [t for t in incumbents if (t not in sel) and (t in agg.index)]
L193     for t in cand:
L194         within_score = (pd.notna(agg[t]) and agg[t] >= thresh)
L195         within_rank  = (t in ranked_all.index) and (ranked_all.index.get_loc(t) < n_target + keep_buffer)
L196         if within_score or within_rank:
L197             non_inc = [x for x in sel if x not in incumbents]
L198             if not non_inc: break
L199             weakest = min(non_inc, key=lambda x: agg.get(x, -np.inf))
L200             if weakest in sel and agg.get(t, -np.inf) >= agg.get(weakest, -np.inf):
L201                 sel.remove(weakest); sel.append(t)
L202     if len(sel) > n_target:
L203         sel = sorted(sel, key=lambda x: agg.get(x, -1e9), reverse=True)[:n_target]
L204     return sel
L205
L206
L207 # === Input：外部I/Oと前処理（CSV/API・欠損補完） ===
L208 class Input:
L209     def __init__(self, cand, exist, bench, price_max, finnhub_api_key=None):
L210         self.cand, self.exist, self.bench, self.price_max = cand, exist, bench, price_max
L211         self.api_key = finnhub_api_key or os.environ.get("FINNHUB_API_KEY")
L212
L213     # ---- （Input専用）EPS補完・FCF算出系 ----
L214     @staticmethod
L215     def _sec_headers():
L216         mail = (os.getenv("SEC_CONTACT_EMAIL") or "yasonba55@gmail.com").strip()
L217         app = (os.getenv("SEC_APP_NAME") or "FactorBot/1.0").strip()
L218         return {
L219             "User-Agent": f"{app} ({mail})",
L220             "From": mail,
L221             "Accept": "application/json",
L222         }
L223
L224     @staticmethod
L225     def _sec_get(url: str, retries: int = 3, backoff: float = 0.5):
L226         for i in range(retries):
L227             r = requests.get(url, headers=Input._sec_headers(), timeout=20)
L228             if r.status_code in (429, 503, 403):
L229                 time.sleep(min(2 ** i * backoff, 8.0))
L230                 continue
L231             r.raise_for_status()
L232             return r.json()
L233         r.raise_for_status()
L234
L235     @staticmethod
L236     def _sec_ticker_map():
L237         j = Input._sec_get("https://data.sec.gov/api/xbrl/company_tickers.json")
L238         mp = {}
L239         for _, v in (j or {}).items():
L240             try:
L241                 mp[str(v["ticker"]).upper()] = f"{int(v['cik_str']):010d}"
L242             except Exception:
L243                 pass
L244         return mp
L245
L246     # --- 追加: ADR/OTC向けの簡易正規化（末尾Y/F, ドット等） ---
L247     @staticmethod
L248     def _normalize_ticker(sym: str) -> list[str]:
L249         s = (sym or "").upper().strip()
L250         # 追加: 先頭の$や全角の記号を除去
L251         s = s.lstrip("$").replace("＄", "").replace("．", ".").replace("－", "-")
L252         cand: list[str] = []
L253
L254         def add(x: str) -> None:
L255             if x and x not in cand:
L256                 cand.append(x)
L257
L258         # 1) 原文を最優先（SECは BRK.B, BF.B など . を正式採用）
L259         add(s)
L260         # 2) Yahoo系バリアント（. と - の揺れを相互に）
L261         if "." in s:
L262             add(s.replace(".", "-"))
L263             add(s.replace(".", ""))
L264         if "-" in s:
L265             add(s.replace("-", "."))
L266             add(s.replace("-", ""))
L267         # 3) ドット・ハイフン・ピリオド無し版（最後の保険）
L268         add(s.replace("-", "").replace(".", ""))
L269         # 4) ADR簡易：末尾Y/Fの除去（SECマップは本体ティッカーを持つことがある）
L270         if len(s) >= 2 and s[-1] in {"Y", "F"}:
L271             add(s[:-1])
L272         return cand
L273
L274     @staticmethod
L275     def _sec_companyfacts(cik: str):
L276         return Input._sec_get(f"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json")
L277
L278     @staticmethod
L279     def _units_for_tags(facts: dict, namespaces: list[str], tags: list[str]) -> list[dict]:
L280         """facts から namespace/tag を横断して units 配列を収集（存在順に連結）。"""
L281         out: list[dict] = []
L282         facts = facts or {}
L283         for ns in namespaces:
L284             try:
L285                 node = facts.get("facts", {}).get(ns, {})
L286             except Exception:
L287                 node = {}
L288             for tg in tags:
L289                 try:
L290                     units = node[tg]["units"]
L291                 except Exception:
L292                     continue
L293                 picks: list[dict] = []
L294                 if "USD/shares" in units:
L295                     picks.extend(list(units["USD/shares"]))
L296                 if "USD" in units:
L297                     picks.extend(list(units["USD"]))
L298                 if not picks:
L299                     for arr in units.values():
L300                         picks.extend(list(arr))
L301                 out.extend(picks)
L302         return out
L303
L304     @staticmethod
L305     def _only_quarterly(arr: list[dict]) -> list[dict]:
L306         """companyfactsの混在配列から『四半期』だけを抽出。
L307
L308         - frame に "Q" を含む（例: CY2024Q2I）
L309         - fp が Q1/Q2/Q3/Q4
L310         - form が 10-Q/10-Q/A/6-K
L311         """
L312         if not arr:
L313             return []
L314         q_forms = {"10-Q", "10-Q/A", "6-K"}
L315
L316         def is_q(x: dict) -> bool:
L317             frame = (x.get("frame") or "").upper()
L318             fp = (x.get("fp") or "").upper()
L319             form = (x.get("form") or "").upper()
L320             return ("Q" in frame) or (fp in {"Q1", "Q2", "Q3", "Q4"}) or (form in q_forms)
L321
L322         out = [x for x in arr if is_q(x)]
L323         out.sort(key=lambda x: (x.get("end") or ""), reverse=True)
L324         return out
L325
L326     @staticmethod
L327     def _series_from_facts_with_dates(arr, key_val="val", key_dt="end", normalize=float):
L328         """companyfactsアイテム配列から (date,value) を返す。dateはYYYY-MM-DDを想定。"""
L329         out: List[Tuple[str, float]] = []
L330         for x in (arr or []):
L331             try:
L332                 v = x.get(key_val)
L333                 d = x.get(key_dt)
L334                 if d is None:
L335                     continue
L336                 out.append((str(d), normalize(v) if v is not None else float("nan")))
L337             except Exception:
L338                 continue
L339         # end(=日付)の降順にソート（最新→古い）
L340         out.sort(key=lambda t: t[0], reverse=True)
L341         return out
L342
L343     def fetch_eps_rev_from_sec(self, tickers: list[str]) -> dict:
L344         out = {}
L345         t2cik = self._sec_ticker_map()
L346         n_map = n_rev = n_eps = 0
L347         miss_map: list[str] = []
L348         miss_facts: list[str] = []
L349         for t in tickers:
L350             candidates: list[str] = []
L351
L352             def add(key: str) -> None:
L353                 if key and key not in candidates:
L354                     candidates.append(key)
L355
L356             add((t or "").upper())
L357             for key in self._normalize_ticker(t):
L358                 add(key)
L359
L360             cik = None
L361             for key in candidates:
L362                 cik = t2cik.get(key)
L363                 if cik:
L364                     break
L365             if not cik:
L366                 out[t] = {}
L367                 miss_map.append(t)
L368                 continue
L369             try:
L370                 j = self._sec_companyfacts(cik)
L371                 facts = j or {}
L372                 rev_tags = [
L373                     "Revenues",
L374                     "RevenueFromContractWithCustomerExcludingAssessedTax",
L375                     "SalesRevenueNet",
L376                     "SalesRevenueGoodsNet",
L377                     "SalesRevenueServicesNet",
L378                     "Revenue",
L379                 ]
L380                 eps_tags = [
L381                     "EarningsPerShareDiluted",
L382                     "EarningsPerShareBasicAndDiluted",
L383                     "EarningsPerShare",
L384                     "EarningsPerShareBasic",
L385                 ]
L386                 rev_arr = self._units_for_tags(facts, ["us-gaap", "ifrs-full"], rev_tags)
L387                 eps_arr = self._units_for_tags(facts, ["us-gaap", "ifrs-full"], eps_tags)
L388                 rev_q_items = self._only_quarterly(rev_arr)
L389                 eps_q_items = self._only_quarterly(eps_arr)
L390                 # (date,value) で取得
L391                 rev_pairs = self._series_from_facts_with_dates(rev_q_items)
L392                 eps_pairs = self._series_from_facts_with_dates(eps_q_items)
L393                 rev_vals = [v for (_d, v) in rev_pairs]
L394                 eps_vals = [v for (_d, v) in eps_pairs]
L395                 rev_q = float(rev_vals[0]) if rev_vals else float("nan")
L396                 eps_q = float(eps_vals[0]) if eps_vals else float("nan")
L397                 rev_ttm = float(sum([v for v in rev_vals[:4] if v == v])) if rev_vals else float("nan")
L398                 eps_ttm = float(sum([v for v in eps_vals[:4] if v == v])) if eps_vals else float("nan")
L399                 out[t] = {
L400                     "eps_q_recent": eps_q,
L401                     "eps_ttm": eps_ttm,
L402                     "rev_q_recent": rev_q,
L403                     "rev_ttm": rev_ttm,
L404                     # 後段でDatetimeIndex化できるよう (date,value) を保持。値だけの互換キーも残す。
L405                     "eps_q_series_pairs": eps_pairs[:16],
L406                     "rev_q_series_pairs": rev_pairs[:16],
L407                     "eps_q_series": eps_vals[:16],
L408                     "rev_q_series": rev_vals[:16],
L409                 }
L410                 n_map += 1
L411                 if rev_vals:
L412                     n_rev += 1
L413                 if eps_vals:
L414                     n_eps += 1
L415             except Exception:
L416                 out[t] = {}
L417                 miss_facts.append(t)
L418             time.sleep(0.30)
L419         # 取得サマリをログ（Actionsで確認しやすいよう print）
L420         try:
L421             total = len(tickers)
L422             print(f"[SEC] map={n_map}/{total}  rev_q_hit={n_rev}  eps_q_hit={n_eps}")
L423             # デバッグ: 取得本数の分布（先頭のみ）
L424             try:
L425                 lens = [len((out.get(t, {}) or {}).get("rev_q_series", [])) for t in tickers]
L426                 print(f"[SEC] rev_q_series length: min={min(lens) if lens else 0} "
L427                       f"p25={np.percentile(lens,25) if lens else 0} median={np.median(lens) if lens else 0} "
L428                       f"p75={np.percentile(lens,75) if lens else 0} max={max(lens) if lens else 0}")
L429             except Exception:
L430                 pass
L431             if miss_map:
L432                 print(f"[SEC] no CIK map: {len(miss_map)} (サンプル例) {miss_map[:20]}")
L433             if miss_facts:
L434                 print(f"[SEC] CIKあり だが対象factなし: {len(miss_facts)} (サンプル例) {miss_facts[:20]}")
L435         except Exception:
L436             pass
L437         return out
L438
L439     def sec_dryrun_sample(self, tickers: list[str] | None = None) -> None:
L440         if not _env_true("SEC_DRYRUN_SAMPLE", False):
L441             return
L442         sample = tickers or ["BRK.B", "BF.B", "GOOGL", "META", "UBER", "PBR.A", "TSM", "NARI", "EVBN", "SWAV"]
L443         print(f"[SEC-DRYRUN] sample tickers: {sample}")
L444         try:
L445             t2cik = self._sec_ticker_map()
L446             hits = 0
L447             for sym in sample:
L448                 candidates: list[str] = []
L449
L450                 def add(key: str) -> None:
L451                     if key and key not in candidates:
L452                         candidates.append(key)
L453
L454                 add((sym or "").upper())
L455                 for alt in self._normalize_ticker(sym):
L456                     add(alt)
L457                 if any(t2cik.get(key) for key in candidates):
L458                     hits += 1
L459             sec_data = self.fetch_eps_rev_from_sec(sample)
L460             rev_hits = sum(1 for v in sec_data.values() if v.get("rev_q_series"))
L461             eps_hits = sum(1 for v in sec_data.values() if v.get("eps_q_series"))
L462             total = len(sample)
L463             print(f"[SEC-DRYRUN] CIK map hit: {hits}/{total}  rev_q_series hits: {rev_hits}  eps_q_series hits: {eps_hits}")
L464         except Exception as e:
L465             print(f"[SEC-DRYRUN] error: {e}")
L466     @staticmethod
L467     def impute_eps_ttm(df: pd.DataFrame, ttm_col: str="eps_ttm", q_col: str="eps_q_recent", out_col: str|None=None) -> pd.DataFrame:
L468         out_col = out_col or ttm_col; df = df.copy(); df["eps_imputed"] = False
L469         cand = df[q_col]*4; ok = df[ttm_col].isna() & cand.replace([np.inf,-np.inf], np.nan).notna()
L470         df.loc[ok, out_col], df.loc[ok,"eps_imputed"] = cand[ok], True; return df
L471
L472     _CF_ALIASES = {"cfo":["Operating Cash Flow","Total Cash From Operating Activities"], "capex":["Capital Expenditure","Capital Expenditures"]}
L473
L474     @staticmethod
L475     def _pick_row(df: pd.DataFrame, names: list[str]) -> pd.Series|None:
L476         if df is None or df.empty: return None
L477         idx_lower={str(i).lower():i for i in df.index}
L478         for n in names:
L479             k=n.lower()
L480             if k in idx_lower: return df.loc[idx_lower[k]]
L481         return None
L482
L483     @staticmethod
L484     def _sum_last_n(s: pd.Series|None, n: int) -> float|None:
L485         if s is None or s.empty: return None
L486         v=s.dropna().astype(float); return None if v.empty else v.iloc[:n].sum()
L487
L488     @staticmethod
L489     def _latest(s: pd.Series|None) -> float|None:
L490         if s is None or s.empty: return None
L491         v=s.dropna().astype(float); return v.iloc[0] if not v.empty else None
L492
L493     def fetch_cfo_capex_ttm_yf(self, tickers: list[str]) -> pd.DataFrame:
L494         from concurrent.futures import ThreadPoolExecutor, as_completed
L495         pick, sumn, latest, aliases = self._pick_row, self._sum_last_n, self._latest, self._CF_ALIASES
L496
L497         def one(t: str):
L498             try:
L499                 tk = yf.Ticker(t)  # ★ セッションは渡さない（YFがcurl_cffiで管理）
L500                 qcf = tk.quarterly_cashflow
L501                 cfo_q, capex_q = pick(qcf, aliases["cfo"]), pick(qcf, aliases["capex"])
L502                 fcf_q = pick(qcf, ["Free Cash Flow","FreeCashFlow","Free cash flow"])
L503                 cfo, capex, fcf = sumn(cfo_q,4), sumn(capex_q,4), sumn(fcf_q,4)
L504                 if any(v is None for v in (cfo, capex, fcf)):
L505                     acf = tk.cashflow
L506                     if cfo   is None: cfo   = latest(pick(acf, aliases["cfo"]))
L507                     if capex is None: capex = latest(pick(acf, aliases["capex"]))
L508                     if fcf   is None: fcf   = latest(pick(acf, ["Free Cash Flow","FreeCashFlow","Free cash flow"]))
L509             except Exception as e:
L510                 print(f"[warn] yf financials error: {t}: {e}"); cfo=capex=fcf=None
L511             n=np.nan
L512             return {"ticker":t,
L513                     "cfo_ttm_yf":   n if cfo   is None else cfo,
L514                     "capex_ttm_yf": n if capex is None else capex,
L515                     "fcf_ttm_yf_direct": n if fcf is None else fcf}
L516
L517         rows, mw = [], int(os.getenv("FIN_THREADS","8"))
L518         with ThreadPoolExecutor(max_workers=mw) as ex:
L519             rows=[f.result() for f in as_completed(ex.submit(one,t) for t in tickers)]
L520         return pd.DataFrame(rows).set_index("ticker")
L521
L522     _FINN_CFO_KEYS = ["netCashProvidedByOperatingActivities","netCashFromOperatingActivities","cashFlowFromOperatingActivities","operatingCashFlow"]
L523     _FINN_CAPEX_KEYS = ["capitalExpenditure","capitalExpenditures","purchaseOfPPE","investmentsInPropertyPlantAndEquipment"]
L524
L525     @staticmethod
L526     def _first_key(d: dict, keys: list[str]):
L527         for k in keys:
L528             if k in d and d[k] is not None: return d[k]
L529         return None
L530
L531     @staticmethod
L532     def _finn_get(session: requests.Session, url: str, params: dict, retries: int=3, sleep_s: float=0.5):
L533         for i in range(retries):
L534             r = session.get(url, params=params, timeout=15)
L535             if r.status_code==429: time.sleep(min(2**i*sleep_s,4.0)); continue
L536             r.raise_for_status(); return r.json()
L537         r.raise_for_status()
L538
L539     def fetch_cfo_capex_ttm_finnhub(self, tickers: list[str], api_key: str|None=None) -> pd.DataFrame:
L540         api_key = api_key or os.getenv("FINNHUB_API_KEY")
L541         if not api_key: raise ValueError("Finnhub API key not provided. Set FINNHUB_API_KEY or pass api_key=")
L542         base, s, rows = "https://finnhub.io/api/v1", requests.Session(), []
L543         for sym in tickers:
L544             cfo_ttm = capex_ttm = None
L545             try:
L546                 j = self._finn_get(s, f"{base}/stock/cash-flow", {"symbol":sym,"frequency":"quarterly","limit":8,"token":api_key})
L547                 arr = j.get("cashFlow") or []; cfo_vals, capex_vals = [], []
L548                 for item in arr[:4]:
L549                     cfo_vals.append(self._first_key(item,self._FINN_CFO_KEYS)); capex_vals.append(self._first_key(item,self._FINN_CAPEX_KEYS))
L550                 if any(v is not None for v in cfo_vals): cfo_ttm = float(np.nansum([np.nan if v is None else float(v) for v in cfo_vals]))
L551                 if any(v is not None for v in capex_vals): capex_ttm = float(np.nansum([np.nan if v is None else float(v) for v in capex_vals]))
L552             except Exception: pass
L553             if cfo_ttm is None or capex_ttm is None:
L554                 try:
L555                     j = self._finn_get(s, f"{base}/stock/cash-flow", {"symbol":sym,"frequency":"annual","limit":1,"token":api_key})
L556                     arr = j.get("cashFlow") or []
L557                     if arr:
L558                         item0 = arr[0]
L559                         if cfo_ttm is None:
L560                             v = self._first_key(item0,self._FINN_CFO_KEYS)
L561                             if v is not None: cfo_ttm = float(v)
L562                         if capex_ttm is None:
L563                             v = self._first_key(item0,self._FINN_CAPEX_KEYS)
L564                             if v is not None: capex_ttm = float(v)
L565                 except Exception: pass
L566             rows.append({"ticker":sym,"cfo_ttm_fh":np.nan if cfo_ttm is None else cfo_ttm,"capex_ttm_fh":np.nan if capex_ttm is None else capex_ttm})
L567         return pd.DataFrame(rows).set_index("ticker")
L568
L569     def compute_fcf_with_fallback(self, tickers: list[str], finnhub_api_key: str|None=None) -> pd.DataFrame:
L570         yf_df = self.fetch_cfo_capex_ttm_yf(tickers)
L571         T.log("financials (yf) done")
L572         miss_mask = yf_df[["cfo_ttm_yf","capex_ttm_yf","fcf_ttm_yf_direct"]].isna().any(axis=1)
L573         need = yf_df.index[miss_mask].tolist(); print(f"[T] yf financials missing: {len(need)} {need[:10]}{'...' if len(need)>10 else ''}")
L574         if need:
L575             fh_df = self.fetch_cfo_capex_ttm_finnhub(need, api_key=finnhub_api_key)
L576             df = yf_df.join(fh_df, how="left")
L577             for col_yf, col_fh in [("cfo_ttm_yf","cfo_ttm_fh"),("capex_ttm_yf","capex_ttm_fh")]:
L578                 df[col_yf] = df[col_yf].fillna(df[col_fh])
L579             print("[T] financials (finnhub) done (fallback only)")
L580         else:
L581             df = yf_df.assign(cfo_ttm_fh=np.nan, capex_ttm_fh=np.nan)
L582             print("[T] financials (finnhub) skipped (no missing)")
L583         df["cfo_ttm"]  = df["cfo_ttm_yf"].where(df["cfo_ttm_yf"].notna(), df["cfo_ttm_fh"])
L584         df["capex_ttm"] = df["capex_ttm_yf"].where(df["capex_ttm_yf"].notna(), df["capex_ttm_fh"])
L585         cfo, capex = pd.to_numeric(df["cfo_ttm"], errors="coerce"), pd.to_numeric(df["capex_ttm"], errors="coerce").abs()
L586         fcf_calc = cfo - capex
L587         fcf_direct = pd.to_numeric(df.get("fcf_ttm_yf_direct"), errors="coerce")
L588         df["fcf_ttm"] = fcf_calc.where(fcf_calc.notna(), fcf_direct)
L589         df["cfo_source"]  = np.where(df["cfo_ttm_yf"].notna(),"yfinance",np.where(df["cfo_ttm_fh"].notna(),"finnhub",""))
L590         df["capex_source"] = np.where(df["capex_ttm_yf"].notna(),"yfinance",np.where(df["capex_ttm_fh"].notna(),"finnhub",""))
L591         df["fcf_imputed"] = df[["cfo_ttm","capex_ttm"]].isna().any(axis=1) & df["fcf_ttm"].notna()
L592         cols = ["cfo_ttm_yf","capex_ttm_yf","cfo_ttm_fh","capex_ttm_fh","cfo_ttm","capex_ttm","fcf_ttm","fcf_ttm_yf_direct","cfo_source","capex_source","fcf_imputed"]
L593         return df[cols].sort_index()
L594
L595     def _build_eps_df(self, tickers, tickers_bulk, info, sec_map: dict | None = None):
L596         eps_rows=[]
L597         for t in tickers:
L598             info_t = info[t]
L599             sec_t = (sec_map or {}).get(t, {})
L600             eps_ttm = sec_t.get("eps_ttm", info_t.get("trailingEps", np.nan))
L601             eps_q = sec_t.get("eps_q_recent", np.nan)
L602             try:
L603                 qearn, so = tickers_bulk.tickers[t].quarterly_earnings, info_t.get("sharesOutstanding")
L604                 if so and qearn is not None and not qearn.empty and "Earnings" in qearn.columns:
L605                     eps_ttm_q = qearn["Earnings"].head(4).sum()/so
L606                     if pd.notna(eps_ttm_q) and (pd.isna(eps_ttm) or (abs(eps_ttm)>0 and abs(eps_ttm/eps_ttm_q)>3)): eps_ttm = eps_ttm_q
L607                     if pd.isna(eps_q):
L608                         eps_q = qearn["Earnings"].iloc[-1]/so
L609             except Exception: pass
L610             rev_ttm = sec_t.get("rev_ttm", np.nan)
L611             rev_q = sec_t.get("rev_q_recent", np.nan)
L612             if (not sec_t) or pd.isna(rev_ttm):
L613                 try:
L614                     tk = tickers_bulk.tickers[t]
L615                     qfin = getattr(tk, "quarterly_financials", None)
L616                     if qfin is not None and not qfin.empty:
L617                         idx_lower = {str(i).lower(): i for i in qfin.index}
L618                         rev_idx = None
L619                         for name in ("Total Revenue", "TotalRevenue"):
L620                             key = name.lower()
L621                             if key in idx_lower:
L622                                 rev_idx = idx_lower[key]
L623                                 break
L624                         if rev_idx is not None:
L625                             rev_series = pd.to_numeric(qfin.loc[rev_idx], errors="coerce").dropna()
L626                             if not rev_series.empty:
L627                                 rev_ttm_yf = float(rev_series.head(4).sum())
L628                                 if pd.isna(rev_ttm):
L629                                     rev_ttm = rev_ttm_yf
L630                                 if pd.isna(rev_q):
L631                                     rev_q = float(rev_series.iloc[0])
L632                 except Exception:
L633                     pass
L634             eps_rows.append({"ticker":t,"eps_ttm":eps_ttm,"eps_q_recent":eps_q,"rev_ttm":rev_ttm,"rev_q_recent":rev_q})
L635         return self.impute_eps_ttm(pd.DataFrame(eps_rows).set_index("ticker"))
L636
L637     def prepare_data(self):
L638         """Fetch price and fundamental data for all tickers."""
L639         self.sec_dryrun_sample()
L640         cand_info = yf.Tickers(" ".join(self.cand)); cand_prices = {}
L641         for t in self.cand:
L642             try: cand_prices[t] = cand_info.tickers[t].fast_info.get("lastPrice", np.inf)
L643             except Exception as e: print(f"{t}: price fetch failed ({e})"); cand_prices[t] = np.inf
L644         cand_f = [t for t,p in cand_prices.items() if p<=self.price_max]
L645         T.log("price cap filter done (CAND_PRICE_MAX)")
L646         # 入力ティッカーの重複を除去し、現行→候補の順序を維持
L647         tickers = list(dict.fromkeys(self.exist + cand_f))
L648         T.log(f"universe prepared: unique={len(tickers)} bench={self.bench}")
L649         data = yf.download(tickers + [self.bench], period="600d",
L650                            auto_adjust=True, progress=False, threads=False)
L651         T.log("yf.download done")
L652         px = data["Close"].dropna(how="all", axis=1).ffill(limit=2)
L653         spx = data["Close"][self.bench].reindex(px.index).ffill()
L654         clip_days = int(os.getenv("PRICE_CLIP_DAYS", "0"))   # 0なら無効（既定）
L655         if clip_days > 0:
L656             px  = px.tail(clip_days + 1)
L657             spx = spx.tail(clip_days + 1)
L658             logger.info("[T] price window clipped by env: %d rows (PRICE_CLIP_DAYS=%d)", len(px), clip_days)
L659         else:
L660             logger.info("[T] price window clip skipped; rows=%d", len(px))
L661         tickers_bulk, info = yf.Tickers(" ".join(tickers)), {}
L662         for t in tickers:
L663             try:
L664                 info[t] = tickers_bulk.tickers[t].info
L665             except Exception as e:
L666                 logger.info("[warn] %s: info fetch failed (%s)", t, e)
L667                 info[t] = {}
L668         try:
L669             sec_map = self.fetch_eps_rev_from_sec(tickers)
L670         except Exception as e:
L671             logger.warning("[SEC] fetch_eps_rev_from_sec failed: %s", e)
L672             sec_map = {}
L673
L674         def _brief_len(s):
L675             try:
L676                 if isinstance(s, pd.Series):
L677                     return int(s.dropna().size)
L678                 if isinstance(s, (list, tuple)):
L679                     return len([v for v in s if pd.notna(v)])
L680                 if isinstance(s, np.ndarray):
L681                     return int(np.count_nonzero(~pd.isna(s)))
L682                 return int(bool(s))
L683             except Exception:
L684                 return 0
L685
L686         def _has_entries(val) -> bool:
L687             try:
L688                 if isinstance(val, pd.Series):
L689                     return not val.dropna().empty
L690                 if isinstance(val, (list, tuple)):
L691                     return any(pd.notna(v) for v in val)
L692                 return bool(val)
L693             except Exception:
L694                 return False
L695
L696         have_rev = 0
L697         have_eps = 0
L698         rev_lens: list[int] = []
L699         eps_lens: list[int] = []
L700         samples: list[tuple[str, int, str, float | None, int, str, float | None]] = []
L701
L702         for t in tickers:
L703             entry = info.get(t, {})
L704             m = (sec_map or {}).get(t) or {}
L705             if entry is None or not isinstance(entry, dict):
L706                 entry = {}
L707                 info[t] = entry
L708
L709             if m:
L710                 pairs_r = m.get("rev_q_series_pairs") or []
L711                 pairs_e = m.get("eps_q_series_pairs") or []
L712                 if pairs_r:
L713                     idx = pd.to_datetime([d for (d, _v) in pairs_r], errors="coerce")
L714                     val = pd.to_numeric([v for (_d, v) in pairs_r], errors="coerce")
L715                     s = pd.Series(val, index=idx).sort_index()
L716                     entry["SEC_REV_Q_SERIES"] = s
L717                 else:
L718                     entry["SEC_REV_Q_SERIES"] = m.get("rev_q_series") or []
L719                 if pairs_e:
L720                     idx = pd.to_datetime([d for (d, _v) in pairs_e], errors="coerce")
L721                     val = pd.to_numeric([v for (_d, v) in pairs_e], errors="coerce")
L722                     s = pd.Series(val, index=idx).sort_index()
L723                     entry["SEC_EPS_Q_SERIES"] = s
L724                 else:
L725                     entry["SEC_EPS_Q_SERIES"] = m.get("eps_q_series") or []
L726
L727             r = entry.get("SEC_REV_Q_SERIES")
L728             e = entry.get("SEC_EPS_Q_SERIES")
L729             if _has_entries(r):
L730                 have_rev += 1
L731             if _has_entries(e):
L732                 have_eps += 1
L733             lr = _brief_len(r)
L734             le = _brief_len(e)
L735             rev_lens.append(lr)
L736             eps_lens.append(le)
L737             if len(samples) < 8:
L738                 try:
L739                     rd = getattr(r, "index", [])[-1] if lr > 0 else None
L740                     rv = float(r.iloc[-1]) if lr > 0 else None
L741                     ed = getattr(e, "index", [])[-1] if le > 0 else None
L742                     ev = float(e.iloc[-1]) if le > 0 else None
L743                     samples.append((t, lr, str(rd) if rd is not None else "-", rv, le, str(ed) if ed is not None else "-", ev))
L744                 except Exception:
L745                     samples.append((t, lr, "-", None, le, "-", None))
L746
L747         logger.info("[SEC] series attach: rev_q=%d/%d, eps_q=%d/%d", have_rev, len(tickers), have_eps, len(tickers))
L748
L749         if rev_lens:
L750             rev_lens_sorted = sorted(rev_lens)
L751             eps_lens_sorted = sorted(eps_lens)
L752             _log(
L753                 "SEC_SERIES",
L754                 f"rev_len min/med/max={rev_lens_sorted[0]}/{rev_lens_sorted[len(rev_lens)//2]}/{rev_lens_sorted[-1]} "
L755                 f"eps_len min/med/max={eps_lens_sorted[0]}/{eps_lens_sorted[len(eps_lens)//2]}/{eps_lens_sorted[-1]}",
L756             )
L757         for (t, lr, rd, rv, le, ed, ev) in samples:
L758             _log("SEC_SERIES_SMP", f"{t}  rev_len={lr} last=({rd},{rv})  eps_len={le} last=({ed},{ev})")
L759         eps_df = self._build_eps_df(tickers, tickers_bulk, info, sec_map=sec_map)
L760         # index 重複があると .loc[t, col] が Series になり代入時に ValueError を誘発する
L761         if not eps_df.index.is_unique:
L762             eps_df = eps_df[~eps_df.index.duplicated(keep="last")]
L763         eps_df = eps_df.assign(
L764             EPS_TTM=eps_df["eps_ttm"],
L765             EPS_Q_LastQ=eps_df["eps_q_recent"],
L766             REV_TTM=eps_df["rev_ttm"],
L767             REV_Q_LastQ=eps_df["rev_q_recent"],
L768         )
L769         # ここで非NaN件数をサマリ表示（欠損状況の即時把握用）
L770         try:
L771             n = len(eps_df)
L772             c_eps = int(eps_df["EPS_TTM"].notna().sum())
L773             c_rev = int(eps_df["REV_TTM"].notna().sum())
L774             print(f"[SEC] eps_ttm non-NaN: {c_eps}/{n}  rev_ttm non-NaN: {c_rev}/{n}")
L775         except Exception:
L776             pass
L777         fcf_df = self.compute_fcf_with_fallback(tickers, finnhub_api_key=self.api_key)
L778         T.log("eps/fcf prep done")
L779         returns = px[tickers].pct_change()
L780         T.log("price prep/returns done")
L781         return dict(cand=cand_f, tickers=tickers, data=data, px=px, spx=spx, tickers_bulk=tickers_bulk, info=info, eps_df=eps_df, fcf_df=fcf_df, returns=returns)
L782
L783 # === Selector：相関低減・選定（スコア＆リターンだけ読む） ===
L784 class Selector:
L785     # ---- DRRS helpers（Selector専用） ----
L786     @staticmethod
L787     def _z_np(X: np.ndarray) -> np.ndarray:
L788         X = np.asarray(X, dtype=np.float32); m = np.nanmean(X, axis=0, keepdims=True); s = np.nanstd(X, axis=0, keepdims=True)+1e-9
L789         return (np.nan_to_num(X)-m)/s
L790
L791     @classmethod
L792     def residual_corr(cls, R: np.ndarray, n_pc: int=3, shrink: float=0.1) -> np.ndarray:
L793         Z = cls._z_np(R); U,S,_ = np.linalg.svd(Z, full_matrices=False); F = U[:,:n_pc]*S[:n_pc]; B = np.linalg.lstsq(F, Z, rcond=None)[0]
L794         E = Z - F@B; C = np.corrcoef(E, rowvar=False)
L795         off = C - np.diag(np.diag(C)); iu = np.triu_indices_from(off,1); avg_abs = np.nanmean(np.abs(off[iu])) if iu[0].size else 0.0
L796         shrink_eff = float(np.clip(shrink + 0.5*avg_abs, 0.1, 0.6)); N = C.shape[0]
L797         return (1.0 - shrink_eff)*C + shrink_eff*np.eye(N, dtype=C.dtype)
L798
L799     @classmethod
L800     def rrqr_like_det(cls, R: np.ndarray, score: np.ndarray, k: int, gamma: float=1.0):
L801         Z, w = cls._z_np(R), (score-score.min())/(np.ptp(score)+1e-12); X = Z*(1.0 + gamma*w)
L802         N, k = X.shape[1], max(0, min(k, X.shape[1]))
L803         if k==0: return []
L804         S, selected, Rres = [], np.zeros(N, dtype=bool), X.copy()
L805         for _ in range(k):
L806             norms = (Rres*Rres).sum(axis=0); cand = np.where(~selected)[0]
L807             j = sorted(cand, key=lambda c:(-norms[c], -w[c], c))[0]
L808             S.append(j); selected[j]=True; u = X[:,j:j+1]; u/=(np.linalg.norm(u)+1e-12); Rres = Rres - u @ (u.T @ Rres)
L809         return sorted(S)
L810
L811     @staticmethod
L812     def _obj(corrM: np.ndarray, score: np.ndarray, idx, lam: float) -> float:
L813         idx = list(idx); P = corrM[np.ix_(idx, idx)]; s = (score-score.mean())/(score.std()+1e-9)
L814         return float(s[idx].sum() - lam*((P.sum()-np.trace(P))/2.0))
L815
L816     @classmethod
L817     def swap_local_det(cls, corrM: np.ndarray, score: np.ndarray, idx, lam: float=0.6, max_pass: int=15):
L818         S, best, improved, passes = sorted(idx), cls._obj(corrM, score, idx, lam), True, 0
L819         while improved and passes<max_pass:
L820             improved, passes = False, passes+1
L821             for i,out in enumerate(list(S)):
L822                 for inn in range(len(score)):
L823                     if inn in S: continue
L824                     cand = sorted(S[:i]+[inn]+S[i+1:]); v = cls._obj(corrM, score, cand, lam)
L825                     if v>best+1e-10: S, best, improved = cand, v, True; break
L826                 if improved: break
L827         return S, best
L828
L829     @staticmethod
L830     def _obj_with_cross(C_within: np.ndarray, C_cross: np.ndarray|None, score: np.ndarray, idx, lam: float, mu: float) -> float:
L831         idx = list(idx); P = C_within[np.ix_(idx, idx)]; s = (score-score.mean())/(score.std()+1e-9)
L832         within = (P.sum()-np.trace(P))/2.0; cross = 0.0
L833         if C_cross is not None and C_cross.size>0: cross = C_cross[idx,:].sum()
L834         return float(s[idx].sum() - lam*within - mu*cross)
L835
L836     @classmethod
L837     def swap_local_det_cross(cls, C_within: np.ndarray, C_cross: np.ndarray|None, score: np.ndarray, idx, lam: float=0.6, mu: float=0.3, max_pass: int=15):
L838         S, best, improved, passes, N = sorted(idx), cls._obj_with_cross(C_within,C_cross,score,idx,lam,mu), True, 0, len(score)
L839         while improved and passes<max_pass:
L840             improved, passes = False, passes+1
L841             for i,out in enumerate(list(S)):
L842                 for inn in range(N):
L843                     if inn in S: continue
L844                     cand = sorted(S[:i]+[inn]+S[i+1:]); v = cls._obj_with_cross(C_within,C_cross,score,cand,lam,mu)
L845                     if v>best+1e-10: S, best, improved = cand, v, True; break
L846                 if improved: break
L847         return S, best
L848
L849     @staticmethod
L850     def avg_corr(C: np.ndarray, idx) -> float:
L851         k = len(idx); P = C[np.ix_(idx, idx)]
L852         return float((P.sum()-np.trace(P))/(k*(k-1)+1e-12))
L853
L854     @classmethod
L855     def select_bucket_drrs(cls, returns_df: pd.DataFrame, score_ser: pd.Series, pool_tickers: list[str], k: int, *, n_pc: int, gamma: float, lam: float, lookback: int, shrink: float=0.10, g_fixed_tickers: list[str]|None=None, mu: float=0.0):
L856         g_fixed = [t for t in (g_fixed_tickers or []) if t in returns_df.columns]
L857         union = [t for t in pool_tickers if t in returns_df.columns]
L858         for t in g_fixed:
L859             if t not in union: union.append(t)
L860         Rdf_all = returns_df[union]; Rdf_all = Rdf_all.iloc[-lookback:] if len(Rdf_all)>=lookback else Rdf_all; Rdf_all = Rdf_all.dropna()
L861         pool_eff, g_eff = [t for t in pool_tickers if t in Rdf_all.columns], [t for t in g_fixed if t in Rdf_all.columns]
L862         if len(pool_eff)==0: return dict(idx=[], tickers=[], avg_res_corr=np.nan, sum_score=0.0, objective=-np.inf)
L863         score = score_ser.reindex(pool_eff).to_numpy(dtype=np.float32)
L864         C_all = cls.residual_corr(Rdf_all.to_numpy(), n_pc=n_pc, shrink=shrink)
L865         col_pos = {c:i for i,c in enumerate(Rdf_all.columns)}; pool_pos = [col_pos[t] for t in pool_eff]
L866         C_within, C_cross = C_all[np.ix_(pool_pos,pool_pos)], None
L867         if len(g_eff)>0 and mu>0.0:
L868             g_pos = [col_pos[t] for t in g_eff]; C_cross = C_all[np.ix_(pool_pos,g_pos)]
L869         R_pool = Rdf_all[pool_eff].to_numpy(); S0 = cls.rrqr_like_det(R_pool, score, k, gamma=gamma)
L870         S, Jn = (cls.swap_local_det_cross(C_within, C_cross, score, S0, lam=lam, mu=mu, max_pass=15) if C_cross is not None else cls.swap_local_det(C_within, score, S0, lam=lam, max_pass=15))
L871         selected_tickers = [pool_eff[i] for i in S]
L872         return dict(idx=S, tickers=selected_tickers, avg_res_corr=cls.avg_corr(C_within,S), sum_score=float(score[S].sum()), objective=float(Jn))
L873
L874     # ---- 選定（スコア Series / returns だけを受ける）----
L875 # === Output：出力整形と送信（表示・Slack） ===
L876 class Output:
L877
L878     def __init__(self, debug=None):
L879         # self.debug は使わない（互換のため引数は受けるが無視）
L880         self.miss_df = self.g_table = self.d_table = self.io_table = self.df_metrics_fmt = self.debug_table = None
L881         self.g_title = self.d_title = ""
L882         self.g_formatters = self.d_formatters = {}
L883         # 低スコア（GSC+DSC）Top10 表示/送信用
L884         self.low10_table = None
L885         self.debug_text = ""   # デバッグ用本文はここに一本化
L886         self._debug_logged = False
L887
L888     # --- 表示（元 display_results のロジックそのまま） ---
L889     def display_results(self, *, exist, bench, df_z, g_score, d_score_all,
L890                         init_G, init_D, top_G, top_D, **kwargs):
L891         logger.info("📌 reached display_results")
L892         pd.set_option('display.float_format','{:.3f}'.format)
L893         print("📈 ファクター分散最適化の結果")
L894         if self.miss_df is not None and not self.miss_df.empty:
L895             print("Missing Data:")
L896             print(self.miss_df.to_string(index=False))
L897
L898         # ---- 表示用：Changes/Near-Miss のスコア源を“最終集計”に統一するプロキシ ----
L899         try:
L900             sc = getattr(self, "_sc", None)
L901             agg_G = getattr(sc, "_agg_G", None)
L902             agg_D = getattr(sc, "_agg_D", None)
L903         except Exception:
L904             sc = agg_G = agg_D = None
L905         class _SeriesProxy:
L906             __slots__ = ("primary", "fallback")
L907             def __init__(self, primary, fallback): self.primary, self.fallback = primary, fallback
L908             def get(self, key, default=None):
L909                 try:
L910                     v = self.primary.get(key) if hasattr(self.primary, "get") else None
L911                     if v is not None and not (isinstance(v, float) and v != v):
L912                         return v
L913                 except Exception:
L914                     pass
L915                 try:
L916                     return self.fallback.get(key) if hasattr(self.fallback, "get") else default
L917                 except Exception:
L918                     return default
L919         g_score = _SeriesProxy(agg_G, g_score)
L920         d_score_all = _SeriesProxy(agg_D, d_score_all)
L921         near_G = getattr(sc, "_near_G", []) if sc else []
L922         near_D = getattr(sc, "_near_D", []) if sc else []
L923
L924         extra_G = [t for t in init_G if t not in top_G][:5]; G_UNI = top_G + extra_G
L925         gsc_series = pd.Series({t: g_score.get(t) for t in G_UNI}, name='GSC')
L926         self.g_table = pd.concat([df_z.loc[G_UNI,['GROWTH_F','MOM','TRD','VOL']], gsc_series], axis=1)
L927         self.g_table.index = [t + ("⭐️" if t in top_G else "") for t in G_UNI]
L928         self.g_formatters = {col:"{:.2f}".format for col in ['GROWTH_F','MOM','TRD','VOL']}; self.g_formatters['GSC'] = "{:.3f}".format
L929         self.g_title = (f"[G枠 / {N_G} / {_fmt_w(g_weights)} / corrM={corrM} / "
L930                         f"LB={DRRS_G['lookback']} nPC={DRRS_G['n_pc']} γ={DRRS_G['gamma']} λ={DRRS_G['lam']} η={DRRS_G['eta']} shrink={DRRS_SHRINK}]")
L931         if near_G:
L932             add = [t for t in near_G if t not in set(G_UNI)][:10]
L933             if len(add) < 10:
L934                 try:
L935                     aggG = getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False)
L936                     out_now = sorted(set(exist) - set(top_G + top_D))  # 今回 OUT
L937                     used = set(G_UNI + add)
L938                     def _push(lst):
L939                         nonlocal add, used
L940                         for t in lst:
L941                             if len(add) == 10: break
L942                             if t in aggG.index and t not in used:
L943                                 add.append(t); used.add(t)
L944                     _push(out_now)           # ① 今回 OUT を優先
L945                     _push(list(aggG.index))  # ② まだ足りなければ上位で充填
L946                 except Exception:
L947                     pass
L948             if add:
L949                 near_tbl = pd.concat([df_z.loc[add,['GROWTH_F','MOM','TRD','VOL']], pd.Series({t: g_score.get(t) for t in add}, name='GSC')], axis=1)
L950                 self.g_table = pd.concat([self.g_table, near_tbl], axis=0)
L951         print(self.g_title); print(self.g_table.to_string(formatters=self.g_formatters))
L952
L953         extra_D = [t for t in init_D if t not in top_D][:5]; D_UNI = top_D + extra_D
L954         cols_D = ['QAL','YLD','VOL','TRD']; d_disp = pd.DataFrame(index=D_UNI)
L955         d_disp['QAL'], d_disp['YLD'], d_disp['VOL'], d_disp['TRD'] = df_z.loc[D_UNI,'D_QAL'], df_z.loc[D_UNI,'D_YLD'], df_z.loc[D_UNI,'D_VOL_RAW'], df_z.loc[D_UNI,'D_TRD']
L956         dsc_series = pd.Series({t: d_score_all.get(t) for t in D_UNI}, name='DSC')
L957         self.d_table = pd.concat([d_disp, dsc_series], axis=1); self.d_table.index = [t + ("⭐️" if t in top_D else "") for t in D_UNI]
L958         self.d_formatters = {col:"{:.2f}".format for col in cols_D}; self.d_formatters['DSC']="{:.3f}".format
L959         import scorer
L960         dw_eff = scorer.D_WEIGHTS_EFF
L961         self.d_title = (f"[D枠 / {N_D} / {_fmt_w(dw_eff)} / corrM={corrM} / "
L962                         f"LB={DRRS_D['lookback']} nPC={DRRS_D['n_pc']} γ={DRRS_D['gamma']} λ={DRRS_D['lam']} μ={CROSS_MU_GD} η={DRRS_D['eta']} shrink={DRRS_SHRINK}]")
L963         if near_D:
L964             add = [t for t in near_D if t not in set(D_UNI)][:10]
L965             if add:
L966                 d_disp2 = pd.DataFrame(index=add)
L967                 d_disp2['QAL'], d_disp2['YLD'], d_disp2['VOL'], d_disp2['TRD'] = df_z.loc[add,'D_QAL'], df_z.loc[add,'D_YLD'], df_z.loc[add,'D_VOL_RAW'], df_z.loc[add,'D_TRD']
L968                 near_tbl = pd.concat([d_disp2, pd.Series({t: d_score_all.get(t) for t in add}, name='DSC')], axis=1)
L969                 self.d_table = pd.concat([self.d_table, near_tbl], axis=0)
L970         print(self.d_title); print(self.d_table.to_string(formatters=self.d_formatters))
L971
L972         # === Changes（IN の GSC/DSC を表示。OUT は銘柄名のみ） ===
L973         in_list = sorted(set(list(top_G)+list(top_D)) - set(exist))
L974         out_list = sorted(set(exist) - set(list(top_G)+list(top_D)))
L975
L976         self.io_table = pd.DataFrame({
L977             'IN': pd.Series(in_list),
L978             '/ OUT': pd.Series(out_list)
L979         })
L980         g_list = [f"{g_score.get(t):.3f}" if pd.notna(g_score.get(t)) else '—' for t in out_list]
L981         d_list = [f"{d_score_all.get(t):.3f}" if pd.notna(d_score_all.get(t)) else '—' for t in out_list]
L982         self.io_table['GSC'] = pd.Series(g_list)
L983         self.io_table['DSC'] = pd.Series(d_list)
L984
L985         print("Changes:")
L986         print(self.io_table.to_string(index=False))
L987
L988         all_tickers = list(set(exist + list(top_G) + list(top_D) + [bench])); prices = yf.download(all_tickers, period='1y', auto_adjust=True, progress=False, threads=False)['Close'].ffill(limit=2)
L989         ret = prices.pct_change(); portfolios = {'CUR':exist,'NEW':list(top_G)+list(top_D)}; metrics={}
L990         for name,ticks in portfolios.items():
L991             pr = ret[ticks].mean(axis=1, skipna=True).dropna(); cum = (1+pr).cumprod()-1; n = len(pr)
L992             if n>=252: ann_ret, ann_vol = (1+cum.iloc[-1])**(252/n)-1, pr.std()*np.sqrt(252)
L993             else: ann_ret, ann_vol = cum.iloc[-1], pr.std()*np.sqrt(n)
L994             sharpe, drawdown = ann_ret/ann_vol, (cum - cum.cummax()).min()
L995             if len(ticks)>=2:
L996                 C_raw = ret[ticks].corr(); RAW_rho = C_raw.mask(np.eye(len(ticks), dtype=bool)).stack().mean()
L997                 R = ret[ticks].dropna().to_numpy(); C_resid = Selector.residual_corr(R, n_pc=3, shrink=DRRS_SHRINK)
L998                 RESID_rho = float((C_resid.sum()-np.trace(C_resid))/(C_resid.shape[0]*(C_resid.shape[0]-1)))
L999             else: RAW_rho = RESID_rho = np.nan
L1000             divy = ttm_div_yield_portfolio(ticks); metrics[name] = {'RET':ann_ret,'VOL':ann_vol,'SHP':sharpe,'MDD':drawdown,'RAWρ':RAW_rho,'RESIDρ':RESID_rho,'DIVY':divy}
L1001         df_metrics = pd.DataFrame(metrics).T; df_metrics_pct = df_metrics.copy(); self.df_metrics = df_metrics
L1002         for col in ['RET','VOL','MDD','DIVY']: df_metrics_pct[col] = df_metrics_pct[col]*100
L1003         cols_order = ['RET','VOL','SHP','MDD','RAWρ','RESIDρ','DIVY']; df_metrics_pct = df_metrics_pct.reindex(columns=cols_order)
L1004         def _fmt_row(s):
L1005             return pd.Series({'RET':f"{s['RET']:.1f}%",'VOL':f"{s['VOL']:.1f}%",'SHP':f"{s['SHP']:.1f}",'MDD':f"{s['MDD']:.1f}%",'RAWρ':(f"{s['RAWρ']:.2f}" if pd.notna(s['RAWρ']) else "NaN"),'RESIDρ':(f"{s['RESIDρ']:.2f}" if pd.notna(s['RESIDρ']) else "NaN"),'DIVY':f"{s['DIVY']:.1f}%"})
L1006         self.df_metrics_fmt = df_metrics_pct.apply(_fmt_row, axis=1); print("Performance Comparison:"); print(self.df_metrics_fmt.to_string())
L1007         # === 追加: GSC+DSC が低い順 TOP10 ===
L1008         try:
L1009             all_scores = pd.DataFrame({'GSC': df_z['GSC'], 'DSC': df_z['DSC']}).copy()
L1010             all_scores['G_plus_D'] = all_scores['GSC'] + all_scores['DSC']
L1011             all_scores = all_scores.dropna(subset=['G_plus_D'])
L1012             self.low10_table = all_scores.sort_values('G_plus_D', ascending=True).head(10).round(3)
L1013             print("Low Score Candidates (GSC+DSC bottom 10):")
L1014             print(self.low10_table.to_string())
L1015         except Exception as e:
L1016             print(f"[warn] low-score ranking failed: {e}")
L1017             self.low10_table = None
L1018         self.debug_text = ""
L1019         if debug_mode:
L1020             logger.info("debug_mode=True: df_z dump handled in scorer; skipping factor-side debug output")
L1021         else:
L1022             logger.debug(
L1023                 "skip debug log: debug_mode=%s debug_text_empty=%s",
L1024                 debug_mode, True
L1025             )
L1026         self._debug_logged = True
L1027
L1028     # --- Slack送信（元 notify_slack のロジックそのまま） ---
L1029     def notify_slack(self):
L1030         SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")
L1031
L1032         if not SLACK_WEBHOOK_URL:
L1033             print("⚠️ SLACK_WEBHOOK_URL not set (main report skipped)")
L1034             return
L1035
L1036         def _filter_suffix_from(spec: dict, group: str) -> str:
L1037             g = spec.get(group, {})
L1038             parts = [str(m) for m in g.get("pre_mask", [])]
L1039             for k, v in (g.get("pre_filter", {}) or {}).items():
L1040                 base, op = (k[:-4], "<") if k.endswith("_max") else ((k[:-4], ">") if k.endswith("_min") else (k, "="))
L1041                 name = {"beta": "β"}.get(base, base)
L1042                 try:
L1043                     val = f"{float(v):g}"
L1044                 except Exception:
L1045                     val = str(v)
L1046                 parts.append(f"{name}{op}{val}")
L1047             return "" if not parts else " / filter:" + " & ".join(parts)
L1048
L1049         def _inject_filter_suffix(title: str, group: str) -> str:
L1050             suf = _filter_suffix_from(FILTER_SPEC, group)
L1051             return f"{title[:-1]}{suf}]" if suf and title.endswith("]") else (title + suf)
L1052
L1053         def _blk(title, tbl, fmt=None, drop=()):
L1054             if tbl is None or getattr(tbl, 'empty', False):
L1055                 return f"{title}\n(選定なし)\n"
L1056             if drop and hasattr(tbl, 'columns'):
L1057                 keep = [c for c in tbl.columns if c not in drop]
L1058                 tbl, fmt = tbl[keep], {k: v for k, v in (fmt or {}).items() if k in keep}
L1059             return f"{title}\n```{tbl.to_string(formatters=fmt)}```\n"
L1060
L1061         message = "📈 ファクター分散最適化の結果\n"
L1062         if self.miss_df is not None and not self.miss_df.empty:
L1063             message += "Missing Data\n```" + self.miss_df.to_string(index=False) + "```\n"
L1064         message += _blk(_inject_filter_suffix(self.g_title, "G"), self.g_table, self.g_formatters, drop=("TRD",))
L1065         message += _blk(_inject_filter_suffix(self.d_title, "D"), self.d_table, self.d_formatters)
L1066         message += "Changes\n" + ("(変更なし)\n" if self.io_table is None or getattr(self.io_table, 'empty', False) else f"```{self.io_table.to_string(index=False)}```\n")
L1067         message += "Performance Comparison:\n```" + self.df_metrics_fmt.to_string() + "```"
L1068
L1069         try:
L1070             r = requests.post(SLACK_WEBHOOK_URL, json={"text": message})
L1071             print(f"[DBG] main_post status={getattr(r, 'status_code', None)} size={len(message)}")
L1072             if r is not None:
L1073                 r.raise_for_status()
L1074         except Exception as e:
L1075             print(f"[ERR] main_post_failed: {e}")
L1076
L1077 def _infer_g_universe(feature_df, selected12=None, near5=None):
L1078     try:
L1079         out = feature_df.index[feature_df['group'].astype(str).str.upper().eq('G')].tolist()
L1080         if out: return out
L1081     except Exception:
L1082         pass
L1083     base = set()
L1084     for lst in (selected12 or []), (near5 or []):
L1085         for x in (lst or []): base.add(x)
L1086     return list(base) if base else list(feature_df.index)
L1087
L1088 def _fmt_with_fire_mark(tickers, feature_df):
L1089     out = []
L1090     for t in tickers or []:
L1091         try:
L1092             br = bool(feature_df.at[t, "G_BREAKOUT_recent_5d"])
L1093             pb = bool(feature_df.at[t, "G_PULLBACK_recent_5d"])
L1094             out.append(f"{t}{' 🔥' if (br or pb) else ''}")
L1095         except Exception:
L1096             out.append(t)
L1097     return out
L1098
L1099 def _label_recent_event(t, feature_df):
L1100     try:
L1101         br = bool(feature_df.at[t, "G_BREAKOUT_recent_5d"]); dbr = str(feature_df.at[t, "G_BREAKOUT_last_date"]) if br else ""
L1102         pb = bool(feature_df.at[t, "G_PULLBACK_recent_5d"]); dpb = str(feature_df.at[t, "G_PULLBACK_last_date"]) if pb else ""
L1103         if   br and not pb: return f"{t}（ブレイクアウト確定 {dbr}）"
L1104         elif pb and not br: return f"{t}（押し目反発 {dpb}）"
L1105         elif br and pb:     return f"{t}（ブレイクアウト確定 {dbr}／押し目反発 {dpb}）"
L1106     except Exception:
L1107         pass
L1108     return t
L1109
L1110 # === パイプライン可視化：G/D共通フロー（出力は不変） ===
L1111
L1112 def io_build_input_bundle() -> InputBundle:
L1113     """
L1114     既存の『データ取得→前処理』を実行し、InputBundle を返す。
L1115     処理内容・列名・丸め・例外・ログ文言は現行どおり（変更禁止）。
L1116     """
L1117     state = Input(cand=cand, exist=exist, bench=bench, price_max=CAND_PRICE_MAX, finnhub_api_key=FINNHUB_API_KEY).prepare_data()
L1118     return InputBundle(cand=state["cand"], tickers=state["tickers"], bench=bench, data=state["data"], px=state["px"], spx=state["spx"], tickers_bulk=state["tickers_bulk"], info=state["info"], eps_df=state["eps_df"], fcf_df=state["fcf_df"], returns=state["returns"])
L1119
L1120 def run_group(sc: Scorer, group: str, inb: InputBundle, cfg: PipelineConfig,
L1121               n_target: int) -> tuple[list, float, float, float]:
L1122     """
L1123     G/Dを同一手順で処理：採点→フィルター→選定（相関低減込み）。
L1124     戻り値：(pick, avg_res_corr, sum_score, objective)
L1125     JSON保存は既存フォーマット（キー名・丸め桁・順序）を踏襲。
L1126     """
L1127     sc.cfg = cfg
L1128
L1129     if hasattr(sc, "score_build_features"):
L1130         feat = sc.score_build_features(inb)
L1131         if not hasattr(sc, "_feat_logged"):
L1132             T.log("features built (scorer)")
L1133             sc._feat_logged = True
L1134         agg = sc.score_aggregate(feat, group, cfg) if hasattr(sc, "score_aggregate") else feat
L1135     else:
L1136         fb = sc.aggregate_scores(inb, cfg)
L1137         if not hasattr(sc, "_feat_logged"):
L1138             T.log("features built (scorer)")
L1139             sc._feat_logged = True
L1140         sc._feat = fb
L1141         agg = fb.g_score if group == "G" else fb.d_score_all
L1142         if group == "D" and hasattr(fb, "df"):
L1143             agg = agg[fb.df['BETA'] < D_BETA_MAX]
L1144
L1145     if hasattr(sc, "filter_candidates"):
L1146         agg = agg[sc.filter_candidates(inb, agg, group, cfg)]
L1147
L1148     selector = Selector()
L1149     if hasattr(sc, "select_diversified"):
L1150         pick, avg_r, sum_sc, obj = sc.select_diversified(agg, group, cfg, n_target,
L1151             selector=selector, prev_tickers=None,
L1152             corrM=cfg.drrs.corrM, shrink=cfg.drrs.shrink,
L1153             cross_mu=cfg.drrs.cross_mu_gd)
L1154     else:
L1155         if group == "G":
L1156             init = agg.nlargest(min(cfg.drrs.corrM, len(agg))).index.tolist()
L1157             res = selector.select_bucket_drrs(returns_df=inb.returns, score_ser=agg, pool_tickers=init, k=n_target,
L1158                 n_pc=cfg.drrs.G.get("n_pc", 3), gamma=cfg.drrs.G.get("gamma", 1.2),
L1159                 lam=cfg.drrs.G.get("lam", 0.68),
L1160                 lookback=cfg.drrs.G.get("lookback", 252),
L1161                 shrink=cfg.drrs.shrink, g_fixed_tickers=None, mu=0.0)
L1162         else:
L1163             init = agg.nlargest(min(cfg.drrs.corrM, len(agg))).index.tolist()
L1164             g_fixed = getattr(sc, "_top_G", None)
L1165             res = selector.select_bucket_drrs(returns_df=inb.returns, score_ser=agg, pool_tickers=init, k=n_target,
L1166                 n_pc=cfg.drrs.D.get("n_pc", 4), gamma=cfg.drrs.D.get("gamma", 0.8),
L1167                 lam=cfg.drrs.D.get("lam", 0.85),
L1168                 lookback=cfg.drrs.D.get("lookback", 504),
L1169                 shrink=cfg.drrs.shrink, g_fixed_tickers=g_fixed,
L1170                 mu=cfg.drrs.cross_mu_gd)
L1171         pick = res["tickers"]; avg_r = res["avg_res_corr"]
L1172         sum_sc = res["sum_score"]; obj = res["objective"]
L1173         if group == "D":
L1174             _, pick = _disjoint_keepG(getattr(sc, "_top_G", []), pick, init)
L1175             T.log("selection finalized (G/D)")
L1176     try:
L1177         inc = [t for t in exist if t in agg.index]
L1178         pick = _sticky_keep_current(
L1179             agg=agg, pick=pick, incumbents=inc, n_target=n_target,
L1180             delta_z=SWAP_DELTA_Z, keep_buffer=SWAP_KEEP_BUFFER
L1181         )
L1182     except Exception as _e:
L1183         print(f"[warn] sticky_keep_current skipped: {str(_e)}")
L1184     # --- Near-Miss: 惜しくも選ばれなかった上位10を保持（Slack表示用） ---
L1185     # 5) Near-Miss と最終集計Seriesを保持（表示専用。計算へ影響なし）
L1186     try:
L1187         pool = agg.drop(index=[t for t in pick if t in agg.index], errors="ignore")
L1188         near10 = list(pool.sort_values(ascending=False).head(10).index)
L1189         setattr(sc, f"_near_{group}", near10)
L1190         setattr(sc, f"_agg_{group}", agg)
L1191     except Exception:
L1192         pass
L1193
L1194     if group == "D":
L1195         T.log("save done")
L1196     if group == "G":
L1197         sc._top_G = pick
L1198     return pick, avg_r, sum_sc, obj
L1199
L1200 def run_pipeline() -> SelectionBundle:
L1201     """
L1202     G/D共通フローの入口。I/Oはここだけで実施し、計算はScorerに委譲。
L1203     Slack文言・丸め・順序は既存の Output を用いて変更しない。
L1204     """
L1205     inb = io_build_input_bundle()
L1206     cfg = PipelineConfig(
L1207         weights=WeightsConfig(g=g_weights, d=D_weights),
L1208         drrs=DRRSParams(
L1209             corrM=corrM, shrink=DRRS_SHRINK,
L1210             G=DRRS_G, D=DRRS_D, cross_mu_gd=CROSS_MU_GD
L1211         ),
L1212         price_max=CAND_PRICE_MAX,
L1213         debug_mode=debug_mode
L1214     )
L1215     sc = Scorer()
L1216     top_G, avgG, sumG, objG = run_group(sc, "G", inb, cfg, N_G)
L1217     poolG = list(getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False).index)
L1218     alpha = Scorer.spx_to_alpha(inb.spx)
L1219     sectors = {t:(inb.info.get(t,{}).get("sector") or "U") for t in poolG}; scores = {t:Scorer.g_score.get(t,0.0) for t in poolG}
L1220     top_G = Scorer.pick_top_softcap(scores, sectors, N=N_G, cap=2, alpha=alpha, hard=5)
L1221     sc._top_G = top_G
L1222     try:
L1223         aggG = getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False)
L1224         sc._near_G = [t for t in aggG.index if t not in set(top_G)][:10]
L1225     except Exception:
L1226         pass
L1227     base = sum(Scorer.g_score.get(t,0.0) for t in poolG[:N_G])
L1228     effs = sum(Scorer.g_score.get(t,0.0) for t in top_G)
L1229     print(f"[soft_cap2] score_cost={(base-effs)/max(1e-9,abs(base)):.2%}, alpha={alpha:.3f}")
L1230     top_D, avgD, sumD, objD = run_group(sc, "D", inb, cfg, N_D)
L1231     fb = getattr(sc, "_feat", None)
L1232     near_G = getattr(sc, "_near_G", [])
L1233     selected12 = list(top_G)
L1234     df = fb.df if fb is not None else pd.DataFrame()
L1235     guni = _infer_g_universe(df, selected12, near_G)
L1236     try:
L1237         fire_recent = [t for t in guni
L1238                        if (str(df.at[t, "G_BREAKOUT_recent_5d"]) == "True") or
L1239                           (str(df.at[t, "G_PULLBACK_recent_5d"]) == "True")]
L1240     except Exception: fire_recent = []
L1241
L1242     lines = [
L1243         "【G枠レポート｜週次モニタ（直近5営業日）】",
L1244         "【凡例】🔥=直近5営業日内に「ブレイクアウト確定」または「押し目反発」を検知",
L1245         f"選定{N_G}: {', '.join(_fmt_with_fire_mark(selected12, df))}" if selected12 else f"選定{N_G}: なし",
L1246         f"次点10: {', '.join(_fmt_with_fire_mark(near_G, df))}" if near_G else "次点10: なし",]
L1247
L1248     if fire_recent:
L1249         fire_list = ", ".join([_label_recent_event(t, df) for t in fire_recent])
L1250         lines.append(f"過去5営業日の検知: {fire_list}")
L1251     else:
L1252         lines.append("過去5営業日の検知: なし")
L1253
L1254     try:
L1255         webhook = os.environ.get("SLACK_WEBHOOK_URL", "")
L1256         if webhook:
L1257             requests.post(webhook, json={"text": "\n".join([s for s in lines if s != ""])}, timeout=10)
L1258     except Exception:
L1259         pass
L1260
L1261     out = Output()
L1262     # 表示側から選定時の集計へアクセスできるように保持（表示専用・副作用なし）
L1263     try: out._sc = sc
L1264     except Exception: pass
L1265     if hasattr(sc, "_feat"):
L1266         try:
L1267             fb = sc._feat
L1268             out.miss_df = fb.missing_logs
L1269             out.display_results(
L1270                 exist=exist,
L1271                 bench=bench,
L1272                 df_z=fb.df_z,
L1273                 g_score=fb.g_score,
L1274                 d_score_all=fb.d_score_all,
L1275                 init_G=top_G,
L1276                 init_D=top_D,
L1277                 top_G=top_G,
L1278                 top_D=top_D,
L1279                 df_full_z=getattr(fb, "df_full_z", None),
L1280                 prev_G=getattr(sc, "_prev_G", exist),
L1281                 prev_D=getattr(sc, "_prev_D", exist),
L1282             )
L1283         except Exception:
L1284             pass
L1285     out.notify_slack()
L1286     sb = SelectionBundle(resG={"tickers": top_G, "avg_res_corr": avgG,
L1287               "sum_score": sumG, "objective": objG},
L1288         resD={"tickers": top_D, "avg_res_corr": avgD,
L1289               "sum_score": sumD, "objective": objD},
L1290         top_G=top_G, top_D=top_D, init_G=top_G, init_D=top_D)
L1291
L1292     # --- Low Score Candidates (GSC+DSC bottom 10) : send before debug dump ---
L1293     try:
L1294         _low_df = (pd.DataFrame({"GSC": fb.g_score, "DSC": fb.d_score_all})
L1295               .assign(G_plus_D=lambda x: x["GSC"] + x["DSC"])
L1296               .sort_values("G_plus_D")
L1297               .head(10)
L1298               .round(3))
L1299         low_msg = "Low Score Candidates (GSC+DSC bottom 10)\n" + _low_df.to_string(index=True, index_names=False)
L1300         _post_slack({"text": f"```{low_msg}```"})
L1301     except Exception as _e:
L1302         _post_slack({"text": f"```Low Score Candidates: 作成失敗: {_e}```"})
L1303
L1304     return sb
L1305
L1306 if __name__ == "__main__":
L1307     run_pipeline()
```

## <scorer.py>
```text
L1 # scorer.py
L2 # kawatest
L3 # =============================================================================
L4 # Scorer: ファクター/指標の生成と合成スコア算出を担う純粋層
L5 #
L6 # 【このファイルだけ読めば分かるポイント】
L7 # - 入力(InputBundle)は「価格/出来高/ベンチ/基本情報/EPS/FCF/リターン」を含むDTO
L8 # - 出力(FeatureBundle)は「raw特徴量 df」「標準化 df_z」「G/D スコア」「欠損ログ」
L9 # - 重み等のコンフィグ(PipelineConfig)は factor から渡す（cfg 必須）
L10 # - 旧カラム名は Scorer 内で自動リネームして受け入れ（後方互換）
L11 #   例) eps_ttm -> EPS_TTM, eps_q_recent -> EPS_Q_LastQ, fcf_ttm -> FCF_TTM
L12 #
L13 # 【I/O契約（Scorerが参照するInputBundleフィールド）】
L14 #   - cand: List[str]    … 候補銘柄（単体実行では未使用）
L15 #   - tickers: List[str] … 対象銘柄リスト
L16 #   - bench: str         … ベンチマークティッカー（例 '^GSPC'）
L17 #   - data: pd.DataFrame … yfinance download結果 ('Close','Volume' 等の階層列)
L18 #   - px: pd.DataFrame   … data['Close'] 相当（終値）
L19 #   - spx: pd.Series     … ベンチマークの終値
L20 #   - tickers_bulk: object         … yfinance.Tickers
L21 #   - info: Dict[str, dict]        … yfinance info per ticker
L22 #   - eps_df: pd.DataFrame         … 必須列: EPS_TTM, EPS_Q_LastQ（旧名も可）
L23 #   - fcf_df: pd.DataFrame         … 必須列: FCF_TTM（旧名も可）
L24 #   - returns: pd.DataFrame        … px[tickers].pct_change() 相当
L25 #
L26 # ※入出力の形式・例外文言は既存実装を変えません（安全な短縮のみ）
L27 # =============================================================================
L28
L29 import logging
L30 import os, sys, warnings
L31 import json
L32 import requests
L33 import numpy as np
L34 import pandas as pd
L35 import yfinance as yf
L36 from typing import Any, TYPE_CHECKING
L37 from scipy.stats import zscore
L38 from datetime import datetime as _dt
L39
L40 if TYPE_CHECKING:
L41     from factor import PipelineConfig  # type: ignore  # 実行時importなし（循環回避）
L42
L43 logger = logging.getLogger(__name__)
L44
L45
L46 def _log(stage, msg):
L47     try:
L48         print(f"[DBG][{_dt.utcnow().isoformat(timespec='seconds')}Z][{stage}] {msg}")
L49     except Exception:
L50         print(f"[DBG][{stage}] {msg}")
L51
L52 # ---- Dividend Helpers -------------------------------------------------------
L53 def _last_close(t, price_map=None):
L54     if price_map and (c := price_map.get(t)) is not None: return float(c)
L55     try:
L56         h = yf.Ticker(t).history(period="5d")["Close"].dropna()
L57         return float(h.iloc[-1]) if len(h) else np.nan
L58     except Exception:
L59         return np.nan
L60
L61 def _ttm_div_sum(t, lookback_days=400):
L62     try:
L63         div = yf.Ticker(t).dividends
L64         if div is None or len(div) == 0: return 0.0
L65         cutoff = pd.Timestamp.utcnow().tz_localize(None) - pd.Timedelta(days=lookback_days)
L66         ttm = float(div[div.index.tz_localize(None) >= cutoff].sum())
L67         return ttm if ttm > 0 else float(div.tail(4).sum())
L68     except Exception:
L69         return 0.0
L70
L71 def ttm_div_yield_portfolio(tickers, price_map=None):
L72     ys = [(lambda c, s: (s/c) if (np.isfinite(c) and c>0 and s>0) else 0.0)(_last_close(t, price_map), _ttm_div_sum(t)) for t in tickers]
L73     return float(np.mean(ys)) if ys else 0.0
L74
L75 # ---- 簡易ユーティリティ（安全な短縮のみ） -----------------------------------
L76 def winsorize_s(s: pd.Series, p=0.02):
L77     if s is None or s.dropna().empty: return s
L78     lo, hi = np.nanpercentile(s.astype(float), [100*p, 100*(1-p)]); return s.clip(lo, hi)
L79
L80 def robust_z(s: pd.Series, p=0.02):
L81     s2 = winsorize_s(s,p); return np.nan_to_num(zscore(s2.fillna(s2.mean())))
L82
L83 def robust_z_keepnan(s: pd.Series) -> pd.Series:
L84     """robust_z variant that preserves NaNs and falls back to rank-z when needed."""
L85     if s is None:
L86         return pd.Series(dtype=float)
L87     v = pd.to_numeric(s, errors="coerce")
L88     m = np.nanmedian(v)
L89     mad = np.nanmedian(np.abs(v - m))
L90     z = (v - m) / (1.4826 * mad + 1e-9)
L91     if np.nanstd(z) < 1e-9:
L92         r = v.rank(method="average", na_option="keep")
L93         z = (r - np.nanmean(r)) / (np.nanstd(r) + 1e-9)
L94     return pd.Series(z, index=v.index, dtype=float)
L95
L96
L97 def _dump_dfz(df_z: pd.DataFrame, debug_mode: bool, max_rows: int = 400, ndigits: int = 3) -> None:
L98     """df_z を System log(INFO) へダンプする簡潔なユーティリティ."""
L99     if not debug_mode:
L100         return
L101     try:
L102         view = df_z.copy()
L103         view = view.apply(
L104             lambda s: s.round(ndigits)
L105             if getattr(getattr(s, "dtype", None), "kind", "") in ("f", "i")
L106             else s
L107         )
L108         if len(view) > max_rows:
L109             view = view.iloc[:max_rows]
L110
L111         # === NaNサマリ（列ごとの欠損件数 上位20） ===
L112         try:
L113             nan_counts = df_z.isna().sum().sort_values(ascending=False)
L114             top_nan = nan_counts[nan_counts > 0].head(20)
L115             if len(top_nan) > 0:
L116                 logger.info("NaN columns (top20):\n%s", top_nan.to_string())
L117             else:
L118                 logger.info("NaN columns: none")
L119         except Exception as exc:
L120             logger.warning("nan summary failed: %s", exc)
L121
L122         # === Zeroサマリ（列ごとのゼロ比率 上位20） ===
L123         try:
L124             zero_counts = ((df_z == 0) & (~df_z.isna())).sum()
L125             nonnull_counts = (~df_z.isna()).sum()
L126             zero_ratio = (zero_counts / nonnull_counts).sort_values(ascending=False)
L127             top_zero = zero_ratio[zero_ratio > 0].head(20)
L128             if len(top_zero) > 0:
L129                 logger.info(
L130                     "Zero-dominated columns (top20):\n%s",
L131                     top_zero.to_string(float_format=lambda x: f"{x:.2%}"),
L132                 )
L133             else:
L134                 logger.info("Zero-dominated columns: none")
L135         except Exception as exc:
L136             logger.warning("zero summary failed: %s", exc)
L137
L138         logger.info("===== DF_Z DUMP START =====")
L139         logger.info("\n%s", view.to_string(max_rows=None, max_cols=None))
L140         logger.info("===== DF_Z DUMP END =====")
L141     except Exception as exc:
L142         logger.warning("df_z dump failed: %s", exc)
L143
L144 def _safe_div(a, b):
L145     try: return np.nan if (b is None or float(b)==0 or pd.isna(b)) else float(a)/float(b)
L146     except Exception: return np.nan
L147
L148 def _safe_last(series: pd.Series, default=np.nan):
L149     try: return float(series.iloc[-1])
L150     except Exception: return default
L151
L152
L153 def _ensure_series(x):
L154     if x is None:
L155         return pd.Series(dtype=float)
L156     if isinstance(x, pd.Series):
L157         return x.dropna()
L158     if isinstance(x, (list, tuple)):
L159         if len(x) and isinstance(x[0], (tuple, list)) and len(x[0]) == 2:
L160             dt = pd.to_datetime([d for d, _ in x], errors="coerce")
L161             v = pd.to_numeric([_v for _, _v in x], errors="coerce")
L162             return pd.Series(v, index=dt).dropna()
L163         return pd.Series(pd.to_numeric(list(x), errors="coerce")).dropna()
L164     try:
L165         return pd.Series(x).dropna()
L166     except Exception:
L167         return pd.Series(dtype=float)
L168
L169
L170 def _to_quarterly(s: pd.Series) -> pd.Series:
L171     if s.empty or not isinstance(s.index, pd.DatetimeIndex):
L172         return s
L173     return s.resample("Q").last().dropna()
L174
L175
L176 def _ttm_yoy_from_quarterly(qs: pd.Series) -> pd.Series:
L177     if qs is None or qs.empty:
L178         return pd.Series(dtype=float)
L179     ttm = qs.rolling(4, min_periods=2).sum()
L180     yoy = ttm.pct_change(4)
L181     return yoy
L182
L183
L184 def _nz(x) -> float:
L185     if x is None:
L186         return 0.0
L187     try:
L188         value = float(x)
L189     except Exception:
L190         return 0.0
L191     if not np.isfinite(value):
L192         return 0.0
L193     return value
L194
L195
L196 def _winsor(x, lo=-2.0, hi=2.0) -> float:
L197     v = _nz(x)
L198     if v < lo:
L199         return float(lo)
L200     if v > hi:
L201         return float(hi)
L202     return float(v)
L203
L204
L205 def _round_debug(x, ndigits: int = 4):
L206     try:
L207         value = float(x)
L208     except Exception:
L209         return None
L210     if not np.isfinite(value):
L211         return None
L212     return round(value, ndigits)
L213
L214
L215 def _calc_grw_flexible(
L216     ticker: str,
L217     info_entry: dict | None,
L218     close_series: pd.Series | None,
L219     volume_series: pd.Series | None,
L220 ):
L221     info_entry = info_entry if isinstance(info_entry, dict) else {}
L222
L223     s_rev_q = _ensure_series(info_entry.get("SEC_REV_Q_SERIES"))
L224     s_eps_q = _ensure_series(info_entry.get("SEC_EPS_Q_SERIES"))
L225     s_rev_y = _ensure_series(info_entry.get("SEC_REV_Y_SERIES"))
L226
L227     nQ = int(getattr(s_rev_q, "size", 0))
L228     nY = int(getattr(s_rev_y, "size", 0))
L229
L230     parts: dict[str, Any] = {"nQ": nQ, "nY": nY}
L231     path = "NONE"
L232     w = 0.0
L233
L234     def _valid_ratio(a, b):
L235         try:
L236             na, nb = float(a), float(b)
L237         except Exception:
L238             return None
L239         if not np.isfinite(na) or not np.isfinite(nb) or nb == 0:
L240             return None
L241         return na, nb
L242
L243     def yoy_q(series: pd.Series) -> float | None:
L244         s = _ensure_series(series)
L245         if s.empty:
L246             return None
L247         s = s.sort_index()
L248         if isinstance(s.index, pd.DatetimeIndex):
L249             last_idx = s.index[-1]
L250             window_start = last_idx - pd.DateOffset(months=15)
L251             window_end = last_idx - pd.DateOffset(months=9)
L252             candidates = s.loc[(s.index >= window_start) & (s.index <= window_end)]
L253             if candidates.empty:
L254                 candidates = s.loc[s.index <= window_end]
L255             if candidates.empty:
L256                 return None
L257             v1 = candidates.iloc[-1]
L258             v0 = s.iloc[-1]
L259         else:
L260             if s.size < 5:
L261                 return None
L262             v0 = s.iloc[-1]
L263             v1 = s.iloc[-5]
L264         pair = _valid_ratio(v0, v1)
L265         if pair is None:
L266             return None
L267         a, b = pair
L268         return float(a / b - 1.0)
L269
L270     def qoq(series: pd.Series) -> float | None:
L271         s = _ensure_series(series)
L272         if s.size < 2:
L273             return None
L274         s = s.sort_index()
L275         v0, v1 = s.iloc[-1], s.iloc[-2]
L276         pair = _valid_ratio(v0, v1)
L277         if pair is None:
L278             return None
L279         a, b = pair
L280         return float(a / b - 1.0)
L281
L282     def ttm_delta(series: pd.Series) -> float | None:
L283         s = _ensure_series(series)
L284         if s.size < 2:
L285             return None
L286         s = s.sort_index()
L287         k = int(min(4, s.size))
L288         cur_slice = s.iloc[-k:]
L289         prev_slice = s.iloc[:-k]
L290         if prev_slice.empty:
L291             return None
L292         prev_k = int(min(k, prev_slice.size))
L293         cur_sum = float(cur_slice.sum())
L294         prev_sum = float(prev_slice.iloc[-prev_k:].sum())
L295         pair = _valid_ratio(cur_sum, prev_sum)
L296         if pair is None:
L297             return None
L298         a, b = pair
L299         return float(a / b - 1.0)
L300
L301     def yoy_y(series: pd.Series) -> float | None:
L302         s = _ensure_series(series)
L303         if s.size < 2:
L304             return None
L305         s = s.sort_index()
L306         v0, v1 = s.iloc[-1], s.iloc[-2]
L307         pair = _valid_ratio(v0, v1)
L308         if pair is None:
L309             return None
L310         a, b = pair
L311         return float(a / b - 1.0)
L312
L313     def price_proxy_growth() -> float | None:
L314         if not isinstance(close_series, pd.Series):
L315             return None
L316         close = close_series.sort_index().dropna()
L317         if close.empty:
L318             return None
L319         hh_window = int(min(126, len(close)))
L320         if hh_window < 20:
L321             return None
L322         hh = close.rolling(hh_window).max().iloc[-1]
L323         prox = None
L324         if np.isfinite(hh) and hh > 0:
L325             prox = float(close.iloc[-1] / hh)
L326         rs6 = None
L327         if len(close) >= 63:
L328             rs6 = float(close.pct_change(63).iloc[-1])
L329         rs12 = None
L330         if len(close) >= 126:
L331             rs12 = float(close.pct_change(126).iloc[-1])
L332         vexp = None
L333         if isinstance(volume_series, pd.Series):
L334             vol = volume_series.reindex(close.index).dropna()
L335             if len(vol) >= 50:
L336                 v20 = vol.rolling(20).mean().iloc[-1]
L337                 v50 = vol.rolling(50).mean().iloc[-1]
L338                 if np.isfinite(v20) and np.isfinite(v50) and v50 > 0:
L339                     vexp = float(v20 / v50 - 1.0)
L340         prox = 0.0 if prox is None or not np.isfinite(prox) else prox
L341         rs6 = 0.0 if rs6 is None or not np.isfinite(rs6) else rs6
L342         rs12 = 0.0 if rs12 is None or not np.isfinite(rs12) else rs12
L343         vexp = 0.0 if vexp is None or not np.isfinite(vexp) else vexp
L344         return 0.5 * prox + 0.3 * rs6 + 0.2 * rs12 + 0.2 * vexp
L345
L346     price_alt = price_proxy_growth() or 0.0
L347     core = 0.0
L348     core_raw = 0.0
L349     price_raw = price_alt
L350
L351     if nQ >= 5:
L352         path = "P5"
L353         yq = yoy_q(s_rev_q)
L354         parts["rev_yoy_q"] = yq
L355         tmp_prev = s_rev_q.iloc[:-1] if s_rev_q.size > 1 else s_rev_q
L356         acc = None
L357         if tmp_prev.size >= 5 and yq is not None:
L358             yq_prev = yoy_q(tmp_prev)
L359             if yq_prev is not None:
L360                 acc = float(yq - yq_prev)
L361         parts["rev_acc_q"] = acc
L362         eps_yoy = yoy_q(s_eps_q) if s_eps_q.size >= 5 else None
L363         parts["eps_yoy_q"] = eps_yoy
L364         eps_acc = None
L365         if eps_yoy is not None and s_eps_q.size > 5:
L366             eps_prev = s_eps_q.iloc[:-1]
L367             if eps_prev.size >= 5:
L368                 eps_prev_yoy = yoy_q(eps_prev)
L369                 if eps_prev_yoy is not None:
L370                     eps_acc = float(eps_yoy - eps_prev_yoy)
L371         parts["eps_acc_q"] = eps_acc
L372         w = 1.0
L373         core_raw = (
L374             0.60 * _nz(yq)
L375             + 0.20 * _nz(acc)
L376             + 0.15 * _nz(eps_yoy)
L377             + 0.05 * _nz(eps_acc)
L378         )
L379         price_alt = 0.0
L380     elif 2 <= nQ <= 4:
L381         path = "P24"
L382         rev_qoq = qoq(s_rev_q)
L383         rev_ttm2 = ttm_delta(s_rev_q)
L384         parts["rev_qoq"] = rev_qoq
L385         parts["rev_ttm2"] = rev_ttm2
L386         eps_qoq = qoq(s_eps_q) if s_eps_q.size >= 2 else None
L387         parts["eps_qoq"] = eps_qoq
L388         w = min(1.0, nQ / 5.0)
L389         core_raw = 0.6 * _nz(rev_qoq) + 0.3 * _nz(rev_ttm2) + 0.1 * _nz(eps_qoq)
L390     else:
L391         path = "P1Y"
L392         rev_yoy_y = yoy_y(s_rev_y) if nY >= 2 else None
L393         parts["rev_yoy_y"] = rev_yoy_y
L394         w = 0.6 * min(1.0, nY / 3.0) if nY >= 2 else 0.4
L395         core_raw = _nz(rev_yoy_y)
L396         if nQ <= 1 and nY < 2 and price_alt == 0.0:
L397             price_alt = price_proxy_growth() or 0.0
L398
L399     core = _winsor(core_raw, lo=-1.5, hi=1.5)
L400     price_alt = _winsor(price_alt, lo=-1.5, hi=1.5)
L401     grw = _winsor(w * core + (1.0 - w) * (0.5 * _nz(price_alt)), lo=-2.0, hi=2.0)
L402
L403     parts.update(
L404         {
L405             "core_raw": core_raw,
L406             "core": core,
L407             "price_proxy_raw": price_raw,
L408             "price_proxy": price_alt,
L409             "weight": w,
L410             "score": grw,
L411         }
L412     )
L413
L414     parts_out: dict[str, Any] = {
L415         "nQ": nQ,
L416         "nY": nY,
L417     }
L418     for key, value in parts.items():
L419         if key in ("nQ", "nY"):
L420             continue
L421         rounded = _round_debug(value)
L422         parts_out[key] = rounded
L423
L424     info_entry["DEBUG_GRW_PATH"] = path
L425     info_entry["DEBUG_GRW_PARTS"] = json.dumps(parts_out, ensure_ascii=False, sort_keys=True)
L426     info_entry["GRW_SCORE"] = grw
L427     info_entry["GRW_WEIGHT"] = w
L428     info_entry["GRW_CORE"] = core
L429     info_entry["GRW_PRICE_PROXY"] = price_alt
L430
L431     return {
L432         "score": grw,
L433         "path": path,
L434         "parts": info_entry["DEBUG_GRW_PARTS"],
L435         "weight": w,
L436         "core": core,
L437         "price_proxy": price_alt,
L438     }
L439
L440
L441 D_WEIGHTS_EFF = None  # 出力表示互換のため
L442
L443
L444 def _scalar(v):
L445     """単一セル代入用に値をスカラーへ正規化する。
L446
L447     - pandas Series -> .iloc[-1]（最後を採用）
L448     - list/tuple/ndarray -> 最後の要素
L449     - それ以外          -> そのまま
L450     取得失敗時は np.nan を返す。
L451     """
L452     import numpy as _np
L453     import pandas as _pd
L454     try:
L455         if isinstance(v, _pd.Series):
L456             return v.iloc[-1] if len(v) else _np.nan
L457         if isinstance(v, (list, tuple, _np.ndarray)):
L458             return v[-1] if len(v) else _np.nan
L459         return v
L460     except Exception:
L461         return _np.nan
L462
L463
L464 # ---- Scorer 本体 -------------------------------------------------------------
L465 class Scorer:
L466     """
L467     - factor.py からは `aggregate_scores(ib, cfg)` を呼ぶだけでOK。
L468     - cfg は必須（factor.PipelineConfig を渡す）。
L469     - 旧カラム名を自動リネームして新スキーマに吸収します。
L470     """
L471
L472     # === 先頭で旧→新カラム名マップ（移行用） ===
L473     EPS_RENAME = {"eps_ttm":"EPS_TTM", "eps_q_recent":"EPS_Q_LastQ"}
L474     FCF_RENAME = {"fcf_ttm":"FCF_TTM"}
L475
L476     # === スキーマ簡易チェック（最低限） ===
L477     @staticmethod
L478     def _validate_ib_for_scorer(ib: Any):
L479         miss = [a for a in ["tickers","bench","data","px","spx","tickers_bulk","info","eps_df","fcf_df","returns"] if not hasattr(ib,a) or getattr(ib,a) is None]
L480         if miss: raise ValueError(f"InputBundle is missing required attributes for Scorer: {miss}")
L481         if any(c in ib.eps_df.columns for c in Scorer.EPS_RENAME): ib.eps_df.rename(columns=Scorer.EPS_RENAME, inplace=True)
L482         if any(c in ib.fcf_df.columns for c in Scorer.FCF_RENAME): ib.fcf_df.rename(columns=Scorer.FCF_RENAME, inplace=True)
L483         need_eps, need_fcf = {"EPS_TTM","EPS_Q_LastQ"},{"FCF_TTM"}
L484         if not need_eps.issubset(ib.eps_df.columns): raise ValueError(f"eps_df must contain columns {need_eps} (accepts old names via auto-rename). Got: {list(ib.eps_df.columns)}")
L485         if not need_fcf.issubset(ib.fcf_df.columns): raise ValueError(f"fcf_df must contain columns {need_fcf} (accepts old names via auto-rename). Got: {list(ib.fcf_df.columns)}")
L486
L487     # ----（Scorer専用）テクニカル・指標系 ----
L488     @staticmethod
L489     def trend(s: pd.Series):
L490         if len(s)<200: return np.nan
L491         sma50, sma150, sma200 = s.rolling(50).mean().iloc[-1], s.rolling(150).mean().iloc[-1], s.rolling(200).mean().iloc[-1]
L492         prev200, p = s.rolling(200).mean().iloc[-21], s.iloc[-1]
L493         lo_52 = s[-252:].min() if len(s)>=252 else s.min(); hi_52 = s[-252:].max() if len(s)>=252 else s.max()
L494         rng = (hi_52 - lo_52) if hi_52>lo_52 else np.nan
L495         clip = lambda x,lo,hi: (np.nan if pd.isna(x) else max(lo,min(hi,x)))
L496         a = clip(p/(s.rolling(50).mean().iloc[-1]) - 1, -0.5, 0.5)
L497         b = clip(sma50/sma150 - 1, -0.5, 0.5)
L498         c = clip(sma150/sma200 - 1, -0.5, 0.5)
L499         d = clip(sma200/prev200 - 1, -0.2, 0.2)
L500         e = clip((p - lo_52) / (rng if rng and rng>0 else np.nan) - 0.5, -0.5, 0.5)
L501         parts = [0.0 if pd.isna(x) else x for x in (a,b,c,d,e)]
L502         return 0.30*parts[0] + 0.20*parts[1] + 0.15*parts[2] + 0.15*parts[3] + 0.20*parts[4]
L503
L504     @staticmethod
L505     def rs(s, b):
L506         n, nb = len(s), len(b)
L507         if n<60 or nb<60: return np.nan
L508         L12 = 252 if n>=252 and nb>=252 else min(n,nb)-1; L1 = 22 if n>=22 and nb>=22 else max(5, min(n,nb)//3)
L509         r12, r1, br12, br1 = s.iloc[-1]/s.iloc[-L12]-1, s.iloc[-1]/s.iloc[-L1]-1, b.iloc[-1]/b.iloc[-L12]-1, b.iloc[-1]/b.iloc[-L1]-1
L510         return (r12 - br12)*0.7 + (r1 - br1)*0.3
L511
L512     @staticmethod
L513     def tr_str(s):
L514         if s is None:
L515             return np.nan
L516         s = s.ffill(limit=2).dropna()
L517         if len(s) < 50:
L518             return np.nan
L519         ma50 = s.rolling(50, min_periods=50).mean()
L520         last_ma = ma50.iloc[-1]
L521         last_px = s.iloc[-1]
L522         return float(last_px/last_ma - 1.0) if pd.notna(last_ma) and pd.notna(last_px) else np.nan
L523
L524     @staticmethod
L525     def rs_line_slope(s: pd.Series, b: pd.Series, win: int) -> float:
L526         r = (s/b).dropna()
L527         if len(r) < win: return np.nan
L528         y, x = np.log(r.iloc[-win:]), np.arange(win, dtype=float)
L529         try: return float(np.polyfit(x, y, 1)[0])
L530         except Exception: return np.nan
L531
L532     @staticmethod
L533     def ev_fallback(info_t: dict, tk: yf.Ticker) -> float:
L534         ev = info_t.get('enterpriseValue', np.nan)
L535         if pd.notna(ev) and ev>0: return float(ev)
L536         mc, debt, cash = info_t.get('marketCap', np.nan), np.nan, np.nan
L537         try:
L538             bs = tk.quarterly_balance_sheet
L539             if bs is not None and not bs.empty:
L540                 c = bs.columns[0]
L541                 for k in ("Total Debt","Long Term Debt","Short Long Term Debt"):
L542                     if k in bs.index: debt = float(bs.loc[k,c]); break
L543                 for k in ("Cash And Cash Equivalents","Cash And Cash Equivalents And Short Term Investments","Cash"):
L544                     if k in bs.index: cash = float(bs.loc[k,c]); break
L545         except Exception: pass
L546         if pd.notna(mc): return float(mc + (0 if pd.isna(debt) else debt) - (0 if pd.isna(cash) else cash))
L547         return np.nan
L548
L549     @staticmethod
L550     def dividend_status(ticker: str) -> str:
L551         t = yf.Ticker(ticker)
L552         try:
L553             if not t.dividends.empty: return "has"
L554         except Exception: return "unknown"
L555         try:
L556             a = t.actions
L557             if (a is not None and not a.empty and "Stock Splits" in a.columns and a["Stock Splits"].abs().sum()>0): return "none_confident"
L558         except Exception: pass
L559         try:
L560             fi = t.fast_info
L561             if any(getattr(fi,k,None) for k in ("last_dividend_date","dividend_rate","dividend_yield")): return "maybe_missing"
L562         except Exception: pass
L563         return "unknown"
L564
L565     @staticmethod
L566     def div_streak(t):
L567         try:
L568             divs = yf.Ticker(t).dividends.dropna(); ann = divs.groupby(divs.index.year).sum(); ann = ann[ann.index<pd.Timestamp.today().year]
L569             years, streak = sorted(ann.index), 0
L570             for i in range(len(years)-1,0,-1):
L571                 if ann[years[i]] > ann[years[i-1]]: streak += 1
L572                 else: break
L573             return streak
L574         except Exception: return 0
L575
L576     @staticmethod
L577     def fetch_finnhub_metrics(symbol):
L578         api_key = os.environ.get("FINNHUB_API_KEY")
L579         if not api_key: return {}
L580         url, params = "https://finnhub.io/api/v1/stock/metric", {"symbol":symbol,"metric":"all","token":api_key}
L581         try:
L582             r = requests.get(url, params=params, timeout=10); r.raise_for_status(); m = r.json().get("metric",{})
L583             return {'EPS':m.get('epsGrowthTTMYoy'),'REV':m.get('revenueGrowthTTMYoy'),'ROE':m.get('roeTTM'),'BETA':m.get('beta'),'DIV':m.get('dividendYieldIndicatedAnnual'),'FCF':(m.get('freeCashFlowTTM')/m.get('enterpriseValue')) if m.get('freeCashFlowTTM') and m.get('enterpriseValue') else None}
L584         except Exception: return {}
L585
L586     @staticmethod
L587     def calc_beta(series: pd.Series, market: pd.Series, lookback=252):
L588         r, m = series.pct_change().dropna(), market.pct_change().dropna()
L589         n = min(len(r), len(m), lookback)
L590         if n<60: return np.nan
L591         r, m = r.iloc[-n:], m.iloc[-n:]; cov, var = np.cov(r, m)[0,1], np.var(m)
L592         return np.nan if var==0 else cov/var
L593
L594     @staticmethod
L595     def spx_to_alpha(spx: pd.Series, bands=(0.03,0.10), w=(0.6,0.4),
L596                      span=5, q=(0.20,0.40), alphas=(0.05,0.08,0.10)) -> float:
L597         """
L598         S&P500指数のみから擬似breadthを作り、履歴分位でαを段階決定。
L599         bands=(±3%, ±10%), w=(50DMA,200DMA), 分位q=(20%,40%), alphas=(低,中,高)
L600         """
L601         ma50, ma200 = spx.rolling(50).mean(), spx.rolling(200).mean()
L602         b50, b200 = ((spx/ma50 - 1)+bands[0])/(2*bands[0]), ((spx/ma200 - 1)+bands[1])/(2*bands[1])
L603         hist = (w[0]*b50 + w[1]*b200).clip(0,1).ewm(span=span).mean()
L604         b, (lo, mid) = float(hist.iloc[-1]), (float(hist.quantile(q[0])), float(hist.quantile(q[1])))
L605         return alphas[0] if b < lo else alphas[1] if b < mid else alphas[2]
L606
L607     @staticmethod
L608     def soft_cap_effective_scores(scores: pd.Series|dict, sectors: dict, cap=2, alpha=0.08) -> pd.Series:
L609         """
L610         同一セクターcap超過（3本目以降）に α×段階減点を課した“有効スコア”Seriesを返す。
L611         戻り値は降順ソート済み。
L612         """
L613         s = pd.Series(scores, dtype=float); order = s.sort_values(ascending=False).index
L614         cnt, pen = {}, {}
L615         for t in order:
L616             sec = sectors.get(t, "U"); cnt[sec] = cnt.get(sec,0) + 1; pen[t] = alpha*max(0, cnt[sec]-cap)
L617         return (s - pd.Series(pen)).sort_values(ascending=False)
L618
L619     @staticmethod
L620     def pick_top_softcap(scores: pd.Series|dict, sectors: dict, N: int, cap=2, alpha=0.08, hard: int|None=5) -> list[str]:
L621         """
L622         soft-cap適用後の上位Nティッカーを返す。hard>0なら非常用ハード上限で同一セクター超過を間引く（既定=5）。
L623         """
L624         eff = Scorer.soft_cap_effective_scores(scores, sectors, cap, alpha)
L625         if not hard:
L626             return list(eff.head(N).index)
L627         pick, used = [], {}
L628         for t in eff.index:
L629             s = sectors.get(t, "U")
L630             if used.get(s,0) < hard:
L631                 pick.append(t); used[s] = used.get(s,0) + 1
L632             if len(pick) == N: break
L633         return pick
L634
L635     @staticmethod
L636     def trend_template_breadth_series(px: pd.DataFrame, spx: pd.Series, win_days: int | None = None) -> pd.Series:
L637         """
L638         各営業日の trend_template 合格本数（合格“本数”=C）を返す。
L639         - px: 列=ticker（ベンチは含めない）
L640         - spx: ベンチマーク Series（px.index に整列）
L641         - win_days: 末尾の計算対象営業日数（None→全体、既定600は呼び出し側指定）
L642         ベクトル化＆rollingのみで軽量。欠損は False 扱い。
L643         """
L644         import numpy as np, pandas as pd
L645         if px is None or px.empty:
L646             return pd.Series(dtype=int)
L647         px = px.dropna(how="all", axis=1)
L648         if win_days and win_days > 0:
L649             px = px.tail(win_days)
L650         if px.empty:
L651             return pd.Series(dtype=int)
L652         spx = spx.reindex(px.index).ffill()
L653
L654         ma50  = px.rolling(50).mean()
L655         ma150 = px.rolling(150).mean()
L656         ma200 = px.rolling(200).mean()
L657
L658         tt = (px > ma150)
L659         tt &= (px > ma200)
L660         tt &= (ma150 > ma200)
L661         tt &= (ma200 - ma200.shift(21) > 0)
L662         tt &= (ma50  > ma150)
L663         tt &= (ma50  > ma200)
L664         tt &= (px    > ma50)
L665
L666         lo252 = px.rolling(252).min()
L667         hi252 = px.rolling(252).max()
L668         tt &= (px.divide(lo252).sub(1.0) >= 0.30)   # P_OVER_LOW52 >= 0.30
L669         tt &= (px >= (0.75 * hi252))                # NEAR_52W_HIGH >= -0.25
L670
L671         r12  = px.divide(px.shift(252)).sub(1.0)
L672         br12 = spx.divide(spx.shift(252)).sub(1.0)
L673         r1   = px.divide(px.shift(22)).sub(1.0)
L674         br1  = spx.divide(spx.shift(22)).sub(1.0)
L675         rs   = 0.7*(r12.sub(br12, axis=0)) + 0.3*(r1.sub(br1, axis=0))
L676         tt &= (rs >= 0.10)
L677
L678         return tt.fillna(False).sum(axis=1).astype(int)
L679
L680     # ---- スコア集計（DTO/Configを受け取り、FeatureBundleを返す） ----
L681     def aggregate_scores(self, ib: Any, cfg):
L682         if cfg is None:
L683             raise ValueError("cfg is required; pass factor.PipelineConfig")
L684         self._validate_ib_for_scorer(ib)
L685
L686         px, spx, tickers = ib.px, ib.spx, ib.tickers
L687         tickers_bulk, info, eps_df, fcf_df = ib.tickers_bulk, ib.info, ib.eps_df, ib.fcf_df
L688
L689         df, missing_logs = pd.DataFrame(index=tickers), []
L690         for t in tickers:
L691             d, s = info[t], px[t]; ev = self.ev_fallback(d, tickers_bulk.tickers[t])
L692             try:
L693                 volume_series_full = ib.data['Volume'][t]
L694             except Exception:
L695                 volume_series_full = None
L696
L697             grw_result = _calc_grw_flexible(t, d, s, volume_series_full)
L698             df.loc[t,'GRW_FLEX_SCORE'] = grw_result.get('score')
L699             df.loc[t,'GRW_FLEX_WEIGHT'] = grw_result.get('weight')
L700             df.loc[t,'GRW_FLEX_CORE'] = grw_result.get('core')
L701             df.loc[t,'GRW_FLEX_PRICE'] = grw_result.get('price_proxy')
L702             df.loc[t,'DEBUG_GRW_PATH'] = grw_result.get('path')
L703             df.loc[t,'DEBUG_GRW_PARTS'] = grw_result.get('parts')
L704
L705             # --- 基本特徴 ---
L706             df.loc[t,'TR']   = self.trend(s)
L707             df.loc[t,'EPS']  = _scalar(eps_df.loc[t,'EPS_TTM']) if t in eps_df.index else np.nan
L708             df.loc[t,'EPS_Q'] = _scalar(eps_df.loc[t,'EPS_Q_LastQ']) if t in eps_df.index else np.nan
L709             df.loc[t,'REV_TTM'] = _scalar(eps_df.loc[t,'REV_TTM']) if t in eps_df.index else np.nan
L710             df.loc[t,'REV_Q']   = _scalar(eps_df.loc[t,'REV_Q_LastQ']) if t in eps_df.index else np.nan
L711             df.loc[t,'REV']  = d.get('revenueGrowth',np.nan)
L712             df.loc[t,'ROE']  = d.get('returnOnEquity',np.nan)
L713             df.loc[t,'BETA'] = self.calc_beta(s, spx, lookback=252)
L714
L715             # --- 配当（欠損補完含む） ---
L716             div = d.get('dividendYield') if d.get('dividendYield') is not None else d.get('trailingAnnualDividendYield')
L717             if div is None or pd.isna(div):
L718                 try:
L719                     divs = yf.Ticker(t).dividends
L720                     if divs is not None and not divs.empty:
L721                         last_close = s.iloc[-1]; div_1y = divs[divs.index >= (divs.index.max() - pd.Timedelta(days=365))].sum()
L722                         if last_close and last_close>0: div = float(div_1y/last_close)
L723                 except Exception: pass
L724             df.loc[t,'DIV'] = 0.0 if (div is None or pd.isna(div)) else float(div)
L725
L726             # --- FCF/EV ---
L727             fcf_val = fcf_df.loc[t,'FCF_TTM'] if t in fcf_df.index else np.nan
L728             df.loc[t,'FCF'] = (fcf_val/ev) if (pd.notna(fcf_val) and pd.notna(ev) and ev>0) else np.nan
L729
L730             # --- モメンタム・ボラ関連 ---
L731             df.loc[t,'RS'], df.loc[t,'TR_str'] = self.rs(s, spx), self.tr_str(s)
L732             r, rm = s.pct_change().dropna(), spx.pct_change().dropna()
L733             n = int(min(len(r), len(rm)))
L734
L735             DOWNSIDE_DEV = np.nan
L736             if n>=60:
L737                 r6 = r.iloc[-min(len(r),126):]; neg = r6[r6<0]
L738                 if len(neg)>=10: DOWNSIDE_DEV = float(neg.std(ddof=0)*np.sqrt(252))
L739             df.loc[t,'DOWNSIDE_DEV'] = DOWNSIDE_DEV
L740
L741             MDD_1Y = np.nan
L742             try:
L743                 w = s.iloc[-min(len(s),252):].dropna()
L744                 if len(w)>=30:
L745                     roll_max = w.cummax(); MDD_1Y = float((w/roll_max - 1.0).min())
L746             except Exception: pass
L747             df.loc[t,'MDD_1Y'] = MDD_1Y
L748
L749             RESID_VOL = np.nan
L750             if n>=120:
L751                 rr, rrm = r.iloc[-n:].align(rm.iloc[-n:], join='inner')
L752                 if len(rr)==len(rrm) and len(rr)>=120 and rrm.var()>0:
L753                     beta = float(np.cov(rr, rrm)[0,1]/np.var(rrm)); resid = rr - beta*rrm
L754                     RESID_VOL = float(resid.std(ddof=0)*np.sqrt(252))
L755             df.loc[t,'RESID_VOL'] = RESID_VOL
L756
L757             DOWN_OUTPERF = np.nan
L758             if n>=60:
L759                 m, x = rm.iloc[-n:], r.iloc[-n:]; mask = m<0
L760                 if mask.sum()>=10:
L761                     mr, sr = float(m[mask].mean()), float(x[mask].mean())
L762                     DOWN_OUTPERF = (sr - mr)/abs(mr) if mr!=0 else np.nan
L763             df.loc[t,'DOWN_OUTPERF'] = DOWN_OUTPERF
L764
L765             # --- 長期移動平均/位置 ---
L766             sma200 = s.rolling(200).mean(); df.loc[t,'EXT_200'] = np.nan
L767             if pd.notna(sma200.iloc[-1]) and sma200.iloc[-1]!=0: df.loc[t,'EXT_200'] = abs(float(s.iloc[-1]/sma200.iloc[-1]-1.0))
L768
L769             # --- 配当の詳細系 ---
L770             DIV_TTM_PS=DIV_VAR5=DIV_YOY=DIV_FCF_COVER=np.nan
L771             try:
L772                 divs = yf.Ticker(t).dividends.dropna()
L773                 if not divs.empty:
L774                     last_close = s.iloc[-1]; div_1y = float(divs[divs.index >= (divs.index.max()-pd.Timedelta(days=365))].sum())
L775                     DIV_TTM_PS = div_1y if div_1y>0 else np.nan
L776                     ann = divs.groupby(divs.index.year).sum()
L777                     if len(ann)>=2 and ann.iloc[-2]!=0: DIV_YOY = float(ann.iloc[-1]/ann.iloc[-2]-1.0)
L778                     tail = ann.iloc[-5:] if len(ann)>=5 else ann
L779                     if len(tail)>=3 and tail.mean()!=0: DIV_VAR5 = float(tail.std(ddof=1)/abs(tail.mean()))
L780                 so = d.get('sharesOutstanding',None)
L781                 if so and pd.notna(DIV_TTM_PS) and pd.notna(fcf_val) and fcf_val!=0:
L782                     DIV_FCF_COVER = float((fcf_val)/(DIV_TTM_PS*float(so)))
L783             except Exception: pass
L784             df.loc[t,'DIV_TTM_PS'], df.loc[t,'DIV_VAR5'], df.loc[t,'DIV_YOY'], df.loc[t,'DIV_FCF_COVER'] = DIV_TTM_PS, DIV_VAR5, DIV_YOY, DIV_FCF_COVER
L785
L786             # --- 財務安定性 ---
L787             df.loc[t,'DEBT2EQ'], df.loc[t,'CURR_RATIO'] = d.get('debtToEquity',np.nan), d.get('currentRatio',np.nan)
L788
L789             # --- EPS 変動 ---
L790             EPS_VAR_8Q = np.nan
L791             try:
L792                 qe, so = tickers_bulk.tickers[t].quarterly_earnings, d.get('sharesOutstanding',None)
L793                 if qe is not None and not qe.empty and so:
L794                     eps_q = (qe['Earnings'].dropna().astype(float)/float(so)).replace([np.inf,-np.inf],np.nan)
L795                     if len(eps_q)>=4: EPS_VAR_8Q = float(eps_q.iloc[-min(8,len(eps_q)):].std(ddof=1))
L796             except Exception: pass
L797             df.loc[t,'EPS_VAR_8Q'] = EPS_VAR_8Q
L798
L799             # --- サイズ/流動性 ---
L800             df.loc[t,'MARKET_CAP'] = d.get('marketCap',np.nan); adv60 = np.nan
L801             try:
L802                 if isinstance(volume_series_full, pd.Series):
L803                     vol_series = volume_series_full.reindex(s.index).dropna()
L804                     if len(vol_series) >= 5:
L805                         aligned_px = s.reindex(vol_series.index).dropna()
L806                         if len(aligned_px) == len(vol_series):
L807                             dv = (vol_series*aligned_px).rolling(60).mean()
L808                             if not dv.dropna().empty:
L809                                 adv60 = float(dv.dropna().iloc[-1])
L810             except Exception:
L811                 pass
L812             df.loc[t,'ADV60_USD'] = adv60
L813
L814             # --- Rule of 40 や周辺 ---
L815             total_rev_ttm = d.get('totalRevenue',np.nan)
L816             FCF_MGN = _safe_div(fcf_val, total_rev_ttm)
L817             df.loc[t,'FCF_MGN'] = FCF_MGN
L818             rule40 = np.nan
L819             try:
L820                 r = df.loc[t,'REV']; rule40 = (r if pd.notna(r) else np.nan) + (FCF_MGN if pd.notna(FCF_MGN) else np.nan)
L821             except Exception: pass
L822             df.loc[t,'RULE40'] = rule40
L823
L824             # --- トレンド補助 ---
L825             sma50  = s.rolling(50).mean()
L826             sma150 = s.rolling(150).mean()
L827             sma200 = s.rolling(200).mean()
L828             p = _safe_last(s)
L829
L830             df.loc[t,'MA50_OVER_150'] = (_safe_last(sma50)/_safe_last(sma150) - 1
L831                 if pd.notna(_safe_last(sma50)) and pd.notna(_safe_last(sma150)) and _safe_last(sma150)!=0 else np.nan)
L832             df.loc[t,'MA150_OVER_200'] = (_safe_last(sma150)/_safe_last(sma200) - 1
L833                 if pd.notna(_safe_last(sma150)) and pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan)
L834
L835             lo52 = s[-252:].min() if len(s)>=252 else s.min()
L836             df.loc[t,'P_OVER_LOW52'] = (p/lo52 - 1) if (lo52 and lo52>0 and pd.notna(p)) else np.nan
L837
L838             df.loc[t,'MA200_SLOPE_1M'] = np.nan
L839             if len(sma200.dropna()) >= 21:
L840                 cur200 = _safe_last(sma200)
L841                 old2001 = float(sma200.iloc[-21])
L842                 if old2001:
L843                     df.loc[t,'MA200_SLOPE_1M'] = cur200/old2001 - 1
L844
L845             df.loc[t,'P_OVER_150'] = p/_safe_last(sma150)-1 if pd.notna(_safe_last(sma150)) and _safe_last(sma150)!=0 else np.nan
L846             df.loc[t,'P_OVER_200'] = p/_safe_last(sma200)-1 if pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L847             df.loc[t,'MA50_OVER_200'] = _safe_last(sma50)/_safe_last(sma200)-1 if pd.notna(_safe_last(sma50)) and pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L848             df.loc[t,'MA200_SLOPE_5M'] = np.nan
L849             if len(sma200.dropna())>=105:
L850                 cur200, old200 = _safe_last(sma200), float(sma200.iloc[-105])
L851                 if old200 and old200!=0: df.loc[t,'MA200_SLOPE_5M'] = cur200/old200 - 1
L852             # NEW: 200日線が連続で上向きの「日数」
L853             df.loc[t,'MA200_UP_STREAK_D'] = np.nan
L854             try:
L855                 s200 = sma200.dropna()
L856                 if len(s200) >= 2:
L857                     diff200 = s200.diff()
L858                     up = 0
L859                     for v in diff200.iloc[::-1]:
L860                         if pd.isna(v) or v <= 0:
L861                             break
L862                         up += 1
L863                     df.loc[t,'MA200_UP_STREAK_D'] = float(up)
L864             except Exception:
L865                 pass
L866             df.loc[t,'LOW52PCT25_EXCESS'] = np.nan if (lo52 is None or lo52<=0 or pd.isna(p)) else (p/(lo52*1.25)-1)
L867             hi52 = s[-252:].max() if len(s)>=252 else s.max(); df.loc[t,'NEAR_52W_HIGH'] = np.nan
L868             if hi52 and hi52>0 and pd.notna(p):
L869                 d_hi = (p/hi52)-1.0; df.loc[t,'NEAR_52W_HIGH'] = -abs(min(0.0, d_hi))
L870             df.loc[t,'RS_SLOPE_6W'] = self.rs_line_slope(s, ib.spx, 30)
L871             df.loc[t,'RS_SLOPE_13W'] = self.rs_line_slope(s, ib.spx, 65)
L872
L873             df.loc[t,'DIV_STREAK'] = self.div_streak(t)
L874
L875             # --- 欠損メモ ---
L876             fin_cols = ['REV','ROE','BETA','DIV','FCF']
L877             need_finnhub = [col for col in fin_cols if pd.isna(df.loc[t,col])]
L878             if need_finnhub:
L879                 fin_data = self.fetch_finnhub_metrics(t)
L880                 for col in need_finnhub:
L881                     val = fin_data.get(col)
L882                     if val is not None and not pd.isna(val): df.loc[t,col] = val
L883             for col in fin_cols + ['EPS','RS','TR_str','DIV_STREAK']:
L884                 if pd.isna(df.loc[t,col]):
L885                     if col=='DIV':
L886                         status = self.dividend_status(t)
L887                         if status!='none_confident': missing_logs.append({'Ticker':t,'Column':col,'Status':status})
L888                     else:
L889                         missing_logs.append({'Ticker':t,'Column':col})
L890
L891         def _pick_series(entry: dict, keys: list[str]):
L892             for k in keys:
L893                 val = entry.get(k) if isinstance(entry, dict) else None
L894                 if val is None:
L895                     continue
L896                 try:
L897                     if hasattr(val, "empty") and getattr(val, "empty"):
L898                         continue
L899                 except Exception:
L900                     pass
L901                 if isinstance(val, (list, tuple)) and len(val) == 0:
L902                     continue
L903                 return val
L904             return None
L905
L906         def _has_sec_series(val) -> bool:
L907             try:
L908                 if isinstance(val, pd.Series):
L909                     return not val.dropna().empty
L910                 if isinstance(val, (list, tuple)):
L911                     return any(pd.notna(v) for v in val)
L912                 return bool(val)
L913             except Exception:
L914                 return False
L915
L916         def _series_len(val) -> int:
L917             try:
L918                 if isinstance(val, pd.Series):
L919                     return int(val.dropna().size)
L920                 if isinstance(val, (list, tuple)):
L921                     return len(val)
L922                 return int(bool(val))
L923             except Exception:
L924                 return 0
L925
L926         cnt_rev_series = sum(1 for _t, d in info.items() if _has_sec_series(d.get("SEC_REV_Q_SERIES")))
L927         cnt_eps_series = sum(1 for _t, d in info.items() if _has_sec_series(d.get("SEC_EPS_Q_SERIES")))
L928         logger.info(
L929             "[DERIV] SEC series presence: REV_Q=%d, EPS_Q=%d (universe=%d)",
L930             cnt_rev_series,
L931             cnt_eps_series,
L932             len(info),
L933         )
L934
L935         rev_q_ge5 = 0
L936         ttm_yoy_avail = 0
L937         wrote_growth = 0
L938
L939         for t in tickers:
L940             try:
L941                 d = info.get(t, {}) or {}
L942                 rev_series = d.get("SEC_REV_Q_SERIES")
L943                 eps_series = d.get("SEC_EPS_Q_SERIES")
L944                 fallback_qearn = False
L945                 try:
L946                     qe = tickers_bulk.tickers[t].quarterly_earnings
L947                     fallback_qearn = bool(qe is not None and not getattr(qe, "empty", True))
L948                 except Exception:
L949                     qe = None
L950                 logger.debug(
L951                     "[DERIV] %s: rev_q_len=%s eps_q_len=%s fallback_qearn=%s",
L952                     t,
L953                     _series_len(rev_series),
L954                     _series_len(eps_series),
L955                     fallback_qearn,
L956                 )
L957
L958                 r_src = _pick_series(d, ["SEC_REV_Q_SERIES", "rev_q_series_pairs", "rev_q_series"])
L959                 e_src = _pick_series(d, ["SEC_EPS_Q_SERIES", "eps_q_series_pairs", "eps_q_series"])
L960                 r_raw = _ensure_series(r_src)
L961                 e_raw = _ensure_series(e_src)
L962                 _log("DERIV_SRC", f"{t} rev_raw_len={r_raw.size} eps_raw_len={e_raw.size}")
L963
L964                 r_q = _to_quarterly(r_raw)
L965                 e_q = _to_quarterly(e_raw)
L966                 _log("DERIV_Q", f"{t} rev_q_len={r_q.size} eps_q_len={e_q.size}")
L967                 if r_q.size >= 5:
L968                     rev_q_ge5 += 1
L969
L970                 r_yoy_ttm = _ttm_yoy_from_quarterly(r_q)
L971                 e_yoy_ttm = _ttm_yoy_from_quarterly(e_q)
L972                 has_ttm = int(not r_yoy_ttm.dropna().empty)
L973                 ttm_yoy_avail += has_ttm
L974                 _log("DERIV_TTM", f"{t} rev_ttm_yoy_len={r_yoy_ttm.dropna().size} eps_ttm_yoy_len={e_yoy_ttm.dropna().size}")
L975
L976                 def _q_yoy(qs):
L977                     return np.nan if qs is None or len(qs) < 5 else float(qs.iloc[-1] / qs.iloc[-5] - 1.0)
L978
L979                 rev_q_yoy = _q_yoy(r_q)
L980                 eps_q_yoy = _q_yoy(e_q)
L981
L982                 def _annual_from(qs: pd.Series, yoy_ttm: pd.Series):
L983                     if isinstance(qs.index, pd.DatetimeIndex) and len(qs) >= 8:
L984                         ann = qs.groupby(qs.index.year).last().pct_change()
L985                         ann_dn = ann.dropna()
L986                         if not ann_dn.empty:
L987                             y = float(ann_dn.iloc[-1])
L988                             acc = float(ann_dn.tail(3).mean()) if ann_dn.size >= 3 else np.nan
L989                             var = float(ann_dn.tail(4).var()) if ann_dn.size >= 4 else np.nan
L990                             return y, acc, var
L991                     yoy_dn = yoy_ttm.dropna()
L992                     if yoy_dn.empty:
L993                         return np.nan, np.nan, np.nan
L994                     return (
L995                         float(yoy_dn.iloc[-1]),
L996                         float(yoy_dn.tail(3).mean() if yoy_dn.size >= 3 else np.nan),
L997                         float(yoy_dn.tail(4).var() if yoy_dn.size >= 4 else np.nan),
L998                     )
L999
L1000                 rev_yoy, rev_acc, rev_var = _annual_from(r_q, r_yoy_ttm)
L1001                 eps_yoy, _, _ = _annual_from(e_q, e_yoy_ttm)
L1002
L1003                 def _pos_streak(s: pd.Series):
L1004                     s = s.dropna()
L1005                     if s.empty:
L1006                         return np.nan
L1007                     b = (s > 0).astype(int).to_numpy()[::-1]
L1008                     k = 0
L1009                     for v in b:
L1010                         if v == 1:
L1011                             k += 1
L1012                         else:
L1013                             break
L1014                     return float(k)
L1015
L1016                 rev_ann_streak = _pos_streak(r_yoy_ttm)
L1017
L1018                 df.loc[t, "REV_Q_YOY"] = rev_q_yoy
L1019                 df.loc[t, "EPS_Q_YOY"] = eps_q_yoy
L1020                 df.loc[t, "REV_YOY"] = rev_yoy
L1021                 df.loc[t, "EPS_YOY"] = eps_yoy
L1022                 df.loc[t, "REV_YOY_ACC"] = rev_acc
L1023                 df.loc[t, "REV_YOY_VAR"] = rev_var
L1024                 df.loc[t, "REV_ANN_STREAK"] = rev_ann_streak
L1025
L1026                 wrote_growth += 1
L1027                 _log(
L1028                     "DERIV_WRITE",
L1029                     f"{t} wrote: Q_YOY(rev={rev_q_yoy}, eps={eps_q_yoy}) ANN(rev_yoy={rev_yoy}, acc={rev_acc}, var={rev_var}) streak={rev_ann_streak}",
L1030                 )
L1031
L1032             except Exception as e:
L1033                 logger.warning("[DERIV_WARN] %s growth-derivatives failed: %s", t, e)
L1034                 _log("DERIV_WARN", f"{t} {type(e).__name__}: {e}")
L1035
L1036         _log("DERIV_SUMMARY", f"rev_q_len>=5: {rev_q_ge5}/{len(tickers)}  ttm_yoy_available: {ttm_yoy_avail}  wrote_growth_for: {wrote_growth}")
L1037
L1038         try:
L1039             cols = [
L1040                 "REV_Q_YOY",
L1041                 "EPS_Q_YOY",
L1042                 "REV_YOY",
L1043                 "EPS_YOY",
L1044                 "REV_YOY_ACC",
L1045                 "REV_YOY_VAR",
L1046                 "REV_ANN_STREAK",
L1047             ]
L1048             cnt = {c: int(df[c].count()) for c in cols if c in df.columns}
L1049             _log("DERIV_NONNAN_COUNTS", str(cnt))
L1050         except Exception as e:
L1051             _log("DERIV_NONNAN_COUNTS", f"error: {e}")
L1052
L1053         def _trend_template_pass(row, rs_alpha_thresh=0.10):
L1054             c1 = (row.get('P_OVER_150', np.nan) > 0) and (row.get('P_OVER_200', np.nan) > 0)
L1055             c2 = (row.get('MA150_OVER_200', np.nan) > 0)
L1056             c3 = (row.get('MA200_SLOPE_1M', np.nan) > 0)
L1057             c4 = (row.get('MA50_OVER_150', np.nan) > 0) and (row.get('MA50_OVER_200', np.nan) > 0)
L1058             c5 = (row.get('TR_str', np.nan) > 0)
L1059             c6 = (row.get('P_OVER_LOW52', np.nan) >= 0.30)
L1060             c7 = (row.get('NEAR_52W_HIGH', np.nan) >= -0.25)
L1061             c8 = (row.get('RS', np.nan) >= 0.10)
L1062             return bool(c1 and c2 and c3 and c4 and c5 and c6 and c7 and c8)
L1063
L1064         if 'trend_template' not in df.columns: df['trend_template'] = df.apply(_trend_template_pass, axis=1).fillna(False)
L1065         assert 'trend_template' in df.columns
L1066
L1067         # === Z化と合成 ===
L1068         for col in ['ROE','FCF','REV','EPS']: df[f'{col}_W'] = winsorize_s(df[col], 0.02)
L1069
L1070         df_z = pd.DataFrame(index=df.index)
L1071         for col in ['EPS','REV','ROE','FCF','RS','TR_str','BETA','DIV','DIV_STREAK']: df_z[col] = robust_z(df[col])
L1072         df_z['REV'], df_z['EPS'], df_z['TR'] = robust_z(df['REV_W']), robust_z(df['EPS_W']), robust_z(df['TR'])
L1073         for col in ['P_OVER_150','P_OVER_200','MA50_OVER_200','MA200_SLOPE_5M','LOW52PCT25_EXCESS','NEAR_52W_HIGH','RS_SLOPE_6W','RS_SLOPE_13W','MA200_UP_STREAK_D']: df_z[col] = robust_z(df[col])
L1074
L1075         # === Growth深掘り系（欠損保持z + RAW併載） ===
L1076         grw_cols = ['REV_Q_YOY','EPS_Q_YOY','REV_YOY','EPS_YOY','REV_YOY_ACC','REV_YOY_VAR','FCF_MGN','RULE40','REV_ANN_STREAK']
L1077         for col in grw_cols:
L1078             if col in df.columns:
L1079                 raw = pd.to_numeric(df[col], errors="coerce")
L1080                 df_z[col] = robust_z_keepnan(raw)
L1081                 df_z[f'{col}_RAW'] = raw
L1082         for k in ("TREND_SLOPE_EPS", "TREND_SLOPE_REV"):
L1083             if k in df.columns and k not in df_z.columns:
L1084                 raw = pd.to_numeric(df[k], errors="coerce")
L1085                 df_z[k] = robust_z_keepnan(raw)
L1086                 df_z[f'{k}_RAW'] = raw
L1087         for col in ['DOWNSIDE_DEV','MDD_1Y','RESID_VOL','DOWN_OUTPERF','EXT_200','DIV_TTM_PS','DIV_VAR5','DIV_YOY','DIV_FCF_COVER','DEBT2EQ','CURR_RATIO','EPS_VAR_8Q','MARKET_CAP','ADV60_USD']: df_z[col] = robust_z(df[col])
L1088
L1089         df_z['SIZE'], df_z['LIQ'] = robust_z(np.log1p(df['MARKET_CAP'])), robust_z(np.log1p(df['ADV60_USD']))
L1090         df_z['QUALITY_F'] = robust_z(0.6*df['FCF_W'] + 0.4*df['ROE_W']).clip(-3.0,3.0)
L1091         df_z['YIELD_F']   = 0.3*df_z['DIV'] + 0.7*df_z['DIV_STREAK']
L1092
L1093         # EPSが赤字でもFCFが黒字なら実質黒字とみなす
L1094         eps_pos_mask = (df['EPS'] > 0) | (df['FCF_MGN'] > 0)
L1095         df_z['EPS_POS'] = df_z['EPS'].where(eps_pos_mask, 0.0)
L1096
L1097         # ===== トレンドスロープ算出 =====
L1098         def zpos(x):
L1099             arr = robust_z(x)
L1100             idx = getattr(x, 'index', df_z.index)
L1101             return pd.Series(arr, index=idx).fillna(0.0)
L1102
L1103         def relu(x):
L1104             ser = x if isinstance(x, pd.Series) else pd.Series(x, index=df_z.index)
L1105             return ser.clip(lower=0).fillna(0.0)
L1106
L1107         # 売上トレンドスロープ（四半期）
L1108         slope_rev = 0.70*zpos(df_z['REV_Q_YOY']) + 0.30*zpos(df_z['REV_YOY_ACC'])
L1109         noise_rev = relu(robust_z(df_z['REV_YOY_VAR']) - 0.8)
L1110         slope_rev_combo = slope_rev - 0.25*noise_rev
L1111         df_z['TREND_SLOPE_REV_RAW'] = slope_rev_combo
L1112         df_z['TREND_SLOPE_REV'] = slope_rev_combo.clip(-3.0, 3.0)
L1113
L1114         # EPSトレンドスロープ（四半期）
L1115         slope_eps = 0.60*zpos(df_z['EPS_Q_YOY']) + 0.40*zpos(df_z['EPS_POS'])
L1116         df_z['TREND_SLOPE_EPS_RAW'] = slope_eps
L1117         df_z['TREND_SLOPE_EPS'] = slope_eps.clip(-3.0, 3.0)
L1118
L1119         # 年次トレンド（サブ）
L1120         slope_rev_yr = zpos(df_z['REV_YOY'])
L1121         slope_eps_yr = zpos(df_z.get('EPS_YOY', pd.Series(0.0, index=df.index)))
L1122         streak_base = df['REV_ANN_STREAK'].clip(lower=0).fillna(0)
L1123         streak_yr = streak_base / (streak_base.abs() + 1.0)
L1124         slope_rev_yr_combo = 0.7*slope_rev_yr + 0.3*streak_yr
L1125         df_z['TREND_SLOPE_REV_YR_RAW'] = slope_rev_yr_combo
L1126         df_z['TREND_SLOPE_REV_YR'] = slope_rev_yr_combo.clip(-3.0, 3.0)
L1127         df_z['TREND_SLOPE_EPS_YR_RAW'] = slope_eps_yr
L1128         df_z['TREND_SLOPE_EPS_YR'] = slope_eps_yr.clip(-3.0, 3.0)
L1129
L1130         # ===== GRW flexible score (variable data paths) =====
L1131         grw_raw = pd.to_numeric(df.get('GRW_FLEX_SCORE'), errors="coerce")
L1132         df_z['GRW_FLEX_SCORE_RAW'] = grw_raw
L1133         df_z['GROWTH_F_RAW'] = grw_raw
L1134         df_z['GROWTH_F'] = robust_z_keepnan(grw_raw).clip(-3.0, 3.0)
L1135         df_z['GRW_FLEX_WEIGHT'] = pd.to_numeric(df.get('GRW_FLEX_WEIGHT'), errors="coerce")
L1136         df_z['GRW_FLEX_CORE_RAW'] = pd.to_numeric(df.get('GRW_FLEX_CORE'), errors="coerce")
L1137         df_z['GRW_FLEX_PRICE_RAW'] = pd.to_numeric(df.get('GRW_FLEX_PRICE'), errors="coerce")
L1138
L1139         # Debug dump for GRW composition (console OFF by default; enable only with env)
L1140         if bool(os.getenv("GRW_CONSOLE_DEBUG")):
L1141             try:
L1142                 cols = ['GROWTH_F', 'GROWTH_F_RAW', 'GRW_FLEX_WEIGHT']
L1143                 use_cols = [c for c in cols if c in df_z.columns]
L1144                 i = df_z[use_cols].copy() if use_cols else pd.DataFrame(index=df_z.index)
L1145                 i.sort_values('GROWTH_F', ascending=False, inplace=True)
L1146                 limit = max(0, min(40, len(i)))
L1147                 print("[DEBUG: GRW]")
L1148                 for t in i.index[:limit]:
L1149                     row = i.loc[t]
L1150                     parts = []
L1151                     if pd.notna(row.get('GROWTH_F')):
L1152                         parts.append(f"GROWTH_F={row.get('GROWTH_F'):.3f}")
L1153                     raw_val = row.get('GROWTH_F_RAW')
L1154                     if pd.notna(raw_val):
L1155                         parts.append(f"GROWTH_F_RAW={raw_val:.3f}")
L1156                     weight_val = row.get('GRW_FLEX_WEIGHT')
L1157                     if pd.notna(weight_val):
L1158                         parts.append(f"w={weight_val:.2f}")
L1159                     path_val = None
L1160                     try:
L1161                         path_val = info.get(t, {}).get('DEBUG_GRW_PATH')
L1162                     except Exception:
L1163                         path_val = None
L1164                     if not path_val and 'DEBUG_GRW_PATH' in df.columns:
L1165                         path_val = df.at[t, 'DEBUG_GRW_PATH']
L1166                     if path_val:
L1167                         parts.append(f"PATH={path_val}")
L1168                     parts_json = None
L1169                     try:
L1170                         parts_json = info.get(t, {}).get('DEBUG_GRW_PARTS')
L1171                     except Exception:
L1172                         parts_json = None
L1173                     if not parts_json and 'DEBUG_GRW_PARTS' in df.columns:
L1174                         parts_json = df.at[t, 'DEBUG_GRW_PARTS']
L1175                     if parts_json:
L1176                         parts.append(f"PARTS={parts_json}")
L1177                     if not parts:
L1178                         parts.append('no-data')
L1179                     print(f"Ticker: {t} | " + " ".join(parts))
L1180                 print()
L1181             except Exception as exc:
L1182                 print(f"[ERR] GRW debug dump failed: {exc}")
L1183
L1184         df_z['MOM_F'] = robust_z(0.40*df_z['RS']
L1185             + 0.15*df_z['TR_str']
L1186             + 0.15*df_z['RS_SLOPE_6W']
L1187             + 0.15*df_z['RS_SLOPE_13W']
L1188             + 0.10*df_z['MA200_SLOPE_5M']
L1189             + 0.10*df_z['MA200_UP_STREAK_D']).clip(-3.0,3.0)
L1190         df_z['VOL'] = robust_z(df['BETA'])
L1191         df_z['QAL'], df_z['YLD'], df_z['MOM'] = df_z['QUALITY_F'], df_z['YIELD_F'], df_z['MOM_F']
L1192         df_z.drop(columns=['QUALITY_F','YIELD_F','MOM_F'], inplace=True, errors='ignore')
L1193
L1194         _dump_dfz(df_z=df_z, debug_mode=getattr(cfg, "debug_mode", False))
L1195
L1196         # === begin: BIO LOSS PENALTY =====================================
L1197         try:
L1198             penalty_z = float(os.getenv("BIO_LOSS_PENALTY_Z", "0.8"))
L1199         except Exception:
L1200             penalty_z = 0.8
L1201
L1202         def _is_bio_like(t: str) -> bool:
L1203             inf = info.get(t, {}) if isinstance(info, dict) else {}
L1204             sec = str(inf.get("sector", "")).lower()
L1205             ind = str(inf.get("industry", "")).lower()
L1206             if "health" not in sec:
L1207                 return False
L1208             keys = ("biotech", "biopharma", "pharma")
L1209             return any(k in ind for k in keys)
L1210
L1211         tickers_s = pd.Index(df_z.index)
L1212         is_bio = pd.Series({t: _is_bio_like(t) for t in tickers_s})
L1213         is_loss = pd.Series({t: (pd.notna(df.loc[t,"EPS"]) and df.loc[t,"EPS"] <= 0) for t in tickers_s})
L1214         mask_bio_loss = (is_bio & is_loss).reindex(df_z.index).fillna(False)
L1215
L1216         if bool(mask_bio_loss.any()) and penalty_z > 0:
L1217             df_z.loc[mask_bio_loss, "GROWTH_F"] = df_z.loc[mask_bio_loss, "GROWTH_F"] - penalty_z
L1218             df_z["GROWTH_F"] = df_z["GROWTH_F"].clip(-3.0, 3.0)
L1219         # === end: BIO LOSS PENALTY =======================================
L1220
L1221         df_z['TRD'] = 0.0  # TRDはスコア寄与から外し、テンプレ判定はフィルタで行う（列は表示互換のため残す）
L1222         if 'BETA' not in df_z.columns: df_z['BETA'] = robust_z(df['BETA'])
L1223
L1224         df_z['D_VOL_RAW'] = robust_z(0.40*df_z['DOWNSIDE_DEV'] + 0.22*df_z['RESID_VOL'] + 0.18*df_z['MDD_1Y'] - 0.10*df_z['DOWN_OUTPERF'] - 0.05*df_z['EXT_200'] - 0.08*df_z['SIZE'] - 0.10*df_z['LIQ'] + 0.10*df_z['BETA'])
L1225         df_z['D_QAL']     = robust_z(0.35*df_z['QAL'] + 0.20*df_z['FCF'] + 0.15*df_z['CURR_RATIO'] - 0.15*df_z['DEBT2EQ'] - 0.15*df_z['EPS_VAR_8Q'])
L1226         df_z['D_YLD']     = robust_z(0.45*df_z['DIV'] + 0.25*df_z['DIV_STREAK'] + 0.20*df_z['DIV_FCF_COVER'] - 0.10*df_z['DIV_VAR5'])
L1227         df_z['D_TRD']     = robust_z(0.40*df_z.get('MA200_SLOPE_5M',0) - 0.30*df_z.get('EXT_200',0) + 0.15*df_z.get('NEAR_52W_HIGH',0) + 0.15*df_z['TR'])
L1228
L1229         # --- 重みは cfg を優先（外部があればそれを使用） ---
L1230         # ① 全銘柄で G/D スコアを算出（unmasked）
L1231         g_score_all = df_z.mul(pd.Series(cfg.weights.g)).sum(axis=1)
L1232
L1233         d_comp = pd.concat({
L1234             'QAL': df_z['D_QAL'],
L1235             'YLD': df_z['D_YLD'],
L1236             'VOL': df_z['D_VOL_RAW'],
L1237             'TRD': df_z['D_TRD']
L1238         }, axis=1)
L1239         dw = pd.Series(cfg.weights.d, dtype=float).reindex(['QAL','YLD','VOL','TRD']).fillna(0.0)
L1240         globals()['D_WEIGHTS_EFF'] = dw.copy()
L1241         d_score_all = d_comp.mul(dw, axis=1).sum(axis=1)
L1242
L1243         # ② テンプレ判定（既存ロジックそのまま）
L1244         mask = df['trend_template']
L1245         if not bool(mask.any()):
L1246             mask = ((df.get('P_OVER_LOW52', np.nan) >= 0.25) &
L1247                 (df.get('NEAR_52W_HIGH', np.nan) >= -0.30) &
L1248                 (df.get('RS', np.nan) >= 0.08) &
L1249                 (df.get('MA200_SLOPE_1M', np.nan) > 0) &
L1250                 (df.get('P_OVER_150', np.nan) > 0) & (df.get('P_OVER_200', np.nan) > 0) &
L1251                 (df.get('MA150_OVER_200', np.nan) > 0) &
L1252                 (df.get('MA50_OVER_150', np.nan) > 0) & (df.get('MA50_OVER_200', np.nan) > 0) &
L1253                 (df.get('TR_str', np.nan) > 0)).fillna(False)
L1254             df['trend_template'] = mask
L1255
L1256         # ③ 採用用は mask、表示/分析用は列で全銘柄保存
L1257         g_score = g_score_all.loc[mask]
L1258         Scorer.g_score = g_score
L1259         df_z['GSC'] = g_score_all
L1260         df_z['DSC'] = d_score_all
L1261
L1262         try:
L1263             current = (pd.read_csv("current_tickers.csv")
L1264                   .iloc[:, 0]
L1265                   .str.upper()
L1266                   .tolist())
L1267         except FileNotFoundError:
L1268             warnings.warn("current_tickers.csv not found — bonus skipped")
L1269             current = []
L1270
L1271         mask_bonus = g_score.index.isin(current)
L1272         if mask_bonus.any():
L1273             # 1) factor.BONUS_COEFF から k を決め、無ければ 0.4
L1274             k = float(getattr(sys.modules.get("factor"), "BONUS_COEFF", 0.4))
L1275             # 2) g 側の σ を取り、NaN なら 0 に丸める
L1276             sigma_g = g_score.std()
L1277             if pd.isna(sigma_g):
L1278                 sigma_g = 0.0
L1279             bonus_g = round(k * sigma_g, 3)
L1280             g_score.loc[mask_bonus] += bonus_g
L1281             Scorer.g_score = g_score
L1282             # 3) D 側も同様に σ の NaN をケア
L1283             sigma_d = d_score_all.std()
L1284             if pd.isna(sigma_d):
L1285                 sigma_d = 0.0
L1286             bonus_d = round(k * sigma_d, 3)
L1287             d_score_all.loc[d_score_all.index.isin(current)] += bonus_d
L1288
L1289         try:
L1290             df = _apply_growth_entry_flags(df, ib, self, win_breakout=5, win_pullback=5)
L1291         except Exception:
L1292             pass
L1293
L1294         df_full = df.copy()
L1295         df_full_z = df_z.copy()
L1296
L1297         from factor import FeatureBundle  # type: ignore  # 実行時importなし（循環回避）
L1298         return FeatureBundle(df=df,
L1299             df_z=df_z,
L1300             g_score=g_score,
L1301             d_score_all=d_score_all,
L1302             missing_logs=pd.DataFrame(missing_logs),
L1303             df_full=df_full,
L1304             df_full_z=df_full_z,
L1305             scaler=None)
L1306
L1307 def _apply_growth_entry_flags(feature_df, bundle, self_obj, win_breakout=5, win_pullback=5):
L1308     """
L1309     G枠ユニバースに対し、ブレイクアウト確定/押し目反発の「直近N営業日内の発火」を判定し、
L1310     次の列を feature_df に追加する（index=ticker）。
L1311       - G_BREAKOUT_recent_5d : bool
L1312       - G_BREAKOUT_last_date : str "YYYY-MM-DD"
L1313       - G_PULLBACK_recent_5d : bool
L1314       - G_PULLBACK_last_date : str "YYYY-MM-DD"
L1315       - G_PIVOT_price        : float
L1316     失敗しても例外は握り潰し、既存処理を阻害しない。
L1317     """
L1318     try:
L1319         px   = bundle.px                      # 終値 DataFrame
L1320         hi   = bundle.data['High']
L1321         lo   = bundle.data['Low']
L1322         vol  = bundle.data['Volume']
L1323         bench= bundle.spx                     # ベンチマーク Series
L1324
L1325         # Gユニバース推定：self.g_universe 優先 → feature_df['group']=='G' → 全銘柄
L1326         g_universe = getattr(self_obj, "g_universe", None)
L1327         if g_universe is None:
L1328             try:
L1329                 g_universe = feature_df.index[feature_df['group'].astype(str).str.upper().eq('G')].tolist()
L1330             except Exception:
L1331                 g_universe = list(feature_df.index)
L1332         if not g_universe:
L1333             return feature_df
L1334
L1335         # 指標
L1336         px = px.ffill(limit=2)
L1337         ema21 = px[g_universe].ewm(span=21, adjust=False).mean()
L1338         ma50  = px[g_universe].rolling(50).mean()
L1339         ma150 = px[g_universe].rolling(150).mean()
L1340         ma200 = px[g_universe].rolling(200).mean()
L1341         atr20 = (hi[g_universe] - lo[g_universe]).rolling(20).mean()
L1342         vol20 = vol[g_universe].rolling(20).mean()
L1343         vol50 = vol[g_universe].rolling(50).mean()
L1344
L1345         # トレンドテンプレート合格
L1346         trend_template_ok = (px[g_universe] > ma50) & (px[g_universe] > ma150) & (px[g_universe] > ma200) \
L1347                             & (ma150 > ma200) & (ma200.diff() > 0)
L1348
L1349         # 汎用ピボット：直近65営業日の高値（当日除外）
L1350         pivot_price = hi[g_universe].rolling(65).max().shift(1)
L1351
L1352         # 相対力：年内高値更新
L1353         bench_aligned = bench.reindex(px.index).ffill()
L1354         rs = px[g_universe].div(bench_aligned, axis=0)
L1355         rs_high = rs.rolling(252).max().shift(1)
L1356
L1357         # ブレイクアウト「発生日」：条件立ち上がり
L1358         breakout_today = trend_template_ok & (px[g_universe] > pivot_price) \
L1359                          & (vol[g_universe] >= 1.5 * vol50) & (rs > rs_high)
L1360         breakout_event = breakout_today & ~breakout_today.shift(1).fillna(False)
L1361
L1362         # 押し目反発「発生日」：EMA21帯×出来高ドライアップ×前日高値越え×終値EMA21上
L1363         near_ema21_band = px[g_universe].between(ema21 - atr20, ema21 + atr20)
L1364         volume_dryup = (vol20 / vol50) <= 1.0
L1365         pullback_bounce_confirmed = (px[g_universe] > hi[g_universe].shift(1)) & (px[g_universe] > ema21)
L1366         pullback_today = trend_template_ok & near_ema21_band & volume_dryup & pullback_bounce_confirmed
L1367         pullback_event = pullback_today & ~pullback_today.shift(1).fillna(False)
L1368
L1369         # 直近N営業日内の発火 / 最終発生日
L1370         rows = []
L1371         for t in g_universe:
L1372             def _recent_and_date(s, win):
L1373                 sw = s[t].iloc[-win:]
L1374                 if sw.any():
L1375                     d = sw[sw].index[-1]
L1376                     return True, d.strftime("%Y-%m-%d")
L1377                 return False, ""
L1378             br_recent, br_date = _recent_and_date(breakout_event, win_breakout)
L1379             pb_recent, pb_date = _recent_and_date(pullback_event, win_pullback)
L1380             rows.append((t, {
L1381                 "G_BREAKOUT_recent_5d": br_recent,
L1382                 "G_BREAKOUT_last_date": br_date,
L1383                 "G_PULLBACK_recent_5d": pb_recent,
L1384                 "G_PULLBACK_last_date": pb_date,
L1385                 "G_PIVOT_price": float(pivot_price[t].iloc[-1]) if t in pivot_price.columns else float('nan'),
L1386             }))
L1387         flags = pd.DataFrame({k: v for k, v in rows}).T
L1388
L1389         # 列を作成・上書き
L1390         cols = ["G_BREAKOUT_recent_5d","G_BREAKOUT_last_date","G_PULLBACK_recent_5d","G_PULLBACK_last_date","G_PIVOT_price"]
L1391         for c in cols:
L1392             if c not in feature_df.columns:
L1393                 feature_df[c] = np.nan
L1394         feature_df.loc[flags.index, flags.columns] = flags
L1395
L1396     except Exception:
L1397         pass
L1398     return feature_df
L1399
```

## <.github/workflows/weekly-report.yml>
```text
L1 name: Weekly Stock Report
L2
L3 on:
L4   push:
L5     branches: [ main ]
L6     paths-ignore:
L7       - 'CodeForChat/**'
L8   schedule:
L9     - cron: '0 0 * * 6'  # UTC 00:00 → JST 09:00（土）
L10   workflow_dispatch:
L11
L12 jobs:
L13   build-and-report:
L14     runs-on: ubuntu-latest
L15     permissions:
L16       contents: write
L17
L18     steps:
L19       - name: Debug start
L20         run: echo '🚀 DEBUGstarted'
L21               
L22       - name: Checkout repository
L23         uses: actions/checkout@v3
L24
L25       - name: Setup Python
L26         uses: actions/setup-python@v5
L27         with:
L28           python-version: '3.x'
L29           cache: 'pip'
L30           cache-dependency-path: requirements.txt
L31
L32       - name: Install dependencies
L33         run: pip install -r requirements.txt
L34
L35       - name: Prepare results directory
L36         run: mkdir -p results
L37
L38       - name: Run factor & scoring
L39         env:
L40           SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
L41           FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
L42           FIN_THREADS: "8"
L43         run: python factor.py
```

## <documents/README.md>
```text
L1 # 運用ルール
L2
L3 ## 基本構成
L4 - 20銘柄を均等配分（現金を除き1銘柄あたり5%）
L5 - moomoo証券で運用
L6 - **Growth枠 12銘柄 / Defense枠 8銘柄**（NORMAL 基準）
L7
L8 ## Barbell Growth-Defense方針
L9 - Growth枠 **12銘柄**：高成長で乖離源となる攻めの銘柄
L10 - Defense枠 **8銘柄**：低ボラで安定成長し配当を増やす守りの銘柄
L11 - 「猛烈に伸びる攻め × 着実に稼ぐ盾」の組合せで乖離→半戻しプレミアムを狙う
L12
L13 ## レジーム判定（trend_template 合格“本数”で判定）
L14 - 合格本数 = current+candidate 全体のうち、trend_template 条件を満たした銘柄の**本数(C)**（基準 N_G=12）
L15 - しきい値は過去~600営業日の分布から**毎回自動採用**（分位点と運用“床”のmax）
L16   - 緊急入り: `max(q05, 12本)`（= N_G）
L17   - 緊急解除: `max(q20, 18本)`（= ceil(1.5×12)）
L18   - 通常復帰: `max(q60, 36本)`（= 3×N_G）
L19 - ヒステリシス: 前回モードに依存（EMERG→解除は23本以上、CAUTION→通常は45本以上）
L20
L21 ## レジーム別の現金・ドリフト
L22  - **通常(NORMAL)** : 現金 **10%** / ドリフト閾値 **12%**
L23  - **警戒(CAUTION)** : 現金 **12.5%** / ドリフト閾値 **14%**
L24  - **緊急(EMERG)** : 現金 **20%** / **ドリフト売買停止**（20×5%に全戻しのみ）
L25
L26 ## モード別の推奨“保有銘柄数”（MMF≒現金）
L27 *各枠=5%（20銘柄均等）。モード移行時は**Gの枠数のみ**調整し、外した枠は現金として保持。*
L28
L29 - **NORMAL:** G **12** / D **8** / 現金化枠 **0**  
L30 - **CAUTION:** G **10** / D **8** / 現金化枠 **2**（= 10%）  
L31 - **EMERG:** G **8**  / D **8** / 現金化枠 **4**（= 20%）  
L32
L33 > 実運用：⭐️低スコアのGから順に外す。解除時はfactor上位から補充。
L34
L35 ## トレーリングストップ
L36 - **基本TS (モード別):** NORMAL **15%** / CAUTION **13%** / EMERG **10%**
L37 - 含み益が **+30% / +60% / +100%** 到達で、基本から **-3pt / -6pt / -8pt** 引き上げ
L38 - TS発動で減少した銘柄は翌日以降に補充（※緊急モード中は補充しない）
L39
L40 ## 半戻し（リバランス）手順
L41 ドリフトチェックで**アラート**が出た場合（合計|drift| がモード閾値を超過、EMERG除く）、翌営業日の米国寄付きで下記を実施する。
L42
L43 1. **売却（必須）**  
L44    Slackテーブルの **Δqty がマイナスの銘柄を売却** する（寄付き成行推奨）。  
L45    これは「半戻し」計算に基づく過重量の削減を意味する。
L46
L47 2. **購入（任意・半戻し目安）**  
L48    半戻し後の合計|drift|を**シミュレーション値（Slackヘッダに表示）**に近づけることを目安に、  
L49    **任意の銘柄を買い増し**してバランスを取る（Δqtyがプラスの銘柄を優先してもよい）。
L50
L51 3. **トレーリングストップの再設定（必須）**  
L52    すべての保有銘柄について、最新の評価額に合わせてTSを**再発注／更新**する。  
L53    ルールは下記（利益到達で段階的にタイト化）：  
L54    - **基本TS:** -15%  
L55    - **+30% 到達 → TS -12%**  
L56    - **+60% 到達 → TS -9%**  
L57    - **+100% 到達 → TS -7%**  
L58    ※ストップ価格の引き上げは許可、**引き下げは不可**（利益保全の原則）。
L59
L60 4. **例外（EMERGモード）**  
L61    緊急(EMERG)では**ドリフト由来の売買は停止（∞）**。20銘柄×各5%への**全戻し**のみ許容。
L62
L63 5. **実行タイミング**
L64    - 判定：米国市場終値直後
L65    - 執行：翌営業日の米国寄付き成行
L66
L67 ## モード移行の実務手順（超シンプル）
L68 モードが変わったら、**MMF≒現金**として扱い、**Gの枠数だけ**を調整する：
L69 1. **Gを削る**（CAUTION/EMERG）  
L70    - ⭐️低スコアのGから順に外す。  
L71    - **`current_tickers.csv` から外すG銘柄の行を削除**（＝その枠は現金化）。
L72 2. **現金として保持**  
L73    - 外した枠は現金（またはMMF相当）でプール。  
L74 3. **復帰時の補充**（NORMALへ）  
L75    - **`current_tickers.csv` に銘柄を追加**（factor上位から）。  
L76    - 以降は日次ドリフト/TSルールに従う。
L77
L78 > driftは `target_ratio = 1/銘柄数` を自動適用。行数に応じて自動で均等比率が再計算される。
L79
L80 ## 入替銘柄選定
L81 - Oxfordキャピタル／インカム、Alpha Investor、Motley Fool Stock Advisor、moomooスクリーニング等を参考にchatGPTで検討
L82 - 年間NISA枠はGrowth群の中から低ボラ銘柄を選定し利用。長期保持にはこだわらない。
L83
L84 ## 再エントリー（クールダウン）
L85 - TSヒット後の同銘柄再INは **8営業日** のクールダウンを設ける（期間中は再IN禁止）
L86
L87 ## 実行タイミング
L88 - 判定：米国市場終値直後
L89 - 執行：翌営業日の米国寄付き成行
```

## <documents/factor_design.md>
```text
L1 # factor.py 詳細設計書
L2
L3 ## 概要
L4 - 既存ポートフォリオの銘柄と検討中の銘柄群を同時に扱う銘柄選定パイプライン。
L5 - 価格・財務データを取り込みスコアリングとDRRS選定を行うことで、以下のアウトプットを得る。
L6   - 採用銘柄と惜しくも漏れた銘柄のスコア一覧
L7   - IN/OUTのティッカーリストとOUT側の低スコア銘柄
L8   - 新旧ポートフォリオの比較表
L9   - 検討中銘柄の低スコアランキング（整理用）
L10
L11 ## 全体フロー
L12 1. **Input** – `current_tickers.csv`と`candidate_tickers.csv`を読み込み、yfinanceやFinnhubのAPIから価格・財務データを収集して`InputBundle`を整備。
L13 2. **Score Calculation** – Scorerが特徴量を計算し因子スコアを合成して`FeatureBundle`を生成。
L14 3. **Correlation Reduction & Selection** – SelectorがDRRSロジックで相関を抑えつつG/D銘柄を選定し`SelectionBundle`を得る。
L15 4. **Output** – 採用結果と周辺情報を表・Slack通知として出力。
L16
L17 ```mermaid
L18 flowchart LR
L19   A[Input\nAPI & 前処理] --> B[Score Calculation\n特徴量・因子合成]
L20   B --> C[Correlation Reduction\nDRRS選定]
L21   C --> D[Output\nSlack通知]
L22 ```
L23
L24 ## 定数・設定
L25 | 変数 | 内容 | 主な用途 |
L26 | --- | --- | --- |
L27 | `exist` / `cand` | 現行ポートフォリオと検討中銘柄のティッカーリスト | スコア対象ユニバースの構成、候補整理 |
L28 | `bench` | ベンチマークティッカー | 相対強さ・β算出、ポート比較 |
L29 | `CAND_PRICE_MAX` | 候補銘柄の許容価格上限 | 高額銘柄の事前除外 |
L30 | `N_G` / `N_D` | G/D採用枠の件数（**既定: 12 / 8**） | 最終的に選ぶ銘柄数の制約 |
L31 | `g_weights` / `D_weights` | 各因子の重みdict | G/Dスコア合成 |
L32 | `D_BETA_MAX` | Dバケットの許容β上限 | 高β銘柄の除外フィルタ |
L33 | `FILTER_SPEC` | G/Dごとの前処理フィルタ | トレンドマスクやβ上限設定 |
L34 | `corrM` | DRRS初期プールの最大件数 | 相関行列サイズ制御 |
L35 | `DRRS_G` / `DRRS_D` | DRRSパラメータdict | バケット別の相関低減設定 |
L36 | `DRRS_SHRINK` | 残差相関の対角シュリンク率 | `residual_corr`の安定化 |
L37 | `CROSS_MU_GD` | G-D間クロス相関ペナルティμ | 2バケット同時最適化で相関抑制 |
L38 | `RESULTS_DIR` | 選定結果保存ディレクトリ | `_save_sel`/`_load_prev`の入出力 |
L39
L40 選定結果は`results/`配下にJSONとして保存し、次回実行時に`_load_prev`で読み込んで選定条件に反映。
L41
L42 ## DTO/Config
L43 各ステップ間で受け渡すデータ構造と設定値。変数の意味合いと利用箇所を以下に示す。
L44
L45 ### InputBundle（Input → Scorer）
L46 | 変数 | 内容 | 主な用途 |
L47 | --- | --- | --- |
L48 | `cand` | 候補銘柄ティッカーのリスト | OUTテーブルや低スコアランキング対象の母集団 |
L49 | `tickers` | 現行+候補を合わせたティッカー一覧 | 価格・出来高ダウンロード、リターン計算 |
L50 | `bench` | ベンチマークティッカー | 相対強さ・β算出、ポート比較 |
L51 | `data` | yfinanceのダウンロード結果（階層列） | `px`/`spx`/リターン等の基礎データ |
L52 | `px` | `data['Close']`だけを抜き出した価格系列 | 指標計算・リターン生成 |
L53 | `spx` | `data['Close'][bench]` のSeries | `rs`や`calc_beta`の基準指数 |
L54 | `tickers_bulk` | `yf.Tickers`オブジェクト | `info`等の一括取得 |
L55 | `info` | ティッカー別のyfinance情報dict | セクター判定やEPS補完 |
L56 | `eps_df` | EPS TTM/直近EPS等をまとめた表 | 成長指標の算出 |
L57 | `fcf_df` | CFO・CapEx・FCF TTMと情報源フラグ | FCF/EVや配当カバレッジ |
L58 | `returns` | `px.pct_change()`のリターン表 | 相関行列・DRRS計算 |
L59
L60 ### FeatureBundle（Scorer → Selector）
L61 | 変数 | 内容 | 主な用途 |
L62 | --- | --- | --- |
L63 | `df` | 計算済み指標の生値テーブル | デバッグ・出力表示 |
L64 | `df_z` | ウィンザー後Zスコア化した指標表 | 因子スコア合成、選定基準 |
L65 | `g_score` | Gバケット総合スコア | G選定、IN/OUT比較 |
L66 | `d_score_all` | Dバケット総合スコア（全銘柄） | D選定、低スコアランキング |
L67 | `missing_logs` | 欠損指標と補完状況のログ | データ品質チェック |
L68
L69 ### SelectionBundle（Selector → Output）
L70 | 変数 | 内容 | 主な用途 |
L71 | --- | --- | --- |
L72 | `resG` | G選定結果の詳細dict（`tickers`、目的値等） | 結果保存・平均相関などの指標表示 |
L73 | `resD` | D選定結果の詳細dict | 同上 |
L74 | `top_G` | 最終採用Gティッカー | 新ポートフォリオ構築 |
L75 | `top_D` | 最終採用Dティッカー | 同上 |
L76 | `init_G` | DRRS前のG初期候補 | 惜しくも外れた銘柄表示 |
L77 | `init_D` | DRRS前のD初期候補 | 同上 |
L78
L79 ### WeightsConfig
L80 | 変数 | 内容 | 主な用途 |
L81 | --- | --- | --- |
L82 | `g` | G因子（GRW/MOM/VOL）の重みdict | `g_score`合成 |
L83 | `d` | D因子（D_QAL/D_YLD/D_VOL_RAW/D_TRD）の重みdict | `d_score_all`合成 |
L84
L85 ### DRRSParams
L86 | 変数 | 内容 | 主な用途 |
L87 | --- | --- | --- |
L88 | `corrM` | DRRS初期プールの最大件数 | 相関行列サイズ制御 |
L89 | `shrink` | 残差相関のシュリンク率 | `residual_corr`の対角強調 |
L90 | `G` | Gバケット用パラメータdict（`lookback`等） | `select_bucket_drrs`設定 |
L91 | `D` | Dバケット用パラメータdict | 同上 |
L92 | `cross_mu_gd` | G-Dクロス相関ペナルティ係数μ | `select_buckets`の目的関数 |
L93
L94 ### PipelineConfig
L95 | 変数 | 内容 | 主な用途 |
L96 | --- | --- | --- |
L97 | `weights` | `WeightsConfig`のインスタンス | スコア合成の重み参照 |
L98 | `drrs` | `DRRSParams`のインスタンス | 選定ステップの設定値 |
L99 | `price_max` | 候補銘柄の許容価格上限 | Input段階でのフィルタ |
L100
L101 ## 共通ユーティリティ
L102 - `winsorize_s` / `robust_z` : 外れ値処理とZスコア化。
L103 - `_safe_div` / `_safe_last` : 例外を潰した分割・末尾取得。
L104 - `_load_prev` / `_save_sel` : 選定結果の読み書き。
L105
L106 ## クラス設計
L107 ### Step1: Input
L108 `current_tickers.csv`の現行銘柄と`candidate_tickers.csv`の検討中銘柄を起点にデータを集約する。外部I/Oと前処理を担当し、`prepare_data`で`InputBundle`を生成。価格・財務データの取得は**yfinanceを優先し、欠損がある指標のみFinnhub APIで補完**する。
L109 主なメソッド:
L110 - `impute_eps_ttm` : 四半期EPS×4でTTMを推定し欠損時のみ差し替え。
L111 - `fetch_cfo_capex_ttm_yf` : yfinanceの四半期/年次キャッシュフローからCFO・CapEx・FCF TTMを算出。
L112 - `fetch_cfo_capex_ttm_finnhub` : yfinanceで欠けた銘柄のみFinnhub APIで補完。
L113 - `compute_fcf_with_fallback` : yfinance値を基準にFinnhub値で穴埋めし、CFO/CapEx/FCFと情報源フラグを返す。
L114 - `_build_eps_df` : `info`や`quarterly_earnings`からEPS TTMと直近EPSを計算し、`impute_eps_ttm`で補完。
L115 - `prepare_data` :
L116     0. CSVから現行銘柄と候補銘柄のティッカー一覧を読み込む。
L117     1. 候補銘柄の現在値を取得し価格上限でフィルタ。
L118     2. 既存+候補から対象ティッカーを決定し、価格・出来高を一括ダウンロード（yfinance）。
L119     3. yfinance値を基にEPS/FCFテーブルやベンチマーク系列、リターンを構築し、欠損セルはFinnhub呼び出しで穴埋め。
L120     4. 上記を`InputBundle`に格納して返す。
L121
L122 ### Step2: Score Calculation (Scorer)
L123 特徴量計算とスコア合成を担当し、`FeatureBundle`を返す。
L124
L125 #### 補助関数
L126 - `trend(s)` : 50/150/200日移動平均や52週レンジから-0.5〜0.5で構成されたトレンド指標。
L127 - `rs(s,b)` / `tr_str(s)` / `rs_line_slope(s,b,win)` : 相対強さや短期トレンド、RS回帰傾きを算出。
L128 - `ev_fallback` : `enterpriseValue`欠損時に負債・現金からEVを推定。
L129 - `dividend_status` / `div_streak` : 配当未設定状況の判定と増配年数カウント。
L130 - `fetch_finnhub_metrics` : Finnhub APIからEPS成長・ROE・βなど不足指標を取得。
L131 - `calc_beta` : ベンチマークとの共分散からβを算出。
L132 - `spx_to_alpha` : S&P500の位置情報からDRRSで用いるαを推定。
L133 - `soft_cap_effective_scores` / `pick_top_softcap` : セクターソフトキャップ付きスコア調整と上位抽出。
L134
L135 **補助関数と生成指標**
L136
L137 | 補助関数 | 生成指標 | 略称 |
L138 | --- | --- | --- |
L139 | `trend` | トレンド総合値 | `TR` |
L140 | `rs` | 相対強さ | `RS` |
L141 | `tr_str` | 価格と50日線の乖離 | `TR_str` |
L142 | `rs_line_slope` | RS線の回帰傾き | `RS_SLOPE_*` |
L143 | `calc_beta` | β | `BETA` |
L144 | `div_streak` | 連続増配年数 | `DIV_STREAK` |
L145
L146 #### `aggregate_scores` 詳細
L147 1. 各銘柄の価格系列や`info`を基に以下を算出。
L148    - **トレンド/モメンタム**: `TR`、`RS`、`TR_str`、多様な移動平均比、`RS_SLOPE_*`など。
L149    - **リスク**: `BETA`、`DOWNSIDE_DEV`、`MDD_1Y`、`RESID_VOL`、`DOWN_OUTPERF`、`EXT_200`等。
L150    - **配当**: `DIV`、`DIV_TTM_PS`、`DIV_VAR5`、`DIV_YOY`、`DIV_FCF_COVER`、`DIV_STREAK`。
L151    - **財務・成長**: `EPS`、`REV`、`ROE`、`FCF/EV`、`REV_Q_YOY`、`EPS_Q_YOY`、`REV_YOY_ACC`、`REV_YOY_VAR`、`REV_ANN_STREAK`、`RULE40`、`FCF_MGN` 等。
L152    - **安定性/サイズ**: `DEBT2EQ`、`CURR_RATIO`、`MARKET_CAP`、`ADV60_USD`、`EPS_VAR_8Q`など。
L153 2. 指標欠損はFinnhub API等で補完し、未取得項目を`missing_logs`に記録。
L154 3. `winsorize_s`→`robust_z`で標準化し`df_z`へ保存。サイズ・流動性は対数変換。
L155 4. 正規化済指標から因子スコアを合成。
L156    - 各因子の構成と重みは以下の通り。
L157      - **GRW**: 0.30×`REV` + 0.20×`EPS_Q_YOY` + 0.15×`REV_Q_YOY` + 0.15×`REV_YOY_ACC` + 0.10×`RULE40` + 0.10×`FCF_MGN` + 0.10×`REV_ANN_STREAK` − 0.05×`REV_YOY_VAR`。
L158      - **MOM**: 0.40×`RS` + 0.15×`TR_str` + 0.15×`RS_SLOPE_6W` + 0.15×`RS_SLOPE_13W` + 0.10×`MA200_SLOPE_5M` + 0.10×`MA200_UP_STREAK_D`。
L159      - **VOL**: `BETA`単体を使用。
L160      - **QAL**: 0.60×`FCF_W` + 0.40×`ROE_W`で作成。
L161      - **YLD**: 0.30×`DIV` + 0.70×`DIV_STREAK`。
L162      - **D_QAL**: 0.35×`QAL` + 0.20×`FCF` + 0.15×`CURR_RATIO` − 0.15×`DEBT2EQ` − 0.15×`EPS_VAR_8Q`。
L163      - **D_YLD**: 0.45×`DIV` + 0.25×`DIV_STREAK` + 0.20×`DIV_FCF_COVER` − 0.10×`DIV_VAR5`。
L164      - **D_VOL_RAW**: 0.40×`DOWNSIDE_DEV` + 0.22×`RESID_VOL` + 0.18×`MDD_1Y` − 0.10×`DOWN_OUTPERF` − 0.05×`EXT_200` − 0.08×`SIZE` − 0.10×`LIQ` + 0.10×`BETA`。
L165      - **D_TRD**: 0.40×`MA200_SLOPE_5M` − 0.30×`EXT_200` + 0.15×`NEAR_52W_HIGH` + 0.15×`TR`。
L166     - 主な指標の略称と意味:
L167
L168       | 略称 | 補助関数 | 概要 |
L169       | --- | --- | --- |
L170       | TR | `trend` | 50/150/200日移動平均と52週レンジを組み合わせたトレンド総合値 |
L171       | RS | `rs` | ベンチマークに対する相対強さ（12M/1Mリターン差） |
L172       | TR_str | `tr_str` | 価格と50日移動平均の乖離 |
L173       | RS_SLOPE_6W | `rs_line_slope` | 相対強さ線の6週回帰傾き |
L174       | RS_SLOPE_13W | `rs_line_slope` | 相対強さ線の13週回帰傾き |
L175       | MA200_SLOPE_5M | - | 200日移動平均の5か月騰落率 |
L176       | MA200_UP_STREAK_D | - | 200日線が連続で上向いた日数 |
L177       | BETA | `calc_beta` | ベンチマークに対するβ |
L178       | DOWNSIDE_DEV | - | 下方リターンのみの年率化標準偏差 |
L179       | RESID_VOL | - | βで調整した残差リターンの年率化標準偏差 |
L180       | MDD_1Y | - | 過去1年の最大ドローダウン |
L181       | DOWN_OUTPERF | - | 市場下落日に対する平均超過リターン |
L182       | EXT_200 | - | 200日移動平均からの絶対乖離率 |
L183       | NEAR_52W_HIGH | - | 52週高値までの下方距離（0=高値） |
L184       | FCF_W | - | ウィンザー処理後のFCF/EV |
L185       | ROE_W | - | ウィンザー処理後のROE |
L186       | FCF | - | FCF/EV |
L187       | QAL | - | FCF_WとROE_Wを組み合わせた品質スコア |
L188       | CURR_RATIO | - | 流動比率 |
L189       | DEBT2EQ | - | 負債資本倍率 |
L190       | EPS_VAR_8Q | - | EPSの8四半期標準偏差 |
L191       | DIV | - | 年率換算配当利回り |
L192       | DIV_STREAK | `div_streak` | 連続増配年数 |
L193       | DIV_FCF_COVER | - | 配当のFCFカバレッジ |
L194       | DIV_VAR5 | - | 5年配当変動率 |
L195       | DIV_TTM_PS | - | 1株当たりTTM配当 |
L196       | DIV_YOY | - | 前年比配当成長率 |
L197       | REV | - | 売上成長率TTM |
L198       | EPS_Q_YOY | - | 四半期EPSの前年同期比 |
L199       | REV_Q_YOY | - | 四半期売上の前年同期比 |
L200       | REV_YOY_ACC | - | 売上成長率の加速分 |
L201       | RULE40 | - | 売上成長率とFCFマージンの合計 |
L202       | FCF_MGN | - | FCFマージン |
L203       | REV_ANN_STREAK | - | 年次売上成長の連続年数 |
L204       | REV_YOY_VAR | - | 年次売上成長率の変動性 |
L205       | SIZE | - | 時価総額の対数値 |
L206       | LIQ | - | 60日平均出来高ドルの対数値 |
L207    - Gバケット: `GRW`、`MOM`、`VOL`を`cfg.weights.g`（0.40/0.45/-0.15）で加重し`g_score`を得る。
L208    - Dバケット: `D_QAL`、`D_YLD`、`D_VOL_RAW`、`D_TRD`を`cfg.weights.d`（0.15/0.15/-0.45/0.25）で加重し`d_score_all`を算出。
L209    - セクターcapによる`soft_cap_effective_scores`を適用し、G採用銘柄にはトレンドテンプレートフィルタを適用。
L210 5. `_apply_growth_entry_flags`でブレイクアウト/押し目発火状況を付加し、`FeatureBundle`を返す。
L211
L212 ### Step3: Correlation Reduction & Selection (Selector)
L213 DRRSアルゴリズムで相関を抑えた銘柄選定を行い、`SelectionBundle`を返す。`results/`に保存された前回選定（`G_selection.json` / `D_selection.json`）を`_load_prev`で読み込み、目的値が大きく悪化しない限り維持する。新しい採用集合は`_save_sel`でJSONに書き出し次回以降の入力に備える。
L214 主なメソッド:
L215 - `residual_corr` : 収益率行列をZスコア化し、上位主成分を除去した残差から相関行列を求め、平均相関に応じてシュリンク。
L216 - `rrqr_like_det` : スコアを重み付けしたQR分解風の手順で初期候補をk件抽出し、スコアの高い非相関な集合を得る。
L217 - `swap_local_det` / `swap_local_det_cross` : `sum(score) - λ*within_corr - μ*cross_corr`を目的関数として、入れ替え探索で局所的に最適化。
L218 - `select_bucket_drrs` : プール銘柄とスコアから残差相関を計算し、上記2段階(初期選択→入れ替え)でk銘柄を決定。過去採用銘柄との比較で目的値が劣化しなければ維持する。
L219 - `select_buckets` : Gバケットを選定後、その結果を除いた候補からDバケットを選ぶ。D選定時はGとの相関ペナルティμを付与し、両バケットの分散を制御する。
L220
L221 #### 相関低減ロジック詳細
L222 1. **残差相関行列の構築 (`residual_corr`)**
L223    - リターン行列`R`をZスコア化。
L224    - SVDで上位`n_pc`主成分`F`を求め、最小二乗で係数`B`を算出し残差`E = Z - F@B`を得る。
L225    - `E`の相関行列`C`を計算し、平均絶対相関に応じてシュリンク量`shrink_eff`を補正して対角を強調。
L226 2. **初期候補の抽出 (`rrqr_like_det`)**
L227    - スコアを0-1正規化した重み`w`とし、`Z*(1+γw)`で列ノルムを強調。
L228    - 残差ノルム最大の列を逐次選び、QRライクなデフレーションを行って非相関かつ高スコアな`k`銘柄集合`S0`を得る。
L229 3. **局所探索 (`swap_local_det` / `swap_local_det_cross`)**
L230    - 目的関数`Σz_score − λ·within_corr − μ·cross_corr`を最大化。
L231    - 選択集合の各銘柄を他候補と入れ替え、改善がなくなるまでまたは`max_pass`回まで探索。
L232    - `swap_local_det_cross`はGバケットとのクロス相関行列`C_cross`を使用し、ペナルティ`μ`を付与。
L233 4. **過去採用の維持とクロスペナルティ (`select_bucket_drrs` / `select_buckets`)**
L234    - 局所探索結果`S`と過去集合`P`の目的値を比較し、`S`が`P`より`η`未満の改善なら`P`を維持。
L235    - `select_buckets`ではGを先に決定し、D選定時にGとの相関ペナルティ`μ`を加えてクロス分散を抑制。
L236
L237 ### Step4: Output
L238 選定結果を可視化し共有する工程。以下の内容をテーブル化して標準出力とSlackへ送る。
L239 - 採用銘柄と惜しくも選外となった銘柄のスコア一覧
L240 - IN/OUTリストとOUT銘柄のスコア（低得点銘柄を確認しやすく）
L241 - 新旧ポートフォリオの比較表（組入れ・除外、スコア変化）
L242 - 検討中銘柄の低スコアランキング
L243
L244 主なメソッド:
L245 - `display_results` : 上記テーブルに加えパフォーマンス指標や分散化指標を表示。
L246 - `notify_slack` : Slack Webhookへ同内容を送信。
L247 - 補助:`_avg_offdiag`、`_resid_avg_rho`、`_raw_avg_rho`、`_cross_block_raw_rho`。
L248
L249 ## エントリポイント
L250 1. `PipelineConfig`を構築。
L251 2. **Step1** `Input.prepare_data`で`InputBundle`を生成。
L252 3. **Step2** `Scorer.aggregate_scores`で`FeatureBundle`を取得。
L253 4. **Step3** `Selector.select_buckets`で`SelectionBundle`を算出。
L254 5. **Step4** `Output.display_results`と`notify_slack`で結果を出力。
```

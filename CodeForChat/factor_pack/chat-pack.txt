# === Chat Paste Pack ===
# Repo: dakara32/GPT_Code @ main
# Files: config.py, factor.py, scorer.py, .github/workflows/weekly-report.yml, documents/README.md, documents/factor_design.md
# ä½œæˆæ—¥æ™‚: 2025-09-19 15:59:09 (JST)
# ä½¿ã„æ–¹: ä¸‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’é †ã«è²¼ã‚Œã°ã“ã®ãƒãƒ£ãƒƒãƒˆã§å…¨ä½“æŠŠæ¡ã§ãã¾ã™ã€‚
# æ³¨è¨˜: å„ãƒ•ã‚¡ã‚¤ãƒ«ã¯å€‹åˆ¥ã« L1.. ã§è¡Œç•ªå·ä»˜ä¸ã€‚
---

## <config.py>
```text
L1 # å…±é€šè¨­å®šï¼ˆfactor / drift ã‹ã‚‰å‚ç…§ï¼‰
L2 from dataclasses import dataclass
L3
L4 TOTAL_TARGETS = 20
L5
L6 # åŸºæº–ã®ãƒã‚±ãƒƒãƒˆæ•°ï¼ˆNORMALï¼‰
L7 COUNTS_BASE = {"G": 12, "D": 8}
L8
L9 # ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®æ¨å¥¨ãƒã‚±ãƒƒãƒˆæ•°
L10 COUNTS_BY_MODE = {
L11     "NORMAL": {"G": 12, "D": 8},
L12     "CAUTION": {"G": 10, "D": 8},
L13     "EMERG": {"G": 8,  "D": 8},
L14 }
L15
L16 # ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®ãƒ‰ãƒªãƒ•ãƒˆé–¾å€¤ï¼ˆ%ï¼‰
L17 DRIFT_THRESHOLD_BY_MODE = {"NORMAL": 12, "CAUTION": 14, "EMERG": float("inf")}
L18
L19 # ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®TSï¼ˆåŸºæœ¬å¹…, å°æ•°=å‰²åˆï¼‰
L20 TS_BASE_BY_MODE = {"NORMAL": 0.15, "CAUTION": 0.13, "EMERG": 0.10}
L21 # åˆ©ç›Šåˆ°é”(+30/+60/+100%)æ™‚ã®æ®µéšã‚¿ã‚¤ãƒˆåŒ–ï¼ˆãƒã‚¤ãƒ³ãƒˆå·®ï¼‰
L22 TS_STEP_DELTAS_PT = (3, 6, 8)
L23
L24 # Breadthã®æ ¡æ­£ã¯ N_G ã«é€£å‹•ï¼ˆç·Šæ€¥è§£é™¤=ceil(1.5*N_G), é€šå¸¸å¾©å¸°=3*N_Gï¼‰
L25 N_G = COUNTS_BASE["G"]
L26 N_D = COUNTS_BASE["D"]
L27
```

## <factor.py>
```text
L1 '''ROLE: Orchestration ONLYï¼ˆå¤–éƒ¨I/Oãƒ»SSOTãƒ»Slackå‡ºåŠ›ï¼‰, è¨ˆç®—ã¯ scorer.py'''
L2 # === NOTE: æ©Ÿèƒ½ãƒ»å…¥å‡ºåŠ›ãƒ»ãƒ­ã‚°æ–‡è¨€ãƒ»ä¾‹å¤–æŒ™å‹•ã¯ä¸å¤‰ã€‚å®‰å…¨ãªçŸ­ç¸®ï¼ˆimportçµ±åˆ/è¤‡æ•°ä»£å…¥/å†…åŒ…è¡¨è¨˜/ãƒ¡ã‚½ãƒƒãƒ‰ãƒã‚§ãƒ¼ãƒ³/ä¸€è¡ŒåŒ–/ç©ºè¡Œåœ§ç¸®ãªã©ï¼‰ã®ã¿é©ç”¨ ===
L3 BONUS_COEFF = 0.55  # æ¨å¥¨: æ”»ã‚=0.45 / ä¸­åº¸=0.55 / å®ˆã‚Š=0.65
L4 SWAP_DELTA_Z = 0.15   # åƒ…å·®åˆ¤å®š: Ïƒã®15%ã€‚(ç·©ã‚=0.10 / æ¨™æº–=0.15 / å›ºã‚=0.20)
L5 SWAP_KEEP_BUFFER = 3  # n_target+ã“ã®é †ä½ä»¥å†…ã®ç¾è¡Œã¯ä¿æŒã€‚(ç²˜ã‚Šå¼±=2 / æ¨™æº–=3 / ç²˜ã‚Šå¼·=4ã€œ5)
L6 import os, time, requests
L7 import logging
L8 from time import perf_counter
L9 from dataclasses import dataclass
L10 from typing import Any, Dict, List
L11 from concurrent.futures import ThreadPoolExecutor
L12 import numpy as np
L13 import pandas as pd
L14 import yfinance as yf
L15 from scipy.stats import zscore  # used via scorer
L16 from scorer import Scorer, ttm_div_yield_portfolio
L17 import config
L18
L19 # ãã®ä»–
L20 debug_mode, FINNHUB_API_KEY = True, os.environ.get("FINNHUB_API_KEY")
L21
L22 logger = logging.getLogger(__name__)
L23 if debug_mode:
L24     logging.basicConfig(level=logging.INFO, force=True)
L25 else:
L26     logging.basicConfig(level=logging.WARNING, force=True)
L27
L28 class T:
L29     t = perf_counter()
L30     log = staticmethod(lambda tag: (lambda now=perf_counter(): (print(f"[T] {tag}: {now - T.t:.2f}s"), setattr(T, "t", now))[-1])())
L31
L32 T.log("start")
L33
L34 # === ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã¨å®šæ•°ï¼ˆå†’é ­ã«å›ºå®šï¼‰ ===
L35 exist, cand = [pd.read_csv(f, header=None)[0].tolist() for f in ("current_tickers.csv","candidate_tickers.csv")]
L36 T.log(f"csv loaded: exist={len(exist)} cand={len(cand)}")
L37 CAND_PRICE_MAX, bench = 450, '^GSPC'  # ä¾¡æ ¼ä¸Šé™ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
L38 N_G, N_D = config.N_G, config.N_D  # G/Dæ ã‚µã‚¤ã‚ºï¼ˆNORMALåŸºæº–: G12/D8ï¼‰
L39 g_weights = {'GROWTH_F':0.35,'MOM':0.55,'VOL':-0.10}
L40 D_BETA_MAX = float(os.environ.get("D_BETA_MAX", "0.8"))
L41 FILTER_SPEC = {"G":{"pre_mask":["trend_template"]},"D":{"pre_filter":{"beta_max":D_BETA_MAX}}}
L42 D_weights = {'QAL':0.1,'YLD':0.3,'VOL':-0.5,'TRD':0.1}
L43 _fmt_w = lambda w: " ".join(f"{k}{int(v*100)}" for k, v in w.items())
L44
L45 # DRRS åˆæœŸãƒ—ãƒ¼ãƒ«ãƒ»å„ç¨®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
L46 corrM = 45
L47 DRRS_G, DRRS_D = dict(lookback=252,n_pc=3,gamma=1.2,lam=0.68,eta=0.8), dict(lookback=504,n_pc=4,gamma=0.8,lam=0.85,eta=0.5)
L48 DRRS_SHRINK = 0.10  # æ®‹å·®ç›¸é–¢ã®å¯¾è§’ã‚·ãƒ¥ãƒªãƒ³ã‚¯ï¼ˆåŸºç¤ï¼‰
L49
L50 # ã‚¯ãƒ­ã‚¹ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆæœªå®šç¾©ãªã‚‰è¨­å®šï¼‰
L51 try: CROSS_MU_GD
L52 except NameError: CROSS_MU_GD = 0.40  # æ¨å¥¨ 0.35â€“0.45ï¼ˆlam=0.85æƒ³å®šï¼‰
L53
L54 # å‡ºåŠ›é–¢é€£
L55 RESULTS_DIR = "results"
L56 os.makedirs(RESULTS_DIR, exist_ok=True)
L57
L58 # === å…±æœ‰DTOï¼ˆã‚¯ãƒ©ã‚¹é–“I/Oå¥‘ç´„ï¼‰ï¼‹ Config ===
L59 @dataclass(frozen=True)
L60 class InputBundle:
L61     # Input â†’ Scorer ã§å—ã‘æ¸¡ã™ç´ æï¼ˆI/Oç¦æ­¢ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ï¼‰
L62     cand: List[str]
L63     tickers: List[str]
L64     bench: str
L65     data: pd.DataFrame              # yfinance downloadçµæœï¼ˆ'Close','Volume'ç­‰ã®éšå±¤åˆ—ï¼‰
L66     px: pd.DataFrame                # data['Close']
L67     spx: pd.Series                  # data['Close'][bench]
L68     tickers_bulk: object            # yfinance.Tickers
L69     info: Dict[str, dict]           # yfinance info per ticker
L70     eps_df: pd.DataFrame            # ['eps_ttm','eps_q_recent',...]
L71     fcf_df: pd.DataFrame            # ['fcf_ttm', ...]
L72     returns: pd.DataFrame           # px[tickers].pct_change()
L73
L74 @dataclass(frozen=True)
L75 class FeatureBundle:
L76     df: pd.DataFrame
L77     df_z: pd.DataFrame
L78     g_score: pd.Series
L79     d_score_all: pd.Series
L80     missing_logs: pd.DataFrame
L81     df_full: pd.DataFrame | None = None
L82     df_full_z: pd.DataFrame | None = None
L83     scaler: Any | None = None
L84
L85 @dataclass(frozen=True)
L86 class SelectionBundle:
L87     resG: dict
L88     resD: dict
L89     top_G: List[str]
L90     top_D: List[str]
L91     init_G: List[str]
L92     init_D: List[str]
L93
L94 @dataclass(frozen=True)
L95 class WeightsConfig:
L96     g: Dict[str,float]
L97     d: Dict[str,float]
L98
L99 @dataclass(frozen=True)
L100 class DRRSParams:
L101     corrM: int
L102     shrink: float
L103     G: Dict[str,float]   # lookback, n_pc, gamma, lam, eta
L104     D: Dict[str,float]
L105     cross_mu_gd: float
L106
L107 @dataclass(frozen=True)
L108 class PipelineConfig:
L109     weights: WeightsConfig
L110     drrs: DRRSParams
L111     price_max: float
L112     debug_mode: bool = False
L113
L114 # === å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆè¤‡æ•°ã‚¯ãƒ©ã‚¹ã§ä½¿ç”¨ï¼‰ ===
L115 # (unused local utils removed â€“ use scorer.py versions if needed)
L116
L117 _env_true = lambda name, default=False: (os.getenv(name) or str(default)).strip().lower() == "true"
L118
L119 def _post_slack(payload: dict):
L120     url = os.getenv("SLACK_WEBHOOK_URL")
L121     if not url: print("âš ï¸ SLACK_WEBHOOK_URL æœªè¨­å®š"); return
L122     try:
L123         requests.post(url, json=payload).raise_for_status()
L124     except Exception as e:
L125         print(f"âš ï¸ Slacké€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
L126
L127 def _slack_send_text_chunks(url: str, text: str, chunk: int = 2800) -> None:
L128     """Slackã¸ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²é€ä¿¡ï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å½¢å¼ï¼‰ã€‚"""
L129
L130     def _post_text(payload: str) -> None:
L131         try:
L132             resp = requests.post(url, json={"text": payload})
L133             print(f"[DBG] debug_post status={getattr(resp, 'status_code', None)} size={len(payload)}")
L134             if resp is not None:
L135                 resp.raise_for_status()
L136         except Exception as e:
L137             print(f"[ERR] debug_post_failed: {e}")
L138
L139     body = str(text or "").strip()
L140     if not body:
L141         print("[DBG] skip debug send: empty body")
L142         return
L143
L144     lines = body.splitlines()
L145     block: list[str] = []
L146     block_len = 0
L147
L148     def _flush() -> None:
L149         nonlocal block, block_len
L150         if not block:
L151             return
L152         payload = "```" + "\n".join(block) + "```"
L153         _post_text(payload)
L154         block, block_len = [], 0
L155
L156     for raw in lines:
L157         line = raw or ""
L158         while len(line) > chunk:
L159             head, line = line[:chunk], line[chunk:]
L160             _flush()
L161             _post_text("```" + head + "```")
L162         add_len = len(line) if not block else len(line) + 1
L163         if block and block_len + add_len > chunk:
L164             _flush()
L165             add_len = len(line)
L166         block.append(line)
L167         block_len += add_len
L168     _flush()
L169
L170 def _disjoint_keepG(top_G, top_D, poolD):
L171     """Gé‡è¤‡ã‚’Dã‹ã‚‰é™¤å»ã—ã€poolDã§é †æ¬¡è£œå……ï¼ˆæ¯æ¸‡æ™‚ã¯å…ƒéŠ˜æŸ„ç¶­æŒï¼‰ã€‚"""
L172     used, D, i = set(top_G), list(top_D), 0
L173     for j, t in enumerate(D):
L174         if t in used:
L175             while i < len(poolD) and (poolD[i] in used or poolD[i] in D):
L176                 i += 1
L177             if i < len(poolD):
L178                 D[j] = poolD[i]; used.add(D[j]); i += 1
L179     return top_G, D
L180
L181
L182 def _sticky_keep_current(agg: pd.Series, pick: list[str], incumbents: list[str],
L183                          n_target: int, delta_z: float, keep_buffer: int) -> list[str]:
L184     import pandas as pd, numpy as np
L185     sel = list(pick)
L186     if not sel: return sel
L187     ranked_sel = agg.reindex(sel).sort_values(ascending=False)
L188     kth = ranked_sel.iloc[min(len(sel), n_target)-1]
L189     sigma = float(agg.std()) if pd.notna(agg.std()) else 0.0
L190     thresh = kth - delta_z * sigma
L191     ranked_all = agg.sort_values(ascending=False)
L192     cand = [t for t in incumbents if (t not in sel) and (t in agg.index)]
L193     for t in cand:
L194         within_score = (pd.notna(agg[t]) and agg[t] >= thresh)
L195         within_rank  = (t in ranked_all.index) and (ranked_all.index.get_loc(t) < n_target + keep_buffer)
L196         if within_score or within_rank:
L197             non_inc = [x for x in sel if x not in incumbents]
L198             if not non_inc: break
L199             weakest = min(non_inc, key=lambda x: agg.get(x, -np.inf))
L200             if weakest in sel and agg.get(t, -np.inf) >= agg.get(weakest, -np.inf):
L201                 sel.remove(weakest); sel.append(t)
L202     if len(sel) > n_target:
L203         sel = sorted(sel, key=lambda x: agg.get(x, -1e9), reverse=True)[:n_target]
L204     return sel
L205
L206
L207 # === Inputï¼šå¤–éƒ¨I/Oã¨å‰å‡¦ç†ï¼ˆCSV/APIãƒ»æ¬ æè£œå®Œï¼‰ ===
L208 class Input:
L209     def __init__(self, cand, exist, bench, price_max, finnhub_api_key=None):
L210         self.cand, self.exist, self.bench, self.price_max = cand, exist, bench, price_max
L211         self.api_key = finnhub_api_key or os.environ.get("FINNHUB_API_KEY")
L212
L213     # ---- ï¼ˆInputå°‚ç”¨ï¼‰EPSè£œå®Œãƒ»FCFç®—å‡ºç³» ----
L214     @staticmethod
L215     def _sec_headers():
L216         mail = (os.getenv("SEC_CONTACT_EMAIL") or "yasonba55@gmail.com").strip()
L217         app = (os.getenv("SEC_APP_NAME") or "FactorBot/1.0").strip()
L218         return {
L219             "User-Agent": f"{app} ({mail})",
L220             "From": mail,
L221             "Accept": "application/json",
L222         }
L223
L224     @staticmethod
L225     def _sec_get(url: str, retries: int = 3, backoff: float = 0.5):
L226         for i in range(retries):
L227             r = requests.get(url, headers=Input._sec_headers(), timeout=20)
L228             if r.status_code in (429, 503):
L229                 time.sleep(min(2 ** i * backoff, 4.0))
L230                 continue
L231             r.raise_for_status()
L232             return r.json()
L233         r.raise_for_status()
L234
L235     @staticmethod
L236     def _sec_ticker_map():
L237         j = Input._sec_get("https://data.sec.gov/api/xbrl/company_tickers.json")
L238         mp = {}
L239         for _, v in (j or {}).items():
L240             try:
L241                 mp[str(v["ticker"]).upper()] = f"{int(v['cik_str']):010d}"
L242             except Exception:
L243                 pass
L244         return mp
L245
L246     # --- è¿½åŠ : ADR/OTCå‘ã‘ã®ç°¡æ˜“æ­£è¦åŒ–ï¼ˆæœ«å°¾Y/F, ãƒ‰ãƒƒãƒˆç­‰ï¼‰ ---
L247     @staticmethod
L248     def _normalize_ticker(sym: str) -> list[str]:
L249         s = (sym or "").upper().strip()
L250         cand: list[str] = []
L251
L252         def add(x: str) -> None:
L253             if x and x not in cand:
L254                 cand.append(x)
L255
L256         add(s)
L257         add(s.replace(".", ""))
L258         if len(s) >= 2 and s[-1] in {"Y", "F"}:
L259             add(s[:-1])
L260         add(s.replace("-", "").replace(".", ""))
L261         return cand
L262
L263     @staticmethod
L264     def _sec_companyfacts(cik: str):
L265         return Input._sec_get(f"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json")
L266
L267     @staticmethod
L268     def _units_for_tags(facts: dict, namespaces: list[str], tags: list[str]) -> list[dict]:
L269         """facts ã‹ã‚‰ namespace/tag ã‚’æ¨ªæ–­ã—ã¦ units é…åˆ—ã‚’åé›†ï¼ˆå­˜åœ¨é †ã«é€£çµï¼‰ã€‚"""
L270         out: list[dict] = []
L271         facts = facts or {}
L272         for ns in namespaces:
L273             try:
L274                 node = facts.get("facts", {}).get(ns, {})
L275             except Exception:
L276                 node = {}
L277             for tg in tags:
L278                 try:
L279                     units = node[tg]["units"]
L280                 except Exception:
L281                     continue
L282                 picks: list[dict] = []
L283                 if "USD/shares" in units:
L284                     picks.extend(list(units["USD/shares"]))
L285                 if "USD" in units:
L286                     picks.extend(list(units["USD"]))
L287                 if not picks:
L288                     for arr in units.values():
L289                         picks.extend(list(arr))
L290                 out.extend(picks)
L291         return out
L292
L293     @staticmethod
L294     def _only_quarterly(arr: list[dict]) -> list[dict]:
L295         """companyfactsã®æ··åœ¨é…åˆ—ã‹ã‚‰ã€å››åŠæœŸã€ã ã‘ã‚’æŠ½å‡ºã€‚
L296
L297         - frame ã« "Q" ã‚’å«ã‚€ï¼ˆä¾‹: CY2024Q2Iï¼‰
L298         - fp ãŒ Q1/Q2/Q3/Q4
L299         - form ãŒ 10-Q/10-Q/A/6-K
L300         """
L301         if not arr:
L302             return []
L303         q_forms = {"10-Q", "10-Q/A", "6-K"}
L304
L305         def is_q(x: dict) -> bool:
L306             frame = (x.get("frame") or "").upper()
L307             fp = (x.get("fp") or "").upper()
L308             form = (x.get("form") or "").upper()
L309             return ("Q" in frame) or (fp in {"Q1", "Q2", "Q3", "Q4"}) or (form in q_forms)
L310
L311         out = [x for x in arr if is_q(x)]
L312         out.sort(key=lambda x: (x.get("end") or ""), reverse=True)
L313         return out
L314
L315     @staticmethod
L316     def _series_from_facts(arr, key="val", normalize=float):
L317         out = []
L318         for x in (arr or []):
L319             try:
L320                 v = x.get(key)
L321                 out.append(normalize(v) if v is not None else float("nan"))
L322             except Exception:
L323                 out.append(float("nan"))
L324         return out
L325
L326     def fetch_eps_rev_from_sec(self, tickers: list[str]) -> dict:
L327         out = {}
L328         t2cik = self._sec_ticker_map()
L329         n_map = n_rev = n_eps = 0
L330         miss_map: list[str] = []
L331         miss_facts: list[str] = []
L332         for t in tickers:
L333             candidates: list[str] = []
L334
L335             def add(key: str) -> None:
L336                 if key and key not in candidates:
L337                     candidates.append(key)
L338
L339             add(t.upper())
L340             for key in self._normalize_ticker(t):
L341                 add(key)
L342
L343             cik = None
L344             for key in candidates:
L345                 cik = t2cik.get(key)
L346                 if cik:
L347                     break
L348             if not cik:
L349                 out[t] = {}
L350                 miss_map.append(t)
L351                 continue
L352             try:
L353                 j = self._sec_companyfacts(cik)
L354                 facts = j or {}
L355                 rev_tags = [
L356                     "Revenues",
L357                     "RevenueFromContractWithCustomerExcludingAssessedTax",
L358                     "SalesRevenueNet",
L359                     "SalesRevenueGoodsNet",
L360                     "SalesRevenueServicesNet",
L361                     "Revenue",
L362                 ]
L363                 eps_tags = [
L364                     "EarningsPerShareDiluted",
L365                     "EarningsPerShareBasicAndDiluted",
L366                     "EarningsPerShare",
L367                     "EarningsPerShareBasic",
L368                 ]
L369                 rev_arr = self._units_for_tags(facts, ["us-gaap", "ifrs-full"], rev_tags)
L370                 eps_arr = self._units_for_tags(facts, ["us-gaap", "ifrs-full"], eps_tags)
L371                 rev_q_items = self._only_quarterly(rev_arr)
L372                 eps_q_items = self._only_quarterly(eps_arr)
L373                 rev_vals = self._series_from_facts(rev_q_items)
L374                 eps_vals = self._series_from_facts(eps_q_items)
L375                 rev_q = float(rev_vals[0]) if rev_vals else float("nan")
L376                 eps_q = float(eps_vals[0]) if eps_vals else float("nan")
L377                 rev_ttm = float(sum([v for v in rev_vals[:4] if v == v])) if rev_vals else float("nan")
L378                 eps_ttm = float(sum([v for v in eps_vals[:4] if v == v])) if eps_vals else float("nan")
L379                 out[t] = {
L380                     "eps_q_recent": eps_q,
L381                     "eps_ttm": eps_ttm,
L382                     "rev_q_recent": rev_q,
L383                     "rev_ttm": rev_ttm,
L384                     "eps_q_series": eps_vals[:8],
L385                     "rev_q_series": rev_vals[:8],
L386                 }
L387                 n_map += 1
L388                 if rev_vals:
L389                     n_rev += 1
L390                 if eps_vals:
L391                     n_eps += 1
L392             except Exception:
L393                 out[t] = {}
L394                 miss_facts.append(t)
L395             time.sleep(0.12)
L396         # å–å¾—ã‚µãƒãƒªã‚’ãƒ­ã‚°ï¼ˆActionsã§ç¢ºèªã—ã‚„ã™ã„ã‚ˆã† printï¼‰
L397         try:
L398             total = len(tickers)
L399             print(f"[SEC] map={n_map}/{total}  rev_q_hit={n_rev}  eps_q_hit={n_eps}")
L400             if miss_map:
L401                 print(f"[SEC] no CIK map: {len(miss_map)} (ä¾‹) {miss_map[:12]}")
L402             if miss_facts:
L403                 print(f"[SEC] CIKã‚ã‚Š ã ãŒå¯¾è±¡factãªã—: {len(miss_facts)} (ä¾‹) {miss_facts[:12]}")
L404         except Exception:
L405             pass
L406         return out
L407     @staticmethod
L408     def impute_eps_ttm(df: pd.DataFrame, ttm_col: str="eps_ttm", q_col: str="eps_q_recent", out_col: str|None=None) -> pd.DataFrame:
L409         out_col = out_col or ttm_col; df = df.copy(); df["eps_imputed"] = False
L410         cand = df[q_col]*4; ok = df[ttm_col].isna() & cand.replace([np.inf,-np.inf], np.nan).notna()
L411         df.loc[ok, out_col], df.loc[ok,"eps_imputed"] = cand[ok], True; return df
L412
L413     _CF_ALIASES = {"cfo":["Operating Cash Flow","Total Cash From Operating Activities"], "capex":["Capital Expenditure","Capital Expenditures"]}
L414
L415     @staticmethod
L416     def _pick_row(df: pd.DataFrame, names: list[str]) -> pd.Series|None:
L417         if df is None or df.empty: return None
L418         idx_lower={str(i).lower():i for i in df.index}
L419         for n in names:
L420             k=n.lower()
L421             if k in idx_lower: return df.loc[idx_lower[k]]
L422         return None
L423
L424     @staticmethod
L425     def _sum_last_n(s: pd.Series|None, n: int) -> float|None:
L426         if s is None or s.empty: return None
L427         v=s.dropna().astype(float); return None if v.empty else v.iloc[:n].sum()
L428
L429     @staticmethod
L430     def _latest(s: pd.Series|None) -> float|None:
L431         if s is None or s.empty: return None
L432         v=s.dropna().astype(float); return v.iloc[0] if not v.empty else None
L433
L434     def fetch_cfo_capex_ttm_yf(self, tickers: list[str]) -> pd.DataFrame:
L435         from concurrent.futures import ThreadPoolExecutor, as_completed
L436         pick, sumn, latest, aliases = self._pick_row, self._sum_last_n, self._latest, self._CF_ALIASES
L437
L438         def one(t: str):
L439             try:
L440                 tk = yf.Ticker(t)  # â˜… ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¯æ¸¡ã•ãªã„ï¼ˆYFãŒcurl_cffiã§ç®¡ç†ï¼‰
L441                 qcf = tk.quarterly_cashflow
L442                 cfo_q, capex_q = pick(qcf, aliases["cfo"]), pick(qcf, aliases["capex"])
L443                 fcf_q = pick(qcf, ["Free Cash Flow","FreeCashFlow","Free cash flow"])
L444                 cfo, capex, fcf = sumn(cfo_q,4), sumn(capex_q,4), sumn(fcf_q,4)
L445                 if any(v is None for v in (cfo, capex, fcf)):
L446                     acf = tk.cashflow
L447                     if cfo   is None: cfo   = latest(pick(acf, aliases["cfo"]))
L448                     if capex is None: capex = latest(pick(acf, aliases["capex"]))
L449                     if fcf   is None: fcf   = latest(pick(acf, ["Free Cash Flow","FreeCashFlow","Free cash flow"]))
L450             except Exception as e:
L451                 print(f"[warn] yf financials error: {t}: {e}"); cfo=capex=fcf=None
L452             n=np.nan
L453             return {"ticker":t,
L454                     "cfo_ttm_yf":   n if cfo   is None else cfo,
L455                     "capex_ttm_yf": n if capex is None else capex,
L456                     "fcf_ttm_yf_direct": n if fcf is None else fcf}
L457
L458         rows, mw = [], int(os.getenv("FIN_THREADS","8"))
L459         with ThreadPoolExecutor(max_workers=mw) as ex:
L460             rows=[f.result() for f in as_completed(ex.submit(one,t) for t in tickers)]
L461         return pd.DataFrame(rows).set_index("ticker")
L462
L463     _FINN_CFO_KEYS = ["netCashProvidedByOperatingActivities","netCashFromOperatingActivities","cashFlowFromOperatingActivities","operatingCashFlow"]
L464     _FINN_CAPEX_KEYS = ["capitalExpenditure","capitalExpenditures","purchaseOfPPE","investmentsInPropertyPlantAndEquipment"]
L465
L466     @staticmethod
L467     def _first_key(d: dict, keys: list[str]):
L468         for k in keys:
L469             if k in d and d[k] is not None: return d[k]
L470         return None
L471
L472     @staticmethod
L473     def _finn_get(session: requests.Session, url: str, params: dict, retries: int=3, sleep_s: float=0.5):
L474         for i in range(retries):
L475             r = session.get(url, params=params, timeout=15)
L476             if r.status_code==429: time.sleep(min(2**i*sleep_s,4.0)); continue
L477             r.raise_for_status(); return r.json()
L478         r.raise_for_status()
L479
L480     def fetch_cfo_capex_ttm_finnhub(self, tickers: list[str], api_key: str|None=None) -> pd.DataFrame:
L481         api_key = api_key or os.getenv("FINNHUB_API_KEY")
L482         if not api_key: raise ValueError("Finnhub API key not provided. Set FINNHUB_API_KEY or pass api_key=")
L483         base, s, rows = "https://finnhub.io/api/v1", requests.Session(), []
L484         for sym in tickers:
L485             cfo_ttm = capex_ttm = None
L486             try:
L487                 j = self._finn_get(s, f"{base}/stock/cash-flow", {"symbol":sym,"frequency":"quarterly","limit":8,"token":api_key})
L488                 arr = j.get("cashFlow") or []; cfo_vals, capex_vals = [], []
L489                 for item in arr[:4]:
L490                     cfo_vals.append(self._first_key(item,self._FINN_CFO_KEYS)); capex_vals.append(self._first_key(item,self._FINN_CAPEX_KEYS))
L491                 if any(v is not None for v in cfo_vals): cfo_ttm = float(np.nansum([np.nan if v is None else float(v) for v in cfo_vals]))
L492                 if any(v is not None for v in capex_vals): capex_ttm = float(np.nansum([np.nan if v is None else float(v) for v in capex_vals]))
L493             except Exception: pass
L494             if cfo_ttm is None or capex_ttm is None:
L495                 try:
L496                     j = self._finn_get(s, f"{base}/stock/cash-flow", {"symbol":sym,"frequency":"annual","limit":1,"token":api_key})
L497                     arr = j.get("cashFlow") or []
L498                     if arr:
L499                         item0 = arr[0]
L500                         if cfo_ttm is None:
L501                             v = self._first_key(item0,self._FINN_CFO_KEYS)
L502                             if v is not None: cfo_ttm = float(v)
L503                         if capex_ttm is None:
L504                             v = self._first_key(item0,self._FINN_CAPEX_KEYS)
L505                             if v is not None: capex_ttm = float(v)
L506                 except Exception: pass
L507             rows.append({"ticker":sym,"cfo_ttm_fh":np.nan if cfo_ttm is None else cfo_ttm,"capex_ttm_fh":np.nan if capex_ttm is None else capex_ttm})
L508         return pd.DataFrame(rows).set_index("ticker")
L509
L510     def compute_fcf_with_fallback(self, tickers: list[str], finnhub_api_key: str|None=None) -> pd.DataFrame:
L511         yf_df = self.fetch_cfo_capex_ttm_yf(tickers)
L512         T.log("financials (yf) done")
L513         miss_mask = yf_df[["cfo_ttm_yf","capex_ttm_yf","fcf_ttm_yf_direct"]].isna().any(axis=1)
L514         need = yf_df.index[miss_mask].tolist(); print(f"[T] yf financials missing: {len(need)} {need[:10]}{'...' if len(need)>10 else ''}")
L515         if need:
L516             fh_df = self.fetch_cfo_capex_ttm_finnhub(need, api_key=finnhub_api_key)
L517             df = yf_df.join(fh_df, how="left")
L518             for col_yf, col_fh in [("cfo_ttm_yf","cfo_ttm_fh"),("capex_ttm_yf","capex_ttm_fh")]:
L519                 df[col_yf] = df[col_yf].fillna(df[col_fh])
L520             print("[T] financials (finnhub) done (fallback only)")
L521         else:
L522             df = yf_df.assign(cfo_ttm_fh=np.nan, capex_ttm_fh=np.nan)
L523             print("[T] financials (finnhub) skipped (no missing)")
L524         df["cfo_ttm"]  = df["cfo_ttm_yf"].where(df["cfo_ttm_yf"].notna(), df["cfo_ttm_fh"])
L525         df["capex_ttm"] = df["capex_ttm_yf"].where(df["capex_ttm_yf"].notna(), df["capex_ttm_fh"])
L526         cfo, capex = pd.to_numeric(df["cfo_ttm"], errors="coerce"), pd.to_numeric(df["capex_ttm"], errors="coerce").abs()
L527         fcf_calc = cfo - capex
L528         fcf_direct = pd.to_numeric(df.get("fcf_ttm_yf_direct"), errors="coerce")
L529         df["fcf_ttm"] = fcf_calc.where(fcf_calc.notna(), fcf_direct)
L530         df["cfo_source"]  = np.where(df["cfo_ttm_yf"].notna(),"yfinance",np.where(df["cfo_ttm_fh"].notna(),"finnhub",""))
L531         df["capex_source"] = np.where(df["capex_ttm_yf"].notna(),"yfinance",np.where(df["capex_ttm_fh"].notna(),"finnhub",""))
L532         df["fcf_imputed"] = df[["cfo_ttm","capex_ttm"]].isna().any(axis=1) & df["fcf_ttm"].notna()
L533         cols = ["cfo_ttm_yf","capex_ttm_yf","cfo_ttm_fh","capex_ttm_fh","cfo_ttm","capex_ttm","fcf_ttm","fcf_ttm_yf_direct","cfo_source","capex_source","fcf_imputed"]
L534         return df[cols].sort_index()
L535
L536     def _build_eps_df(self, tickers, tickers_bulk, info, sec_map: dict | None = None):
L537         eps_rows=[]
L538         for t in tickers:
L539             info_t = info[t]
L540             sec_t = (sec_map or {}).get(t, {})
L541             eps_ttm = sec_t.get("eps_ttm", info_t.get("trailingEps", np.nan))
L542             eps_q = sec_t.get("eps_q_recent", np.nan)
L543             try:
L544                 qearn, so = tickers_bulk.tickers[t].quarterly_earnings, info_t.get("sharesOutstanding")
L545                 if so and qearn is not None and not qearn.empty and "Earnings" in qearn.columns:
L546                     eps_ttm_q = qearn["Earnings"].head(4).sum()/so
L547                     if pd.notna(eps_ttm_q) and (pd.isna(eps_ttm) or (abs(eps_ttm)>0 and abs(eps_ttm/eps_ttm_q)>3)): eps_ttm = eps_ttm_q
L548                     if pd.isna(eps_q):
L549                         eps_q = qearn["Earnings"].iloc[-1]/so
L550             except Exception: pass
L551             rev_ttm = sec_t.get("rev_ttm", np.nan)
L552             rev_q = sec_t.get("rev_q_recent", np.nan)
L553             eps_rows.append({"ticker":t,"eps_ttm":eps_ttm,"eps_q_recent":eps_q,"rev_ttm":rev_ttm,"rev_q_recent":rev_q})
L554         return self.impute_eps_ttm(pd.DataFrame(eps_rows).set_index("ticker"))
L555
L556     def prepare_data(self):
L557         """Fetch price and fundamental data for all tickers."""
L558         cand_info = yf.Tickers(" ".join(self.cand)); cand_prices = {}
L559         for t in self.cand:
L560             try: cand_prices[t] = cand_info.tickers[t].fast_info.get("lastPrice", np.inf)
L561             except Exception as e: print(f"{t}: price fetch failed ({e})"); cand_prices[t] = np.inf
L562         cand_f = [t for t,p in cand_prices.items() if p<=self.price_max]
L563         T.log("price cap filter done (CAND_PRICE_MAX)")
L564         # å…¥åŠ›ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã®é‡è¤‡ã‚’é™¤å»ã—ã€ç¾è¡Œâ†’å€™è£œã®é †åºã‚’ç¶­æŒ
L565         tickers = list(dict.fromkeys(self.exist + cand_f))
L566         T.log(f"universe prepared: unique={len(tickers)} bench={self.bench}")
L567         data = yf.download(tickers + [self.bench], period="600d",
L568                            auto_adjust=True, progress=False, threads=False)
L569         T.log("yf.download done")
L570         px = data["Close"].dropna(how="all", axis=1).ffill(limit=2)
L571         spx = data["Close"][self.bench].reindex(px.index).ffill()
L572         clip_days = int(os.getenv("PRICE_CLIP_DAYS", "0"))   # 0ãªã‚‰ç„¡åŠ¹ï¼ˆæ—¢å®šï¼‰
L573         if clip_days > 0:
L574             px  = px.tail(clip_days + 1)
L575             spx = spx.tail(clip_days + 1)
L576             logger.info("[T] price window clipped by env: %d rows (PRICE_CLIP_DAYS=%d)", len(px), clip_days)
L577         else:
L578             logger.info("[T] price window clip skipped; rows=%d", len(px))
L579         tickers_bulk, info = yf.Tickers(" ".join(tickers)), {}
L580         for t in tickers:
L581             try:
L582                 info[t] = tickers_bulk.tickers[t].info
L583             except Exception as e:
L584                 logger.info("[warn] %s: info fetch failed (%s)", t, e)
L585                 info[t] = {}
L586         try:
L587             sec_map = self.fetch_eps_rev_from_sec(tickers)
L588             for t in tickers:
L589                 if t in info and sec_map.get(t):
L590                     info[t]["SEC_REV_Q_SERIES"] = sec_map[t].get("rev_q_series") or []
L591                     info[t]["SEC_EPS_Q_SERIES"] = sec_map[t].get("eps_q_series") or []
L592         except Exception:
L593             sec_map = None
L594         eps_df = self._build_eps_df(tickers, tickers_bulk, info, sec_map=sec_map)
L595         # index é‡è¤‡ãŒã‚ã‚‹ã¨ .loc[t, col] ãŒ Series ã«ãªã‚Šä»£å…¥æ™‚ã« ValueError ã‚’èª˜ç™ºã™ã‚‹
L596         if not eps_df.index.is_unique:
L597             eps_df = eps_df[~eps_df.index.duplicated(keep="last")]
L598         eps_df = eps_df.assign(
L599             EPS_TTM=eps_df["eps_ttm"],
L600             EPS_Q_LastQ=eps_df["eps_q_recent"],
L601             REV_TTM=eps_df["rev_ttm"],
L602             REV_Q_LastQ=eps_df["rev_q_recent"],
L603         )
L604         # ã“ã“ã§éNaNä»¶æ•°ã‚’ã‚µãƒãƒªè¡¨ç¤ºï¼ˆæ¬ æçŠ¶æ³ã®å³æ™‚æŠŠæ¡ç”¨ï¼‰
L605         try:
L606             n = len(eps_df)
L607             c_eps = int(eps_df["EPS_TTM"].notna().sum())
L608             c_rev = int(eps_df["REV_TTM"].notna().sum())
L609             print(f"[SEC] eps_ttm non-NaN: {c_eps}/{n}  rev_ttm non-NaN: {c_rev}/{n}")
L610         except Exception:
L611             pass
L612         fcf_df = self.compute_fcf_with_fallback(tickers, finnhub_api_key=self.api_key)
L613         T.log("eps/fcf prep done")
L614         returns = px[tickers].pct_change()
L615         T.log("price prep/returns done")
L616         return dict(cand=cand_f, tickers=tickers, data=data, px=px, spx=spx, tickers_bulk=tickers_bulk, info=info, eps_df=eps_df, fcf_df=fcf_df, returns=returns)
L617
L618 # === Selectorï¼šç›¸é–¢ä½æ¸›ãƒ»é¸å®šï¼ˆã‚¹ã‚³ã‚¢ï¼†ãƒªã‚¿ãƒ¼ãƒ³ã ã‘èª­ã‚€ï¼‰ ===
L619 class Selector:
L620     # ---- DRRS helpersï¼ˆSelectorå°‚ç”¨ï¼‰ ----
L621     @staticmethod
L622     def _z_np(X: np.ndarray) -> np.ndarray:
L623         X = np.asarray(X, dtype=np.float32); m = np.nanmean(X, axis=0, keepdims=True); s = np.nanstd(X, axis=0, keepdims=True)+1e-9
L624         return (np.nan_to_num(X)-m)/s
L625
L626     @classmethod
L627     def residual_corr(cls, R: np.ndarray, n_pc: int=3, shrink: float=0.1) -> np.ndarray:
L628         Z = cls._z_np(R); U,S,_ = np.linalg.svd(Z, full_matrices=False); F = U[:,:n_pc]*S[:n_pc]; B = np.linalg.lstsq(F, Z, rcond=None)[0]
L629         E = Z - F@B; C = np.corrcoef(E, rowvar=False)
L630         off = C - np.diag(np.diag(C)); iu = np.triu_indices_from(off,1); avg_abs = np.nanmean(np.abs(off[iu])) if iu[0].size else 0.0
L631         shrink_eff = float(np.clip(shrink + 0.5*avg_abs, 0.1, 0.6)); N = C.shape[0]
L632         return (1.0 - shrink_eff)*C + shrink_eff*np.eye(N, dtype=C.dtype)
L633
L634     @classmethod
L635     def rrqr_like_det(cls, R: np.ndarray, score: np.ndarray, k: int, gamma: float=1.0):
L636         Z, w = cls._z_np(R), (score-score.min())/(np.ptp(score)+1e-12); X = Z*(1.0 + gamma*w)
L637         N, k = X.shape[1], max(0, min(k, X.shape[1]))
L638         if k==0: return []
L639         S, selected, Rres = [], np.zeros(N, dtype=bool), X.copy()
L640         for _ in range(k):
L641             norms = (Rres*Rres).sum(axis=0); cand = np.where(~selected)[0]
L642             j = sorted(cand, key=lambda c:(-norms[c], -w[c], c))[0]
L643             S.append(j); selected[j]=True; u = X[:,j:j+1]; u/=(np.linalg.norm(u)+1e-12); Rres = Rres - u @ (u.T @ Rres)
L644         return sorted(S)
L645
L646     @staticmethod
L647     def _obj(corrM: np.ndarray, score: np.ndarray, idx, lam: float) -> float:
L648         idx = list(idx); P = corrM[np.ix_(idx, idx)]; s = (score-score.mean())/(score.std()+1e-9)
L649         return float(s[idx].sum() - lam*((P.sum()-np.trace(P))/2.0))
L650
L651     @classmethod
L652     def swap_local_det(cls, corrM: np.ndarray, score: np.ndarray, idx, lam: float=0.6, max_pass: int=15):
L653         S, best, improved, passes = sorted(idx), cls._obj(corrM, score, idx, lam), True, 0
L654         while improved and passes<max_pass:
L655             improved, passes = False, passes+1
L656             for i,out in enumerate(list(S)):
L657                 for inn in range(len(score)):
L658                     if inn in S: continue
L659                     cand = sorted(S[:i]+[inn]+S[i+1:]); v = cls._obj(corrM, score, cand, lam)
L660                     if v>best+1e-10: S, best, improved = cand, v, True; break
L661                 if improved: break
L662         return S, best
L663
L664     @staticmethod
L665     def _obj_with_cross(C_within: np.ndarray, C_cross: np.ndarray|None, score: np.ndarray, idx, lam: float, mu: float) -> float:
L666         idx = list(idx); P = C_within[np.ix_(idx, idx)]; s = (score-score.mean())/(score.std()+1e-9)
L667         within = (P.sum()-np.trace(P))/2.0; cross = 0.0
L668         if C_cross is not None and C_cross.size>0: cross = C_cross[idx,:].sum()
L669         return float(s[idx].sum() - lam*within - mu*cross)
L670
L671     @classmethod
L672     def swap_local_det_cross(cls, C_within: np.ndarray, C_cross: np.ndarray|None, score: np.ndarray, idx, lam: float=0.6, mu: float=0.3, max_pass: int=15):
L673         S, best, improved, passes, N = sorted(idx), cls._obj_with_cross(C_within,C_cross,score,idx,lam,mu), True, 0, len(score)
L674         while improved and passes<max_pass:
L675             improved, passes = False, passes+1
L676             for i,out in enumerate(list(S)):
L677                 for inn in range(N):
L678                     if inn in S: continue
L679                     cand = sorted(S[:i]+[inn]+S[i+1:]); v = cls._obj_with_cross(C_within,C_cross,score,cand,lam,mu)
L680                     if v>best+1e-10: S, best, improved = cand, v, True; break
L681                 if improved: break
L682         return S, best
L683
L684     @staticmethod
L685     def avg_corr(C: np.ndarray, idx) -> float:
L686         k = len(idx); P = C[np.ix_(idx, idx)]
L687         return float((P.sum()-np.trace(P))/(k*(k-1)+1e-12))
L688
L689     @classmethod
L690     def select_bucket_drrs(cls, returns_df: pd.DataFrame, score_ser: pd.Series, pool_tickers: list[str], k: int, *, n_pc: int, gamma: float, lam: float, lookback: int, shrink: float=0.10, g_fixed_tickers: list[str]|None=None, mu: float=0.0):
L691         g_fixed = [t for t in (g_fixed_tickers or []) if t in returns_df.columns]
L692         union = [t for t in pool_tickers if t in returns_df.columns]
L693         for t in g_fixed:
L694             if t not in union: union.append(t)
L695         Rdf_all = returns_df[union]; Rdf_all = Rdf_all.iloc[-lookback:] if len(Rdf_all)>=lookback else Rdf_all; Rdf_all = Rdf_all.dropna()
L696         pool_eff, g_eff = [t for t in pool_tickers if t in Rdf_all.columns], [t for t in g_fixed if t in Rdf_all.columns]
L697         if len(pool_eff)==0: return dict(idx=[], tickers=[], avg_res_corr=np.nan, sum_score=0.0, objective=-np.inf)
L698         score = score_ser.reindex(pool_eff).to_numpy(dtype=np.float32)
L699         C_all = cls.residual_corr(Rdf_all.to_numpy(), n_pc=n_pc, shrink=shrink)
L700         col_pos = {c:i for i,c in enumerate(Rdf_all.columns)}; pool_pos = [col_pos[t] for t in pool_eff]
L701         C_within, C_cross = C_all[np.ix_(pool_pos,pool_pos)], None
L702         if len(g_eff)>0 and mu>0.0:
L703             g_pos = [col_pos[t] for t in g_eff]; C_cross = C_all[np.ix_(pool_pos,g_pos)]
L704         R_pool = Rdf_all[pool_eff].to_numpy(); S0 = cls.rrqr_like_det(R_pool, score, k, gamma=gamma)
L705         S, Jn = (cls.swap_local_det_cross(C_within, C_cross, score, S0, lam=lam, mu=mu, max_pass=15) if C_cross is not None else cls.swap_local_det(C_within, score, S0, lam=lam, max_pass=15))
L706         selected_tickers = [pool_eff[i] for i in S]
L707         return dict(idx=S, tickers=selected_tickers, avg_res_corr=cls.avg_corr(C_within,S), sum_score=float(score[S].sum()), objective=float(Jn))
L708
L709     # ---- é¸å®šï¼ˆã‚¹ã‚³ã‚¢ Series / returns ã ã‘ã‚’å—ã‘ã‚‹ï¼‰----
L710 # === Outputï¼šå‡ºåŠ›æ•´å½¢ã¨é€ä¿¡ï¼ˆè¡¨ç¤ºãƒ»Slackï¼‰ ===
L711 class Output:
L712
L713     def __init__(self, debug=None):
L714         # self.debug ã¯ä½¿ã‚ãªã„ï¼ˆäº’æ›ã®ãŸã‚å¼•æ•°ã¯å—ã‘ã‚‹ãŒç„¡è¦–ï¼‰
L715         self.miss_df = self.g_table = self.d_table = self.io_table = self.df_metrics_fmt = self.debug_table = None
L716         self.g_title = self.d_title = ""
L717         self.g_formatters = self.d_formatters = {}
L718         # ä½ã‚¹ã‚³ã‚¢ï¼ˆGSC+DSCï¼‰Top10 è¡¨ç¤º/é€ä¿¡ç”¨
L719         self.low10_table = None
L720         self.debug_text = ""   # ãƒ‡ãƒãƒƒã‚°ç”¨æœ¬æ–‡ã¯ã“ã“ã«ä¸€æœ¬åŒ–
L721         self._debug_logged = False
L722
L723     # --- è¡¨ç¤ºï¼ˆå…ƒ display_results ã®ãƒ­ã‚¸ãƒƒã‚¯ãã®ã¾ã¾ï¼‰ ---
L724     def display_results(self, *, exist, bench, df_z, g_score, d_score_all,
L725                         init_G, init_D, top_G, top_D, **kwargs):
L726         logger.info("ğŸ“Œ reached display_results")
L727         pd.set_option('display.float_format','{:.3f}'.format)
L728         print("ğŸ“ˆ ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼åˆ†æ•£æœ€é©åŒ–ã®çµæœ")
L729         if self.miss_df is not None and not self.miss_df.empty:
L730             print("Missing Data:")
L731             print(self.miss_df.to_string(index=False))
L732
L733         # ---- è¡¨ç¤ºç”¨ï¼šChanges/Near-Miss ã®ã‚¹ã‚³ã‚¢æºã‚’â€œæœ€çµ‚é›†è¨ˆâ€ã«çµ±ä¸€ã™ã‚‹ãƒ—ãƒ­ã‚­ã‚· ----
L734         try:
L735             sc = getattr(self, "_sc", None)
L736             agg_G = getattr(sc, "_agg_G", None)
L737             agg_D = getattr(sc, "_agg_D", None)
L738         except Exception:
L739             sc = agg_G = agg_D = None
L740         class _SeriesProxy:
L741             __slots__ = ("primary", "fallback")
L742             def __init__(self, primary, fallback): self.primary, self.fallback = primary, fallback
L743             def get(self, key, default=None):
L744                 try:
L745                     v = self.primary.get(key) if hasattr(self.primary, "get") else None
L746                     if v is not None and not (isinstance(v, float) and v != v):
L747                         return v
L748                 except Exception:
L749                     pass
L750                 try:
L751                     return self.fallback.get(key) if hasattr(self.fallback, "get") else default
L752                 except Exception:
L753                     return default
L754         g_score = _SeriesProxy(agg_G, g_score)
L755         d_score_all = _SeriesProxy(agg_D, d_score_all)
L756         near_G = getattr(sc, "_near_G", []) if sc else []
L757         near_D = getattr(sc, "_near_D", []) if sc else []
L758
L759         extra_G = [t for t in init_G if t not in top_G][:5]; G_UNI = top_G + extra_G
L760         gsc_series = pd.Series({t: g_score.get(t) for t in G_UNI}, name='GSC')
L761         self.g_table = pd.concat([df_z.loc[G_UNI,['GROWTH_F','MOM','TRD','VOL']], gsc_series], axis=1)
L762         self.g_table.index = [t + ("â­ï¸" if t in top_G else "") for t in G_UNI]
L763         self.g_formatters = {col:"{:.2f}".format for col in ['GROWTH_F','MOM','TRD','VOL']}; self.g_formatters['GSC'] = "{:.3f}".format
L764         self.g_title = (f"[Gæ  / {N_G} / {_fmt_w(g_weights)} / corrM={corrM} / "
L765                         f"LB={DRRS_G['lookback']} nPC={DRRS_G['n_pc']} Î³={DRRS_G['gamma']} Î»={DRRS_G['lam']} Î·={DRRS_G['eta']} shrink={DRRS_SHRINK}]")
L766         if near_G:
L767             add = [t for t in near_G if t not in set(G_UNI)][:10]
L768             if len(add) < 10:
L769                 try:
L770                     aggG = getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False)
L771                     out_now = sorted(set(exist) - set(top_G + top_D))  # ä»Šå› OUT
L772                     used = set(G_UNI + add)
L773                     def _push(lst):
L774                         nonlocal add, used
L775                         for t in lst:
L776                             if len(add) == 10: break
L777                             if t in aggG.index and t not in used:
L778                                 add.append(t); used.add(t)
L779                     _push(out_now)           # â‘  ä»Šå› OUT ã‚’å„ªå…ˆ
L780                     _push(list(aggG.index))  # â‘¡ ã¾ã è¶³ã‚Šãªã‘ã‚Œã°ä¸Šä½ã§å……å¡«
L781                 except Exception:
L782                     pass
L783             if add:
L784                 near_tbl = pd.concat([df_z.loc[add,['GROWTH_F','MOM','TRD','VOL']], pd.Series({t: g_score.get(t) for t in add}, name='GSC')], axis=1)
L785                 self.g_table = pd.concat([self.g_table, near_tbl], axis=0)
L786         print(self.g_title); print(self.g_table.to_string(formatters=self.g_formatters))
L787
L788         extra_D = [t for t in init_D if t not in top_D][:5]; D_UNI = top_D + extra_D
L789         cols_D = ['QAL','YLD','VOL','TRD']; d_disp = pd.DataFrame(index=D_UNI)
L790         d_disp['QAL'], d_disp['YLD'], d_disp['VOL'], d_disp['TRD'] = df_z.loc[D_UNI,'D_QAL'], df_z.loc[D_UNI,'D_YLD'], df_z.loc[D_UNI,'D_VOL_RAW'], df_z.loc[D_UNI,'D_TRD']
L791         dsc_series = pd.Series({t: d_score_all.get(t) for t in D_UNI}, name='DSC')
L792         self.d_table = pd.concat([d_disp, dsc_series], axis=1); self.d_table.index = [t + ("â­ï¸" if t in top_D else "") for t in D_UNI]
L793         self.d_formatters = {col:"{:.2f}".format for col in cols_D}; self.d_formatters['DSC']="{:.3f}".format
L794         import scorer
L795         dw_eff = scorer.D_WEIGHTS_EFF
L796         self.d_title = (f"[Dæ  / {N_D} / {_fmt_w(dw_eff)} / corrM={corrM} / "
L797                         f"LB={DRRS_D['lookback']} nPC={DRRS_D['n_pc']} Î³={DRRS_D['gamma']} Î»={DRRS_D['lam']} Î¼={CROSS_MU_GD} Î·={DRRS_D['eta']} shrink={DRRS_SHRINK}]")
L798         if near_D:
L799             add = [t for t in near_D if t not in set(D_UNI)][:10]
L800             if add:
L801                 d_disp2 = pd.DataFrame(index=add)
L802                 d_disp2['QAL'], d_disp2['YLD'], d_disp2['VOL'], d_disp2['TRD'] = df_z.loc[add,'D_QAL'], df_z.loc[add,'D_YLD'], df_z.loc[add,'D_VOL_RAW'], df_z.loc[add,'D_TRD']
L803                 near_tbl = pd.concat([d_disp2, pd.Series({t: d_score_all.get(t) for t in add}, name='DSC')], axis=1)
L804                 self.d_table = pd.concat([self.d_table, near_tbl], axis=0)
L805         print(self.d_title); print(self.d_table.to_string(formatters=self.d_formatters))
L806
L807         # === Changesï¼ˆIN ã® GSC/DSC ã‚’è¡¨ç¤ºã€‚OUT ã¯éŠ˜æŸ„åã®ã¿ï¼‰ ===
L808         in_list = sorted(set(list(top_G)+list(top_D)) - set(exist))
L809         out_list = sorted(set(exist) - set(list(top_G)+list(top_D)))
L810
L811         self.io_table = pd.DataFrame({
L812             'IN': pd.Series(in_list),
L813             '/ OUT': pd.Series(out_list)
L814         })
L815         g_list = [f"{g_score.get(t):.3f}" if pd.notna(g_score.get(t)) else 'â€”' for t in out_list]
L816         d_list = [f"{d_score_all.get(t):.3f}" if pd.notna(d_score_all.get(t)) else 'â€”' for t in out_list]
L817         self.io_table['GSC'] = pd.Series(g_list)
L818         self.io_table['DSC'] = pd.Series(d_list)
L819
L820         print("Changes:")
L821         print(self.io_table.to_string(index=False))
L822
L823         all_tickers = list(set(exist + list(top_G) + list(top_D) + [bench])); prices = yf.download(all_tickers, period='1y', auto_adjust=True, progress=False, threads=False)['Close'].ffill(limit=2)
L824         ret = prices.pct_change(); portfolios = {'CUR':exist,'NEW':list(top_G)+list(top_D)}; metrics={}
L825         for name,ticks in portfolios.items():
L826             pr = ret[ticks].mean(axis=1, skipna=True).dropna(); cum = (1+pr).cumprod()-1; n = len(pr)
L827             if n>=252: ann_ret, ann_vol = (1+cum.iloc[-1])**(252/n)-1, pr.std()*np.sqrt(252)
L828             else: ann_ret, ann_vol = cum.iloc[-1], pr.std()*np.sqrt(n)
L829             sharpe, drawdown = ann_ret/ann_vol, (cum - cum.cummax()).min()
L830             if len(ticks)>=2:
L831                 C_raw = ret[ticks].corr(); RAW_rho = C_raw.mask(np.eye(len(ticks), dtype=bool)).stack().mean()
L832                 R = ret[ticks].dropna().to_numpy(); C_resid = Selector.residual_corr(R, n_pc=3, shrink=DRRS_SHRINK)
L833                 RESID_rho = float((C_resid.sum()-np.trace(C_resid))/(C_resid.shape[0]*(C_resid.shape[0]-1)))
L834             else: RAW_rho = RESID_rho = np.nan
L835             divy = ttm_div_yield_portfolio(ticks); metrics[name] = {'RET':ann_ret,'VOL':ann_vol,'SHP':sharpe,'MDD':drawdown,'RAWÏ':RAW_rho,'RESIDÏ':RESID_rho,'DIVY':divy}
L836         df_metrics = pd.DataFrame(metrics).T; df_metrics_pct = df_metrics.copy(); self.df_metrics = df_metrics
L837         for col in ['RET','VOL','MDD','DIVY']: df_metrics_pct[col] = df_metrics_pct[col]*100
L838         cols_order = ['RET','VOL','SHP','MDD','RAWÏ','RESIDÏ','DIVY']; df_metrics_pct = df_metrics_pct.reindex(columns=cols_order)
L839         def _fmt_row(s):
L840             return pd.Series({'RET':f"{s['RET']:.1f}%",'VOL':f"{s['VOL']:.1f}%",'SHP':f"{s['SHP']:.1f}",'MDD':f"{s['MDD']:.1f}%",'RAWÏ':(f"{s['RAWÏ']:.2f}" if pd.notna(s['RAWÏ']) else "NaN"),'RESIDÏ':(f"{s['RESIDÏ']:.2f}" if pd.notna(s['RESIDÏ']) else "NaN"),'DIVY':f"{s['DIVY']:.1f}%"})
L841         self.df_metrics_fmt = df_metrics_pct.apply(_fmt_row, axis=1); print("Performance Comparison:"); print(self.df_metrics_fmt.to_string())
L842         # === è¿½åŠ : GSC+DSC ãŒä½ã„é † TOP10 ===
L843         try:
L844             all_scores = pd.DataFrame({'GSC': df_z['GSC'], 'DSC': df_z['DSC']}).copy()
L845             all_scores['G_plus_D'] = all_scores['GSC'] + all_scores['DSC']
L846             all_scores = all_scores.dropna(subset=['G_plus_D'])
L847             self.low10_table = all_scores.sort_values('G_plus_D', ascending=True).head(10).round(3)
L848             print("Low Score Candidates (GSC+DSC bottom 10):")
L849             print(self.low10_table.to_string())
L850         except Exception as e:
L851             print(f"[warn] low-score ranking failed: {e}")
L852             self.low10_table = None
L853         self.debug_text = ""
L854         if debug_mode:
L855             logger.info("debug_mode=True: df_z dump handled in scorer; skipping factor-side debug output")
L856         else:
L857             logger.debug(
L858                 "skip debug log: debug_mode=%s debug_text_empty=%s",
L859                 debug_mode, True
L860             )
L861         self._debug_logged = True
L862
L863     # --- Slacké€ä¿¡ï¼ˆå…ƒ notify_slack ã®ãƒ­ã‚¸ãƒƒã‚¯ãã®ã¾ã¾ï¼‰ ---
L864     def notify_slack(self):
L865         SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")
L866
L867         if not SLACK_WEBHOOK_URL:
L868             print("âš ï¸ SLACK_WEBHOOK_URL not set (main report skipped)")
L869             return
L870
L871         def _filter_suffix_from(spec: dict, group: str) -> str:
L872             g = spec.get(group, {})
L873             parts = [str(m) for m in g.get("pre_mask", [])]
L874             for k, v in (g.get("pre_filter", {}) or {}).items():
L875                 base, op = (k[:-4], "<") if k.endswith("_max") else ((k[:-4], ">") if k.endswith("_min") else (k, "="))
L876                 name = {"beta": "Î²"}.get(base, base)
L877                 try:
L878                     val = f"{float(v):g}"
L879                 except Exception:
L880                     val = str(v)
L881                 parts.append(f"{name}{op}{val}")
L882             return "" if not parts else " / filter:" + " & ".join(parts)
L883
L884         def _inject_filter_suffix(title: str, group: str) -> str:
L885             suf = _filter_suffix_from(FILTER_SPEC, group)
L886             return f"{title[:-1]}{suf}]" if suf and title.endswith("]") else (title + suf)
L887
L888         def _blk(title, tbl, fmt=None, drop=()):
L889             if tbl is None or getattr(tbl, 'empty', False):
L890                 return f"{title}\n(é¸å®šãªã—)\n"
L891             if drop and hasattr(tbl, 'columns'):
L892                 keep = [c for c in tbl.columns if c not in drop]
L893                 tbl, fmt = tbl[keep], {k: v for k, v in (fmt or {}).items() if k in keep}
L894             return f"{title}\n```{tbl.to_string(formatters=fmt)}```\n"
L895
L896         message = "ğŸ“ˆ ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼åˆ†æ•£æœ€é©åŒ–ã®çµæœ\n"
L897         if self.miss_df is not None and not self.miss_df.empty:
L898             message += "Missing Data\n```" + self.miss_df.to_string(index=False) + "```\n"
L899         message += _blk(_inject_filter_suffix(self.g_title, "G"), self.g_table, self.g_formatters, drop=("TRD",))
L900         message += _blk(_inject_filter_suffix(self.d_title, "D"), self.d_table, self.d_formatters)
L901         message += "Changes\n" + ("(å¤‰æ›´ãªã—)\n" if self.io_table is None or getattr(self.io_table, 'empty', False) else f"```{self.io_table.to_string(index=False)}```\n")
L902         message += "Performance Comparison:\n```" + self.df_metrics_fmt.to_string() + "```"
L903
L904         try:
L905             r = requests.post(SLACK_WEBHOOK_URL, json={"text": message})
L906             print(f"[DBG] main_post status={getattr(r, 'status_code', None)} size={len(message)}")
L907             if r is not None:
L908                 r.raise_for_status()
L909         except Exception as e:
L910             print(f"[ERR] main_post_failed: {e}")
L911
L912 def _infer_g_universe(feature_df, selected12=None, near5=None):
L913     try:
L914         out = feature_df.index[feature_df['group'].astype(str).str.upper().eq('G')].tolist()
L915         if out: return out
L916     except Exception:
L917         pass
L918     base = set()
L919     for lst in (selected12 or []), (near5 or []):
L920         for x in (lst or []): base.add(x)
L921     return list(base) if base else list(feature_df.index)
L922
L923 def _fmt_with_fire_mark(tickers, feature_df):
L924     out = []
L925     for t in tickers or []:
L926         try:
L927             br = bool(feature_df.at[t, "G_BREAKOUT_recent_5d"])
L928             pb = bool(feature_df.at[t, "G_PULLBACK_recent_5d"])
L929             out.append(f"{t}{' ğŸ”¥' if (br or pb) else ''}")
L930         except Exception:
L931             out.append(t)
L932     return out
L933
L934 def _label_recent_event(t, feature_df):
L935     try:
L936         br = bool(feature_df.at[t, "G_BREAKOUT_recent_5d"]); dbr = str(feature_df.at[t, "G_BREAKOUT_last_date"]) if br else ""
L937         pb = bool(feature_df.at[t, "G_PULLBACK_recent_5d"]); dpb = str(feature_df.at[t, "G_PULLBACK_last_date"]) if pb else ""
L938         if   br and not pb: return f"{t}ï¼ˆãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®š {dbr}ï¼‰"
L939         elif pb and not br: return f"{t}ï¼ˆæŠ¼ã—ç›®åç™º {dpb}ï¼‰"
L940         elif br and pb:     return f"{t}ï¼ˆãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®š {dbr}ï¼æŠ¼ã—ç›®åç™º {dpb}ï¼‰"
L941     except Exception:
L942         pass
L943     return t
L944
L945 # === ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å¯è¦–åŒ–ï¼šG/Då…±é€šãƒ•ãƒ­ãƒ¼ï¼ˆå‡ºåŠ›ã¯ä¸å¤‰ï¼‰ ===
L946
L947 def io_build_input_bundle() -> InputBundle:
L948     """
L949     æ—¢å­˜ã®ã€ãƒ‡ãƒ¼ã‚¿å–å¾—â†’å‰å‡¦ç†ã€ã‚’å®Ÿè¡Œã—ã€InputBundle ã‚’è¿”ã™ã€‚
L950     å‡¦ç†å†…å®¹ãƒ»åˆ—åãƒ»ä¸¸ã‚ãƒ»ä¾‹å¤–ãƒ»ãƒ­ã‚°æ–‡è¨€ã¯ç¾è¡Œã©ãŠã‚Šï¼ˆå¤‰æ›´ç¦æ­¢ï¼‰ã€‚
L951     """
L952     state = Input(cand=cand, exist=exist, bench=bench, price_max=CAND_PRICE_MAX, finnhub_api_key=FINNHUB_API_KEY).prepare_data()
L953     return InputBundle(cand=state["cand"], tickers=state["tickers"], bench=bench, data=state["data"], px=state["px"], spx=state["spx"], tickers_bulk=state["tickers_bulk"], info=state["info"], eps_df=state["eps_df"], fcf_df=state["fcf_df"], returns=state["returns"])
L954
L955 def run_group(sc: Scorer, group: str, inb: InputBundle, cfg: PipelineConfig,
L956               n_target: int) -> tuple[list, float, float, float]:
L957     """
L958     G/Dã‚’åŒä¸€æ‰‹é †ã§å‡¦ç†ï¼šæ¡ç‚¹â†’ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼â†’é¸å®šï¼ˆç›¸é–¢ä½æ¸›è¾¼ã¿ï¼‰ã€‚
L959     æˆ»ã‚Šå€¤ï¼š(pick, avg_res_corr, sum_score, objective)
L960     JSONä¿å­˜ã¯æ—¢å­˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆã‚­ãƒ¼åãƒ»ä¸¸ã‚æ¡ãƒ»é †åºï¼‰ã‚’è¸è¥²ã€‚
L961     """
L962     sc.cfg = cfg
L963
L964     if hasattr(sc, "score_build_features"):
L965         feat = sc.score_build_features(inb)
L966         if not hasattr(sc, "_feat_logged"):
L967             T.log("features built (scorer)")
L968             sc._feat_logged = True
L969         agg = sc.score_aggregate(feat, group, cfg) if hasattr(sc, "score_aggregate") else feat
L970     else:
L971         fb = sc.aggregate_scores(inb, cfg)
L972         if not hasattr(sc, "_feat_logged"):
L973             T.log("features built (scorer)")
L974             sc._feat_logged = True
L975         sc._feat = fb
L976         agg = fb.g_score if group == "G" else fb.d_score_all
L977         if group == "D" and hasattr(fb, "df"):
L978             agg = agg[fb.df['BETA'] < D_BETA_MAX]
L979
L980     if hasattr(sc, "filter_candidates"):
L981         agg = agg[sc.filter_candidates(inb, agg, group, cfg)]
L982
L983     selector = Selector()
L984     if hasattr(sc, "select_diversified"):
L985         pick, avg_r, sum_sc, obj = sc.select_diversified(agg, group, cfg, n_target,
L986             selector=selector, prev_tickers=None,
L987             corrM=cfg.drrs.corrM, shrink=cfg.drrs.shrink,
L988             cross_mu=cfg.drrs.cross_mu_gd)
L989     else:
L990         if group == "G":
L991             init = agg.nlargest(min(cfg.drrs.corrM, len(agg))).index.tolist()
L992             res = selector.select_bucket_drrs(returns_df=inb.returns, score_ser=agg, pool_tickers=init, k=n_target,
L993                 n_pc=cfg.drrs.G.get("n_pc", 3), gamma=cfg.drrs.G.get("gamma", 1.2),
L994                 lam=cfg.drrs.G.get("lam", 0.68),
L995                 lookback=cfg.drrs.G.get("lookback", 252),
L996                 shrink=cfg.drrs.shrink, g_fixed_tickers=None, mu=0.0)
L997         else:
L998             init = agg.nlargest(min(cfg.drrs.corrM, len(agg))).index.tolist()
L999             g_fixed = getattr(sc, "_top_G", None)
L1000             res = selector.select_bucket_drrs(returns_df=inb.returns, score_ser=agg, pool_tickers=init, k=n_target,
L1001                 n_pc=cfg.drrs.D.get("n_pc", 4), gamma=cfg.drrs.D.get("gamma", 0.8),
L1002                 lam=cfg.drrs.D.get("lam", 0.85),
L1003                 lookback=cfg.drrs.D.get("lookback", 504),
L1004                 shrink=cfg.drrs.shrink, g_fixed_tickers=g_fixed,
L1005                 mu=cfg.drrs.cross_mu_gd)
L1006         pick = res["tickers"]; avg_r = res["avg_res_corr"]
L1007         sum_sc = res["sum_score"]; obj = res["objective"]
L1008         if group == "D":
L1009             _, pick = _disjoint_keepG(getattr(sc, "_top_G", []), pick, init)
L1010             T.log("selection finalized (G/D)")
L1011     try:
L1012         inc = [t for t in exist if t in agg.index]
L1013         pick = _sticky_keep_current(
L1014             agg=agg, pick=pick, incumbents=inc, n_target=n_target,
L1015             delta_z=SWAP_DELTA_Z, keep_buffer=SWAP_KEEP_BUFFER
L1016         )
L1017     except Exception as _e:
L1018         print(f"[warn] sticky_keep_current skipped: {str(_e)}")
L1019     # --- Near-Miss: æƒœã—ãã‚‚é¸ã°ã‚Œãªã‹ã£ãŸä¸Šä½10ã‚’ä¿æŒï¼ˆSlackè¡¨ç¤ºç”¨ï¼‰ ---
L1020     # 5) Near-Miss ã¨æœ€çµ‚é›†è¨ˆSeriesã‚’ä¿æŒï¼ˆè¡¨ç¤ºå°‚ç”¨ã€‚è¨ˆç®—ã¸å½±éŸ¿ãªã—ï¼‰
L1021     try:
L1022         pool = agg.drop(index=[t for t in pick if t in agg.index], errors="ignore")
L1023         near10 = list(pool.sort_values(ascending=False).head(10).index)
L1024         setattr(sc, f"_near_{group}", near10)
L1025         setattr(sc, f"_agg_{group}", agg)
L1026     except Exception:
L1027         pass
L1028
L1029     if group == "D":
L1030         T.log("save done")
L1031     if group == "G":
L1032         sc._top_G = pick
L1033     return pick, avg_r, sum_sc, obj
L1034
L1035 def run_pipeline() -> SelectionBundle:
L1036     """
L1037     G/Då…±é€šãƒ•ãƒ­ãƒ¼ã®å…¥å£ã€‚I/Oã¯ã“ã“ã ã‘ã§å®Ÿæ–½ã—ã€è¨ˆç®—ã¯Scorerã«å§”è­²ã€‚
L1038     Slackæ–‡è¨€ãƒ»ä¸¸ã‚ãƒ»é †åºã¯æ—¢å­˜ã® Output ã‚’ç”¨ã„ã¦å¤‰æ›´ã—ãªã„ã€‚
L1039     """
L1040     inb = io_build_input_bundle()
L1041     cfg = PipelineConfig(
L1042         weights=WeightsConfig(g=g_weights, d=D_weights),
L1043         drrs=DRRSParams(
L1044             corrM=corrM, shrink=DRRS_SHRINK,
L1045             G=DRRS_G, D=DRRS_D, cross_mu_gd=CROSS_MU_GD
L1046         ),
L1047         price_max=CAND_PRICE_MAX,
L1048         debug_mode=debug_mode
L1049     )
L1050     sc = Scorer()
L1051     top_G, avgG, sumG, objG = run_group(sc, "G", inb, cfg, N_G)
L1052     poolG = list(getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False).index)
L1053     alpha = Scorer.spx_to_alpha(inb.spx)
L1054     sectors = {t:(inb.info.get(t,{}).get("sector") or "U") for t in poolG}; scores = {t:Scorer.g_score.get(t,0.0) for t in poolG}
L1055     top_G = Scorer.pick_top_softcap(scores, sectors, N=N_G, cap=2, alpha=alpha, hard=5)
L1056     sc._top_G = top_G
L1057     try:
L1058         aggG = getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False)
L1059         sc._near_G = [t for t in aggG.index if t not in set(top_G)][:10]
L1060     except Exception:
L1061         pass
L1062     base = sum(Scorer.g_score.get(t,0.0) for t in poolG[:N_G])
L1063     effs = sum(Scorer.g_score.get(t,0.0) for t in top_G)
L1064     print(f"[soft_cap2] score_cost={(base-effs)/max(1e-9,abs(base)):.2%}, alpha={alpha:.3f}")
L1065     top_D, avgD, sumD, objD = run_group(sc, "D", inb, cfg, N_D)
L1066     fb = getattr(sc, "_feat", None)
L1067     near_G = getattr(sc, "_near_G", [])
L1068     selected12 = list(top_G)
L1069     df = fb.df if fb is not None else pd.DataFrame()
L1070     guni = _infer_g_universe(df, selected12, near_G)
L1071     try:
L1072         fire_recent = [t for t in guni
L1073                        if (str(df.at[t, "G_BREAKOUT_recent_5d"]) == "True") or
L1074                           (str(df.at[t, "G_PULLBACK_recent_5d"]) == "True")]
L1075     except Exception: fire_recent = []
L1076
L1077     lines = [
L1078         "ã€Gæ ãƒ¬ãƒãƒ¼ãƒˆï½œé€±æ¬¡ãƒ¢ãƒ‹ã‚¿ï¼ˆç›´è¿‘5å–¶æ¥­æ—¥ï¼‰ã€‘",
L1079         "ã€å‡¡ä¾‹ã€‘ğŸ”¥=ç›´è¿‘5å–¶æ¥­æ—¥å†…ã«ã€Œãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®šã€ã¾ãŸã¯ã€ŒæŠ¼ã—ç›®åç™ºã€ã‚’æ¤œçŸ¥",
L1080         f"é¸å®š{N_G}: {', '.join(_fmt_with_fire_mark(selected12, df))}" if selected12 else f"é¸å®š{N_G}: ãªã—",
L1081         f"æ¬¡ç‚¹10: {', '.join(_fmt_with_fire_mark(near_G, df))}" if near_G else "æ¬¡ç‚¹10: ãªã—",]
L1082
L1083     if fire_recent:
L1084         fire_list = ", ".join([_label_recent_event(t, df) for t in fire_recent])
L1085         lines.append(f"éå»5å–¶æ¥­æ—¥ã®æ¤œçŸ¥: {fire_list}")
L1086     else:
L1087         lines.append("éå»5å–¶æ¥­æ—¥ã®æ¤œçŸ¥: ãªã—")
L1088
L1089     try:
L1090         webhook = os.environ.get("SLACK_WEBHOOK_URL", "")
L1091         if webhook:
L1092             requests.post(webhook, json={"text": "\n".join([s for s in lines if s != ""])}, timeout=10)
L1093     except Exception:
L1094         pass
L1095
L1096     out = Output()
L1097     # è¡¨ç¤ºå´ã‹ã‚‰é¸å®šæ™‚ã®é›†è¨ˆã¸ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†ã«ä¿æŒï¼ˆè¡¨ç¤ºå°‚ç”¨ãƒ»å‰¯ä½œç”¨ãªã—ï¼‰
L1098     try: out._sc = sc
L1099     except Exception: pass
L1100     if hasattr(sc, "_feat"):
L1101         try:
L1102             fb = sc._feat
L1103             out.miss_df = fb.missing_logs
L1104             out.display_results(
L1105                 exist=exist,
L1106                 bench=bench,
L1107                 df_z=fb.df_z,
L1108                 g_score=fb.g_score,
L1109                 d_score_all=fb.d_score_all,
L1110                 init_G=top_G,
L1111                 init_D=top_D,
L1112                 top_G=top_G,
L1113                 top_D=top_D,
L1114                 df_full_z=getattr(fb, "df_full_z", None),
L1115                 prev_G=getattr(sc, "_prev_G", exist),
L1116                 prev_D=getattr(sc, "_prev_D", exist),
L1117             )
L1118         except Exception:
L1119             pass
L1120     out.notify_slack()
L1121     sb = SelectionBundle(resG={"tickers": top_G, "avg_res_corr": avgG,
L1122               "sum_score": sumG, "objective": objG},
L1123         resD={"tickers": top_D, "avg_res_corr": avgD,
L1124               "sum_score": sumD, "objective": objD},
L1125         top_G=top_G, top_D=top_D, init_G=top_G, init_D=top_D)
L1126
L1127     # --- Low Score Candidates (GSC+DSC bottom 10) : send before debug dump ---
L1128     try:
L1129         _low_df = (pd.DataFrame({"GSC": fb.g_score, "DSC": fb.d_score_all})
L1130               .assign(G_plus_D=lambda x: x["GSC"] + x["DSC"])
L1131               .sort_values("G_plus_D")
L1132               .head(10)
L1133               .round(3))
L1134         low_msg = "Low Score Candidates (GSC+DSC bottom 10)\n" + _low_df.to_string(index=True, index_names=False)
L1135         _post_slack({"text": f"```{low_msg}```"})
L1136     except Exception as _e:
L1137         _post_slack({"text": f"```Low Score Candidates: ä½œæˆå¤±æ•—: {_e}```"})
L1138
L1139     return sb
L1140
L1141 if __name__ == "__main__":
L1142     run_pipeline()
```

## <scorer.py>
```text
L1 # scorer.py
L2 # kawatest
L3 # =============================================================================
L4 # Scorer: ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼/æŒ‡æ¨™ã®ç”Ÿæˆã¨åˆæˆã‚¹ã‚³ã‚¢ç®—å‡ºã‚’æ‹…ã†ç´”ç²‹å±¤
L5 #
L6 # ã€ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã ã‘èª­ã‚ã°åˆ†ã‹ã‚‹ãƒã‚¤ãƒ³ãƒˆã€‘
L7 # - å…¥åŠ›(InputBundle)ã¯ã€Œä¾¡æ ¼/å‡ºæ¥é«˜/ãƒ™ãƒ³ãƒ/åŸºæœ¬æƒ…å ±/EPS/FCF/ãƒªã‚¿ãƒ¼ãƒ³ã€ã‚’å«ã‚€DTO
L8 # - å‡ºåŠ›(FeatureBundle)ã¯ã€Œrawç‰¹å¾´é‡ dfã€ã€Œæ¨™æº–åŒ– df_zã€ã€ŒG/D ã‚¹ã‚³ã‚¢ã€ã€Œæ¬ æãƒ­ã‚°ã€
L9 # - é‡ã¿ç­‰ã®ã‚³ãƒ³ãƒ•ã‚£ã‚°(PipelineConfig)ã¯ factor ã‹ã‚‰æ¸¡ã™ï¼ˆcfg å¿…é ˆï¼‰
L10 # - æ—§ã‚«ãƒ©ãƒ åã¯ Scorer å†…ã§è‡ªå‹•ãƒªãƒãƒ¼ãƒ ã—ã¦å—ã‘å…¥ã‚Œï¼ˆå¾Œæ–¹äº’æ›ï¼‰
L11 #   ä¾‹) eps_ttm -> EPS_TTM, eps_q_recent -> EPS_Q_LastQ, fcf_ttm -> FCF_TTM
L12 #
L13 # ã€I/Oå¥‘ç´„ï¼ˆScorerãŒå‚ç…§ã™ã‚‹InputBundleãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰ã€‘
L14 #   - cand: List[str]    â€¦ å€™è£œéŠ˜æŸ„ï¼ˆå˜ä½“å®Ÿè¡Œã§ã¯æœªä½¿ç”¨ï¼‰
L15 #   - tickers: List[str] â€¦ å¯¾è±¡éŠ˜æŸ„ãƒªã‚¹ãƒˆ
L16 #   - bench: str         â€¦ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼ï¼ˆä¾‹ '^GSPC'ï¼‰
L17 #   - data: pd.DataFrame â€¦ yfinance downloadçµæœ ('Close','Volume' ç­‰ã®éšå±¤åˆ—)
L18 #   - px: pd.DataFrame   â€¦ data['Close'] ç›¸å½“ï¼ˆçµ‚å€¤ï¼‰
L19 #   - spx: pd.Series     â€¦ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®çµ‚å€¤
L20 #   - tickers_bulk: object         â€¦ yfinance.Tickers
L21 #   - info: Dict[str, dict]        â€¦ yfinance info per ticker
L22 #   - eps_df: pd.DataFrame         â€¦ å¿…é ˆåˆ—: EPS_TTM, EPS_Q_LastQï¼ˆæ—§åã‚‚å¯ï¼‰
L23 #   - fcf_df: pd.DataFrame         â€¦ å¿…é ˆåˆ—: FCF_TTMï¼ˆæ—§åã‚‚å¯ï¼‰
L24 #   - returns: pd.DataFrame        â€¦ px[tickers].pct_change() ç›¸å½“
L25 #
L26 # â€»å…¥å‡ºåŠ›ã®å½¢å¼ãƒ»ä¾‹å¤–æ–‡è¨€ã¯æ—¢å­˜å®Ÿè£…ã‚’å¤‰ãˆã¾ã›ã‚“ï¼ˆå®‰å…¨ãªçŸ­ç¸®ã®ã¿ï¼‰
L27 # =============================================================================
L28
L29 import logging
L30 import os, sys, warnings
L31 import requests
L32 import numpy as np
L33 import pandas as pd
L34 import yfinance as yf
L35 from typing import Any, TYPE_CHECKING
L36 from scipy.stats import zscore
L37
L38 if TYPE_CHECKING:
L39     from factor import PipelineConfig  # type: ignore  # å®Ÿè¡Œæ™‚importãªã—ï¼ˆå¾ªç’°å›é¿ï¼‰
L40
L41 logger = logging.getLogger(__name__)
L42
L43 # ---- Dividend Helpers -------------------------------------------------------
L44 def _last_close(t, price_map=None):
L45     if price_map and (c := price_map.get(t)) is not None: return float(c)
L46     try:
L47         h = yf.Ticker(t).history(period="5d")["Close"].dropna()
L48         return float(h.iloc[-1]) if len(h) else np.nan
L49     except Exception:
L50         return np.nan
L51
L52 def _ttm_div_sum(t, lookback_days=400):
L53     try:
L54         div = yf.Ticker(t).dividends
L55         if div is None or len(div) == 0: return 0.0
L56         cutoff = pd.Timestamp.utcnow().tz_localize(None) - pd.Timedelta(days=lookback_days)
L57         ttm = float(div[div.index.tz_localize(None) >= cutoff].sum())
L58         return ttm if ttm > 0 else float(div.tail(4).sum())
L59     except Exception:
L60         return 0.0
L61
L62 def ttm_div_yield_portfolio(tickers, price_map=None):
L63     ys = [(lambda c, s: (s/c) if (np.isfinite(c) and c>0 and s>0) else 0.0)(_last_close(t, price_map), _ttm_div_sum(t)) for t in tickers]
L64     return float(np.mean(ys)) if ys else 0.0
L65
L66 # ---- ç°¡æ˜“ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆå®‰å…¨ãªçŸ­ç¸®ã®ã¿ï¼‰ -----------------------------------
L67 def winsorize_s(s: pd.Series, p=0.02):
L68     if s is None or s.dropna().empty: return s
L69     lo, hi = np.nanpercentile(s.astype(float), [100*p, 100*(1-p)]); return s.clip(lo, hi)
L70
L71 def robust_z(s: pd.Series, p=0.02):
L72     s2 = winsorize_s(s,p); return np.nan_to_num(zscore(s2.fillna(s2.mean())))
L73
L74 def robust_z_keepnan(s: pd.Series) -> pd.Series:
L75     """robust_z variant that preserves NaNs and falls back to rank-z when needed."""
L76     if s is None:
L77         return pd.Series(dtype=float)
L78     v = pd.to_numeric(s, errors="coerce")
L79     m = np.nanmedian(v)
L80     mad = np.nanmedian(np.abs(v - m))
L81     z = (v - m) / (1.4826 * mad + 1e-9)
L82     if np.nanstd(z) < 1e-9:
L83         r = v.rank(method="average", na_option="keep")
L84         z = (r - np.nanmean(r)) / (np.nanstd(r) + 1e-9)
L85     return pd.Series(z, index=v.index, dtype=float)
L86
L87
L88 def _dump_dfz(df_z: pd.DataFrame, debug_mode: bool, max_rows: int = 400, ndigits: int = 3) -> None:
L89     """df_z ã‚’ System log(INFO) ã¸ãƒ€ãƒ³ãƒ—ã™ã‚‹ç°¡æ½”ãªãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£."""
L90     if not debug_mode:
L91         return
L92     try:
L93         view = df_z.copy()
L94         view = view.apply(
L95             lambda s: s.round(ndigits)
L96             if getattr(getattr(s, "dtype", None), "kind", "") in ("f", "i")
L97             else s
L98         )
L99         if len(view) > max_rows:
L100             view = view.iloc[:max_rows]
L101
L102         # === NaNã‚µãƒãƒªï¼ˆåˆ—ã”ã¨ã®æ¬ æä»¶æ•° ä¸Šä½20ï¼‰ ===
L103         try:
L104             nan_counts = df_z.isna().sum().sort_values(ascending=False)
L105             top_nan = nan_counts[nan_counts > 0].head(20)
L106             if len(top_nan) > 0:
L107                 logger.info("NaN columns (top20):\n%s", top_nan.to_string())
L108             else:
L109                 logger.info("NaN columns: none")
L110         except Exception as exc:
L111             logger.warning("nan summary failed: %s", exc)
L112
L113         # === Zeroã‚µãƒãƒªï¼ˆåˆ—ã”ã¨ã®ã‚¼ãƒ­æ¯”ç‡ ä¸Šä½20ï¼‰ ===
L114         try:
L115             zero_counts = ((df_z == 0) & (~df_z.isna())).sum()
L116             nonnull_counts = (~df_z.isna()).sum()
L117             zero_ratio = (zero_counts / nonnull_counts).sort_values(ascending=False)
L118             top_zero = zero_ratio[zero_ratio > 0].head(20)
L119             if len(top_zero) > 0:
L120                 logger.info(
L121                     "Zero-dominated columns (top20):\n%s",
L122                     top_zero.to_string(float_format=lambda x: f"{x:.2%}"),
L123                 )
L124             else:
L125                 logger.info("Zero-dominated columns: none")
L126         except Exception as exc:
L127             logger.warning("zero summary failed: %s", exc)
L128
L129         logger.info("===== DF_Z DUMP START =====")
L130         logger.info("\n%s", view.to_string(max_rows=None, max_cols=None))
L131         logger.info("===== DF_Z DUMP END =====")
L132     except Exception as exc:
L133         logger.warning("df_z dump failed: %s", exc)
L134
L135 def _safe_div(a, b):
L136     try: return np.nan if (b is None or float(b)==0 or pd.isna(b)) else float(a)/float(b)
L137     except Exception: return np.nan
L138
L139 def _safe_last(series: pd.Series, default=np.nan):
L140     try: return float(series.iloc[-1])
L141     except Exception: return default
L142
L143 D_WEIGHTS_EFF = None  # å‡ºåŠ›è¡¨ç¤ºäº’æ›ã®ãŸã‚
L144
L145
L146 def _scalar(v):
L147     """å˜ä¸€ã‚»ãƒ«ä»£å…¥ç”¨ã«å€¤ã‚’ã‚¹ã‚«ãƒ©ãƒ¼ã¸æ­£è¦åŒ–ã™ã‚‹ã€‚
L148
L149     - pandas Series -> .iloc[-1]ï¼ˆæœ€å¾Œã‚’æ¡ç”¨ï¼‰
L150     - list/tuple/ndarray -> æœ€å¾Œã®è¦ç´ 
L151     - ãã‚Œä»¥å¤–          -> ãã®ã¾ã¾
L152     å–å¾—å¤±æ•—æ™‚ã¯ np.nan ã‚’è¿”ã™ã€‚
L153     """
L154     import numpy as _np
L155     import pandas as _pd
L156     try:
L157         if isinstance(v, _pd.Series):
L158             return v.iloc[-1] if len(v) else _np.nan
L159         if isinstance(v, (list, tuple, _np.ndarray)):
L160             return v[-1] if len(v) else _np.nan
L161         return v
L162     except Exception:
L163         return _np.nan
L164
L165
L166 # ---- Scorer æœ¬ä½“ -------------------------------------------------------------
L167 class Scorer:
L168     """
L169     - factor.py ã‹ã‚‰ã¯ `aggregate_scores(ib, cfg)` ã‚’å‘¼ã¶ã ã‘ã§OKã€‚
L170     - cfg ã¯å¿…é ˆï¼ˆfactor.PipelineConfig ã‚’æ¸¡ã™ï¼‰ã€‚
L171     - æ—§ã‚«ãƒ©ãƒ åã‚’è‡ªå‹•ãƒªãƒãƒ¼ãƒ ã—ã¦æ–°ã‚¹ã‚­ãƒ¼ãƒã«å¸åã—ã¾ã™ã€‚
L172     """
L173
L174     # === å…ˆé ­ã§æ—§â†’æ–°ã‚«ãƒ©ãƒ åãƒãƒƒãƒ—ï¼ˆç§»è¡Œç”¨ï¼‰ ===
L175     EPS_RENAME = {"eps_ttm":"EPS_TTM", "eps_q_recent":"EPS_Q_LastQ"}
L176     FCF_RENAME = {"fcf_ttm":"FCF_TTM"}
L177
L178     # === ã‚¹ã‚­ãƒ¼ãƒç°¡æ˜“ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€ä½é™ï¼‰ ===
L179     @staticmethod
L180     def _validate_ib_for_scorer(ib: Any):
L181         miss = [a for a in ["tickers","bench","data","px","spx","tickers_bulk","info","eps_df","fcf_df","returns"] if not hasattr(ib,a) or getattr(ib,a) is None]
L182         if miss: raise ValueError(f"InputBundle is missing required attributes for Scorer: {miss}")
L183         if any(c in ib.eps_df.columns for c in Scorer.EPS_RENAME): ib.eps_df.rename(columns=Scorer.EPS_RENAME, inplace=True)
L184         if any(c in ib.fcf_df.columns for c in Scorer.FCF_RENAME): ib.fcf_df.rename(columns=Scorer.FCF_RENAME, inplace=True)
L185         need_eps, need_fcf = {"EPS_TTM","EPS_Q_LastQ"},{"FCF_TTM"}
L186         if not need_eps.issubset(ib.eps_df.columns): raise ValueError(f"eps_df must contain columns {need_eps} (accepts old names via auto-rename). Got: {list(ib.eps_df.columns)}")
L187         if not need_fcf.issubset(ib.fcf_df.columns): raise ValueError(f"fcf_df must contain columns {need_fcf} (accepts old names via auto-rename). Got: {list(ib.fcf_df.columns)}")
L188
L189     # ----ï¼ˆScorerå°‚ç”¨ï¼‰ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ»æŒ‡æ¨™ç³» ----
L190     @staticmethod
L191     def trend(s: pd.Series):
L192         if len(s)<200: return np.nan
L193         sma50, sma150, sma200 = s.rolling(50).mean().iloc[-1], s.rolling(150).mean().iloc[-1], s.rolling(200).mean().iloc[-1]
L194         prev200, p = s.rolling(200).mean().iloc[-21], s.iloc[-1]
L195         lo_52 = s[-252:].min() if len(s)>=252 else s.min(); hi_52 = s[-252:].max() if len(s)>=252 else s.max()
L196         rng = (hi_52 - lo_52) if hi_52>lo_52 else np.nan
L197         clip = lambda x,lo,hi: (np.nan if pd.isna(x) else max(lo,min(hi,x)))
L198         a = clip(p/(s.rolling(50).mean().iloc[-1]) - 1, -0.5, 0.5)
L199         b = clip(sma50/sma150 - 1, -0.5, 0.5)
L200         c = clip(sma150/sma200 - 1, -0.5, 0.5)
L201         d = clip(sma200/prev200 - 1, -0.2, 0.2)
L202         e = clip((p - lo_52) / (rng if rng and rng>0 else np.nan) - 0.5, -0.5, 0.5)
L203         parts = [0.0 if pd.isna(x) else x for x in (a,b,c,d,e)]
L204         return 0.30*parts[0] + 0.20*parts[1] + 0.15*parts[2] + 0.15*parts[3] + 0.20*parts[4]
L205
L206     @staticmethod
L207     def rs(s, b):
L208         n, nb = len(s), len(b)
L209         if n<60 or nb<60: return np.nan
L210         L12 = 252 if n>=252 and nb>=252 else min(n,nb)-1; L1 = 22 if n>=22 and nb>=22 else max(5, min(n,nb)//3)
L211         r12, r1, br12, br1 = s.iloc[-1]/s.iloc[-L12]-1, s.iloc[-1]/s.iloc[-L1]-1, b.iloc[-1]/b.iloc[-L12]-1, b.iloc[-1]/b.iloc[-L1]-1
L212         return (r12 - br12)*0.7 + (r1 - br1)*0.3
L213
L214     @staticmethod
L215     def tr_str(s):
L216         if s is None:
L217             return np.nan
L218         s = s.ffill(limit=2).dropna()
L219         if len(s) < 50:
L220             return np.nan
L221         ma50 = s.rolling(50, min_periods=50).mean()
L222         last_ma = ma50.iloc[-1]
L223         last_px = s.iloc[-1]
L224         return float(last_px/last_ma - 1.0) if pd.notna(last_ma) and pd.notna(last_px) else np.nan
L225
L226     @staticmethod
L227     def rs_line_slope(s: pd.Series, b: pd.Series, win: int) -> float:
L228         r = (s/b).dropna()
L229         if len(r) < win: return np.nan
L230         y, x = np.log(r.iloc[-win:]), np.arange(win, dtype=float)
L231         try: return float(np.polyfit(x, y, 1)[0])
L232         except Exception: return np.nan
L233
L234     @staticmethod
L235     def ev_fallback(info_t: dict, tk: yf.Ticker) -> float:
L236         ev = info_t.get('enterpriseValue', np.nan)
L237         if pd.notna(ev) and ev>0: return float(ev)
L238         mc, debt, cash = info_t.get('marketCap', np.nan), np.nan, np.nan
L239         try:
L240             bs = tk.quarterly_balance_sheet
L241             if bs is not None and not bs.empty:
L242                 c = bs.columns[0]
L243                 for k in ("Total Debt","Long Term Debt","Short Long Term Debt"):
L244                     if k in bs.index: debt = float(bs.loc[k,c]); break
L245                 for k in ("Cash And Cash Equivalents","Cash And Cash Equivalents And Short Term Investments","Cash"):
L246                     if k in bs.index: cash = float(bs.loc[k,c]); break
L247         except Exception: pass
L248         if pd.notna(mc): return float(mc + (0 if pd.isna(debt) else debt) - (0 if pd.isna(cash) else cash))
L249         return np.nan
L250
L251     @staticmethod
L252     def dividend_status(ticker: str) -> str:
L253         t = yf.Ticker(ticker)
L254         try:
L255             if not t.dividends.empty: return "has"
L256         except Exception: return "unknown"
L257         try:
L258             a = t.actions
L259             if (a is not None and not a.empty and "Stock Splits" in a.columns and a["Stock Splits"].abs().sum()>0): return "none_confident"
L260         except Exception: pass
L261         try:
L262             fi = t.fast_info
L263             if any(getattr(fi,k,None) for k in ("last_dividend_date","dividend_rate","dividend_yield")): return "maybe_missing"
L264         except Exception: pass
L265         return "unknown"
L266
L267     @staticmethod
L268     def div_streak(t):
L269         try:
L270             divs = yf.Ticker(t).dividends.dropna(); ann = divs.groupby(divs.index.year).sum(); ann = ann[ann.index<pd.Timestamp.today().year]
L271             years, streak = sorted(ann.index), 0
L272             for i in range(len(years)-1,0,-1):
L273                 if ann[years[i]] > ann[years[i-1]]: streak += 1
L274                 else: break
L275             return streak
L276         except Exception: return 0
L277
L278     @staticmethod
L279     def fetch_finnhub_metrics(symbol):
L280         api_key = os.environ.get("FINNHUB_API_KEY")
L281         if not api_key: return {}
L282         url, params = "https://finnhub.io/api/v1/stock/metric", {"symbol":symbol,"metric":"all","token":api_key}
L283         try:
L284             r = requests.get(url, params=params, timeout=10); r.raise_for_status(); m = r.json().get("metric",{})
L285             return {'EPS':m.get('epsGrowthTTMYoy'),'REV':m.get('revenueGrowthTTMYoy'),'ROE':m.get('roeTTM'),'BETA':m.get('beta'),'DIV':m.get('dividendYieldIndicatedAnnual'),'FCF':(m.get('freeCashFlowTTM')/m.get('enterpriseValue')) if m.get('freeCashFlowTTM') and m.get('enterpriseValue') else None}
L286         except Exception: return {}
L287
L288     @staticmethod
L289     def calc_beta(series: pd.Series, market: pd.Series, lookback=252):
L290         r, m = series.pct_change().dropna(), market.pct_change().dropna()
L291         n = min(len(r), len(m), lookback)
L292         if n<60: return np.nan
L293         r, m = r.iloc[-n:], m.iloc[-n:]; cov, var = np.cov(r, m)[0,1], np.var(m)
L294         return np.nan if var==0 else cov/var
L295
L296     @staticmethod
L297     def spx_to_alpha(spx: pd.Series, bands=(0.03,0.10), w=(0.6,0.4),
L298                      span=5, q=(0.20,0.40), alphas=(0.05,0.08,0.10)) -> float:
L299         """
L300         S&P500æŒ‡æ•°ã®ã¿ã‹ã‚‰æ“¬ä¼¼breadthã‚’ä½œã‚Šã€å±¥æ­´åˆ†ä½ã§Î±ã‚’æ®µéšæ±ºå®šã€‚
L301         bands=(Â±3%, Â±10%), w=(50DMA,200DMA), åˆ†ä½q=(20%,40%), alphas=(ä½,ä¸­,é«˜)
L302         """
L303         ma50, ma200 = spx.rolling(50).mean(), spx.rolling(200).mean()
L304         b50, b200 = ((spx/ma50 - 1)+bands[0])/(2*bands[0]), ((spx/ma200 - 1)+bands[1])/(2*bands[1])
L305         hist = (w[0]*b50 + w[1]*b200).clip(0,1).ewm(span=span).mean()
L306         b, (lo, mid) = float(hist.iloc[-1]), (float(hist.quantile(q[0])), float(hist.quantile(q[1])))
L307         return alphas[0] if b < lo else alphas[1] if b < mid else alphas[2]
L308
L309     @staticmethod
L310     def soft_cap_effective_scores(scores: pd.Series|dict, sectors: dict, cap=2, alpha=0.08) -> pd.Series:
L311         """
L312         åŒä¸€ã‚»ã‚¯ã‚¿ãƒ¼capè¶…éï¼ˆ3æœ¬ç›®ä»¥é™ï¼‰ã« Î±Ã—æ®µéšæ¸›ç‚¹ã‚’èª²ã—ãŸâ€œæœ‰åŠ¹ã‚¹ã‚³ã‚¢â€Seriesã‚’è¿”ã™ã€‚
L313         æˆ»ã‚Šå€¤ã¯é™é †ã‚½ãƒ¼ãƒˆæ¸ˆã¿ã€‚
L314         """
L315         s = pd.Series(scores, dtype=float); order = s.sort_values(ascending=False).index
L316         cnt, pen = {}, {}
L317         for t in order:
L318             sec = sectors.get(t, "U"); cnt[sec] = cnt.get(sec,0) + 1; pen[t] = alpha*max(0, cnt[sec]-cap)
L319         return (s - pd.Series(pen)).sort_values(ascending=False)
L320
L321     @staticmethod
L322     def pick_top_softcap(scores: pd.Series|dict, sectors: dict, N: int, cap=2, alpha=0.08, hard: int|None=5) -> list[str]:
L323         """
L324         soft-capé©ç”¨å¾Œã®ä¸Šä½Nãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚’è¿”ã™ã€‚hard>0ãªã‚‰éå¸¸ç”¨ãƒãƒ¼ãƒ‰ä¸Šé™ã§åŒä¸€ã‚»ã‚¯ã‚¿ãƒ¼è¶…éã‚’é–“å¼•ãï¼ˆæ—¢å®š=5ï¼‰ã€‚
L325         """
L326         eff = Scorer.soft_cap_effective_scores(scores, sectors, cap, alpha)
L327         if not hard:
L328             return list(eff.head(N).index)
L329         pick, used = [], {}
L330         for t in eff.index:
L331             s = sectors.get(t, "U")
L332             if used.get(s,0) < hard:
L333                 pick.append(t); used[s] = used.get(s,0) + 1
L334             if len(pick) == N: break
L335         return pick
L336
L337     @staticmethod
L338     def trend_template_breadth_series(px: pd.DataFrame, spx: pd.Series, win_days: int | None = None) -> pd.Series:
L339         """
L340         å„å–¶æ¥­æ—¥ã® trend_template åˆæ ¼æœ¬æ•°ï¼ˆåˆæ ¼â€œæœ¬æ•°â€=Cï¼‰ã‚’è¿”ã™ã€‚
L341         - px: åˆ—=tickerï¼ˆãƒ™ãƒ³ãƒã¯å«ã‚ãªã„ï¼‰
L342         - spx: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ Seriesï¼ˆpx.index ã«æ•´åˆ—ï¼‰
L343         - win_days: æœ«å°¾ã®è¨ˆç®—å¯¾è±¡å–¶æ¥­æ—¥æ•°ï¼ˆNoneâ†’å…¨ä½“ã€æ—¢å®š600ã¯å‘¼ã³å‡ºã—å´æŒ‡å®šï¼‰
L344         ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼†rollingã®ã¿ã§è»½é‡ã€‚æ¬ æã¯ False æ‰±ã„ã€‚
L345         """
L346         import numpy as np, pandas as pd
L347         if px is None or px.empty:
L348             return pd.Series(dtype=int)
L349         px = px.dropna(how="all", axis=1)
L350         if win_days and win_days > 0:
L351             px = px.tail(win_days)
L352         if px.empty:
L353             return pd.Series(dtype=int)
L354         spx = spx.reindex(px.index).ffill()
L355
L356         ma50  = px.rolling(50).mean()
L357         ma150 = px.rolling(150).mean()
L358         ma200 = px.rolling(200).mean()
L359
L360         tt = (px > ma150)
L361         tt &= (px > ma200)
L362         tt &= (ma150 > ma200)
L363         tt &= (ma200 - ma200.shift(21) > 0)
L364         tt &= (ma50  > ma150)
L365         tt &= (ma50  > ma200)
L366         tt &= (px    > ma50)
L367
L368         lo252 = px.rolling(252).min()
L369         hi252 = px.rolling(252).max()
L370         tt &= (px.divide(lo252).sub(1.0) >= 0.30)   # P_OVER_LOW52 >= 0.30
L371         tt &= (px >= (0.75 * hi252))                # NEAR_52W_HIGH >= -0.25
L372
L373         r12  = px.divide(px.shift(252)).sub(1.0)
L374         br12 = spx.divide(spx.shift(252)).sub(1.0)
L375         r1   = px.divide(px.shift(22)).sub(1.0)
L376         br1  = spx.divide(spx.shift(22)).sub(1.0)
L377         rs   = 0.7*(r12.sub(br12, axis=0)) + 0.3*(r1.sub(br1, axis=0))
L378         tt &= (rs >= 0.10)
L379
L380         return tt.fillna(False).sum(axis=1).astype(int)
L381
L382     # ---- ã‚¹ã‚³ã‚¢é›†è¨ˆï¼ˆDTO/Configã‚’å—ã‘å–ã‚Šã€FeatureBundleã‚’è¿”ã™ï¼‰ ----
L383     def aggregate_scores(self, ib: Any, cfg):
L384         if cfg is None:
L385             raise ValueError("cfg is required; pass factor.PipelineConfig")
L386         self._validate_ib_for_scorer(ib)
L387
L388         px, spx, tickers = ib.px, ib.spx, ib.tickers
L389         tickers_bulk, info, eps_df, fcf_df = ib.tickers_bulk, ib.info, ib.eps_df, ib.fcf_df
L390
L391         df, missing_logs = pd.DataFrame(index=tickers), []
L392         for t in tickers:
L393             d, s = info[t], px[t]; ev = self.ev_fallback(d, tickers_bulk.tickers[t])
L394             # --- åŸºæœ¬ç‰¹å¾´ ---
L395             df.loc[t,'TR']   = self.trend(s)
L396             df.loc[t,'EPS']  = _scalar(eps_df.loc[t,'EPS_TTM']) if t in eps_df.index else np.nan
L397             df.loc[t,'EPS_Q'] = _scalar(eps_df.loc[t,'EPS_Q_LastQ']) if t in eps_df.index else np.nan
L398             df.loc[t,'REV_TTM'] = _scalar(eps_df.loc[t,'REV_TTM']) if t in eps_df.index else np.nan
L399             df.loc[t,'REV_Q']   = _scalar(eps_df.loc[t,'REV_Q_LastQ']) if t in eps_df.index else np.nan
L400             df.loc[t,'REV']  = d.get('revenueGrowth',np.nan)
L401             df.loc[t,'ROE']  = d.get('returnOnEquity',np.nan)
L402             df.loc[t,'BETA'] = self.calc_beta(s, spx, lookback=252)
L403
L404             # --- é…å½“ï¼ˆæ¬ æè£œå®Œå«ã‚€ï¼‰ ---
L405             div = d.get('dividendYield') if d.get('dividendYield') is not None else d.get('trailingAnnualDividendYield')
L406             if div is None or pd.isna(div):
L407                 try:
L408                     divs = yf.Ticker(t).dividends
L409                     if divs is not None and not divs.empty:
L410                         last_close = s.iloc[-1]; div_1y = divs[divs.index >= (divs.index.max() - pd.Timedelta(days=365))].sum()
L411                         if last_close and last_close>0: div = float(div_1y/last_close)
L412                 except Exception: pass
L413             df.loc[t,'DIV'] = 0.0 if (div is None or pd.isna(div)) else float(div)
L414
L415             # --- FCF/EV ---
L416             fcf_val = fcf_df.loc[t,'FCF_TTM'] if t in fcf_df.index else np.nan
L417             df.loc[t,'FCF'] = (fcf_val/ev) if (pd.notna(fcf_val) and pd.notna(ev) and ev>0) else np.nan
L418
L419             # --- ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ãƒ»ãƒœãƒ©é–¢é€£ ---
L420             df.loc[t,'RS'], df.loc[t,'TR_str'] = self.rs(s, spx), self.tr_str(s)
L421             r, rm = s.pct_change().dropna(), spx.pct_change().dropna()
L422             n = int(min(len(r), len(rm)))
L423
L424             DOWNSIDE_DEV = np.nan
L425             if n>=60:
L426                 r6 = r.iloc[-min(len(r),126):]; neg = r6[r6<0]
L427                 if len(neg)>=10: DOWNSIDE_DEV = float(neg.std(ddof=0)*np.sqrt(252))
L428             df.loc[t,'DOWNSIDE_DEV'] = DOWNSIDE_DEV
L429
L430             MDD_1Y = np.nan
L431             try:
L432                 w = s.iloc[-min(len(s),252):].dropna()
L433                 if len(w)>=30:
L434                     roll_max = w.cummax(); MDD_1Y = float((w/roll_max - 1.0).min())
L435             except Exception: pass
L436             df.loc[t,'MDD_1Y'] = MDD_1Y
L437
L438             RESID_VOL = np.nan
L439             if n>=120:
L440                 rr, rrm = r.iloc[-n:].align(rm.iloc[-n:], join='inner')
L441                 if len(rr)==len(rrm) and len(rr)>=120 and rrm.var()>0:
L442                     beta = float(np.cov(rr, rrm)[0,1]/np.var(rrm)); resid = rr - beta*rrm
L443                     RESID_VOL = float(resid.std(ddof=0)*np.sqrt(252))
L444             df.loc[t,'RESID_VOL'] = RESID_VOL
L445
L446             DOWN_OUTPERF = np.nan
L447             if n>=60:
L448                 m, x = rm.iloc[-n:], r.iloc[-n:]; mask = m<0
L449                 if mask.sum()>=10:
L450                     mr, sr = float(m[mask].mean()), float(x[mask].mean())
L451                     DOWN_OUTPERF = (sr - mr)/abs(mr) if mr!=0 else np.nan
L452             df.loc[t,'DOWN_OUTPERF'] = DOWN_OUTPERF
L453
L454             # --- é•·æœŸç§»å‹•å¹³å‡/ä½ç½® ---
L455             sma200 = s.rolling(200).mean(); df.loc[t,'EXT_200'] = np.nan
L456             if pd.notna(sma200.iloc[-1]) and sma200.iloc[-1]!=0: df.loc[t,'EXT_200'] = abs(float(s.iloc[-1]/sma200.iloc[-1]-1.0))
L457
L458             # --- é…å½“ã®è©³ç´°ç³» ---
L459             DIV_TTM_PS=DIV_VAR5=DIV_YOY=DIV_FCF_COVER=np.nan
L460             try:
L461                 divs = yf.Ticker(t).dividends.dropna()
L462                 if not divs.empty:
L463                     last_close = s.iloc[-1]; div_1y = float(divs[divs.index >= (divs.index.max()-pd.Timedelta(days=365))].sum())
L464                     DIV_TTM_PS = div_1y if div_1y>0 else np.nan
L465                     ann = divs.groupby(divs.index.year).sum()
L466                     if len(ann)>=2 and ann.iloc[-2]!=0: DIV_YOY = float(ann.iloc[-1]/ann.iloc[-2]-1.0)
L467                     tail = ann.iloc[-5:] if len(ann)>=5 else ann
L468                     if len(tail)>=3 and tail.mean()!=0: DIV_VAR5 = float(tail.std(ddof=1)/abs(tail.mean()))
L469                 so = d.get('sharesOutstanding',None)
L470                 if so and pd.notna(DIV_TTM_PS) and pd.notna(fcf_val) and fcf_val!=0:
L471                     DIV_FCF_COVER = float((fcf_val)/(DIV_TTM_PS*float(so)))
L472             except Exception: pass
L473             df.loc[t,'DIV_TTM_PS'], df.loc[t,'DIV_VAR5'], df.loc[t,'DIV_YOY'], df.loc[t,'DIV_FCF_COVER'] = DIV_TTM_PS, DIV_VAR5, DIV_YOY, DIV_FCF_COVER
L474
L475             # --- è²¡å‹™å®‰å®šæ€§ ---
L476             df.loc[t,'DEBT2EQ'], df.loc[t,'CURR_RATIO'] = d.get('debtToEquity',np.nan), d.get('currentRatio',np.nan)
L477
L478             # --- EPS å¤‰å‹• ---
L479             EPS_VAR_8Q = np.nan
L480             try:
L481                 qe, so = tickers_bulk.tickers[t].quarterly_earnings, d.get('sharesOutstanding',None)
L482                 if qe is not None and not qe.empty and so:
L483                     eps_q = (qe['Earnings'].dropna().astype(float)/float(so)).replace([np.inf,-np.inf],np.nan)
L484                     if len(eps_q)>=4: EPS_VAR_8Q = float(eps_q.iloc[-min(8,len(eps_q)):].std(ddof=1))
L485             except Exception: pass
L486             df.loc[t,'EPS_VAR_8Q'] = EPS_VAR_8Q
L487
L488             # --- ã‚µã‚¤ã‚º/æµå‹•æ€§ ---
L489             df.loc[t,'MARKET_CAP'] = d.get('marketCap',np.nan); adv60 = np.nan
L490             try:
L491                 vol_series = ib.data['Volume'][t].dropna()
L492                 if len(vol_series)>=5 and len(s)==len(vol_series):
L493                     dv = (vol_series*s).rolling(60).mean(); adv60 = float(dv.iloc[-1])
L494             except Exception: pass
L495             df.loc[t,'ADV60_USD'] = adv60
L496
L497             # --- å£²ä¸Š/åˆ©ç›Šã®åŠ é€Ÿåº¦ç­‰ ---
L498             REV_Q_YOY=EPS_Q_YOY=REV_YOY_ACC=REV_YOY_VAR=np.nan
L499             REV_ANNUAL_STREAK = REV_YOY = np.nan
L500             EPS_YOY = np.nan
L501             try:
L502                 qe, so = tickers_bulk.tickers[t].quarterly_earnings, d.get('sharesOutstanding',None)
L503                 sec_rev_series = (d.get('SEC_REV_Q_SERIES') or [])
L504                 if sec_rev_series:
L505                     rev = pd.Series(sec_rev_series, dtype=float).dropna()
L506                 elif qe is not None and not qe.empty and 'Revenue' in qe.columns:
L507                     rev = qe['Revenue'].dropna().astype(float)
L508                 else:
L509                     rev = pd.Series([], dtype=float)
L510                 if not rev.empty:
L511                     if len(rev)>=5: REV_Q_YOY = _safe_div(rev.iloc[-1]-rev.iloc[-5], rev.iloc[-5])
L512                     if len(rev)>=6:
L513                         yoy_now = _safe_div(rev.iloc[-1]-rev.iloc[-5], rev.iloc[-5]); yoy_prev = _safe_div(rev.iloc[-2]-rev.iloc[-6], rev.iloc[-6])
L514                         if pd.notna(yoy_now) and pd.notna(yoy_prev): REV_YOY_ACC = yoy_now - yoy_prev
L515                     yoy_list=[]
L516                     for k in range(1,5):
L517                         if len(rev)>=4+k:
L518                             y = _safe_div(rev.iloc[-k]-rev.iloc[-(k+4)], rev.iloc[-(k+4)])
L519                             if pd.notna(y): yoy_list.append(y)
L520                     if len(yoy_list)>=2: REV_YOY_VAR = float(np.std(yoy_list, ddof=1))
L521                     # NEW: å¹´æ¬¡ã®æŒç¶šæ€§ï¼ˆç›´è¿‘ã‹ã‚‰é¡ã£ã¦å‰å¹´æ¯”ãƒ—ãƒ©ã‚¹ãŒä½•å¹´é€£ç¶šã‹ã€å››åŠæœŸ4æœ¬æƒã†å®Œå…¨å¹´ã®ã¿ï¼‰
L522                     try:
L523                         if isinstance(rev.index, pd.DatetimeIndex):
L524                             g = rev.groupby(rev.index.year)
L525                             ann_sum, cnt = g.sum(), g.count()
L526                             ann_sum = ann_sum[cnt >= 4]
L527                             if len(ann_sum) >= 2:
L528                                 yoy = ann_sum.pct_change().dropna()
L529                                 if not yoy.empty:
L530                                     REV_YOY = float(yoy.iloc[-1])
L531                                 streak = 0
L532                                 for v in yoy.iloc[::-1]:
L533                                     if pd.isna(v) or v <= 0:
L534                                         break
L535                                     streak += 1
L536                                 REV_ANNUAL_STREAK = float(streak)
L537                     except Exception:
L538                         pass
L539                 sec_eps_series = (d.get('SEC_EPS_Q_SERIES') or [])
L540                 if sec_eps_series:
L541                     eps_series = pd.Series(sec_eps_series, dtype=float).replace([np.inf,-np.inf],np.nan)
L542                 elif qe is not None and not qe.empty and 'Earnings' in qe.columns and so:
L543                     eps_series = (qe['Earnings'].dropna().astype(float)/float(so)).replace([np.inf,-np.inf],np.nan)
L544                 else:
L545                     eps_series = pd.Series([], dtype=float)
L546                 if not eps_series.empty:
L547                     if len(eps_series)>=5 and pd.notna(eps_series.iloc[-5]) and eps_series.iloc[-5]!=0:
L548                         EPS_Q_YOY = _safe_div(eps_series.iloc[-1]-eps_series.iloc[-5], eps_series.iloc[-5])
L549                     try:
L550                         if isinstance(eps_series.index, pd.DatetimeIndex):
L551                             g_eps = eps_series.groupby(eps_series.index.year)
L552                             ann_eps, cnt_eps = g_eps.sum(), g_eps.count()
L553                             ann_eps = ann_eps[cnt_eps >= 4]
L554                             if len(ann_eps) >= 2:
L555                                 eps_yoy = ann_eps.pct_change().dropna()
L556                                 if not eps_yoy.empty:
L557                                     EPS_YOY = float(eps_yoy.iloc[-1])
L558                     except Exception:
L559                         pass
L560             except Exception: pass
L561             df.loc[t,'REV_Q_YOY'], df.loc[t,'EPS_Q_YOY'] = REV_Q_YOY, EPS_Q_YOY
L562             df.loc[t,'REV_YOY_ACC'], df.loc[t,'REV_YOY_VAR'] = REV_YOY_ACC, REV_YOY_VAR
L563             df.loc[t,'REV_YOY'] = REV_YOY
L564             df.loc[t,'REV_ANN_STREAK'] = REV_ANNUAL_STREAK
L565             df.loc[t,'EPS_YOY'] = EPS_YOY
L566
L567             # --- Rule of 40 ã‚„å‘¨è¾º ---
L568             total_rev_ttm = d.get('totalRevenue',np.nan)
L569             FCF_MGN = _safe_div(fcf_val, total_rev_ttm)
L570             df.loc[t,'FCF_MGN'] = FCF_MGN
L571             rule40 = np.nan
L572             try:
L573                 r = df.loc[t,'REV']; rule40 = (r if pd.notna(r) else np.nan) + (FCF_MGN if pd.notna(FCF_MGN) else np.nan)
L574             except Exception: pass
L575             df.loc[t,'RULE40'] = rule40
L576
L577             # --- ãƒˆãƒ¬ãƒ³ãƒ‰è£œåŠ© ---
L578             sma50  = s.rolling(50).mean()
L579             sma150 = s.rolling(150).mean()
L580             sma200 = s.rolling(200).mean()
L581             p = _safe_last(s)
L582
L583             df.loc[t,'MA50_OVER_150'] = (_safe_last(sma50)/_safe_last(sma150) - 1
L584                 if pd.notna(_safe_last(sma50)) and pd.notna(_safe_last(sma150)) and _safe_last(sma150)!=0 else np.nan)
L585             df.loc[t,'MA150_OVER_200'] = (_safe_last(sma150)/_safe_last(sma200) - 1
L586                 if pd.notna(_safe_last(sma150)) and pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan)
L587
L588             lo52 = s[-252:].min() if len(s)>=252 else s.min()
L589             df.loc[t,'P_OVER_LOW52'] = (p/lo52 - 1) if (lo52 and lo52>0 and pd.notna(p)) else np.nan
L590
L591             df.loc[t,'MA200_SLOPE_1M'] = np.nan
L592             if len(sma200.dropna()) >= 21:
L593                 cur200 = _safe_last(sma200)
L594                 old2001 = float(sma200.iloc[-21])
L595                 if old2001:
L596                     df.loc[t,'MA200_SLOPE_1M'] = cur200/old2001 - 1
L597
L598             df.loc[t,'P_OVER_150'] = p/_safe_last(sma150)-1 if pd.notna(_safe_last(sma150)) and _safe_last(sma150)!=0 else np.nan
L599             df.loc[t,'P_OVER_200'] = p/_safe_last(sma200)-1 if pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L600             df.loc[t,'MA50_OVER_200'] = _safe_last(sma50)/_safe_last(sma200)-1 if pd.notna(_safe_last(sma50)) and pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L601             df.loc[t,'MA200_SLOPE_5M'] = np.nan
L602             if len(sma200.dropna())>=105:
L603                 cur200, old200 = _safe_last(sma200), float(sma200.iloc[-105])
L604                 if old200 and old200!=0: df.loc[t,'MA200_SLOPE_5M'] = cur200/old200 - 1
L605             # NEW: 200æ—¥ç·šãŒé€£ç¶šã§ä¸Šå‘ãã®ã€Œæ—¥æ•°ã€
L606             df.loc[t,'MA200_UP_STREAK_D'] = np.nan
L607             try:
L608                 s200 = sma200.dropna()
L609                 if len(s200) >= 2:
L610                     diff200 = s200.diff()
L611                     up = 0
L612                     for v in diff200.iloc[::-1]:
L613                         if pd.isna(v) or v <= 0:
L614                             break
L615                         up += 1
L616                     df.loc[t,'MA200_UP_STREAK_D'] = float(up)
L617             except Exception:
L618                 pass
L619             df.loc[t,'LOW52PCT25_EXCESS'] = np.nan if (lo52 is None or lo52<=0 or pd.isna(p)) else (p/(lo52*1.25)-1)
L620             hi52 = s[-252:].max() if len(s)>=252 else s.max(); df.loc[t,'NEAR_52W_HIGH'] = np.nan
L621             if hi52 and hi52>0 and pd.notna(p):
L622                 d_hi = (p/hi52)-1.0; df.loc[t,'NEAR_52W_HIGH'] = -abs(min(0.0, d_hi))
L623             df.loc[t,'RS_SLOPE_6W'] = self.rs_line_slope(s, ib.spx, 30)
L624             df.loc[t,'RS_SLOPE_13W'] = self.rs_line_slope(s, ib.spx, 65)
L625
L626             df.loc[t,'DIV_STREAK'] = self.div_streak(t)
L627
L628             # --- æ¬ æãƒ¡ãƒ¢ ---
L629             fin_cols = ['REV','ROE','BETA','DIV','FCF']
L630             need_finnhub = [col for col in fin_cols if pd.isna(df.loc[t,col])]
L631             if need_finnhub:
L632                 fin_data = self.fetch_finnhub_metrics(t)
L633                 for col in need_finnhub:
L634                     val = fin_data.get(col)
L635                     if val is not None and not pd.isna(val): df.loc[t,col] = val
L636             for col in fin_cols + ['EPS','RS','TR_str','DIV_STREAK']:
L637                 if pd.isna(df.loc[t,col]):
L638                     if col=='DIV':
L639                         status = self.dividend_status(t)
L640                         if status!='none_confident': missing_logs.append({'Ticker':t,'Column':col,'Status':status})
L641                     else:
L642                         missing_logs.append({'Ticker':t,'Column':col})
L643
L644         def _trend_template_pass(row, rs_alpha_thresh=0.10):
L645             c1 = (row.get('P_OVER_150', np.nan) > 0) and (row.get('P_OVER_200', np.nan) > 0)
L646             c2 = (row.get('MA150_OVER_200', np.nan) > 0)
L647             c3 = (row.get('MA200_SLOPE_1M', np.nan) > 0)
L648             c4 = (row.get('MA50_OVER_150', np.nan) > 0) and (row.get('MA50_OVER_200', np.nan) > 0)
L649             c5 = (row.get('TR_str', np.nan) > 0)
L650             c6 = (row.get('P_OVER_LOW52', np.nan) >= 0.30)
L651             c7 = (row.get('NEAR_52W_HIGH', np.nan) >= -0.25)
L652             c8 = (row.get('RS', np.nan) >= 0.10)
L653             return bool(c1 and c2 and c3 and c4 and c5 and c6 and c7 and c8)
L654
L655         if 'trend_template' not in df.columns: df['trend_template'] = df.apply(_trend_template_pass, axis=1).fillna(False)
L656         assert 'trend_template' in df.columns
L657
L658         # === ZåŒ–ã¨åˆæˆ ===
L659         for col in ['ROE','FCF','REV','EPS']: df[f'{col}_W'] = winsorize_s(df[col], 0.02)
L660
L661         df_z = pd.DataFrame(index=df.index)
L662         for col in ['EPS','REV','ROE','FCF','RS','TR_str','BETA','DIV','DIV_STREAK']: df_z[col] = robust_z(df[col])
L663         df_z['REV'], df_z['EPS'], df_z['TR'] = robust_z(df['REV_W']), robust_z(df['EPS_W']), robust_z(df['TR'])
L664         for col in ['P_OVER_150','P_OVER_200','MA50_OVER_200','MA200_SLOPE_5M','LOW52PCT25_EXCESS','NEAR_52W_HIGH','RS_SLOPE_6W','RS_SLOPE_13W','MA200_UP_STREAK_D']: df_z[col] = robust_z(df[col])
L665
L666         # === Growthæ·±æ˜ã‚Šç³»ï¼ˆæ¬ æä¿æŒz + RAWä½µè¼‰ï¼‰ ===
L667         grw_cols = ['REV_Q_YOY','EPS_Q_YOY','REV_YOY','EPS_YOY','REV_YOY_ACC','REV_YOY_VAR','FCF_MGN','RULE40','REV_ANN_STREAK']
L668         for col in grw_cols:
L669             if col in df.columns:
L670                 raw = pd.to_numeric(df[col], errors="coerce")
L671                 df_z[col] = robust_z_keepnan(raw)
L672                 df_z[f'{col}_RAW'] = raw
L673         for k in ("TREND_SLOPE_EPS", "TREND_SLOPE_REV"):
L674             if k in df.columns and k not in df_z.columns:
L675                 raw = pd.to_numeric(df[k], errors="coerce")
L676                 df_z[k] = robust_z_keepnan(raw)
L677                 df_z[f'{k}_RAW'] = raw
L678         for col in ['DOWNSIDE_DEV','MDD_1Y','RESID_VOL','DOWN_OUTPERF','EXT_200','DIV_TTM_PS','DIV_VAR5','DIV_YOY','DIV_FCF_COVER','DEBT2EQ','CURR_RATIO','EPS_VAR_8Q','MARKET_CAP','ADV60_USD']: df_z[col] = robust_z(df[col])
L679
L680         df_z['SIZE'], df_z['LIQ'] = robust_z(np.log1p(df['MARKET_CAP'])), robust_z(np.log1p(df['ADV60_USD']))
L681         df_z['QUALITY_F'] = robust_z(0.6*df['FCF_W'] + 0.4*df['ROE_W']).clip(-3.0,3.0)
L682         df_z['YIELD_F']   = 0.3*df_z['DIV'] + 0.7*df_z['DIV_STREAK']
L683
L684         # EPSãŒèµ¤å­—ã§ã‚‚FCFãŒé»’å­—ãªã‚‰å®Ÿè³ªé»’å­—ã¨ã¿ãªã™
L685         eps_pos_mask = (df['EPS'] > 0) | (df['FCF_MGN'] > 0)
L686         df_z['EPS_POS'] = df_z['EPS'].where(eps_pos_mask, 0.0)
L687
L688         # ===== ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ãƒ­ãƒ¼ãƒ—ç®—å‡º =====
L689         def zpos(x):
L690             arr = robust_z(x)
L691             idx = getattr(x, 'index', df_z.index)
L692             return pd.Series(arr, index=idx).fillna(0.0)
L693
L694         def relu(x):
L695             ser = x if isinstance(x, pd.Series) else pd.Series(x, index=df_z.index)
L696             return ser.clip(lower=0).fillna(0.0)
L697
L698         # å£²ä¸Šãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ãƒ­ãƒ¼ãƒ—ï¼ˆå››åŠæœŸï¼‰
L699         slope_rev = 0.70*zpos(df_z['REV_Q_YOY']) + 0.30*zpos(df_z['REV_YOY_ACC'])
L700         noise_rev = relu(robust_z(df_z['REV_YOY_VAR']) - 0.8)
L701         slope_rev_combo = slope_rev - 0.25*noise_rev
L702         df_z['TREND_SLOPE_REV_RAW'] = slope_rev_combo
L703         df_z['TREND_SLOPE_REV'] = slope_rev_combo.clip(-3.0, 3.0)
L704
L705         # EPSãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ãƒ­ãƒ¼ãƒ—ï¼ˆå››åŠæœŸï¼‰
L706         slope_eps = 0.60*zpos(df_z['EPS_Q_YOY']) + 0.40*zpos(df_z['EPS_POS'])
L707         df_z['TREND_SLOPE_EPS_RAW'] = slope_eps
L708         df_z['TREND_SLOPE_EPS'] = slope_eps.clip(-3.0, 3.0)
L709
L710         # å¹´æ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆã‚µãƒ–ï¼‰
L711         slope_rev_yr = zpos(df_z['REV_YOY'])
L712         slope_eps_yr = zpos(df_z.get('EPS_YOY', pd.Series(0.0, index=df.index)))
L713         streak_base = df['REV_ANN_STREAK'].clip(lower=0).fillna(0)
L714         streak_yr = streak_base / (streak_base.abs() + 1.0)
L715         slope_rev_yr_combo = 0.7*slope_rev_yr + 0.3*streak_yr
L716         df_z['TREND_SLOPE_REV_YR_RAW'] = slope_rev_yr_combo
L717         df_z['TREND_SLOPE_REV_YR'] = slope_rev_yr_combo.clip(-3.0, 3.0)
L718         df_z['TREND_SLOPE_EPS_YR_RAW'] = slope_eps_yr
L719         df_z['TREND_SLOPE_EPS_YR'] = slope_eps_yr.clip(-3.0, 3.0)
L720
L721         # ===== æ–°GRWåˆæˆå¼ï¼ˆSEPAå¯„ã‚Šã‚·ãƒ•ãƒˆï¼‰ =====
L722         _nz = lambda name: df_z.get(name, pd.Series(0.0, index=df_z.index)).fillna(0.0)
L723         grw_combo = (
L724               0.20*_nz('REV_Q_YOY')
L725             + 0.10*_nz('REV_YOY_ACC')
L726             + 0.10*_nz('REV_ANN_STREAK')
L727             - 0.05*_nz('REV_YOY_VAR')
L728             + 0.10*_nz('TREND_SLOPE_REV')
L729             + 0.15*_nz('EPS_Q_YOY')
L730             + 0.05*_nz('EPS_POS')
L731             + 0.20*_nz('TREND_SLOPE_EPS')
L732             + 0.05*_nz('TREND_SLOPE_REV_YR')
L733             + 0.03*_nz('TREND_SLOPE_EPS_YR')
L734             + 0.10*_nz('FCF_MGN')
L735             + 0.05*_nz('RULE40')
L736         )
L737         df_z['GROWTH_F_RAW'] = grw_combo
L738         df_z['GROWTH_F'] = robust_z(grw_combo).clip(-3.0, 3.0)
L739
L740         # Debug dump for GRW composition (console OFF by default; enable only with env)
L741         if bool(os.getenv("GRW_CONSOLE_DEBUG")):
L742             try:
L743                 i = df_z[['GROWTH_F', 'GROWTH_F_RAW']].copy()
L744                 i.sort_values('GROWTH_F', ascending=False, inplace=True)
L745                 limit = max(0, min(40, len(i)))
L746                 print("[DEBUG: GRW]")
L747                 for t in i.index[:limit]:
L748                     row = i.loc[t]
L749                     parts = [f"GROWTH_F={row['GROWTH_F']:.3f}"]
L750                     if pd.notna(row.get('GROWTH_F_RAW')):
L751                         parts.append(f"GROWTH_F_RAW={row['GROWTH_F_RAW']:.3f}")
L752                     print(f"Ticker: {t} | " + " ".join(parts))
L753                 print()
L754             except Exception as exc:
L755                 print(f"[ERR] GRW debug dump failed: {exc}")
L756
L757         df_z['MOM_F'] = robust_z(0.40*df_z['RS']
L758             + 0.15*df_z['TR_str']
L759             + 0.15*df_z['RS_SLOPE_6W']
L760             + 0.15*df_z['RS_SLOPE_13W']
L761             + 0.10*df_z['MA200_SLOPE_5M']
L762             + 0.10*df_z['MA200_UP_STREAK_D']).clip(-3.0,3.0)
L763         df_z['VOL'] = robust_z(df['BETA'])
L764         df_z['QAL'], df_z['YLD'], df_z['MOM'] = df_z['QUALITY_F'], df_z['YIELD_F'], df_z['MOM_F']
L765         df_z.drop(columns=['QUALITY_F','YIELD_F','MOM_F'], inplace=True, errors='ignore')
L766
L767         _dump_dfz(df_z=df_z, debug_mode=getattr(cfg, "debug_mode", False))
L768
L769         # === begin: BIO LOSS PENALTY =====================================
L770         try:
L771             penalty_z = float(os.getenv("BIO_LOSS_PENALTY_Z", "0.8"))
L772         except Exception:
L773             penalty_z = 0.8
L774
L775         def _is_bio_like(t: str) -> bool:
L776             inf = info.get(t, {}) if isinstance(info, dict) else {}
L777             sec = str(inf.get("sector", "")).lower()
L778             ind = str(inf.get("industry", "")).lower()
L779             if "health" not in sec:
L780                 return False
L781             keys = ("biotech", "biopharma", "pharma")
L782             return any(k in ind for k in keys)
L783
L784         tickers_s = pd.Index(df_z.index)
L785         is_bio = pd.Series({t: _is_bio_like(t) for t in tickers_s})
L786         is_loss = pd.Series({t: (pd.notna(df.loc[t,"EPS"]) and df.loc[t,"EPS"] <= 0) for t in tickers_s})
L787         mask_bio_loss = (is_bio & is_loss).reindex(df_z.index).fillna(False)
L788
L789         if bool(mask_bio_loss.any()) and penalty_z > 0:
L790             df_z.loc[mask_bio_loss, "GROWTH_F"] = df_z.loc[mask_bio_loss, "GROWTH_F"] - penalty_z
L791             df_z["GROWTH_F"] = df_z["GROWTH_F"].clip(-3.0, 3.0)
L792         # === end: BIO LOSS PENALTY =======================================
L793
L794         df_z['TRD'] = 0.0  # TRDã¯ã‚¹ã‚³ã‚¢å¯„ä¸ã‹ã‚‰å¤–ã—ã€ãƒ†ãƒ³ãƒ—ãƒ¬åˆ¤å®šã¯ãƒ•ã‚£ãƒ«ã‚¿ã§è¡Œã†ï¼ˆåˆ—ã¯è¡¨ç¤ºäº’æ›ã®ãŸã‚æ®‹ã™ï¼‰
L795         if 'BETA' not in df_z.columns: df_z['BETA'] = robust_z(df['BETA'])
L796
L797         df_z['D_VOL_RAW'] = robust_z(0.40*df_z['DOWNSIDE_DEV'] + 0.22*df_z['RESID_VOL'] + 0.18*df_z['MDD_1Y'] - 0.10*df_z['DOWN_OUTPERF'] - 0.05*df_z['EXT_200'] - 0.08*df_z['SIZE'] - 0.10*df_z['LIQ'] + 0.10*df_z['BETA'])
L798         df_z['D_QAL']     = robust_z(0.35*df_z['QAL'] + 0.20*df_z['FCF'] + 0.15*df_z['CURR_RATIO'] - 0.15*df_z['DEBT2EQ'] - 0.15*df_z['EPS_VAR_8Q'])
L799         df_z['D_YLD']     = robust_z(0.45*df_z['DIV'] + 0.25*df_z['DIV_STREAK'] + 0.20*df_z['DIV_FCF_COVER'] - 0.10*df_z['DIV_VAR5'])
L800         df_z['D_TRD']     = robust_z(0.40*df_z.get('MA200_SLOPE_5M',0) - 0.30*df_z.get('EXT_200',0) + 0.15*df_z.get('NEAR_52W_HIGH',0) + 0.15*df_z['TR'])
L801
L802         # --- é‡ã¿ã¯ cfg ã‚’å„ªå…ˆï¼ˆå¤–éƒ¨ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ï¼‰ ---
L803         # â‘  å…¨éŠ˜æŸ„ã§ G/D ã‚¹ã‚³ã‚¢ã‚’ç®—å‡ºï¼ˆunmaskedï¼‰
L804         g_score_all = df_z.mul(pd.Series(cfg.weights.g)).sum(axis=1)
L805
L806         d_comp = pd.concat({
L807             'QAL': df_z['D_QAL'],
L808             'YLD': df_z['D_YLD'],
L809             'VOL': df_z['D_VOL_RAW'],
L810             'TRD': df_z['D_TRD']
L811         }, axis=1)
L812         dw = pd.Series(cfg.weights.d, dtype=float).reindex(['QAL','YLD','VOL','TRD']).fillna(0.0)
L813         globals()['D_WEIGHTS_EFF'] = dw.copy()
L814         d_score_all = d_comp.mul(dw, axis=1).sum(axis=1)
L815
L816         # â‘¡ ãƒ†ãƒ³ãƒ—ãƒ¬åˆ¤å®šï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ãã®ã¾ã¾ï¼‰
L817         mask = df['trend_template']
L818         if not bool(mask.any()):
L819             mask = ((df.get('P_OVER_LOW52', np.nan) >= 0.25) &
L820                 (df.get('NEAR_52W_HIGH', np.nan) >= -0.30) &
L821                 (df.get('RS', np.nan) >= 0.08) &
L822                 (df.get('MA200_SLOPE_1M', np.nan) > 0) &
L823                 (df.get('P_OVER_150', np.nan) > 0) & (df.get('P_OVER_200', np.nan) > 0) &
L824                 (df.get('MA150_OVER_200', np.nan) > 0) &
L825                 (df.get('MA50_OVER_150', np.nan) > 0) & (df.get('MA50_OVER_200', np.nan) > 0) &
L826                 (df.get('TR_str', np.nan) > 0)).fillna(False)
L827             df['trend_template'] = mask
L828
L829         # â‘¢ æ¡ç”¨ç”¨ã¯ maskã€è¡¨ç¤º/åˆ†æç”¨ã¯åˆ—ã§å…¨éŠ˜æŸ„ä¿å­˜
L830         g_score = g_score_all.loc[mask]
L831         Scorer.g_score = g_score
L832         df_z['GSC'] = g_score_all
L833         df_z['DSC'] = d_score_all
L834
L835         try:
L836             current = (pd.read_csv("current_tickers.csv")
L837                   .iloc[:, 0]
L838                   .str.upper()
L839                   .tolist())
L840         except FileNotFoundError:
L841             warnings.warn("current_tickers.csv not found â€” bonus skipped")
L842             current = []
L843
L844         mask_bonus = g_score.index.isin(current)
L845         if mask_bonus.any():
L846             # 1) factor.BONUS_COEFF ã‹ã‚‰ k ã‚’æ±ºã‚ã€ç„¡ã‘ã‚Œã° 0.4
L847             k = float(getattr(sys.modules.get("factor"), "BONUS_COEFF", 0.4))
L848             # 2) g å´ã® Ïƒ ã‚’å–ã‚Šã€NaN ãªã‚‰ 0 ã«ä¸¸ã‚ã‚‹
L849             sigma_g = g_score.std()
L850             if pd.isna(sigma_g):
L851                 sigma_g = 0.0
L852             bonus_g = round(k * sigma_g, 3)
L853             g_score.loc[mask_bonus] += bonus_g
L854             Scorer.g_score = g_score
L855             # 3) D å´ã‚‚åŒæ§˜ã« Ïƒ ã® NaN ã‚’ã‚±ã‚¢
L856             sigma_d = d_score_all.std()
L857             if pd.isna(sigma_d):
L858                 sigma_d = 0.0
L859             bonus_d = round(k * sigma_d, 3)
L860             d_score_all.loc[d_score_all.index.isin(current)] += bonus_d
L861
L862         try:
L863             df = _apply_growth_entry_flags(df, ib, self, win_breakout=5, win_pullback=5)
L864         except Exception:
L865             pass
L866
L867         df_full = df.copy()
L868         df_full_z = df_z.copy()
L869
L870         from factor import FeatureBundle  # type: ignore  # å®Ÿè¡Œæ™‚importãªã—ï¼ˆå¾ªç’°å›é¿ï¼‰
L871         return FeatureBundle(df=df,
L872             df_z=df_z,
L873             g_score=g_score,
L874             d_score_all=d_score_all,
L875             missing_logs=pd.DataFrame(missing_logs),
L876             df_full=df_full,
L877             df_full_z=df_full_z,
L878             scaler=None)
L879
L880 def _apply_growth_entry_flags(feature_df, bundle, self_obj, win_breakout=5, win_pullback=5):
L881     """
L882     Gæ ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã«å¯¾ã—ã€ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®š/æŠ¼ã—ç›®åç™ºã®ã€Œç›´è¿‘Nå–¶æ¥­æ—¥å†…ã®ç™ºç«ã€ã‚’åˆ¤å®šã—ã€
L883     æ¬¡ã®åˆ—ã‚’ feature_df ã«è¿½åŠ ã™ã‚‹ï¼ˆindex=tickerï¼‰ã€‚
L884       - G_BREAKOUT_recent_5d : bool
L885       - G_BREAKOUT_last_date : str "YYYY-MM-DD"
L886       - G_PULLBACK_recent_5d : bool
L887       - G_PULLBACK_last_date : str "YYYY-MM-DD"
L888       - G_PIVOT_price        : float
L889     å¤±æ•—ã—ã¦ã‚‚ä¾‹å¤–ã¯æ¡ã‚Šæ½°ã—ã€æ—¢å­˜å‡¦ç†ã‚’é˜»å®³ã—ãªã„ã€‚
L890     """
L891     try:
L892         px   = bundle.px                      # çµ‚å€¤ DataFrame
L893         hi   = bundle.data['High']
L894         lo   = bundle.data['Low']
L895         vol  = bundle.data['Volume']
L896         bench= bundle.spx                     # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ Series
L897
L898         # Gãƒ¦ãƒ‹ãƒãƒ¼ã‚¹æ¨å®šï¼šself.g_universe å„ªå…ˆ â†’ feature_df['group']=='G' â†’ å…¨éŠ˜æŸ„
L899         g_universe = getattr(self_obj, "g_universe", None)
L900         if g_universe is None:
L901             try:
L902                 g_universe = feature_df.index[feature_df['group'].astype(str).str.upper().eq('G')].tolist()
L903             except Exception:
L904                 g_universe = list(feature_df.index)
L905         if not g_universe:
L906             return feature_df
L907
L908         # æŒ‡æ¨™
L909         px = px.ffill(limit=2)
L910         ema21 = px[g_universe].ewm(span=21, adjust=False).mean()
L911         ma50  = px[g_universe].rolling(50).mean()
L912         ma150 = px[g_universe].rolling(150).mean()
L913         ma200 = px[g_universe].rolling(200).mean()
L914         atr20 = (hi[g_universe] - lo[g_universe]).rolling(20).mean()
L915         vol20 = vol[g_universe].rolling(20).mean()
L916         vol50 = vol[g_universe].rolling(50).mean()
L917
L918         # ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆåˆæ ¼
L919         trend_template_ok = (px[g_universe] > ma50) & (px[g_universe] > ma150) & (px[g_universe] > ma200) \
L920                             & (ma150 > ma200) & (ma200.diff() > 0)
L921
L922         # æ±ç”¨ãƒ”ãƒœãƒƒãƒˆï¼šç›´è¿‘65å–¶æ¥­æ—¥ã®é«˜å€¤ï¼ˆå½“æ—¥é™¤å¤–ï¼‰
L923         pivot_price = hi[g_universe].rolling(65).max().shift(1)
L924
L925         # ç›¸å¯¾åŠ›ï¼šå¹´å†…é«˜å€¤æ›´æ–°
L926         bench_aligned = bench.reindex(px.index).ffill()
L927         rs = px[g_universe].div(bench_aligned, axis=0)
L928         rs_high = rs.rolling(252).max().shift(1)
L929
L930         # ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆã€Œç™ºç”Ÿæ—¥ã€ï¼šæ¡ä»¶ç«‹ã¡ä¸ŠãŒã‚Š
L931         breakout_today = trend_template_ok & (px[g_universe] > pivot_price) \
L932                          & (vol[g_universe] >= 1.5 * vol50) & (rs > rs_high)
L933         breakout_event = breakout_today & ~breakout_today.shift(1).fillna(False)
L934
L935         # æŠ¼ã—ç›®åç™ºã€Œç™ºç”Ÿæ—¥ã€ï¼šEMA21å¸¯Ã—å‡ºæ¥é«˜ãƒ‰ãƒ©ã‚¤ã‚¢ãƒƒãƒ—Ã—å‰æ—¥é«˜å€¤è¶ŠãˆÃ—çµ‚å€¤EMA21ä¸Š
L936         near_ema21_band = px[g_universe].between(ema21 - atr20, ema21 + atr20)
L937         volume_dryup = (vol20 / vol50) <= 1.0
L938         pullback_bounce_confirmed = (px[g_universe] > hi[g_universe].shift(1)) & (px[g_universe] > ema21)
L939         pullback_today = trend_template_ok & near_ema21_band & volume_dryup & pullback_bounce_confirmed
L940         pullback_event = pullback_today & ~pullback_today.shift(1).fillna(False)
L941
L942         # ç›´è¿‘Nå–¶æ¥­æ—¥å†…ã®ç™ºç« / æœ€çµ‚ç™ºç”Ÿæ—¥
L943         rows = []
L944         for t in g_universe:
L945             def _recent_and_date(s, win):
L946                 sw = s[t].iloc[-win:]
L947                 if sw.any():
L948                     d = sw[sw].index[-1]
L949                     return True, d.strftime("%Y-%m-%d")
L950                 return False, ""
L951             br_recent, br_date = _recent_and_date(breakout_event, win_breakout)
L952             pb_recent, pb_date = _recent_and_date(pullback_event, win_pullback)
L953             rows.append((t, {
L954                 "G_BREAKOUT_recent_5d": br_recent,
L955                 "G_BREAKOUT_last_date": br_date,
L956                 "G_PULLBACK_recent_5d": pb_recent,
L957                 "G_PULLBACK_last_date": pb_date,
L958                 "G_PIVOT_price": float(pivot_price[t].iloc[-1]) if t in pivot_price.columns else float('nan'),
L959             }))
L960         flags = pd.DataFrame({k: v for k, v in rows}).T
L961
L962         # åˆ—ã‚’ä½œæˆãƒ»ä¸Šæ›¸ã
L963         cols = ["G_BREAKOUT_recent_5d","G_BREAKOUT_last_date","G_PULLBACK_recent_5d","G_PULLBACK_last_date","G_PIVOT_price"]
L964         for c in cols:
L965             if c not in feature_df.columns:
L966                 feature_df[c] = np.nan
L967         feature_df.loc[flags.index, flags.columns] = flags
L968
L969     except Exception:
L970         pass
L971     return feature_df
L972
```

## <.github/workflows/weekly-report.yml>
```text
L1 name: Weekly Stock Report
L2
L3 on:
L4   push:
L5     branches: [ main ]
L6     paths-ignore:
L7       - 'CodeForChat/**'
L8   schedule:
L9     - cron: '0 0 * * 6'  # UTC 00:00 â†’ JST 09:00ï¼ˆåœŸï¼‰
L10   workflow_dispatch:
L11
L12 jobs:
L13   build-and-report:
L14     runs-on: ubuntu-latest
L15     permissions:
L16       contents: write
L17
L18     steps:
L19       - name: Debug start
L20         run: echo 'ğŸš€ DEBUGstarted'
L21               
L22       - name: Checkout repository
L23         uses: actions/checkout@v3
L24
L25       - name: Setup Python
L26         uses: actions/setup-python@v5
L27         with:
L28           python-version: '3.x'
L29           cache: 'pip'
L30           cache-dependency-path: requirements.txt
L31
L32       - name: Install dependencies
L33         run: pip install -r requirements.txt
L34
L35       - name: Prepare results directory
L36         run: mkdir -p results
L37
L38       - name: Run factor & scoring
L39         env:
L40           SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
L41           FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
L42           FIN_THREADS: "8"
L43         run: python factor.py
```

## <documents/README.md>
```text
L1 # é‹ç”¨ãƒ«ãƒ¼ãƒ«
L2
L3 ## åŸºæœ¬æ§‹æˆ
L4 - 20éŠ˜æŸ„ã‚’å‡ç­‰é…åˆ†ï¼ˆç¾é‡‘ã‚’é™¤ã1éŠ˜æŸ„ã‚ãŸã‚Š5%ï¼‰
L5 - moomooè¨¼åˆ¸ã§é‹ç”¨
L6 - **Growthæ  12éŠ˜æŸ„ / Defenseæ  8éŠ˜æŸ„**ï¼ˆNORMAL åŸºæº–ï¼‰
L7
L8 ## Barbell Growth-Defenseæ–¹é‡
L9 - Growthæ  **12éŠ˜æŸ„**ï¼šé«˜æˆé•·ã§ä¹–é›¢æºã¨ãªã‚‹æ”»ã‚ã®éŠ˜æŸ„
L10 - Defenseæ  **8éŠ˜æŸ„**ï¼šä½ãƒœãƒ©ã§å®‰å®šæˆé•·ã—é…å½“ã‚’å¢—ã‚„ã™å®ˆã‚Šã®éŠ˜æŸ„
L11 - ã€ŒçŒ›çƒˆã«ä¼¸ã³ã‚‹æ”»ã‚ Ã— ç€å®Ÿã«ç¨¼ãç›¾ã€ã®çµ„åˆã›ã§ä¹–é›¢â†’åŠæˆ»ã—ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ã‚’ç‹™ã†
L12
L13 ## ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¤å®šï¼ˆtrend_template åˆæ ¼â€œæœ¬æ•°â€ã§åˆ¤å®šï¼‰
L14 - åˆæ ¼æœ¬æ•° = current+candidate å…¨ä½“ã®ã†ã¡ã€trend_template æ¡ä»¶ã‚’æº€ãŸã—ãŸéŠ˜æŸ„ã®**æœ¬æ•°(C)**ï¼ˆåŸºæº– N_G=12ï¼‰
L15 - ã—ãã„å€¤ã¯éå»~600å–¶æ¥­æ—¥ã®åˆ†å¸ƒã‹ã‚‰**æ¯å›è‡ªå‹•æ¡ç”¨**ï¼ˆåˆ†ä½ç‚¹ã¨é‹ç”¨â€œåºŠâ€ã®maxï¼‰
L16   - ç·Šæ€¥å…¥ã‚Š: `max(q05, 12æœ¬)`ï¼ˆ= N_Gï¼‰
L17   - ç·Šæ€¥è§£é™¤: `max(q20, 18æœ¬)`ï¼ˆ= ceil(1.5Ã—12)ï¼‰
L18   - é€šå¸¸å¾©å¸°: `max(q60, 36æœ¬)`ï¼ˆ= 3Ã—N_Gï¼‰
L19 - ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹: å‰å›ãƒ¢ãƒ¼ãƒ‰ã«ä¾å­˜ï¼ˆEMERGâ†’è§£é™¤ã¯23æœ¬ä»¥ä¸Šã€CAUTIONâ†’é€šå¸¸ã¯45æœ¬ä»¥ä¸Šï¼‰
L20
L21 ## ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¥ã®ç¾é‡‘ãƒ»ãƒ‰ãƒªãƒ•ãƒˆ
L22  - **é€šå¸¸(NORMAL)** : ç¾é‡‘ **10%** / ãƒ‰ãƒªãƒ•ãƒˆé–¾å€¤ **12%**
L23  - **è­¦æˆ’(CAUTION)** : ç¾é‡‘ **12.5%** / ãƒ‰ãƒªãƒ•ãƒˆé–¾å€¤ **14%**
L24  - **ç·Šæ€¥(EMERG)** : ç¾é‡‘ **20%** / **ãƒ‰ãƒªãƒ•ãƒˆå£²è²·åœæ­¢**ï¼ˆ20Ã—5%ã«å…¨æˆ»ã—ã®ã¿ï¼‰
L25
L26 ## ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®æ¨å¥¨â€œä¿æœ‰éŠ˜æŸ„æ•°â€ï¼ˆMMFâ‰’ç¾é‡‘ï¼‰
L27 *å„æ =5%ï¼ˆ20éŠ˜æŸ„å‡ç­‰ï¼‰ã€‚ãƒ¢ãƒ¼ãƒ‰ç§»è¡Œæ™‚ã¯**Gã®æ æ•°ã®ã¿**èª¿æ•´ã—ã€å¤–ã—ãŸæ ã¯ç¾é‡‘ã¨ã—ã¦ä¿æŒã€‚*
L28
L29 - **NORMAL:** G **12** / D **8** / ç¾é‡‘åŒ–æ  **0**  
L30 - **CAUTION:** G **10** / D **8** / ç¾é‡‘åŒ–æ  **2**ï¼ˆ= 10%ï¼‰  
L31 - **EMERG:** G **8**  / D **8** / ç¾é‡‘åŒ–æ  **4**ï¼ˆ= 20%ï¼‰  
L32
L33 > å®Ÿé‹ç”¨ï¼šâ­ï¸ä½ã‚¹ã‚³ã‚¢ã®Gã‹ã‚‰é †ã«å¤–ã™ã€‚è§£é™¤æ™‚ã¯factorä¸Šä½ã‹ã‚‰è£œå……ã€‚
L34
L35 ## ãƒˆãƒ¬ãƒ¼ãƒªãƒ³ã‚°ã‚¹ãƒˆãƒƒãƒ—
L36 - **åŸºæœ¬TS (ãƒ¢ãƒ¼ãƒ‰åˆ¥):** NORMAL **15%** / CAUTION **13%** / EMERG **10%**
L37 - å«ã¿ç›ŠãŒ **+30% / +60% / +100%** åˆ°é”ã§ã€åŸºæœ¬ã‹ã‚‰ **-3pt / -6pt / -8pt** å¼•ãä¸Šã’
L38 - TSç™ºå‹•ã§æ¸›å°‘ã—ãŸéŠ˜æŸ„ã¯ç¿Œæ—¥ä»¥é™ã«è£œå……ï¼ˆâ€»ç·Šæ€¥ãƒ¢ãƒ¼ãƒ‰ä¸­ã¯è£œå……ã—ãªã„ï¼‰
L39
L40 ## åŠæˆ»ã—ï¼ˆãƒªãƒãƒ©ãƒ³ã‚¹ï¼‰æ‰‹é †
L41 ãƒ‰ãƒªãƒ•ãƒˆãƒã‚§ãƒƒã‚¯ã§**ã‚¢ãƒ©ãƒ¼ãƒˆ**ãŒå‡ºãŸå ´åˆï¼ˆåˆè¨ˆ|drift| ãŒãƒ¢ãƒ¼ãƒ‰é–¾å€¤ã‚’è¶…éã€EMERGé™¤ãï¼‰ã€ç¿Œå–¶æ¥­æ—¥ã®ç±³å›½å¯„ä»˜ãã§ä¸‹è¨˜ã‚’å®Ÿæ–½ã™ã‚‹ã€‚
L42
L43 1. **å£²å´ï¼ˆå¿…é ˆï¼‰**  
L44    Slackãƒ†ãƒ¼ãƒ–ãƒ«ã® **Î”qty ãŒãƒã‚¤ãƒŠã‚¹ã®éŠ˜æŸ„ã‚’å£²å´** ã™ã‚‹ï¼ˆå¯„ä»˜ãæˆè¡Œæ¨å¥¨ï¼‰ã€‚  
L45    ã“ã‚Œã¯ã€ŒåŠæˆ»ã—ã€è¨ˆç®—ã«åŸºã¥ãéé‡é‡ã®å‰Šæ¸›ã‚’æ„å‘³ã™ã‚‹ã€‚
L46
L47 2. **è³¼å…¥ï¼ˆä»»æ„ãƒ»åŠæˆ»ã—ç›®å®‰ï¼‰**  
L48    åŠæˆ»ã—å¾Œã®åˆè¨ˆ|drift|ã‚’**ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤ï¼ˆSlackãƒ˜ãƒƒãƒ€ã«è¡¨ç¤ºï¼‰**ã«è¿‘ã¥ã‘ã‚‹ã“ã¨ã‚’ç›®å®‰ã«ã€  
L49    **ä»»æ„ã®éŠ˜æŸ„ã‚’è²·ã„å¢—ã—**ã—ã¦ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹ï¼ˆÎ”qtyãŒãƒ—ãƒ©ã‚¹ã®éŠ˜æŸ„ã‚’å„ªå…ˆã—ã¦ã‚‚ã‚ˆã„ï¼‰ã€‚
L50
L51 3. **ãƒˆãƒ¬ãƒ¼ãƒªãƒ³ã‚°ã‚¹ãƒˆãƒƒãƒ—ã®å†è¨­å®šï¼ˆå¿…é ˆï¼‰**  
L52    ã™ã¹ã¦ã®ä¿æœ‰éŠ˜æŸ„ã«ã¤ã„ã¦ã€æœ€æ–°ã®è©•ä¾¡é¡ã«åˆã‚ã›ã¦TSã‚’**å†ç™ºæ³¨ï¼æ›´æ–°**ã™ã‚‹ã€‚  
L53    ãƒ«ãƒ¼ãƒ«ã¯ä¸‹è¨˜ï¼ˆåˆ©ç›Šåˆ°é”ã§æ®µéšçš„ã«ã‚¿ã‚¤ãƒˆåŒ–ï¼‰ï¼š  
L54    - **åŸºæœ¬TS:** -15%  
L55    - **+30% åˆ°é” â†’ TS -12%**  
L56    - **+60% åˆ°é” â†’ TS -9%**  
L57    - **+100% åˆ°é” â†’ TS -7%**  
L58    â€»ã‚¹ãƒˆãƒƒãƒ—ä¾¡æ ¼ã®å¼•ãä¸Šã’ã¯è¨±å¯ã€**å¼•ãä¸‹ã’ã¯ä¸å¯**ï¼ˆåˆ©ç›Šä¿å…¨ã®åŸå‰‡ï¼‰ã€‚
L59
L60 4. **ä¾‹å¤–ï¼ˆEMERGãƒ¢ãƒ¼ãƒ‰ï¼‰**  
L61    ç·Šæ€¥(EMERG)ã§ã¯**ãƒ‰ãƒªãƒ•ãƒˆç”±æ¥ã®å£²è²·ã¯åœæ­¢ï¼ˆâˆï¼‰**ã€‚20éŠ˜æŸ„Ã—å„5%ã¸ã®**å…¨æˆ»ã—**ã®ã¿è¨±å®¹ã€‚
L62
L63 5. **å®Ÿè¡Œã‚¿ã‚¤ãƒŸãƒ³ã‚°**
L64    - åˆ¤å®šï¼šç±³å›½å¸‚å ´çµ‚å€¤ç›´å¾Œ
L65    - åŸ·è¡Œï¼šç¿Œå–¶æ¥­æ—¥ã®ç±³å›½å¯„ä»˜ãæˆè¡Œ
L66
L67 ## ãƒ¢ãƒ¼ãƒ‰ç§»è¡Œã®å®Ÿå‹™æ‰‹é †ï¼ˆè¶…ã‚·ãƒ³ãƒ—ãƒ«ï¼‰
L68 ãƒ¢ãƒ¼ãƒ‰ãŒå¤‰ã‚ã£ãŸã‚‰ã€**MMFâ‰’ç¾é‡‘**ã¨ã—ã¦æ‰±ã„ã€**Gã®æ æ•°ã ã‘**ã‚’èª¿æ•´ã™ã‚‹ï¼š
L69 1. **Gã‚’å‰Šã‚‹**ï¼ˆCAUTION/EMERGï¼‰  
L70    - â­ï¸ä½ã‚¹ã‚³ã‚¢ã®Gã‹ã‚‰é †ã«å¤–ã™ã€‚  
L71    - **`current_tickers.csv` ã‹ã‚‰å¤–ã™GéŠ˜æŸ„ã®è¡Œã‚’å‰Šé™¤**ï¼ˆï¼ãã®æ ã¯ç¾é‡‘åŒ–ï¼‰ã€‚
L72 2. **ç¾é‡‘ã¨ã—ã¦ä¿æŒ**  
L73    - å¤–ã—ãŸæ ã¯ç¾é‡‘ï¼ˆã¾ãŸã¯MMFç›¸å½“ï¼‰ã§ãƒ—ãƒ¼ãƒ«ã€‚  
L74 3. **å¾©å¸°æ™‚ã®è£œå……**ï¼ˆNORMALã¸ï¼‰  
L75    - **`current_tickers.csv` ã«éŠ˜æŸ„ã‚’è¿½åŠ **ï¼ˆfactorä¸Šä½ã‹ã‚‰ï¼‰ã€‚  
L76    - ä»¥é™ã¯æ—¥æ¬¡ãƒ‰ãƒªãƒ•ãƒˆ/TSãƒ«ãƒ¼ãƒ«ã«å¾“ã†ã€‚
L77
L78 > driftã¯ `target_ratio = 1/éŠ˜æŸ„æ•°` ã‚’è‡ªå‹•é©ç”¨ã€‚è¡Œæ•°ã«å¿œã˜ã¦è‡ªå‹•ã§å‡ç­‰æ¯”ç‡ãŒå†è¨ˆç®—ã•ã‚Œã‚‹ã€‚
L79
L80 ## å…¥æ›¿éŠ˜æŸ„é¸å®š
L81 - Oxfordã‚­ãƒ£ãƒ”ã‚¿ãƒ«ï¼ã‚¤ãƒ³ã‚«ãƒ ã€Alpha Investorã€Motley Fool Stock Advisorã€moomooã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ç­‰ã‚’å‚è€ƒã«chatGPTã§æ¤œè¨
L82 - å¹´é–“NISAæ ã¯Growthç¾¤ã®ä¸­ã‹ã‚‰ä½ãƒœãƒ©éŠ˜æŸ„ã‚’é¸å®šã—åˆ©ç”¨ã€‚é•·æœŸä¿æŒã«ã¯ã“ã ã‚ã‚‰ãªã„ã€‚
L83
L84 ## å†ã‚¨ãƒ³ãƒˆãƒªãƒ¼ï¼ˆã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ï¼‰
L85 - TSãƒ’ãƒƒãƒˆå¾Œã®åŒéŠ˜æŸ„å†INã¯ **8å–¶æ¥­æ—¥** ã®ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ã‚’è¨­ã‘ã‚‹ï¼ˆæœŸé–“ä¸­ã¯å†INç¦æ­¢ï¼‰
L86
L87 ## å®Ÿè¡Œã‚¿ã‚¤ãƒŸãƒ³ã‚°
L88 - åˆ¤å®šï¼šç±³å›½å¸‚å ´çµ‚å€¤ç›´å¾Œ
L89 - åŸ·è¡Œï¼šç¿Œå–¶æ¥­æ—¥ã®ç±³å›½å¯„ä»˜ãæˆè¡Œ
```

## <documents/factor_design.md>
```text
L1 # factor.py è©³ç´°è¨­è¨ˆæ›¸
L2
L3 ## æ¦‚è¦
L4 - æ—¢å­˜ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®éŠ˜æŸ„ã¨æ¤œè¨ä¸­ã®éŠ˜æŸ„ç¾¤ã‚’åŒæ™‚ã«æ‰±ã†éŠ˜æŸ„é¸å®šãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€‚
L5 - ä¾¡æ ¼ãƒ»è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šè¾¼ã¿ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã¨DRRSé¸å®šã‚’è¡Œã†ã“ã¨ã§ã€ä»¥ä¸‹ã®ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆã‚’å¾—ã‚‹ã€‚
L6   - æ¡ç”¨éŠ˜æŸ„ã¨æƒœã—ãã‚‚æ¼ã‚ŒãŸéŠ˜æŸ„ã®ã‚¹ã‚³ã‚¢ä¸€è¦§
L7   - IN/OUTã®ãƒ†ã‚£ãƒƒã‚«ãƒ¼ãƒªã‚¹ãƒˆã¨OUTå´ã®ä½ã‚¹ã‚³ã‚¢éŠ˜æŸ„
L8   - æ–°æ—§ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®æ¯”è¼ƒè¡¨
L9   - æ¤œè¨ä¸­éŠ˜æŸ„ã®ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆæ•´ç†ç”¨ï¼‰
L10
L11 ## å…¨ä½“ãƒ•ãƒ­ãƒ¼
L12 1. **Input** â€“ `current_tickers.csv`ã¨`candidate_tickers.csv`ã‚’èª­ã¿è¾¼ã¿ã€yfinanceã‚„Finnhubã®APIã‹ã‚‰ä¾¡æ ¼ãƒ»è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã¦`InputBundle`ã‚’æ•´å‚™ã€‚
L13 2. **Score Calculation** â€“ ScorerãŒç‰¹å¾´é‡ã‚’è¨ˆç®—ã—å› å­ã‚¹ã‚³ã‚¢ã‚’åˆæˆã—ã¦`FeatureBundle`ã‚’ç”Ÿæˆã€‚
L14 3. **Correlation Reduction & Selection** â€“ SelectorãŒDRRSãƒ­ã‚¸ãƒƒã‚¯ã§ç›¸é–¢ã‚’æŠ‘ãˆã¤ã¤G/DéŠ˜æŸ„ã‚’é¸å®šã—`SelectionBundle`ã‚’å¾—ã‚‹ã€‚
L15 4. **Output** â€“ æ¡ç”¨çµæœã¨å‘¨è¾ºæƒ…å ±ã‚’è¡¨ãƒ»Slacké€šçŸ¥ã¨ã—ã¦å‡ºåŠ›ã€‚
L16
L17 ```mermaid
L18 flowchart LR
L19   A[Input\nAPI & å‰å‡¦ç†] --> B[Score Calculation\nç‰¹å¾´é‡ãƒ»å› å­åˆæˆ]
L20   B --> C[Correlation Reduction\nDRRSé¸å®š]
L21   C --> D[Output\nSlacké€šçŸ¥]
L22 ```
L23
L24 ## å®šæ•°ãƒ»è¨­å®š
L25 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L26 | --- | --- | --- |
L27 | `exist` / `cand` | ç¾è¡Œãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã¨æ¤œè¨ä¸­éŠ˜æŸ„ã®ãƒ†ã‚£ãƒƒã‚«ãƒ¼ãƒªã‚¹ãƒˆ | ã‚¹ã‚³ã‚¢å¯¾è±¡ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã®æ§‹æˆã€å€™è£œæ•´ç† |
L28 | `bench` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼ | ç›¸å¯¾å¼·ã•ãƒ»Î²ç®—å‡ºã€ãƒãƒ¼ãƒˆæ¯”è¼ƒ |
L29 | `CAND_PRICE_MAX` | å€™è£œéŠ˜æŸ„ã®è¨±å®¹ä¾¡æ ¼ä¸Šé™ | é«˜é¡éŠ˜æŸ„ã®äº‹å‰é™¤å¤– |
L30 | `N_G` / `N_D` | G/Dæ¡ç”¨æ ã®ä»¶æ•°ï¼ˆ**æ—¢å®š: 12 / 8**ï¼‰ | æœ€çµ‚çš„ã«é¸ã¶éŠ˜æŸ„æ•°ã®åˆ¶ç´„ |
L31 | `g_weights` / `D_weights` | å„å› å­ã®é‡ã¿dict | G/Dã‚¹ã‚³ã‚¢åˆæˆ |
L32 | `D_BETA_MAX` | Dãƒã‚±ãƒƒãƒˆã®è¨±å®¹Î²ä¸Šé™ | é«˜Î²éŠ˜æŸ„ã®é™¤å¤–ãƒ•ã‚£ãƒ«ã‚¿ |
L33 | `FILTER_SPEC` | G/Dã”ã¨ã®å‰å‡¦ç†ãƒ•ã‚£ãƒ«ã‚¿ | ãƒˆãƒ¬ãƒ³ãƒ‰ãƒã‚¹ã‚¯ã‚„Î²ä¸Šé™è¨­å®š |
L34 | `corrM` | DRRSåˆæœŸãƒ—ãƒ¼ãƒ«ã®æœ€å¤§ä»¶æ•° | ç›¸é–¢è¡Œåˆ—ã‚µã‚¤ã‚ºåˆ¶å¾¡ |
L35 | `DRRS_G` / `DRRS_D` | DRRSãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿dict | ãƒã‚±ãƒƒãƒˆåˆ¥ã®ç›¸é–¢ä½æ¸›è¨­å®š |
L36 | `DRRS_SHRINK` | æ®‹å·®ç›¸é–¢ã®å¯¾è§’ã‚·ãƒ¥ãƒªãƒ³ã‚¯ç‡ | `residual_corr`ã®å®‰å®šåŒ– |
L37 | `CROSS_MU_GD` | G-Dé–“ã‚¯ãƒ­ã‚¹ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£Î¼ | 2ãƒã‚±ãƒƒãƒˆåŒæ™‚æœ€é©åŒ–ã§ç›¸é–¢æŠ‘åˆ¶ |
L38 | `RESULTS_DIR` | é¸å®šçµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª | `_save_sel`/`_load_prev`ã®å…¥å‡ºåŠ› |
L39
L40 é¸å®šçµæœã¯`results/`é…ä¸‹ã«JSONã¨ã—ã¦ä¿å­˜ã—ã€æ¬¡å›å®Ÿè¡Œæ™‚ã«`_load_prev`ã§èª­ã¿è¾¼ã‚“ã§é¸å®šæ¡ä»¶ã«åæ˜ ã€‚
L41
L42 ## DTO/Config
L43 å„ã‚¹ãƒ†ãƒƒãƒ—é–“ã§å—ã‘æ¸¡ã™ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¨è¨­å®šå€¤ã€‚å¤‰æ•°ã®æ„å‘³åˆã„ã¨åˆ©ç”¨ç®‡æ‰€ã‚’ä»¥ä¸‹ã«ç¤ºã™ã€‚
L44
L45 ### InputBundleï¼ˆInput â†’ Scorerï¼‰
L46 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L47 | --- | --- | --- |
L48 | `cand` | å€™è£œéŠ˜æŸ„ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã®ãƒªã‚¹ãƒˆ | OUTãƒ†ãƒ¼ãƒ–ãƒ«ã‚„ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°å¯¾è±¡ã®æ¯é›†å›£ |
L49 | `tickers` | ç¾è¡Œ+å€™è£œã‚’åˆã‚ã›ãŸãƒ†ã‚£ãƒƒã‚«ãƒ¼ä¸€è¦§ | ä¾¡æ ¼ãƒ»å‡ºæ¥é«˜ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€ãƒªã‚¿ãƒ¼ãƒ³è¨ˆç®— |
L50 | `bench` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼ | ç›¸å¯¾å¼·ã•ãƒ»Î²ç®—å‡ºã€ãƒãƒ¼ãƒˆæ¯”è¼ƒ |
L51 | `data` | yfinanceã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœï¼ˆéšå±¤åˆ—ï¼‰ | `px`/`spx`/ãƒªã‚¿ãƒ¼ãƒ³ç­‰ã®åŸºç¤ãƒ‡ãƒ¼ã‚¿ |
L52 | `px` | `data['Close']`ã ã‘ã‚’æŠœãå‡ºã—ãŸä¾¡æ ¼ç³»åˆ— | æŒ‡æ¨™è¨ˆç®—ãƒ»ãƒªã‚¿ãƒ¼ãƒ³ç”Ÿæˆ |
L53 | `spx` | `data['Close'][bench]` ã®Series | `rs`ã‚„`calc_beta`ã®åŸºæº–æŒ‡æ•° |
L54 | `tickers_bulk` | `yf.Tickers`ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ | `info`ç­‰ã®ä¸€æ‹¬å–å¾— |
L55 | `info` | ãƒ†ã‚£ãƒƒã‚«ãƒ¼åˆ¥ã®yfinanceæƒ…å ±dict | ã‚»ã‚¯ã‚¿ãƒ¼åˆ¤å®šã‚„EPSè£œå®Œ |
L56 | `eps_df` | EPS TTM/ç›´è¿‘EPSç­‰ã‚’ã¾ã¨ã‚ãŸè¡¨ | æˆé•·æŒ‡æ¨™ã®ç®—å‡º |
L57 | `fcf_df` | CFOãƒ»CapExãƒ»FCF TTMã¨æƒ…å ±æºãƒ•ãƒ©ã‚° | FCF/EVã‚„é…å½“ã‚«ãƒãƒ¬ãƒƒã‚¸ |
L58 | `returns` | `px.pct_change()`ã®ãƒªã‚¿ãƒ¼ãƒ³è¡¨ | ç›¸é–¢è¡Œåˆ—ãƒ»DRRSè¨ˆç®— |
L59
L60 ### FeatureBundleï¼ˆScorer â†’ Selectorï¼‰
L61 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L62 | --- | --- | --- |
L63 | `df` | è¨ˆç®—æ¸ˆã¿æŒ‡æ¨™ã®ç”Ÿå€¤ãƒ†ãƒ¼ãƒ–ãƒ« | ãƒ‡ãƒãƒƒã‚°ãƒ»å‡ºåŠ›è¡¨ç¤º |
L64 | `df_z` | ã‚¦ã‚£ãƒ³ã‚¶ãƒ¼å¾ŒZã‚¹ã‚³ã‚¢åŒ–ã—ãŸæŒ‡æ¨™è¡¨ | å› å­ã‚¹ã‚³ã‚¢åˆæˆã€é¸å®šåŸºæº– |
L65 | `g_score` | Gãƒã‚±ãƒƒãƒˆç·åˆã‚¹ã‚³ã‚¢ | Gé¸å®šã€IN/OUTæ¯”è¼ƒ |
L66 | `d_score_all` | Dãƒã‚±ãƒƒãƒˆç·åˆã‚¹ã‚³ã‚¢ï¼ˆå…¨éŠ˜æŸ„ï¼‰ | Dé¸å®šã€ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚° |
L67 | `missing_logs` | æ¬ ææŒ‡æ¨™ã¨è£œå®ŒçŠ¶æ³ã®ãƒ­ã‚° | ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ |
L68
L69 ### SelectionBundleï¼ˆSelector â†’ Outputï¼‰
L70 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L71 | --- | --- | --- |
L72 | `resG` | Gé¸å®šçµæœã®è©³ç´°dictï¼ˆ`tickers`ã€ç›®çš„å€¤ç­‰ï¼‰ | çµæœä¿å­˜ãƒ»å¹³å‡ç›¸é–¢ãªã©ã®æŒ‡æ¨™è¡¨ç¤º |
L73 | `resD` | Dé¸å®šçµæœã®è©³ç´°dict | åŒä¸Š |
L74 | `top_G` | æœ€çµ‚æ¡ç”¨Gãƒ†ã‚£ãƒƒã‚«ãƒ¼ | æ–°ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæ§‹ç¯‰ |
L75 | `top_D` | æœ€çµ‚æ¡ç”¨Dãƒ†ã‚£ãƒƒã‚«ãƒ¼ | åŒä¸Š |
L76 | `init_G` | DRRSå‰ã®GåˆæœŸå€™è£œ | æƒœã—ãã‚‚å¤–ã‚ŒãŸéŠ˜æŸ„è¡¨ç¤º |
L77 | `init_D` | DRRSå‰ã®DåˆæœŸå€™è£œ | åŒä¸Š |
L78
L79 ### WeightsConfig
L80 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L81 | --- | --- | --- |
L82 | `g` | Gå› å­ï¼ˆGRW/MOM/VOLï¼‰ã®é‡ã¿dict | `g_score`åˆæˆ |
L83 | `d` | Då› å­ï¼ˆD_QAL/D_YLD/D_VOL_RAW/D_TRDï¼‰ã®é‡ã¿dict | `d_score_all`åˆæˆ |
L84
L85 ### DRRSParams
L86 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L87 | --- | --- | --- |
L88 | `corrM` | DRRSåˆæœŸãƒ—ãƒ¼ãƒ«ã®æœ€å¤§ä»¶æ•° | ç›¸é–¢è¡Œåˆ—ã‚µã‚¤ã‚ºåˆ¶å¾¡ |
L89 | `shrink` | æ®‹å·®ç›¸é–¢ã®ã‚·ãƒ¥ãƒªãƒ³ã‚¯ç‡ | `residual_corr`ã®å¯¾è§’å¼·èª¿ |
L90 | `G` | Gãƒã‚±ãƒƒãƒˆç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿dictï¼ˆ`lookback`ç­‰ï¼‰ | `select_bucket_drrs`è¨­å®š |
L91 | `D` | Dãƒã‚±ãƒƒãƒˆç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿dict | åŒä¸Š |
L92 | `cross_mu_gd` | G-Dã‚¯ãƒ­ã‚¹ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£ä¿‚æ•°Î¼ | `select_buckets`ã®ç›®çš„é–¢æ•° |
L93
L94 ### PipelineConfig
L95 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L96 | --- | --- | --- |
L97 | `weights` | `WeightsConfig`ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ | ã‚¹ã‚³ã‚¢åˆæˆã®é‡ã¿å‚ç…§ |
L98 | `drrs` | `DRRSParams`ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ | é¸å®šã‚¹ãƒ†ãƒƒãƒ—ã®è¨­å®šå€¤ |
L99 | `price_max` | å€™è£œéŠ˜æŸ„ã®è¨±å®¹ä¾¡æ ¼ä¸Šé™ | Inputæ®µéšã§ã®ãƒ•ã‚£ãƒ«ã‚¿ |
L100
L101 ## å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
L102 - `winsorize_s` / `robust_z` : å¤–ã‚Œå€¤å‡¦ç†ã¨Zã‚¹ã‚³ã‚¢åŒ–ã€‚
L103 - `_safe_div` / `_safe_last` : ä¾‹å¤–ã‚’æ½°ã—ãŸåˆ†å‰²ãƒ»æœ«å°¾å–å¾—ã€‚
L104 - `_load_prev` / `_save_sel` : é¸å®šçµæœã®èª­ã¿æ›¸ãã€‚
L105
L106 ## ã‚¯ãƒ©ã‚¹è¨­è¨ˆ
L107 ### Step1: Input
L108 `current_tickers.csv`ã®ç¾è¡ŒéŠ˜æŸ„ã¨`candidate_tickers.csv`ã®æ¤œè¨ä¸­éŠ˜æŸ„ã‚’èµ·ç‚¹ã«ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„ã™ã‚‹ã€‚å¤–éƒ¨I/Oã¨å‰å‡¦ç†ã‚’æ‹…å½“ã—ã€`prepare_data`ã§`InputBundle`ã‚’ç”Ÿæˆã€‚ä¾¡æ ¼ãƒ»è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã¯**yfinanceã‚’å„ªå…ˆã—ã€æ¬ æãŒã‚ã‚‹æŒ‡æ¨™ã®ã¿Finnhub APIã§è£œå®Œ**ã™ã‚‹ã€‚
L109 ä¸»ãªãƒ¡ã‚½ãƒƒãƒ‰:
L110 - `impute_eps_ttm` : å››åŠæœŸEPSÃ—4ã§TTMã‚’æ¨å®šã—æ¬ ææ™‚ã®ã¿å·®ã—æ›¿ãˆã€‚
L111 - `fetch_cfo_capex_ttm_yf` : yfinanceã®å››åŠæœŸ/å¹´æ¬¡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼ã‹ã‚‰CFOãƒ»CapExãƒ»FCF TTMã‚’ç®—å‡ºã€‚
L112 - `fetch_cfo_capex_ttm_finnhub` : yfinanceã§æ¬ ã‘ãŸéŠ˜æŸ„ã®ã¿Finnhub APIã§è£œå®Œã€‚
L113 - `compute_fcf_with_fallback` : yfinanceå€¤ã‚’åŸºæº–ã«Finnhubå€¤ã§ç©´åŸ‹ã‚ã—ã€CFO/CapEx/FCFã¨æƒ…å ±æºãƒ•ãƒ©ã‚°ã‚’è¿”ã™ã€‚
L114 - `_build_eps_df` : `info`ã‚„`quarterly_earnings`ã‹ã‚‰EPS TTMã¨ç›´è¿‘EPSã‚’è¨ˆç®—ã—ã€`impute_eps_ttm`ã§è£œå®Œã€‚
L115 - `prepare_data` :
L116     0. CSVã‹ã‚‰ç¾è¡ŒéŠ˜æŸ„ã¨å€™è£œéŠ˜æŸ„ã®ãƒ†ã‚£ãƒƒã‚«ãƒ¼ä¸€è¦§ã‚’èª­ã¿è¾¼ã‚€ã€‚
L117     1. å€™è£œéŠ˜æŸ„ã®ç¾åœ¨å€¤ã‚’å–å¾—ã—ä¾¡æ ¼ä¸Šé™ã§ãƒ•ã‚£ãƒ«ã‚¿ã€‚
L118     2. æ—¢å­˜+å€™è£œã‹ã‚‰å¯¾è±¡ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚’æ±ºå®šã—ã€ä¾¡æ ¼ãƒ»å‡ºæ¥é«˜ã‚’ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆyfinanceï¼‰ã€‚
L119     3. yfinanceå€¤ã‚’åŸºã«EPS/FCFãƒ†ãƒ¼ãƒ–ãƒ«ã‚„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç³»åˆ—ã€ãƒªã‚¿ãƒ¼ãƒ³ã‚’æ§‹ç¯‰ã—ã€æ¬ æã‚»ãƒ«ã¯Finnhubå‘¼ã³å‡ºã—ã§ç©´åŸ‹ã‚ã€‚
L120     4. ä¸Šè¨˜ã‚’`InputBundle`ã«æ ¼ç´ã—ã¦è¿”ã™ã€‚
L121
L122 ### Step2: Score Calculation (Scorer)
L123 ç‰¹å¾´é‡è¨ˆç®—ã¨ã‚¹ã‚³ã‚¢åˆæˆã‚’æ‹…å½“ã—ã€`FeatureBundle`ã‚’è¿”ã™ã€‚
L124
L125 #### è£œåŠ©é–¢æ•°
L126 - `trend(s)` : 50/150/200æ—¥ç§»å‹•å¹³å‡ã‚„52é€±ãƒ¬ãƒ³ã‚¸ã‹ã‚‰-0.5ã€œ0.5ã§æ§‹æˆã•ã‚ŒãŸãƒˆãƒ¬ãƒ³ãƒ‰æŒ‡æ¨™ã€‚
L127 - `rs(s,b)` / `tr_str(s)` / `rs_line_slope(s,b,win)` : ç›¸å¯¾å¼·ã•ã‚„çŸ­æœŸãƒˆãƒ¬ãƒ³ãƒ‰ã€RSå›å¸°å‚¾ãã‚’ç®—å‡ºã€‚
L128 - `ev_fallback` : `enterpriseValue`æ¬ ææ™‚ã«è² å‚µãƒ»ç¾é‡‘ã‹ã‚‰EVã‚’æ¨å®šã€‚
L129 - `dividend_status` / `div_streak` : é…å½“æœªè¨­å®šçŠ¶æ³ã®åˆ¤å®šã¨å¢—é…å¹´æ•°ã‚«ã‚¦ãƒ³ãƒˆã€‚
L130 - `fetch_finnhub_metrics` : Finnhub APIã‹ã‚‰EPSæˆé•·ãƒ»ROEãƒ»Î²ãªã©ä¸è¶³æŒ‡æ¨™ã‚’å–å¾—ã€‚
L131 - `calc_beta` : ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ã®å…±åˆ†æ•£ã‹ã‚‰Î²ã‚’ç®—å‡ºã€‚
L132 - `spx_to_alpha` : S&P500ã®ä½ç½®æƒ…å ±ã‹ã‚‰DRRSã§ç”¨ã„ã‚‹Î±ã‚’æ¨å®šã€‚
L133 - `soft_cap_effective_scores` / `pick_top_softcap` : ã‚»ã‚¯ã‚¿ãƒ¼ã‚½ãƒ•ãƒˆã‚­ãƒ£ãƒƒãƒ—ä»˜ãã‚¹ã‚³ã‚¢èª¿æ•´ã¨ä¸Šä½æŠ½å‡ºã€‚
L134
L135 **è£œåŠ©é–¢æ•°ã¨ç”ŸæˆæŒ‡æ¨™**
L136
L137 | è£œåŠ©é–¢æ•° | ç”ŸæˆæŒ‡æ¨™ | ç•¥ç§° |
L138 | --- | --- | --- |
L139 | `trend` | ãƒˆãƒ¬ãƒ³ãƒ‰ç·åˆå€¤ | `TR` |
L140 | `rs` | ç›¸å¯¾å¼·ã• | `RS` |
L141 | `tr_str` | ä¾¡æ ¼ã¨50æ—¥ç·šã®ä¹–é›¢ | `TR_str` |
L142 | `rs_line_slope` | RSç·šã®å›å¸°å‚¾ã | `RS_SLOPE_*` |
L143 | `calc_beta` | Î² | `BETA` |
L144 | `div_streak` | é€£ç¶šå¢—é…å¹´æ•° | `DIV_STREAK` |
L145
L146 #### `aggregate_scores` è©³ç´°
L147 1. å„éŠ˜æŸ„ã®ä¾¡æ ¼ç³»åˆ—ã‚„`info`ã‚’åŸºã«ä»¥ä¸‹ã‚’ç®—å‡ºã€‚
L148    - **ãƒˆãƒ¬ãƒ³ãƒ‰/ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ **: `TR`ã€`RS`ã€`TR_str`ã€å¤šæ§˜ãªç§»å‹•å¹³å‡æ¯”ã€`RS_SLOPE_*`ãªã©ã€‚
L149    - **ãƒªã‚¹ã‚¯**: `BETA`ã€`DOWNSIDE_DEV`ã€`MDD_1Y`ã€`RESID_VOL`ã€`DOWN_OUTPERF`ã€`EXT_200`ç­‰ã€‚
L150    - **é…å½“**: `DIV`ã€`DIV_TTM_PS`ã€`DIV_VAR5`ã€`DIV_YOY`ã€`DIV_FCF_COVER`ã€`DIV_STREAK`ã€‚
L151    - **è²¡å‹™ãƒ»æˆé•·**: `EPS`ã€`REV`ã€`ROE`ã€`FCF/EV`ã€`REV_Q_YOY`ã€`EPS_Q_YOY`ã€`REV_YOY_ACC`ã€`REV_YOY_VAR`ã€`REV_ANN_STREAK`ã€`RULE40`ã€`FCF_MGN` ç­‰ã€‚
L152    - **å®‰å®šæ€§/ã‚µã‚¤ã‚º**: `DEBT2EQ`ã€`CURR_RATIO`ã€`MARKET_CAP`ã€`ADV60_USD`ã€`EPS_VAR_8Q`ãªã©ã€‚
L153 2. æŒ‡æ¨™æ¬ æã¯Finnhub APIç­‰ã§è£œå®Œã—ã€æœªå–å¾—é …ç›®ã‚’`missing_logs`ã«è¨˜éŒ²ã€‚
L154 3. `winsorize_s`â†’`robust_z`ã§æ¨™æº–åŒ–ã—`df_z`ã¸ä¿å­˜ã€‚ã‚µã‚¤ã‚ºãƒ»æµå‹•æ€§ã¯å¯¾æ•°å¤‰æ›ã€‚
L155 4. æ­£è¦åŒ–æ¸ˆæŒ‡æ¨™ã‹ã‚‰å› å­ã‚¹ã‚³ã‚¢ã‚’åˆæˆã€‚
L156    - å„å› å­ã®æ§‹æˆã¨é‡ã¿ã¯ä»¥ä¸‹ã®é€šã‚Šã€‚
L157      - **GRW**: 0.30Ã—`REV` + 0.20Ã—`EPS_Q_YOY` + 0.15Ã—`REV_Q_YOY` + 0.15Ã—`REV_YOY_ACC` + 0.10Ã—`RULE40` + 0.10Ã—`FCF_MGN` + 0.10Ã—`REV_ANN_STREAK` âˆ’ 0.05Ã—`REV_YOY_VAR`ã€‚
L158      - **MOM**: 0.40Ã—`RS` + 0.15Ã—`TR_str` + 0.15Ã—`RS_SLOPE_6W` + 0.15Ã—`RS_SLOPE_13W` + 0.10Ã—`MA200_SLOPE_5M` + 0.10Ã—`MA200_UP_STREAK_D`ã€‚
L159      - **VOL**: `BETA`å˜ä½“ã‚’ä½¿ç”¨ã€‚
L160      - **QAL**: 0.60Ã—`FCF_W` + 0.40Ã—`ROE_W`ã§ä½œæˆã€‚
L161      - **YLD**: 0.30Ã—`DIV` + 0.70Ã—`DIV_STREAK`ã€‚
L162      - **D_QAL**: 0.35Ã—`QAL` + 0.20Ã—`FCF` + 0.15Ã—`CURR_RATIO` âˆ’ 0.15Ã—`DEBT2EQ` âˆ’ 0.15Ã—`EPS_VAR_8Q`ã€‚
L163      - **D_YLD**: 0.45Ã—`DIV` + 0.25Ã—`DIV_STREAK` + 0.20Ã—`DIV_FCF_COVER` âˆ’ 0.10Ã—`DIV_VAR5`ã€‚
L164      - **D_VOL_RAW**: 0.40Ã—`DOWNSIDE_DEV` + 0.22Ã—`RESID_VOL` + 0.18Ã—`MDD_1Y` âˆ’ 0.10Ã—`DOWN_OUTPERF` âˆ’ 0.05Ã—`EXT_200` âˆ’ 0.08Ã—`SIZE` âˆ’ 0.10Ã—`LIQ` + 0.10Ã—`BETA`ã€‚
L165      - **D_TRD**: 0.40Ã—`MA200_SLOPE_5M` âˆ’ 0.30Ã—`EXT_200` + 0.15Ã—`NEAR_52W_HIGH` + 0.15Ã—`TR`ã€‚
L166     - ä¸»ãªæŒ‡æ¨™ã®ç•¥ç§°ã¨æ„å‘³:
L167
L168       | ç•¥ç§° | è£œåŠ©é–¢æ•° | æ¦‚è¦ |
L169       | --- | --- | --- |
L170       | TR | `trend` | 50/150/200æ—¥ç§»å‹•å¹³å‡ã¨52é€±ãƒ¬ãƒ³ã‚¸ã‚’çµ„ã¿åˆã‚ã›ãŸãƒˆãƒ¬ãƒ³ãƒ‰ç·åˆå€¤ |
L171       | RS | `rs` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«å¯¾ã™ã‚‹ç›¸å¯¾å¼·ã•ï¼ˆ12M/1Mãƒªã‚¿ãƒ¼ãƒ³å·®ï¼‰ |
L172       | TR_str | `tr_str` | ä¾¡æ ¼ã¨50æ—¥ç§»å‹•å¹³å‡ã®ä¹–é›¢ |
L173       | RS_SLOPE_6W | `rs_line_slope` | ç›¸å¯¾å¼·ã•ç·šã®6é€±å›å¸°å‚¾ã |
L174       | RS_SLOPE_13W | `rs_line_slope` | ç›¸å¯¾å¼·ã•ç·šã®13é€±å›å¸°å‚¾ã |
L175       | MA200_SLOPE_5M | - | 200æ—¥ç§»å‹•å¹³å‡ã®5ã‹æœˆé¨°è½ç‡ |
L176       | MA200_UP_STREAK_D | - | 200æ—¥ç·šãŒé€£ç¶šã§ä¸Šå‘ã„ãŸæ—¥æ•° |
L177       | BETA | `calc_beta` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«å¯¾ã™ã‚‹Î² |
L178       | DOWNSIDE_DEV | - | ä¸‹æ–¹ãƒªã‚¿ãƒ¼ãƒ³ã®ã¿ã®å¹´ç‡åŒ–æ¨™æº–åå·® |
L179       | RESID_VOL | - | Î²ã§èª¿æ•´ã—ãŸæ®‹å·®ãƒªã‚¿ãƒ¼ãƒ³ã®å¹´ç‡åŒ–æ¨™æº–åå·® |
L180       | MDD_1Y | - | éå»1å¹´ã®æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³ |
L181       | DOWN_OUTPERF | - | å¸‚å ´ä¸‹è½æ—¥ã«å¯¾ã™ã‚‹å¹³å‡è¶…éãƒªã‚¿ãƒ¼ãƒ³ |
L182       | EXT_200 | - | 200æ—¥ç§»å‹•å¹³å‡ã‹ã‚‰ã®çµ¶å¯¾ä¹–é›¢ç‡ |
L183       | NEAR_52W_HIGH | - | 52é€±é«˜å€¤ã¾ã§ã®ä¸‹æ–¹è·é›¢ï¼ˆ0=é«˜å€¤ï¼‰ |
L184       | FCF_W | - | ã‚¦ã‚£ãƒ³ã‚¶ãƒ¼å‡¦ç†å¾Œã®FCF/EV |
L185       | ROE_W | - | ã‚¦ã‚£ãƒ³ã‚¶ãƒ¼å‡¦ç†å¾Œã®ROE |
L186       | FCF | - | FCF/EV |
L187       | QAL | - | FCF_Wã¨ROE_Wã‚’çµ„ã¿åˆã‚ã›ãŸå“è³ªã‚¹ã‚³ã‚¢ |
L188       | CURR_RATIO | - | æµå‹•æ¯”ç‡ |
L189       | DEBT2EQ | - | è² å‚µè³‡æœ¬å€ç‡ |
L190       | EPS_VAR_8Q | - | EPSã®8å››åŠæœŸæ¨™æº–åå·® |
L191       | DIV | - | å¹´ç‡æ›ç®—é…å½“åˆ©å›ã‚Š |
L192       | DIV_STREAK | `div_streak` | é€£ç¶šå¢—é…å¹´æ•° |
L193       | DIV_FCF_COVER | - | é…å½“ã®FCFã‚«ãƒãƒ¬ãƒƒã‚¸ |
L194       | DIV_VAR5 | - | 5å¹´é…å½“å¤‰å‹•ç‡ |
L195       | DIV_TTM_PS | - | 1æ ªå½“ãŸã‚ŠTTMé…å½“ |
L196       | DIV_YOY | - | å‰å¹´æ¯”é…å½“æˆé•·ç‡ |
L197       | REV | - | å£²ä¸Šæˆé•·ç‡TTM |
L198       | EPS_Q_YOY | - | å››åŠæœŸEPSã®å‰å¹´åŒæœŸæ¯” |
L199       | REV_Q_YOY | - | å››åŠæœŸå£²ä¸Šã®å‰å¹´åŒæœŸæ¯” |
L200       | REV_YOY_ACC | - | å£²ä¸Šæˆé•·ç‡ã®åŠ é€Ÿåˆ† |
L201       | RULE40 | - | å£²ä¸Šæˆé•·ç‡ã¨FCFãƒãƒ¼ã‚¸ãƒ³ã®åˆè¨ˆ |
L202       | FCF_MGN | - | FCFãƒãƒ¼ã‚¸ãƒ³ |
L203       | REV_ANN_STREAK | - | å¹´æ¬¡å£²ä¸Šæˆé•·ã®é€£ç¶šå¹´æ•° |
L204       | REV_YOY_VAR | - | å¹´æ¬¡å£²ä¸Šæˆé•·ç‡ã®å¤‰å‹•æ€§ |
L205       | SIZE | - | æ™‚ä¾¡ç·é¡ã®å¯¾æ•°å€¤ |
L206       | LIQ | - | 60æ—¥å¹³å‡å‡ºæ¥é«˜ãƒ‰ãƒ«ã®å¯¾æ•°å€¤ |
L207    - Gãƒã‚±ãƒƒãƒˆ: `GRW`ã€`MOM`ã€`VOL`ã‚’`cfg.weights.g`ï¼ˆ0.40/0.45/-0.15ï¼‰ã§åŠ é‡ã—`g_score`ã‚’å¾—ã‚‹ã€‚
L208    - Dãƒã‚±ãƒƒãƒˆ: `D_QAL`ã€`D_YLD`ã€`D_VOL_RAW`ã€`D_TRD`ã‚’`cfg.weights.d`ï¼ˆ0.15/0.15/-0.45/0.25ï¼‰ã§åŠ é‡ã—`d_score_all`ã‚’ç®—å‡ºã€‚
L209    - ã‚»ã‚¯ã‚¿ãƒ¼capã«ã‚ˆã‚‹`soft_cap_effective_scores`ã‚’é©ç”¨ã—ã€Gæ¡ç”¨éŠ˜æŸ„ã«ã¯ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨ã€‚
L210 5. `_apply_growth_entry_flags`ã§ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆ/æŠ¼ã—ç›®ç™ºç«çŠ¶æ³ã‚’ä»˜åŠ ã—ã€`FeatureBundle`ã‚’è¿”ã™ã€‚
L211
L212 ### Step3: Correlation Reduction & Selection (Selector)
L213 DRRSã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ç›¸é–¢ã‚’æŠ‘ãˆãŸéŠ˜æŸ„é¸å®šã‚’è¡Œã„ã€`SelectionBundle`ã‚’è¿”ã™ã€‚`results/`ã«ä¿å­˜ã•ã‚ŒãŸå‰å›é¸å®šï¼ˆ`G_selection.json` / `D_selection.json`ï¼‰ã‚’`_load_prev`ã§èª­ã¿è¾¼ã¿ã€ç›®çš„å€¤ãŒå¤§ããæ‚ªåŒ–ã—ãªã„é™ã‚Šç¶­æŒã™ã‚‹ã€‚æ–°ã—ã„æ¡ç”¨é›†åˆã¯`_save_sel`ã§JSONã«æ›¸ãå‡ºã—æ¬¡å›ä»¥é™ã®å…¥åŠ›ã«å‚™ãˆã‚‹ã€‚
L214 ä¸»ãªãƒ¡ã‚½ãƒƒãƒ‰:
L215 - `residual_corr` : åç›Šç‡è¡Œåˆ—ã‚’Zã‚¹ã‚³ã‚¢åŒ–ã—ã€ä¸Šä½ä¸»æˆåˆ†ã‚’é™¤å»ã—ãŸæ®‹å·®ã‹ã‚‰ç›¸é–¢è¡Œåˆ—ã‚’æ±‚ã‚ã€å¹³å‡ç›¸é–¢ã«å¿œã˜ã¦ã‚·ãƒ¥ãƒªãƒ³ã‚¯ã€‚
L216 - `rrqr_like_det` : ã‚¹ã‚³ã‚¢ã‚’é‡ã¿ä»˜ã‘ã—ãŸQRåˆ†è§£é¢¨ã®æ‰‹é †ã§åˆæœŸå€™è£œã‚’kä»¶æŠ½å‡ºã—ã€ã‚¹ã‚³ã‚¢ã®é«˜ã„éç›¸é–¢ãªé›†åˆã‚’å¾—ã‚‹ã€‚
L217 - `swap_local_det` / `swap_local_det_cross` : `sum(score) - Î»*within_corr - Î¼*cross_corr`ã‚’ç›®çš„é–¢æ•°ã¨ã—ã¦ã€å…¥ã‚Œæ›¿ãˆæ¢ç´¢ã§å±€æ‰€çš„ã«æœ€é©åŒ–ã€‚
L218 - `select_bucket_drrs` : ãƒ—ãƒ¼ãƒ«éŠ˜æŸ„ã¨ã‚¹ã‚³ã‚¢ã‹ã‚‰æ®‹å·®ç›¸é–¢ã‚’è¨ˆç®—ã—ã€ä¸Šè¨˜2æ®µéš(åˆæœŸé¸æŠâ†’å…¥ã‚Œæ›¿ãˆ)ã§kéŠ˜æŸ„ã‚’æ±ºå®šã€‚éå»æ¡ç”¨éŠ˜æŸ„ã¨ã®æ¯”è¼ƒã§ç›®çš„å€¤ãŒåŠ£åŒ–ã—ãªã‘ã‚Œã°ç¶­æŒã™ã‚‹ã€‚
L219 - `select_buckets` : Gãƒã‚±ãƒƒãƒˆã‚’é¸å®šå¾Œã€ãã®çµæœã‚’é™¤ã„ãŸå€™è£œã‹ã‚‰Dãƒã‚±ãƒƒãƒˆã‚’é¸ã¶ã€‚Dé¸å®šæ™‚ã¯Gã¨ã®ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£Î¼ã‚’ä»˜ä¸ã—ã€ä¸¡ãƒã‚±ãƒƒãƒˆã®åˆ†æ•£ã‚’åˆ¶å¾¡ã™ã‚‹ã€‚
L220
L221 #### ç›¸é–¢ä½æ¸›ãƒ­ã‚¸ãƒƒã‚¯è©³ç´°
L222 1. **æ®‹å·®ç›¸é–¢è¡Œåˆ—ã®æ§‹ç¯‰ (`residual_corr`)**
L223    - ãƒªã‚¿ãƒ¼ãƒ³è¡Œåˆ—`R`ã‚’Zã‚¹ã‚³ã‚¢åŒ–ã€‚
L224    - SVDã§ä¸Šä½`n_pc`ä¸»æˆåˆ†`F`ã‚’æ±‚ã‚ã€æœ€å°äºŒä¹—ã§ä¿‚æ•°`B`ã‚’ç®—å‡ºã—æ®‹å·®`E = Z - F@B`ã‚’å¾—ã‚‹ã€‚
L225    - `E`ã®ç›¸é–¢è¡Œåˆ—`C`ã‚’è¨ˆç®—ã—ã€å¹³å‡çµ¶å¯¾ç›¸é–¢ã«å¿œã˜ã¦ã‚·ãƒ¥ãƒªãƒ³ã‚¯é‡`shrink_eff`ã‚’è£œæ­£ã—ã¦å¯¾è§’ã‚’å¼·èª¿ã€‚
L226 2. **åˆæœŸå€™è£œã®æŠ½å‡º (`rrqr_like_det`)**
L227    - ã‚¹ã‚³ã‚¢ã‚’0-1æ­£è¦åŒ–ã—ãŸé‡ã¿`w`ã¨ã—ã€`Z*(1+Î³w)`ã§åˆ—ãƒãƒ«ãƒ ã‚’å¼·èª¿ã€‚
L228    - æ®‹å·®ãƒãƒ«ãƒ æœ€å¤§ã®åˆ—ã‚’é€æ¬¡é¸ã³ã€QRãƒ©ã‚¤ã‚¯ãªãƒ‡ãƒ•ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã£ã¦éç›¸é–¢ã‹ã¤é«˜ã‚¹ã‚³ã‚¢ãª`k`éŠ˜æŸ„é›†åˆ`S0`ã‚’å¾—ã‚‹ã€‚
L229 3. **å±€æ‰€æ¢ç´¢ (`swap_local_det` / `swap_local_det_cross`)**
L230    - ç›®çš„é–¢æ•°`Î£z_score âˆ’ Î»Â·within_corr âˆ’ Î¼Â·cross_corr`ã‚’æœ€å¤§åŒ–ã€‚
L231    - é¸æŠé›†åˆã®å„éŠ˜æŸ„ã‚’ä»–å€™è£œã¨å…¥ã‚Œæ›¿ãˆã€æ”¹å–„ãŒãªããªã‚‹ã¾ã§ã¾ãŸã¯`max_pass`å›ã¾ã§æ¢ç´¢ã€‚
L232    - `swap_local_det_cross`ã¯Gãƒã‚±ãƒƒãƒˆã¨ã®ã‚¯ãƒ­ã‚¹ç›¸é–¢è¡Œåˆ—`C_cross`ã‚’ä½¿ç”¨ã—ã€ãƒšãƒŠãƒ«ãƒ†ã‚£`Î¼`ã‚’ä»˜ä¸ã€‚
L233 4. **éå»æ¡ç”¨ã®ç¶­æŒã¨ã‚¯ãƒ­ã‚¹ãƒšãƒŠãƒ«ãƒ†ã‚£ (`select_bucket_drrs` / `select_buckets`)**
L234    - å±€æ‰€æ¢ç´¢çµæœ`S`ã¨éå»é›†åˆ`P`ã®ç›®çš„å€¤ã‚’æ¯”è¼ƒã—ã€`S`ãŒ`P`ã‚ˆã‚Š`Î·`æœªæº€ã®æ”¹å–„ãªã‚‰`P`ã‚’ç¶­æŒã€‚
L235    - `select_buckets`ã§ã¯Gã‚’å…ˆã«æ±ºå®šã—ã€Dé¸å®šæ™‚ã«Gã¨ã®ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£`Î¼`ã‚’åŠ ãˆã¦ã‚¯ãƒ­ã‚¹åˆ†æ•£ã‚’æŠ‘åˆ¶ã€‚
L236
L237 ### Step4: Output
L238 é¸å®šçµæœã‚’å¯è¦–åŒ–ã—å…±æœ‰ã™ã‚‹å·¥ç¨‹ã€‚ä»¥ä¸‹ã®å†…å®¹ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«åŒ–ã—ã¦æ¨™æº–å‡ºåŠ›ã¨Slackã¸é€ã‚‹ã€‚
L239 - æ¡ç”¨éŠ˜æŸ„ã¨æƒœã—ãã‚‚é¸å¤–ã¨ãªã£ãŸéŠ˜æŸ„ã®ã‚¹ã‚³ã‚¢ä¸€è¦§
L240 - IN/OUTãƒªã‚¹ãƒˆã¨OUTéŠ˜æŸ„ã®ã‚¹ã‚³ã‚¢ï¼ˆä½å¾—ç‚¹éŠ˜æŸ„ã‚’ç¢ºèªã—ã‚„ã™ãï¼‰
L241 - æ–°æ—§ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®æ¯”è¼ƒè¡¨ï¼ˆçµ„å…¥ã‚Œãƒ»é™¤å¤–ã€ã‚¹ã‚³ã‚¢å¤‰åŒ–ï¼‰
L242 - æ¤œè¨ä¸­éŠ˜æŸ„ã®ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°
L243
L244 ä¸»ãªãƒ¡ã‚½ãƒƒãƒ‰:
L245 - `display_results` : ä¸Šè¨˜ãƒ†ãƒ¼ãƒ–ãƒ«ã«åŠ ãˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã‚„åˆ†æ•£åŒ–æŒ‡æ¨™ã‚’è¡¨ç¤ºã€‚
L246 - `notify_slack` : Slack Webhookã¸åŒå†…å®¹ã‚’é€ä¿¡ã€‚
L247 - è£œåŠ©:`_avg_offdiag`ã€`_resid_avg_rho`ã€`_raw_avg_rho`ã€`_cross_block_raw_rho`ã€‚
L248
L249 ## ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
L250 1. `PipelineConfig`ã‚’æ§‹ç¯‰ã€‚
L251 2. **Step1** `Input.prepare_data`ã§`InputBundle`ã‚’ç”Ÿæˆã€‚
L252 3. **Step2** `Scorer.aggregate_scores`ã§`FeatureBundle`ã‚’å–å¾—ã€‚
L253 4. **Step3** `Selector.select_buckets`ã§`SelectionBundle`ã‚’ç®—å‡ºã€‚
L254 5. **Step4** `Output.display_results`ã¨`notify_slack`ã§çµæœã‚’å‡ºåŠ›ã€‚
```

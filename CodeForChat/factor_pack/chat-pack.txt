# === Chat Paste Pack ===
# Repo: dakara32/GPT_Code @ main
# Files: config.py, factor.py, scorer.py, .github/workflows/weekly-report.yml, documents/README.md, documents/factor_design.md
# ä½œæˆæ—¥æ™‚: 2025-09-18 20:52:51 (JST)
# ä½¿ã„æ–¹: ä¸‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’é †ã«è²¼ã‚Œã°ã“ã®ãƒãƒ£ãƒƒãƒˆã§å…¨ä½“æŠŠæ¡ã§ãã¾ã™ã€‚
# æ³¨è¨˜: å„ãƒ•ã‚¡ã‚¤ãƒ«ã¯å€‹åˆ¥ã« L1.. ã§è¡Œç•ªå·ä»˜ä¸ã€‚
---

## <config.py>
```text
L1 # å…±é€šè¨­å®šï¼ˆfactor / drift ã‹ã‚‰å‚ç…§ï¼‰
L2 from dataclasses import dataclass
L3
L4 TOTAL_TARGETS = 20
L5
L6 # åŸºæº–ã®ãƒã‚±ãƒƒãƒˆæ•°ï¼ˆNORMALï¼‰
L7 COUNTS_BASE = {"G": 12, "D": 8}
L8
L9 # ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®æ¨å¥¨ãƒã‚±ãƒƒãƒˆæ•°
L10 COUNTS_BY_MODE = {
L11     "NORMAL": {"G": 12, "D": 8},
L12     "CAUTION": {"G": 10, "D": 8},
L13     "EMERG": {"G": 8,  "D": 8},
L14 }
L15
L16 # ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®ãƒ‰ãƒªãƒ•ãƒˆé–¾å€¤ï¼ˆ%ï¼‰
L17 DRIFT_THRESHOLD_BY_MODE = {"NORMAL": 12, "CAUTION": 14, "EMERG": float("inf")}
L18
L19 # ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®TSï¼ˆåŸºæœ¬å¹…, å°æ•°=å‰²åˆï¼‰
L20 TS_BASE_BY_MODE = {"NORMAL": 0.15, "CAUTION": 0.13, "EMERG": 0.10}
L21 # åˆ©ç›Šåˆ°é”(+30/+60/+100%)æ™‚ã®æ®µéšã‚¿ã‚¤ãƒˆåŒ–ï¼ˆãƒã‚¤ãƒ³ãƒˆå·®ï¼‰
L22 TS_STEP_DELTAS_PT = (3, 6, 8)
L23
L24 # Breadthã®æ ¡æ­£ã¯ N_G ã«é€£å‹•ï¼ˆç·Šæ€¥è§£é™¤=ceil(1.5*N_G), é€šå¸¸å¾©å¸°=3*N_Gï¼‰
L25 N_G = COUNTS_BASE["G"]
L26 N_D = COUNTS_BASE["D"]
L27
```

## <factor.py>
```text
L1 '''ROLE: Orchestration ONLYï¼ˆå¤–éƒ¨I/Oãƒ»SSOTãƒ»Slackå‡ºåŠ›ï¼‰, è¨ˆç®—ã¯ scorer.py'''
L2 # === NOTE: æ©Ÿèƒ½ãƒ»å…¥å‡ºåŠ›ãƒ»ãƒ­ã‚°æ–‡è¨€ãƒ»ä¾‹å¤–æŒ™å‹•ã¯ä¸å¤‰ã€‚å®‰å…¨ãªçŸ­ç¸®ï¼ˆimportçµ±åˆ/è¤‡æ•°ä»£å…¥/å†…åŒ…è¡¨è¨˜/ãƒ¡ã‚½ãƒƒãƒ‰ãƒã‚§ãƒ¼ãƒ³/ä¸€è¡ŒåŒ–/ç©ºè¡Œåœ§ç¸®ãªã©ï¼‰ã®ã¿é©ç”¨ ===
L3 BONUS_COEFF = 0.55  # æ¨å¥¨: æ”»ã‚=0.45 / ä¸­åº¸=0.55 / å®ˆã‚Š=0.65
L4 SWAP_DELTA_Z = 0.15   # åƒ…å·®åˆ¤å®š: Ïƒã®15%ã€‚(ç·©ã‚=0.10 / æ¨™æº–=0.15 / å›ºã‚=0.20)
L5 SWAP_KEEP_BUFFER = 3  # n_target+ã“ã®é †ä½ä»¥å†…ã®ç¾è¡Œã¯ä¿æŒã€‚(ç²˜ã‚Šå¼±=2 / æ¨™æº–=3 / ç²˜ã‚Šå¼·=4ã€œ5)
L6 import os, time, requests
L7 import logging
L8 from time import perf_counter
L9 from dataclasses import dataclass
L10 from typing import Any, Dict, List
L11 from concurrent.futures import ThreadPoolExecutor
L12 from types import SimpleNamespace
L13 import numpy as np
L14 import pandas as pd
L15 import yfinance as yf
L16 from scipy.stats import zscore  # used via scorer
L17 from scorer import Scorer, ttm_div_yield_portfolio
L18 import config
L19
L20 # ãã®ä»–
L21 debug_mode, FINNHUB_API_KEY = True, os.environ.get("FINNHUB_API_KEY")
L22
L23 logger = logging.getLogger(__name__)
L24 if debug_mode:
L25     logging.basicConfig(level=logging.INFO, force=True)
L26 else:
L27     logging.basicConfig(level=logging.WARNING, force=True)
L28
L29 class T:
L30     t = perf_counter()
L31     log = staticmethod(lambda tag: (lambda now=perf_counter(): (print(f"[T] {tag}: {now - T.t:.2f}s"), setattr(T, "t", now))[-1])())
L32
L33 T.log("start")
L34
L35 # === ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã¨å®šæ•°ï¼ˆå†’é ­ã«å›ºå®šï¼‰ ===
L36 exist, cand = [pd.read_csv(f, header=None)[0].tolist() for f in ("current_tickers.csv","candidate_tickers.csv")]
L37 T.log(f"csv loaded: exist={len(exist)} cand={len(cand)}")
L38 CAND_PRICE_MAX, bench = 450, '^GSPC'  # ä¾¡æ ¼ä¸Šé™ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
L39 N_G, N_D = config.N_G, config.N_D  # G/Dæ ã‚µã‚¤ã‚ºï¼ˆNORMALåŸºæº–: G12/D8ï¼‰
L40 g_weights = {'GROWTH_F':0.35,'MOM':0.55,'VOL':-0.10}
L41 D_BETA_MAX = float(os.environ.get("D_BETA_MAX", "0.8"))
L42 FILTER_SPEC = {"G":{"pre_mask":["trend_template"]},"D":{"pre_filter":{"beta_max":D_BETA_MAX}}}
L43 D_weights = {'QAL':0.1,'YLD':0.3,'VOL':-0.5,'TRD':0.1}
L44 _fmt_w = lambda w: " ".join(f"{k}{int(v*100)}" for k, v in w.items())
L45
L46 # DRRS åˆæœŸãƒ—ãƒ¼ãƒ«ãƒ»å„ç¨®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
L47 corrM = 45
L48 DRRS_G, DRRS_D = dict(lookback=252,n_pc=3,gamma=1.2,lam=0.68,eta=0.8), dict(lookback=504,n_pc=4,gamma=0.8,lam=0.85,eta=0.5)
L49 DRRS_SHRINK = 0.10  # æ®‹å·®ç›¸é–¢ã®å¯¾è§’ã‚·ãƒ¥ãƒªãƒ³ã‚¯ï¼ˆåŸºç¤ï¼‰
L50
L51 # ã‚¯ãƒ­ã‚¹ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆæœªå®šç¾©ãªã‚‰è¨­å®šï¼‰
L52 try: CROSS_MU_GD
L53 except NameError: CROSS_MU_GD = 0.40  # æ¨å¥¨ 0.35â€“0.45ï¼ˆlam=0.85æƒ³å®šï¼‰
L54
L55 # å‡ºåŠ›é–¢é€£
L56 RESULTS_DIR = "results"
L57 os.makedirs(RESULTS_DIR, exist_ok=True)
L58
L59 # === å…±æœ‰DTOï¼ˆã‚¯ãƒ©ã‚¹é–“I/Oå¥‘ç´„ï¼‰ï¼‹ Config ===
L60 @dataclass(frozen=True)
L61 class InputBundle:
L62     # Input â†’ Scorer ã§å—ã‘æ¸¡ã™ç´ æï¼ˆI/Oç¦æ­¢ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ï¼‰
L63     cand: List[str]
L64     tickers: List[str]
L65     bench: str
L66     data: pd.DataFrame              # yfinance downloadçµæœï¼ˆ'Close','Volume'ç­‰ã®éšå±¤åˆ—ï¼‰
L67     px: pd.DataFrame                # data['Close']
L68     spx: pd.Series                  # data['Close'][bench]
L69     tickers_bulk: object            # yfinance.Tickers
L70     info: Dict[str, dict]           # yfinance info per ticker
L71     eps_df: pd.DataFrame            # ['eps_ttm','eps_q_recent',...]
L72     fcf_df: pd.DataFrame            # ['fcf_ttm', ...]
L73     returns: pd.DataFrame           # px[tickers].pct_change()
L74
L75 @dataclass(frozen=True)
L76 class FeatureBundle:
L77     df: pd.DataFrame
L78     df_z: pd.DataFrame
L79     g_score: pd.Series
L80     d_score_all: pd.Series
L81     missing_logs: pd.DataFrame
L82     df_full: pd.DataFrame | None = None
L83     df_full_z: pd.DataFrame | None = None
L84     scaler: Any | None = None
L85
L86 @dataclass(frozen=True)
L87 class SelectionBundle:
L88     resG: dict
L89     resD: dict
L90     top_G: List[str]
L91     top_D: List[str]
L92     init_G: List[str]
L93     init_D: List[str]
L94
L95 @dataclass(frozen=True)
L96 class WeightsConfig:
L97     g: Dict[str,float]
L98     d: Dict[str,float]
L99
L100 @dataclass(frozen=True)
L101 class DRRSParams:
L102     corrM: int
L103     shrink: float
L104     G: Dict[str,float]   # lookback, n_pc, gamma, lam, eta
L105     D: Dict[str,float]
L106     cross_mu_gd: float
L107
L108 @dataclass(frozen=True)
L109 class PipelineConfig:
L110     weights: WeightsConfig
L111     drrs: DRRSParams
L112     price_max: float
L113
L114 # === å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆè¤‡æ•°ã‚¯ãƒ©ã‚¹ã§ä½¿ç”¨ï¼‰ ===
L115 # (unused local utils removed â€“ use scorer.py versions if needed)
L116
L117 _env_true = lambda name, default=False: (os.getenv(name) or str(default)).strip().lower() == "true"
L118
L119 _DEBUG_COL_ALIAS = {
L120     "GROWTH_F": "GRW",
L121     "GROWTH_F_RAW": "GRW_RAW",
L122     "TREND_SLOPE_EPS": "TR_EPS",
L123     "TREND_SLOPE_EPS_RAW": "TR_EPS_RAW",
L124     "TREND_SLOPE_REV": "TR_REV",
L125     "TREND_SLOPE_REV_RAW": "TR_REV_RAW",
L126     "TREND_SLOPE_EPS_YR": "TR_EPS_YR",
L127     "TREND_SLOPE_EPS_YR_RAW": "TR_EPS_YR_RAW",
L128     "TREND_SLOPE_REV_YR": "TR_REV_YR",
L129     "TREND_SLOPE_REV_YR_RAW": "TR_REV_YR_RAW",
L130     "BETA": "BETA_RAW",
L131 }
L132
L133 _DEBUG_COL_ORDER = [
L134     "GRW", "GRW_RAW",
L135     "TR_EPS", "TR_EPS_RAW", "TR_REV", "TR_REV_RAW",
L136     "TR_EPS_YR", "TR_EPS_YR_RAW", "TR_REV_YR", "TR_REV_YR_RAW",
L137     "EPS_Q_YOY", "EPS_Q_YOY_RAW", "EPS_YOY", "EPS_YOY_RAW",
L138     "REV_Q_YOY", "REV_Q_YOY_RAW", "REV_YOY", "REV_YOY_RAW",
L139     "REV_YOY_ACC", "REV_YOY_ACC_RAW", "REV_YOY_VAR", "REV_YOY_VAR_RAW", "REV_ANN_STREAK",
L140     "RULE40", "RULE40_RAW", "FCF_MGN", "FCF_MGN_RAW",
L141     "FCF", "ROE", "RS", "TR_str", "BETA_RAW", "DIV_STREAK",
L142 ]
L143
L144 DEBUG_COLS = [
L145     "GRW", "GRW_RAW",
L146     "TR_EPS", "TR_EPS_RAW", "TR_REV", "TR_REV_RAW",
L147     "TR_EPS_YR", "TR_EPS_YR_RAW", "TR_REV_YR", "TR_REV_YR_RAW",
L148     "EPS_Q_YOY", "EPS_Q_YOY_RAW", "EPS_YOY", "EPS_YOY_RAW",
L149     "REV_Q_YOY", "REV_Q_YOY_RAW", "REV_YOY", "REV_YOY_RAW",
L150     "REV_YOY_ACC", "REV_YOY_ACC_RAW", "REV_YOY_VAR", "REV_YOY_VAR_RAW",
L151     "REV_ANN_STREAK", "RULE40", "RULE40_RAW",
L152     "FCF_MGN", "FCF_MGN_RAW", "FCF", "ROE", "RS", "TR_str", "BETA_RAW", "DIV_STREAK",
L153     "GSC", "DSC"
L154 ]
L155
L156 MUST_DEBUG_COLS = {"TR_EPS", "TR_REV", "REV_Q_YOY", "REV_YOY_ACC", "RULE40", "FCF_MGN"}
L157
L158 def _post_slack(payload: dict):
L159     url = os.getenv("SLACK_WEBHOOK_URL")
L160     if not url: print("âš ï¸ SLACK_WEBHOOK_URL æœªè¨­å®š"); return
L161     try:
L162         requests.post(url, json=payload).raise_for_status()
L163     except Exception as e:
L164         print(f"âš ï¸ Slacké€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
L165
L166 def _slack_send_text_chunks(url: str, text: str, chunk: int = 2800) -> None:
L167     """Slackã¸ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²é€ä¿¡ï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å½¢å¼ï¼‰ã€‚"""
L168
L169     def _post_text(payload: str) -> None:
L170         try:
L171             resp = requests.post(url, json={"text": payload})
L172             print(f"[DBG] debug_post status={getattr(resp, 'status_code', None)} size={len(payload)}")
L173             if resp is not None:
L174                 resp.raise_for_status()
L175         except Exception as e:
L176             print(f"[ERR] debug_post_failed: {e}")
L177
L178     body = str(text or "").strip()
L179     if not body:
L180         print("[DBG] skip debug send: empty body")
L181         return
L182
L183     lines = body.splitlines()
L184     block: list[str] = []
L185     block_len = 0
L186
L187     def _flush() -> None:
L188         nonlocal block, block_len
L189         if not block:
L190             return
L191         payload = "```" + "\n".join(block) + "```"
L192         _post_text(payload)
L193         block, block_len = [], 0
L194
L195     for raw in lines:
L196         line = raw or ""
L197         while len(line) > chunk:
L198             head, line = line[:chunk], line[chunk:]
L199             _flush()
L200             _post_text("```" + head + "```")
L201         add_len = len(line) if not block else len(line) + 1
L202         if block and block_len + add_len > chunk:
L203             _flush()
L204             add_len = len(line)
L205         block.append(line)
L206         block_len += add_len
L207     _flush()
L208
L209 def _compact_debug(fb, sb, prevG, prevD, max_rows=140):
L210     src = getattr(fb, "df_full", None)
L211     if not isinstance(src, pd.DataFrame) or src.empty:
L212         src = getattr(fb, "df_full_z", None)
L213     if not isinstance(src, pd.DataFrame) or src.empty:
L214         src = getattr(fb, "df_z", None)
L215     if not isinstance(src, pd.DataFrame) or src.empty:
L216         return ""
L217
L218     df_show = src.apply(pd.to_numeric, errors="coerce").rename(
L219         columns={k: v for k, v in _DEBUG_COL_ALIAS.items() if k in src.columns}
L220     )
L221
L222     missing = sorted(c for c in MUST_DEBUG_COLS if c not in df_show.columns)
L223     if missing:
L224         logger.warning("[debug] missing cols: %s", missing)
L225
L226     all_cols = _env_true("DEBUG_ALL_COLS", False)
L227     ordered = [c for c in _DEBUG_COL_ORDER if c in df_show.columns]
L228     cols = list(df_show.columns) if all_cols else ordered
L229     if not cols:
L230         cols = [c for c in df_show.columns if c not in ("GSC", "DSC")]
L231
L232     g_series = getattr(fb, "g_score", None)
L233     d_series = getattr(fb, "d_score_all", None)
L234     show_near = _env_true("DEBUG_NEAR5", True)
L235     g_sorted = g_series.sort_values(ascending=False) if show_near and hasattr(g_series, "sort_values") else None
L236     d_sorted = d_series.sort_values(ascending=False) if show_near and hasattr(d_series, "sort_values") else None
L237
L238     g_pick = list(getattr(sb, "top_G", []) or [])
L239     d_pick = list(getattr(sb, "top_D", []) or [])
L240     prevG = list(prevG or [])
L241     prevD = list(prevD or [])
L242     Gp, Dp = set(prevG), set(prevD)
L243     g_new = [t for t in g_pick if t not in Gp]
L244     g_out = [t for t in prevG if t not in g_pick]
L245     d_new = [t for t in d_pick if t not in Dp]
L246     d_out = [t for t in prevD if t not in d_pick]
L247
L248     g_miss = [t for t in (g_sorted.index if g_sorted is not None else []) if t not in g_pick][:10]
L249     used_d = set(g_pick + d_pick)
L250     d_miss = [t for t in (d_sorted.index if d_sorted is not None else []) if t not in used_d][:10]
L251
L252     def _merge_rows(*seqs):
L253         seen, out = set(), []
L254         for seq in seqs:
L255             for t in seq or []:
L256                 if t in df_show.index and t not in seen:
L257                     seen.add(t)
L258                     out.append(t)
L259         return out
L260
L261     all_rows = _env_true("DEBUG_ALL_ROWS", False)
L262     focus = df_show.index.tolist() if all_rows else _merge_rows(g_pick + d_pick, prevG + prevD, g_miss, d_miss)
L263     if not focus:
L264         focus = df_show.index.tolist()
L265     focus = focus[:max_rows]
L266
L267     if cols:
L268         df_focus = df_show.loc[focus, cols].copy()
L269     else:
L270         df_focus = df_show.loc[focus].copy()
L271     if "GSC" not in df_focus.columns and g_series is not None:
L272         df_focus["GSC"] = [g_series.get(t, np.nan) if hasattr(g_series, "get") else np.nan for t in df_focus.index]
L273     if "DSC" not in df_focus.columns and d_series is not None:
L274         df_focus["DSC"] = [d_series.get(t, np.nan) if hasattr(d_series, "get") else np.nan for t in df_focus.index]
L275
L276     if not all_cols:
L277         extra = [c for c in df_focus.columns if c not in cols]
L278         cols = cols + [c for c in ("GSC", "DSC") if c not in cols]
L279         cols += [c for c in extra if c not in cols]
L280     else:
L281         cols = [c for c in df_focus.columns if c not in ("GSC", "DSC")]
L282         cols += [c for c in ("GSC", "DSC") if c not in cols]
L283
L284     body = df_focus.reindex(columns=cols).round(3).to_string(max_rows=None, max_cols=None, na_rep="nan") if focus else "(no rows)"
L285
L286     def _fmt_near(lbl, ser, lst):
L287         if ser is None:
L288             return f"{lbl}: off"
L289         get = ser.get
L290         parts = []
L291         for t in lst:
L292             val = get(t, np.nan)
L293             parts.append(f"{t}:{val:.3f}" if pd.notna(val) else f"{t}:nan")
L294         return f"{lbl}: " + (", ".join(parts) if parts else "-")
L295
L296     head = [
L297         f"G new/out: {len(g_new)}/{len(g_out)}  D new/out: {len(d_new)}/{len(d_out)}",
L298         _fmt_near("G near10", g_sorted, g_miss),
L299         _fmt_near("D near10", d_sorted, d_miss),
L300         f"Filters: G pre_mask=['trend_template'], D pre_filter={{'beta_max': {D_BETA_MAX}}}",
L301         f"Cols={'ALL' if all_cols else 'MIN'}  Rows={'ALL' if all_rows else 'SELECTED+CURRENT+NEAR'}",
L302     ]
L303
L304     miss_txt = ""
L305     if _env_true("DEBUG_MISSING_LOGS", False):
L306         miss = getattr(fb, "missing_logs", None)
L307         if miss is not None and not miss.empty:
L308             miss_txt = "\nMissing data (head)\n" + miss.head(10).to_string(index=False)
L309
L310     lines = head + ["", "Changed/Selected (+ Near Miss + Current)", "GRWæ·±æ˜ã‚Šè¡¨", body]
L311     return "\n".join(lines) + miss_txt
L312
L313 def _disjoint_keepG(top_G, top_D, poolD):
L314     """Gé‡è¤‡ã‚’Dã‹ã‚‰é™¤å»ã—ã€poolDã§é †æ¬¡è£œå……ï¼ˆæ¯æ¸‡æ™‚ã¯å…ƒéŠ˜æŸ„ç¶­æŒï¼‰ã€‚"""
L315     used, D, i = set(top_G), list(top_D), 0
L316     for j, t in enumerate(D):
L317         if t in used:
L318             while i < len(poolD) and (poolD[i] in used or poolD[i] in D):
L319                 i += 1
L320             if i < len(poolD):
L321                 D[j] = poolD[i]; used.add(D[j]); i += 1
L322     return top_G, D
L323
L324
L325 def _sticky_keep_current(agg: pd.Series, pick: list[str], incumbents: list[str],
L326                          n_target: int, delta_z: float, keep_buffer: int) -> list[str]:
L327     import pandas as pd, numpy as np
L328     sel = list(pick)
L329     if not sel: return sel
L330     ranked_sel = agg.reindex(sel).sort_values(ascending=False)
L331     kth = ranked_sel.iloc[min(len(sel), n_target)-1]
L332     sigma = float(agg.std()) if pd.notna(agg.std()) else 0.0
L333     thresh = kth - delta_z * sigma
L334     ranked_all = agg.sort_values(ascending=False)
L335     cand = [t for t in incumbents if (t not in sel) and (t in agg.index)]
L336     for t in cand:
L337         within_score = (pd.notna(agg[t]) and agg[t] >= thresh)
L338         within_rank  = (t in ranked_all.index) and (ranked_all.index.get_loc(t) < n_target + keep_buffer)
L339         if within_score or within_rank:
L340             non_inc = [x for x in sel if x not in incumbents]
L341             if not non_inc: break
L342             weakest = min(non_inc, key=lambda x: agg.get(x, -np.inf))
L343             if weakest in sel and agg.get(t, -np.inf) >= agg.get(weakest, -np.inf):
L344                 sel.remove(weakest); sel.append(t)
L345     if len(sel) > n_target:
L346         sel = sorted(sel, key=lambda x: agg.get(x, -1e9), reverse=True)[:n_target]
L347     return sel
L348
L349
L350 # === Inputï¼šå¤–éƒ¨I/Oã¨å‰å‡¦ç†ï¼ˆCSV/APIãƒ»æ¬ æè£œå®Œï¼‰ ===
L351 class Input:
L352     def __init__(self, cand, exist, bench, price_max, finnhub_api_key=None):
L353         self.cand, self.exist, self.bench, self.price_max = cand, exist, bench, price_max
L354         self.api_key = finnhub_api_key or os.environ.get("FINNHUB_API_KEY")
L355
L356     # ---- ï¼ˆInputå°‚ç”¨ï¼‰EPSè£œå®Œãƒ»FCFç®—å‡ºç³» ----
L357     @staticmethod
L358     def impute_eps_ttm(df: pd.DataFrame, ttm_col: str="eps_ttm", q_col: str="eps_q_recent", out_col: str|None=None) -> pd.DataFrame:
L359         out_col = out_col or ttm_col; df = df.copy(); df["eps_imputed"] = False
L360         cand = df[q_col]*4; ok = df[ttm_col].isna() & cand.replace([np.inf,-np.inf], np.nan).notna()
L361         df.loc[ok, out_col], df.loc[ok,"eps_imputed"] = cand[ok], True; return df
L362
L363     _CF_ALIASES = {"cfo":["Operating Cash Flow","Total Cash From Operating Activities"], "capex":["Capital Expenditure","Capital Expenditures"]}
L364
L365     @staticmethod
L366     def _pick_row(df: pd.DataFrame, names: list[str]) -> pd.Series|None:
L367         if df is None or df.empty: return None
L368         idx_lower={str(i).lower():i for i in df.index}
L369         for n in names:
L370             k=n.lower()
L371             if k in idx_lower: return df.loc[idx_lower[k]]
L372         return None
L373
L374     @staticmethod
L375     def _sum_last_n(s: pd.Series|None, n: int) -> float|None:
L376         if s is None or s.empty: return None
L377         v=s.dropna().astype(float); return None if v.empty else v.iloc[:n].sum()
L378
L379     @staticmethod
L380     def _latest(s: pd.Series|None) -> float|None:
L381         if s is None or s.empty: return None
L382         v=s.dropna().astype(float); return v.iloc[0] if not v.empty else None
L383
L384     def fetch_cfo_capex_ttm_yf(self, tickers: list[str]) -> pd.DataFrame:
L385         from concurrent.futures import ThreadPoolExecutor, as_completed
L386         pick, sumn, latest, aliases = self._pick_row, self._sum_last_n, self._latest, self._CF_ALIASES
L387
L388         def one(t: str):
L389             try:
L390                 tk = yf.Ticker(t)  # â˜… ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¯æ¸¡ã•ãªã„ï¼ˆYFãŒcurl_cffiã§ç®¡ç†ï¼‰
L391                 qcf = tk.quarterly_cashflow
L392                 cfo_q, capex_q = pick(qcf, aliases["cfo"]), pick(qcf, aliases["capex"])
L393                 fcf_q = pick(qcf, ["Free Cash Flow","FreeCashFlow","Free cash flow"])
L394                 cfo, capex, fcf = sumn(cfo_q,4), sumn(capex_q,4), sumn(fcf_q,4)
L395                 if any(v is None for v in (cfo, capex, fcf)):
L396                     acf = tk.cashflow
L397                     if cfo   is None: cfo   = latest(pick(acf, aliases["cfo"]))
L398                     if capex is None: capex = latest(pick(acf, aliases["capex"]))
L399                     if fcf   is None: fcf   = latest(pick(acf, ["Free Cash Flow","FreeCashFlow","Free cash flow"]))
L400             except Exception as e:
L401                 print(f"[warn] yf financials error: {t}: {e}"); cfo=capex=fcf=None
L402             n=np.nan
L403             return {"ticker":t,
L404                     "cfo_ttm_yf":   n if cfo   is None else cfo,
L405                     "capex_ttm_yf": n if capex is None else capex,
L406                     "fcf_ttm_yf_direct": n if fcf is None else fcf}
L407
L408         rows, mw = [], int(os.getenv("FIN_THREADS","8"))
L409         with ThreadPoolExecutor(max_workers=mw) as ex:
L410             rows=[f.result() for f in as_completed(ex.submit(one,t) for t in tickers)]
L411         return pd.DataFrame(rows).set_index("ticker")
L412
L413     _FINN_CFO_KEYS = ["netCashProvidedByOperatingActivities","netCashFromOperatingActivities","cashFlowFromOperatingActivities","operatingCashFlow"]
L414     _FINN_CAPEX_KEYS = ["capitalExpenditure","capitalExpenditures","purchaseOfPPE","investmentsInPropertyPlantAndEquipment"]
L415
L416     @staticmethod
L417     def _first_key(d: dict, keys: list[str]):
L418         for k in keys:
L419             if k in d and d[k] is not None: return d[k]
L420         return None
L421
L422     @staticmethod
L423     def _finn_get(session: requests.Session, url: str, params: dict, retries: int=3, sleep_s: float=0.5):
L424         for i in range(retries):
L425             r = session.get(url, params=params, timeout=15)
L426             if r.status_code==429: time.sleep(min(2**i*sleep_s,4.0)); continue
L427             r.raise_for_status(); return r.json()
L428         r.raise_for_status()
L429
L430     def fetch_cfo_capex_ttm_finnhub(self, tickers: list[str], api_key: str|None=None) -> pd.DataFrame:
L431         api_key = api_key or os.getenv("FINNHUB_API_KEY")
L432         if not api_key: raise ValueError("Finnhub API key not provided. Set FINNHUB_API_KEY or pass api_key=")
L433         base, s, rows = "https://finnhub.io/api/v1", requests.Session(), []
L434         for sym in tickers:
L435             cfo_ttm = capex_ttm = None
L436             try:
L437                 j = self._finn_get(s, f"{base}/stock/cash-flow", {"symbol":sym,"frequency":"quarterly","limit":8,"token":api_key})
L438                 arr = j.get("cashFlow") or []; cfo_vals, capex_vals = [], []
L439                 for item in arr[:4]:
L440                     cfo_vals.append(self._first_key(item,self._FINN_CFO_KEYS)); capex_vals.append(self._first_key(item,self._FINN_CAPEX_KEYS))
L441                 if any(v is not None for v in cfo_vals): cfo_ttm = float(np.nansum([np.nan if v is None else float(v) for v in cfo_vals]))
L442                 if any(v is not None for v in capex_vals): capex_ttm = float(np.nansum([np.nan if v is None else float(v) for v in capex_vals]))
L443             except Exception: pass
L444             if cfo_ttm is None or capex_ttm is None:
L445                 try:
L446                     j = self._finn_get(s, f"{base}/stock/cash-flow", {"symbol":sym,"frequency":"annual","limit":1,"token":api_key})
L447                     arr = j.get("cashFlow") or []
L448                     if arr:
L449                         item0 = arr[0]
L450                         if cfo_ttm is None:
L451                             v = self._first_key(item0,self._FINN_CFO_KEYS)
L452                             if v is not None: cfo_ttm = float(v)
L453                         if capex_ttm is None:
L454                             v = self._first_key(item0,self._FINN_CAPEX_KEYS)
L455                             if v is not None: capex_ttm = float(v)
L456                 except Exception: pass
L457             rows.append({"ticker":sym,"cfo_ttm_fh":np.nan if cfo_ttm is None else cfo_ttm,"capex_ttm_fh":np.nan if capex_ttm is None else capex_ttm})
L458         return pd.DataFrame(rows).set_index("ticker")
L459
L460     def compute_fcf_with_fallback(self, tickers: list[str], finnhub_api_key: str|None=None) -> pd.DataFrame:
L461         yf_df = self.fetch_cfo_capex_ttm_yf(tickers)
L462         T.log("financials (yf) done")
L463         miss_mask = yf_df[["cfo_ttm_yf","capex_ttm_yf","fcf_ttm_yf_direct"]].isna().any(axis=1)
L464         need = yf_df.index[miss_mask].tolist(); print(f"[T] yf financials missing: {len(need)} {need[:10]}{'...' if len(need)>10 else ''}")
L465         if need:
L466             fh_df = self.fetch_cfo_capex_ttm_finnhub(need, api_key=finnhub_api_key)
L467             df = yf_df.join(fh_df, how="left")
L468             for col_yf, col_fh in [("cfo_ttm_yf","cfo_ttm_fh"),("capex_ttm_yf","capex_ttm_fh")]:
L469                 df[col_yf] = df[col_yf].fillna(df[col_fh])
L470             print("[T] financials (finnhub) done (fallback only)")
L471         else:
L472             df = yf_df.assign(cfo_ttm_fh=np.nan, capex_ttm_fh=np.nan)
L473             print("[T] financials (finnhub) skipped (no missing)")
L474         df["cfo_ttm"]  = df["cfo_ttm_yf"].where(df["cfo_ttm_yf"].notna(), df["cfo_ttm_fh"])
L475         df["capex_ttm"] = df["capex_ttm_yf"].where(df["capex_ttm_yf"].notna(), df["capex_ttm_fh"])
L476         cfo, capex = pd.to_numeric(df["cfo_ttm"], errors="coerce"), pd.to_numeric(df["capex_ttm"], errors="coerce").abs()
L477         fcf_calc = cfo - capex
L478         fcf_direct = pd.to_numeric(df.get("fcf_ttm_yf_direct"), errors="coerce")
L479         df["fcf_ttm"] = fcf_calc.where(fcf_calc.notna(), fcf_direct)
L480         df["cfo_source"]  = np.where(df["cfo_ttm_yf"].notna(),"yfinance",np.where(df["cfo_ttm_fh"].notna(),"finnhub",""))
L481         df["capex_source"] = np.where(df["capex_ttm_yf"].notna(),"yfinance",np.where(df["capex_ttm_fh"].notna(),"finnhub",""))
L482         df["fcf_imputed"] = df[["cfo_ttm","capex_ttm"]].isna().any(axis=1) & df["fcf_ttm"].notna()
L483         cols = ["cfo_ttm_yf","capex_ttm_yf","cfo_ttm_fh","capex_ttm_fh","cfo_ttm","capex_ttm","fcf_ttm","fcf_ttm_yf_direct","cfo_source","capex_source","fcf_imputed"]
L484         return df[cols].sort_index()
L485
L486     def _build_eps_df(self, tickers, tickers_bulk, info):
L487         eps_rows=[]
L488         for t in tickers:
L489             info_t, eps_ttm, eps_q = info[t], info[t].get("trailingEps", np.nan), np.nan
L490             try:
L491                 qearn, so = tickers_bulk.tickers[t].quarterly_earnings, info_t.get("sharesOutstanding")
L492                 if so and qearn is not None and not qearn.empty and "Earnings" in qearn.columns:
L493                     eps_ttm_q = qearn["Earnings"].head(4).sum()/so
L494                     if pd.notna(eps_ttm_q) and (pd.isna(eps_ttm) or (abs(eps_ttm)>0 and abs(eps_ttm/eps_ttm_q)>3)): eps_ttm = eps_ttm_q
L495                     eps_q = qearn["Earnings"].iloc[-1]/so
L496             except Exception: pass
L497             eps_rows.append({"ticker":t,"eps_ttm":eps_ttm,"eps_q_recent":eps_q})
L498         return self.impute_eps_ttm(pd.DataFrame(eps_rows).set_index("ticker"))
L499
L500     def prepare_data(self):
L501         """Fetch price and fundamental data for all tickers."""
L502         cand_info = yf.Tickers(" ".join(self.cand)); cand_prices = {}
L503         for t in self.cand:
L504             try: cand_prices[t] = cand_info.tickers[t].fast_info.get("lastPrice", np.inf)
L505             except Exception as e: print(f"{t}: price fetch failed ({e})"); cand_prices[t] = np.inf
L506         cand_f = [t for t,p in cand_prices.items() if p<=self.price_max]
L507         T.log("price cap filter done (CAND_PRICE_MAX)")
L508         tickers = sorted(set(self.exist + cand_f))
L509         T.log(f"universe prepared: unique={len(tickers)} bench={self.bench}")
L510         data = yf.download(tickers + [self.bench], period="600d",
L511                            auto_adjust=True, progress=False, threads=False)
L512         T.log("yf.download done")
L513         px = data["Close"].dropna(how="all", axis=1).ffill(limit=2)
L514         spx = data["Close"][self.bench].reindex(px.index).ffill()
L515         clip_days = int(os.getenv("PRICE_CLIP_DAYS", "0"))   # 0ãªã‚‰ç„¡åŠ¹ï¼ˆæ—¢å®šï¼‰
L516         if clip_days > 0:
L517             px  = px.tail(clip_days + 1)
L518             spx = spx.tail(clip_days + 1)
L519             logger.info("[T] price window clipped by env: %d rows (PRICE_CLIP_DAYS=%d)", len(px), clip_days)
L520         else:
L521             logger.info("[T] price window clip skipped; rows=%d", len(px))
L522         tickers_bulk, info = yf.Tickers(" ".join(tickers)), {}
L523         for t in tickers:
L524             try:
L525                 info[t] = tickers_bulk.tickers[t].info
L526             except Exception as e:
L527                 logger.info("[warn] %s: info fetch failed (%s)", t, e)
L528                 info[t] = {}
L529         eps_df = self._build_eps_df(tickers, tickers_bulk, info)
L530         fcf_df = self.compute_fcf_with_fallback(tickers, finnhub_api_key=self.api_key)
L531         T.log("eps/fcf prep done")
L532         returns = px[tickers].pct_change()
L533         T.log("price prep/returns done")
L534         return dict(cand=cand_f, tickers=tickers, data=data, px=px, spx=spx, tickers_bulk=tickers_bulk, info=info, eps_df=eps_df, fcf_df=fcf_df, returns=returns)
L535
L536 # === Selectorï¼šç›¸é–¢ä½æ¸›ãƒ»é¸å®šï¼ˆã‚¹ã‚³ã‚¢ï¼†ãƒªã‚¿ãƒ¼ãƒ³ã ã‘èª­ã‚€ï¼‰ ===
L537 class Selector:
L538     # ---- DRRS helpersï¼ˆSelectorå°‚ç”¨ï¼‰ ----
L539     @staticmethod
L540     def _z_np(X: np.ndarray) -> np.ndarray:
L541         X = np.asarray(X, dtype=np.float32); m = np.nanmean(X, axis=0, keepdims=True); s = np.nanstd(X, axis=0, keepdims=True)+1e-9
L542         return (np.nan_to_num(X)-m)/s
L543
L544     @classmethod
L545     def residual_corr(cls, R: np.ndarray, n_pc: int=3, shrink: float=0.1) -> np.ndarray:
L546         Z = cls._z_np(R); U,S,_ = np.linalg.svd(Z, full_matrices=False); F = U[:,:n_pc]*S[:n_pc]; B = np.linalg.lstsq(F, Z, rcond=None)[0]
L547         E = Z - F@B; C = np.corrcoef(E, rowvar=False)
L548         off = C - np.diag(np.diag(C)); iu = np.triu_indices_from(off,1); avg_abs = np.nanmean(np.abs(off[iu])) if iu[0].size else 0.0
L549         shrink_eff = float(np.clip(shrink + 0.5*avg_abs, 0.1, 0.6)); N = C.shape[0]
L550         return (1.0 - shrink_eff)*C + shrink_eff*np.eye(N, dtype=C.dtype)
L551
L552     @classmethod
L553     def rrqr_like_det(cls, R: np.ndarray, score: np.ndarray, k: int, gamma: float=1.0):
L554         Z, w = cls._z_np(R), (score-score.min())/(np.ptp(score)+1e-12); X = Z*(1.0 + gamma*w)
L555         N, k = X.shape[1], max(0, min(k, X.shape[1]))
L556         if k==0: return []
L557         S, selected, Rres = [], np.zeros(N, dtype=bool), X.copy()
L558         for _ in range(k):
L559             norms = (Rres*Rres).sum(axis=0); cand = np.where(~selected)[0]
L560             j = sorted(cand, key=lambda c:(-norms[c], -w[c], c))[0]
L561             S.append(j); selected[j]=True; u = X[:,j:j+1]; u/=(np.linalg.norm(u)+1e-12); Rres = Rres - u @ (u.T @ Rres)
L562         return sorted(S)
L563
L564     @staticmethod
L565     def _obj(corrM: np.ndarray, score: np.ndarray, idx, lam: float) -> float:
L566         idx = list(idx); P = corrM[np.ix_(idx, idx)]; s = (score-score.mean())/(score.std()+1e-9)
L567         return float(s[idx].sum() - lam*((P.sum()-np.trace(P))/2.0))
L568
L569     @classmethod
L570     def swap_local_det(cls, corrM: np.ndarray, score: np.ndarray, idx, lam: float=0.6, max_pass: int=15):
L571         S, best, improved, passes = sorted(idx), cls._obj(corrM, score, idx, lam), True, 0
L572         while improved and passes<max_pass:
L573             improved, passes = False, passes+1
L574             for i,out in enumerate(list(S)):
L575                 for inn in range(len(score)):
L576                     if inn in S: continue
L577                     cand = sorted(S[:i]+[inn]+S[i+1:]); v = cls._obj(corrM, score, cand, lam)
L578                     if v>best+1e-10: S, best, improved = cand, v, True; break
L579                 if improved: break
L580         return S, best
L581
L582     @staticmethod
L583     def _obj_with_cross(C_within: np.ndarray, C_cross: np.ndarray|None, score: np.ndarray, idx, lam: float, mu: float) -> float:
L584         idx = list(idx); P = C_within[np.ix_(idx, idx)]; s = (score-score.mean())/(score.std()+1e-9)
L585         within = (P.sum()-np.trace(P))/2.0; cross = 0.0
L586         if C_cross is not None and C_cross.size>0: cross = C_cross[idx,:].sum()
L587         return float(s[idx].sum() - lam*within - mu*cross)
L588
L589     @classmethod
L590     def swap_local_det_cross(cls, C_within: np.ndarray, C_cross: np.ndarray|None, score: np.ndarray, idx, lam: float=0.6, mu: float=0.3, max_pass: int=15):
L591         S, best, improved, passes, N = sorted(idx), cls._obj_with_cross(C_within,C_cross,score,idx,lam,mu), True, 0, len(score)
L592         while improved and passes<max_pass:
L593             improved, passes = False, passes+1
L594             for i,out in enumerate(list(S)):
L595                 for inn in range(N):
L596                     if inn in S: continue
L597                     cand = sorted(S[:i]+[inn]+S[i+1:]); v = cls._obj_with_cross(C_within,C_cross,score,cand,lam,mu)
L598                     if v>best+1e-10: S, best, improved = cand, v, True; break
L599                 if improved: break
L600         return S, best
L601
L602     @staticmethod
L603     def avg_corr(C: np.ndarray, idx) -> float:
L604         k = len(idx); P = C[np.ix_(idx, idx)]
L605         return float((P.sum()-np.trace(P))/(k*(k-1)+1e-12))
L606
L607     @classmethod
L608     def select_bucket_drrs(cls, returns_df: pd.DataFrame, score_ser: pd.Series, pool_tickers: list[str], k: int, *, n_pc: int, gamma: float, lam: float, lookback: int, shrink: float=0.10, g_fixed_tickers: list[str]|None=None, mu: float=0.0):
L609         g_fixed = [t for t in (g_fixed_tickers or []) if t in returns_df.columns]
L610         union = [t for t in pool_tickers if t in returns_df.columns]
L611         for t in g_fixed:
L612             if t not in union: union.append(t)
L613         Rdf_all = returns_df[union]; Rdf_all = Rdf_all.iloc[-lookback:] if len(Rdf_all)>=lookback else Rdf_all; Rdf_all = Rdf_all.dropna()
L614         pool_eff, g_eff = [t for t in pool_tickers if t in Rdf_all.columns], [t for t in g_fixed if t in Rdf_all.columns]
L615         if len(pool_eff)==0: return dict(idx=[], tickers=[], avg_res_corr=np.nan, sum_score=0.0, objective=-np.inf)
L616         score = score_ser.reindex(pool_eff).to_numpy(dtype=np.float32)
L617         C_all = cls.residual_corr(Rdf_all.to_numpy(), n_pc=n_pc, shrink=shrink)
L618         col_pos = {c:i for i,c in enumerate(Rdf_all.columns)}; pool_pos = [col_pos[t] for t in pool_eff]
L619         C_within, C_cross = C_all[np.ix_(pool_pos,pool_pos)], None
L620         if len(g_eff)>0 and mu>0.0:
L621             g_pos = [col_pos[t] for t in g_eff]; C_cross = C_all[np.ix_(pool_pos,g_pos)]
L622         R_pool = Rdf_all[pool_eff].to_numpy(); S0 = cls.rrqr_like_det(R_pool, score, k, gamma=gamma)
L623         S, Jn = (cls.swap_local_det_cross(C_within, C_cross, score, S0, lam=lam, mu=mu, max_pass=15) if C_cross is not None else cls.swap_local_det(C_within, score, S0, lam=lam, max_pass=15))
L624         selected_tickers = [pool_eff[i] for i in S]
L625         return dict(idx=S, tickers=selected_tickers, avg_res_corr=cls.avg_corr(C_within,S), sum_score=float(score[S].sum()), objective=float(Jn))
L626
L627     # ---- é¸å®šï¼ˆã‚¹ã‚³ã‚¢ Series / returns ã ã‘ã‚’å—ã‘ã‚‹ï¼‰----
L628 # === Outputï¼šå‡ºåŠ›æ•´å½¢ã¨é€ä¿¡ï¼ˆè¡¨ç¤ºãƒ»Slackï¼‰ ===
L629 class Output:
L630
L631     def __init__(self, debug=None):
L632         # self.debug ã¯ä½¿ã‚ãªã„ï¼ˆäº’æ›ã®ãŸã‚å¼•æ•°ã¯å—ã‘ã‚‹ãŒç„¡è¦–ï¼‰
L633         self.miss_df = self.g_table = self.d_table = self.io_table = self.df_metrics_fmt = self.debug_table = None
L634         self.g_title = self.d_title = ""
L635         self.g_formatters = self.d_formatters = {}
L636         # ä½ã‚¹ã‚³ã‚¢ï¼ˆGSC+DSCï¼‰Top10 è¡¨ç¤º/é€ä¿¡ç”¨
L637         self.low10_table = None
L638         self.debug_text = ""   # ãƒ‡ãƒãƒƒã‚°ç”¨æœ¬æ–‡ã¯ã“ã“ã«ä¸€æœ¬åŒ–
L639         self._debug_logged = False
L640
L641     # --- è¡¨ç¤ºï¼ˆå…ƒ display_results ã®ãƒ­ã‚¸ãƒƒã‚¯ãã®ã¾ã¾ï¼‰ ---
L642     def display_results(self, *, exist, bench, df_z, g_score, d_score_all,
L643                         init_G, init_D, top_G, top_D, **kwargs):
L644         logger.info("ğŸ“Œ reached display_results")
L645         pd.set_option('display.float_format','{:.3f}'.format)
L646         print("ğŸ“ˆ ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼åˆ†æ•£æœ€é©åŒ–ã®çµæœ")
L647         if self.miss_df is not None and not self.miss_df.empty:
L648             print("Missing Data:")
L649             print(self.miss_df.to_string(index=False))
L650
L651         # ---- è¡¨ç¤ºç”¨ï¼šChanges/Near-Miss ã®ã‚¹ã‚³ã‚¢æºã‚’â€œæœ€çµ‚é›†è¨ˆâ€ã«çµ±ä¸€ã™ã‚‹ãƒ—ãƒ­ã‚­ã‚· ----
L652         try:
L653             sc = getattr(self, "_sc", None)
L654             agg_G = getattr(sc, "_agg_G", None)
L655             agg_D = getattr(sc, "_agg_D", None)
L656         except Exception:
L657             sc = agg_G = agg_D = None
L658         class _SeriesProxy:
L659             __slots__ = ("primary", "fallback")
L660             def __init__(self, primary, fallback): self.primary, self.fallback = primary, fallback
L661             def get(self, key, default=None):
L662                 try:
L663                     v = self.primary.get(key) if hasattr(self.primary, "get") else None
L664                     if v is not None and not (isinstance(v, float) and v != v):
L665                         return v
L666                 except Exception:
L667                     pass
L668                 try:
L669                     return self.fallback.get(key) if hasattr(self.fallback, "get") else default
L670                 except Exception:
L671                     return default
L672         g_score = _SeriesProxy(agg_G, g_score)
L673         d_score_all = _SeriesProxy(agg_D, d_score_all)
L674         near_G = getattr(sc, "_near_G", []) if sc else []
L675         near_D = getattr(sc, "_near_D", []) if sc else []
L676
L677         extra_G = [t for t in init_G if t not in top_G][:5]; G_UNI = top_G + extra_G
L678         gsc_series = pd.Series({t: g_score.get(t) for t in G_UNI}, name='GSC')
L679         self.g_table = pd.concat([df_z.loc[G_UNI,['GROWTH_F','MOM','TRD','VOL']], gsc_series], axis=1)
L680         self.g_table.index = [t + ("â­ï¸" if t in top_G else "") for t in G_UNI]
L681         self.g_formatters = {col:"{:.2f}".format for col in ['GROWTH_F','MOM','TRD','VOL']}; self.g_formatters['GSC'] = "{:.3f}".format
L682         self.g_title = (f"[Gæ  / {N_G} / {_fmt_w(g_weights)} / corrM={corrM} / "
L683                         f"LB={DRRS_G['lookback']} nPC={DRRS_G['n_pc']} Î³={DRRS_G['gamma']} Î»={DRRS_G['lam']} Î·={DRRS_G['eta']} shrink={DRRS_SHRINK}]")
L684         if near_G:
L685             add = [t for t in near_G if t not in set(G_UNI)][:10]
L686             if len(add) < 10:
L687                 try:
L688                     aggG = getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False)
L689                     out_now = sorted(set(exist) - set(top_G + top_D))  # ä»Šå› OUT
L690                     used = set(G_UNI + add)
L691                     def _push(lst):
L692                         nonlocal add, used
L693                         for t in lst:
L694                             if len(add) == 10: break
L695                             if t in aggG.index and t not in used:
L696                                 add.append(t); used.add(t)
L697                     _push(out_now)           # â‘  ä»Šå› OUT ã‚’å„ªå…ˆ
L698                     _push(list(aggG.index))  # â‘¡ ã¾ã è¶³ã‚Šãªã‘ã‚Œã°ä¸Šä½ã§å……å¡«
L699                 except Exception:
L700                     pass
L701             if add:
L702                 near_tbl = pd.concat([df_z.loc[add,['GROWTH_F','MOM','TRD','VOL']], pd.Series({t: g_score.get(t) for t in add}, name='GSC')], axis=1)
L703                 self.g_table = pd.concat([self.g_table, near_tbl], axis=0)
L704         print(self.g_title); print(self.g_table.to_string(formatters=self.g_formatters))
L705
L706         extra_D = [t for t in init_D if t not in top_D][:5]; D_UNI = top_D + extra_D
L707         cols_D = ['QAL','YLD','VOL','TRD']; d_disp = pd.DataFrame(index=D_UNI)
L708         d_disp['QAL'], d_disp['YLD'], d_disp['VOL'], d_disp['TRD'] = df_z.loc[D_UNI,'D_QAL'], df_z.loc[D_UNI,'D_YLD'], df_z.loc[D_UNI,'D_VOL_RAW'], df_z.loc[D_UNI,'D_TRD']
L709         dsc_series = pd.Series({t: d_score_all.get(t) for t in D_UNI}, name='DSC')
L710         self.d_table = pd.concat([d_disp, dsc_series], axis=1); self.d_table.index = [t + ("â­ï¸" if t in top_D else "") for t in D_UNI]
L711         self.d_formatters = {col:"{:.2f}".format for col in cols_D}; self.d_formatters['DSC']="{:.3f}".format
L712         import scorer
L713         dw_eff = scorer.D_WEIGHTS_EFF
L714         self.d_title = (f"[Dæ  / {N_D} / {_fmt_w(dw_eff)} / corrM={corrM} / "
L715                         f"LB={DRRS_D['lookback']} nPC={DRRS_D['n_pc']} Î³={DRRS_D['gamma']} Î»={DRRS_D['lam']} Î¼={CROSS_MU_GD} Î·={DRRS_D['eta']} shrink={DRRS_SHRINK}]")
L716         if near_D:
L717             add = [t for t in near_D if t not in set(D_UNI)][:10]
L718             if add:
L719                 d_disp2 = pd.DataFrame(index=add)
L720                 d_disp2['QAL'], d_disp2['YLD'], d_disp2['VOL'], d_disp2['TRD'] = df_z.loc[add,'D_QAL'], df_z.loc[add,'D_YLD'], df_z.loc[add,'D_VOL_RAW'], df_z.loc[add,'D_TRD']
L721                 near_tbl = pd.concat([d_disp2, pd.Series({t: d_score_all.get(t) for t in add}, name='DSC')], axis=1)
L722                 self.d_table = pd.concat([self.d_table, near_tbl], axis=0)
L723         print(self.d_title); print(self.d_table.to_string(formatters=self.d_formatters))
L724
L725         # === Changesï¼ˆIN ã® GSC/DSC ã‚’è¡¨ç¤ºã€‚OUT ã¯éŠ˜æŸ„åã®ã¿ï¼‰ ===
L726         in_list = sorted(set(list(top_G)+list(top_D)) - set(exist))
L727         out_list = sorted(set(exist) - set(list(top_G)+list(top_D)))
L728
L729         self.io_table = pd.DataFrame({
L730             'IN': pd.Series(in_list),
L731             '/ OUT': pd.Series(out_list)
L732         })
L733         g_list = [f"{g_score.get(t):.3f}" if pd.notna(g_score.get(t)) else 'â€”' for t in out_list]
L734         d_list = [f"{d_score_all.get(t):.3f}" if pd.notna(d_score_all.get(t)) else 'â€”' for t in out_list]
L735         self.io_table['GSC'] = pd.Series(g_list)
L736         self.io_table['DSC'] = pd.Series(d_list)
L737
L738         print("Changes:")
L739         print(self.io_table.to_string(index=False))
L740
L741         all_tickers = list(set(exist + list(top_G) + list(top_D) + [bench])); prices = yf.download(all_tickers, period='1y', auto_adjust=True, progress=False, threads=False)['Close'].ffill(limit=2)
L742         ret = prices.pct_change(); portfolios = {'CUR':exist,'NEW':list(top_G)+list(top_D)}; metrics={}
L743         for name,ticks in portfolios.items():
L744             pr = ret[ticks].mean(axis=1, skipna=True).dropna(); cum = (1+pr).cumprod()-1; n = len(pr)
L745             if n>=252: ann_ret, ann_vol = (1+cum.iloc[-1])**(252/n)-1, pr.std()*np.sqrt(252)
L746             else: ann_ret, ann_vol = cum.iloc[-1], pr.std()*np.sqrt(n)
L747             sharpe, drawdown = ann_ret/ann_vol, (cum - cum.cummax()).min()
L748             if len(ticks)>=2:
L749                 C_raw = ret[ticks].corr(); RAW_rho = C_raw.mask(np.eye(len(ticks), dtype=bool)).stack().mean()
L750                 R = ret[ticks].dropna().to_numpy(); C_resid = Selector.residual_corr(R, n_pc=3, shrink=DRRS_SHRINK)
L751                 RESID_rho = float((C_resid.sum()-np.trace(C_resid))/(C_resid.shape[0]*(C_resid.shape[0]-1)))
L752             else: RAW_rho = RESID_rho = np.nan
L753             divy = ttm_div_yield_portfolio(ticks); metrics[name] = {'RET':ann_ret,'VOL':ann_vol,'SHP':sharpe,'MDD':drawdown,'RAWÏ':RAW_rho,'RESIDÏ':RESID_rho,'DIVY':divy}
L754         df_metrics = pd.DataFrame(metrics).T; df_metrics_pct = df_metrics.copy(); self.df_metrics = df_metrics
L755         for col in ['RET','VOL','MDD','DIVY']: df_metrics_pct[col] = df_metrics_pct[col]*100
L756         cols_order = ['RET','VOL','SHP','MDD','RAWÏ','RESIDÏ','DIVY']; df_metrics_pct = df_metrics_pct.reindex(columns=cols_order)
L757         def _fmt_row(s):
L758             return pd.Series({'RET':f"{s['RET']:.1f}%",'VOL':f"{s['VOL']:.1f}%",'SHP':f"{s['SHP']:.1f}",'MDD':f"{s['MDD']:.1f}%",'RAWÏ':(f"{s['RAWÏ']:.2f}" if pd.notna(s['RAWÏ']) else "NaN"),'RESIDÏ':(f"{s['RESIDÏ']:.2f}" if pd.notna(s['RESIDÏ']) else "NaN"),'DIVY':f"{s['DIVY']:.1f}%"})
L759         self.df_metrics_fmt = df_metrics_pct.apply(_fmt_row, axis=1); print("Performance Comparison:"); print(self.df_metrics_fmt.to_string())
L760         # === è¿½åŠ : GSC+DSC ãŒä½ã„é † TOP10 ===
L761         try:
L762             all_scores = pd.DataFrame({'GSC': df_z['GSC'], 'DSC': df_z['DSC']}).copy()
L763             all_scores['G_plus_D'] = all_scores['GSC'] + all_scores['DSC']
L764             all_scores = all_scores.dropna(subset=['G_plus_D'])
L765             self.low10_table = all_scores.sort_values('G_plus_D', ascending=True).head(10).round(3)
L766             print("Low Score Candidates (GSC+DSC bottom 10):")
L767             print(self.low10_table.to_string())
L768             try:
L769                 df_full_src = getattr(getattr(self, "_sc", None), "_feat", None)
L770                 df_full = getattr(df_full_src, "df_full", None) or kwargs.get("df_full")
L771                 df_full_z_pass = getattr(df_full_src, "df_full_z", None) or kwargs.get("df_full_z")
L772                 fb_like = SimpleNamespace(
L773                     df_full=df_full,
L774                     df_z=df_z,
L775                     df_full_z=df_full_z_pass,
L776                     g_score=g_score,
L777                     d_score_all=d_score_all,
L778                     missing_logs=self.miss_df,
L779                 )
L780                 sb_like = SimpleNamespace(top_G=top_G, top_D=top_D)
L781                 if debug_mode:
L782                     logger.info("ğŸ“Œ entering debug output block")
L783                     try:
L784                         # æ—¢ã« self.debug_text ãŒç”Ÿæˆæ¸ˆã¿ã§ã‚‚ä¸Šæ›¸ãã§OKï¼ˆå¸¸ã«æœ€æ–°ã‚’æ¡ç”¨ï¼‰
L785                         self.debug_text = _compact_debug(
L786                             fb_like, sb_like,
L787                             prevG=kwargs.get("prev_G", exist),
L788                             prevD=kwargs.get("prev_D", exist),
L789                             max_rows=int(os.getenv("DEBUG_MAX_ROWS", "120")),
L790                         )
L791
L792                         logger.info("ğŸ“Œ after _compact_debug call, length=%s", len(self.debug_text or ""))
L793
L794                         if not (self.debug_text or "").strip():
L795                             src_df = df_full or df_full_z_pass or df_z
L796                             if src_df is not None and getattr(src_df, "empty", False) is False:
L797                                 core = [c for c in [
L798                                     "GRW","TR_EPS","TR_REV","EPS_Q_YOY","REV_Q_YOY","REV_YOY_ACC",
L799                                     "RULE40","FCF_MGN","GSC","DSC"
L800                                 ] if c in src_df.columns]
L801                                 if core:
L802                                     pick: list[str] = []
L803                                     for t in (self.g_table, self.d_table, self.low10_table):
L804                                         if t is not None and getattr(t, "empty", False) is False:
L805                                             pick += [i for i in list(t.index) if i in src_df.index]
L806                                     view = (src_df.loc[pick, core] if pick else src_df[core]).head(int(os.getenv("DEBUG_MAX_ROWS", "80")))
L807                                     self.debug_text = "DEBUG fallback\n" + view.to_string()
L808                                 else:
L809                                     self.debug_text = "(no debug columns)"
L810                             else:
L811                                 self.debug_text = "(no dataframe)"
L812                         dt = (self.debug_text or "").strip()
L813                         logger.info("===== DEBUG (after Low Score) START =====")
L814                         if dt:
L815                             logger.info("\n%s", dt)
L816                         else:
L817                             logger.info("<empty debug_text>")
L818                         logger.info("===== DEBUG (after Low Score) END =====")
L819
L820                     except Exception as e:
L821                         logger.warning("debug output failed: %s", e)
L822             except Exception as e:
L823                 logger.warning("debug dump failed: %s", e)
L824         except Exception as e:
L825             print(f"[warn] low-score ranking failed: {e}")
L826             self.low10_table = None
L827         if not debug_mode:
L828             self.debug_text = ""
L829         if debug_mode:
L830             self._debug_logged = True
L831         else:
L832             logger.debug(
L833                 "skip debug log: debug_mode=%s debug_text_empty=%s",
L834                 debug_mode, not bool((self.debug_text or '').strip())
L835             )
L836             self._debug_logged = True
L837
L838     # --- Slacké€ä¿¡ï¼ˆå…ƒ notify_slack ã®ãƒ­ã‚¸ãƒƒã‚¯ãã®ã¾ã¾ï¼‰ ---
L839     def notify_slack(self):
L840         SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")
L841
L842         if not SLACK_WEBHOOK_URL:
L843             print("âš ï¸ SLACK_WEBHOOK_URL not set (main report skipped)")
L844             return
L845
L846         def _filter_suffix_from(spec: dict, group: str) -> str:
L847             g = spec.get(group, {})
L848             parts = [str(m) for m in g.get("pre_mask", [])]
L849             for k, v in (g.get("pre_filter", {}) or {}).items():
L850                 base, op = (k[:-4], "<") if k.endswith("_max") else ((k[:-4], ">") if k.endswith("_min") else (k, "="))
L851                 name = {"beta": "Î²"}.get(base, base)
L852                 try:
L853                     val = f"{float(v):g}"
L854                 except Exception:
L855                     val = str(v)
L856                 parts.append(f"{name}{op}{val}")
L857             return "" if not parts else " / filter:" + " & ".join(parts)
L858
L859         def _inject_filter_suffix(title: str, group: str) -> str:
L860             suf = _filter_suffix_from(FILTER_SPEC, group)
L861             return f"{title[:-1]}{suf}]" if suf and title.endswith("]") else (title + suf)
L862
L863         def _blk(title, tbl, fmt=None, drop=()):
L864             if tbl is None or getattr(tbl, 'empty', False):
L865                 return f"{title}\n(é¸å®šãªã—)\n"
L866             if drop and hasattr(tbl, 'columns'):
L867                 keep = [c for c in tbl.columns if c not in drop]
L868                 tbl, fmt = tbl[keep], {k: v for k, v in (fmt or {}).items() if k in keep}
L869             return f"{title}\n```{tbl.to_string(formatters=fmt)}```\n"
L870
L871         message = "ğŸ“ˆ ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼åˆ†æ•£æœ€é©åŒ–ã®çµæœ\n"
L872         if self.miss_df is not None and not self.miss_df.empty:
L873             message += "Missing Data\n```" + self.miss_df.to_string(index=False) + "```\n"
L874         message += _blk(_inject_filter_suffix(self.g_title, "G"), self.g_table, self.g_formatters, drop=("TRD",))
L875         message += _blk(_inject_filter_suffix(self.d_title, "D"), self.d_table, self.d_formatters)
L876         message += "Changes\n" + ("(å¤‰æ›´ãªã—)\n" if self.io_table is None or getattr(self.io_table, 'empty', False) else f"```{self.io_table.to_string(index=False)}```\n")
L877         message += "Performance Comparison:\n```" + self.df_metrics_fmt.to_string() + "```"
L878
L879         try:
L880             r = requests.post(SLACK_WEBHOOK_URL, json={"text": message})
L881             print(f"[DBG] main_post status={getattr(r, 'status_code', None)} size={len(message)}")
L882             if r is not None:
L883                 r.raise_for_status()
L884         except Exception as e:
L885             print(f"[ERR] main_post_failed: {e}")
L886
L887 def _infer_g_universe(feature_df, selected12=None, near5=None):
L888     try:
L889         out = feature_df.index[feature_df['group'].astype(str).str.upper().eq('G')].tolist()
L890         if out: return out
L891     except Exception:
L892         pass
L893     base = set()
L894     for lst in (selected12 or []), (near5 or []):
L895         for x in (lst or []): base.add(x)
L896     return list(base) if base else list(feature_df.index)
L897
L898 def _fmt_with_fire_mark(tickers, feature_df):
L899     out = []
L900     for t in tickers or []:
L901         try:
L902             br = bool(feature_df.at[t, "G_BREAKOUT_recent_5d"])
L903             pb = bool(feature_df.at[t, "G_PULLBACK_recent_5d"])
L904             out.append(f"{t}{' ğŸ”¥' if (br or pb) else ''}")
L905         except Exception:
L906             out.append(t)
L907     return out
L908
L909 def _label_recent_event(t, feature_df):
L910     try:
L911         br = bool(feature_df.at[t, "G_BREAKOUT_recent_5d"]); dbr = str(feature_df.at[t, "G_BREAKOUT_last_date"]) if br else ""
L912         pb = bool(feature_df.at[t, "G_PULLBACK_recent_5d"]); dpb = str(feature_df.at[t, "G_PULLBACK_last_date"]) if pb else ""
L913         if   br and not pb: return f"{t}ï¼ˆãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®š {dbr}ï¼‰"
L914         elif pb and not br: return f"{t}ï¼ˆæŠ¼ã—ç›®åç™º {dpb}ï¼‰"
L915         elif br and pb:     return f"{t}ï¼ˆãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®š {dbr}ï¼æŠ¼ã—ç›®åç™º {dpb}ï¼‰"
L916     except Exception:
L917         pass
L918     return t
L919
L920 # === ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å¯è¦–åŒ–ï¼šG/Då…±é€šãƒ•ãƒ­ãƒ¼ï¼ˆå‡ºåŠ›ã¯ä¸å¤‰ï¼‰ ===
L921
L922 def io_build_input_bundle() -> InputBundle:
L923     """
L924     æ—¢å­˜ã®ã€ãƒ‡ãƒ¼ã‚¿å–å¾—â†’å‰å‡¦ç†ã€ã‚’å®Ÿè¡Œã—ã€InputBundle ã‚’è¿”ã™ã€‚
L925     å‡¦ç†å†…å®¹ãƒ»åˆ—åãƒ»ä¸¸ã‚ãƒ»ä¾‹å¤–ãƒ»ãƒ­ã‚°æ–‡è¨€ã¯ç¾è¡Œã©ãŠã‚Šï¼ˆå¤‰æ›´ç¦æ­¢ï¼‰ã€‚
L926     """
L927     state = Input(cand=cand, exist=exist, bench=bench, price_max=CAND_PRICE_MAX, finnhub_api_key=FINNHUB_API_KEY).prepare_data()
L928     return InputBundle(cand=state["cand"], tickers=state["tickers"], bench=bench, data=state["data"], px=state["px"], spx=state["spx"], tickers_bulk=state["tickers_bulk"], info=state["info"], eps_df=state["eps_df"], fcf_df=state["fcf_df"], returns=state["returns"])
L929
L930 def run_group(sc: Scorer, group: str, inb: InputBundle, cfg: PipelineConfig,
L931               n_target: int) -> tuple[list, float, float, float]:
L932     """
L933     G/Dã‚’åŒä¸€æ‰‹é †ã§å‡¦ç†ï¼šæ¡ç‚¹â†’ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼â†’é¸å®šï¼ˆç›¸é–¢ä½æ¸›è¾¼ã¿ï¼‰ã€‚
L934     æˆ»ã‚Šå€¤ï¼š(pick, avg_res_corr, sum_score, objective)
L935     JSONä¿å­˜ã¯æ—¢å­˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆã‚­ãƒ¼åãƒ»ä¸¸ã‚æ¡ãƒ»é †åºï¼‰ã‚’è¸è¥²ã€‚
L936     """
L937     sc.cfg = cfg
L938
L939     if hasattr(sc, "score_build_features"):
L940         feat = sc.score_build_features(inb)
L941         if not hasattr(sc, "_feat_logged"):
L942             T.log("features built (scorer)")
L943             sc._feat_logged = True
L944         agg = sc.score_aggregate(feat, group, cfg) if hasattr(sc, "score_aggregate") else feat
L945     else:
L946         fb = sc.aggregate_scores(inb, cfg)
L947         if not hasattr(sc, "_feat_logged"):
L948             T.log("features built (scorer)")
L949             sc._feat_logged = True
L950         sc._feat = fb
L951         agg = fb.g_score if group == "G" else fb.d_score_all
L952         if group == "D" and hasattr(fb, "df"):
L953             agg = agg[fb.df['BETA'] < D_BETA_MAX]
L954
L955     if hasattr(sc, "filter_candidates"):
L956         agg = agg[sc.filter_candidates(inb, agg, group, cfg)]
L957
L958     selector = Selector()
L959     if hasattr(sc, "select_diversified"):
L960         pick, avg_r, sum_sc, obj = sc.select_diversified(agg, group, cfg, n_target,
L961             selector=selector, prev_tickers=None,
L962             corrM=cfg.drrs.corrM, shrink=cfg.drrs.shrink,
L963             cross_mu=cfg.drrs.cross_mu_gd)
L964     else:
L965         if group == "G":
L966             init = agg.nlargest(min(cfg.drrs.corrM, len(agg))).index.tolist()
L967             res = selector.select_bucket_drrs(returns_df=inb.returns, score_ser=agg, pool_tickers=init, k=n_target,
L968                 n_pc=cfg.drrs.G.get("n_pc", 3), gamma=cfg.drrs.G.get("gamma", 1.2),
L969                 lam=cfg.drrs.G.get("lam", 0.68),
L970                 lookback=cfg.drrs.G.get("lookback", 252),
L971                 shrink=cfg.drrs.shrink, g_fixed_tickers=None, mu=0.0)
L972         else:
L973             init = agg.nlargest(min(cfg.drrs.corrM, len(agg))).index.tolist()
L974             g_fixed = getattr(sc, "_top_G", None)
L975             res = selector.select_bucket_drrs(returns_df=inb.returns, score_ser=agg, pool_tickers=init, k=n_target,
L976                 n_pc=cfg.drrs.D.get("n_pc", 4), gamma=cfg.drrs.D.get("gamma", 0.8),
L977                 lam=cfg.drrs.D.get("lam", 0.85),
L978                 lookback=cfg.drrs.D.get("lookback", 504),
L979                 shrink=cfg.drrs.shrink, g_fixed_tickers=g_fixed,
L980                 mu=cfg.drrs.cross_mu_gd)
L981         pick = res["tickers"]; avg_r = res["avg_res_corr"]
L982         sum_sc = res["sum_score"]; obj = res["objective"]
L983         if group == "D":
L984             _, pick = _disjoint_keepG(getattr(sc, "_top_G", []), pick, init)
L985             T.log("selection finalized (G/D)")
L986     try:
L987         inc = [t for t in exist if t in agg.index]
L988         pick = _sticky_keep_current(
L989             agg=agg, pick=pick, incumbents=inc, n_target=n_target,
L990             delta_z=SWAP_DELTA_Z, keep_buffer=SWAP_KEEP_BUFFER
L991         )
L992     except Exception as _e:
L993         print(f"[warn] sticky_keep_current skipped: {str(_e)}")
L994     # --- Near-Miss: æƒœã—ãã‚‚é¸ã°ã‚Œãªã‹ã£ãŸä¸Šä½10ã‚’ä¿æŒï¼ˆSlackè¡¨ç¤ºç”¨ï¼‰ ---
L995     # 5) Near-Miss ã¨æœ€çµ‚é›†è¨ˆSeriesã‚’ä¿æŒï¼ˆè¡¨ç¤ºå°‚ç”¨ã€‚è¨ˆç®—ã¸å½±éŸ¿ãªã—ï¼‰
L996     try:
L997         pool = agg.drop(index=[t for t in pick if t in agg.index], errors="ignore")
L998         near10 = list(pool.sort_values(ascending=False).head(10).index)
L999         setattr(sc, f"_near_{group}", near10)
L1000         setattr(sc, f"_agg_{group}", agg)
L1001     except Exception:
L1002         pass
L1003
L1004     if group == "D":
L1005         T.log("save done")
L1006     if group == "G":
L1007         sc._top_G = pick
L1008     return pick, avg_r, sum_sc, obj
L1009
L1010 def run_pipeline() -> SelectionBundle:
L1011     """
L1012     G/Då…±é€šãƒ•ãƒ­ãƒ¼ã®å…¥å£ã€‚I/Oã¯ã“ã“ã ã‘ã§å®Ÿæ–½ã—ã€è¨ˆç®—ã¯Scorerã«å§”è­²ã€‚
L1013     Slackæ–‡è¨€ãƒ»ä¸¸ã‚ãƒ»é †åºã¯æ—¢å­˜ã® Output ã‚’ç”¨ã„ã¦å¤‰æ›´ã—ãªã„ã€‚
L1014     """
L1015     inb = io_build_input_bundle()
L1016     cfg = PipelineConfig(weights=WeightsConfig(g=g_weights, d=D_weights),
L1017         drrs=DRRSParams(corrM=corrM, shrink=DRRS_SHRINK,
L1018                          G=DRRS_G, D=DRRS_D, cross_mu_gd=CROSS_MU_GD),
L1019         price_max=CAND_PRICE_MAX)
L1020     sc = Scorer()
L1021     top_G, avgG, sumG, objG = run_group(sc, "G", inb, cfg, N_G)
L1022     poolG = list(getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False).index)
L1023     alpha = Scorer.spx_to_alpha(inb.spx)
L1024     sectors = {t:(inb.info.get(t,{}).get("sector") or "U") for t in poolG}; scores = {t:Scorer.g_score.get(t,0.0) for t in poolG}
L1025     top_G = Scorer.pick_top_softcap(scores, sectors, N=N_G, cap=2, alpha=alpha, hard=5)
L1026     sc._top_G = top_G
L1027     try:
L1028         aggG = getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False)
L1029         sc._near_G = [t for t in aggG.index if t not in set(top_G)][:10]
L1030     except Exception:
L1031         pass
L1032     base = sum(Scorer.g_score.get(t,0.0) for t in poolG[:N_G])
L1033     effs = sum(Scorer.g_score.get(t,0.0) for t in top_G)
L1034     print(f"[soft_cap2] score_cost={(base-effs)/max(1e-9,abs(base)):.2%}, alpha={alpha:.3f}")
L1035     top_D, avgD, sumD, objD = run_group(sc, "D", inb, cfg, N_D)
L1036     fb = getattr(sc, "_feat", None)
L1037     near_G = getattr(sc, "_near_G", [])
L1038     selected12 = list(top_G)
L1039     df = fb.df if fb is not None else pd.DataFrame()
L1040     guni = _infer_g_universe(df, selected12, near_G)
L1041     try:
L1042         fire_recent = [t for t in guni
L1043                        if (str(df.at[t, "G_BREAKOUT_recent_5d"]) == "True") or
L1044                           (str(df.at[t, "G_PULLBACK_recent_5d"]) == "True")]
L1045     except Exception: fire_recent = []
L1046
L1047     lines = [
L1048         "ã€Gæ ãƒ¬ãƒãƒ¼ãƒˆï½œé€±æ¬¡ãƒ¢ãƒ‹ã‚¿ï¼ˆç›´è¿‘5å–¶æ¥­æ—¥ï¼‰ã€‘",
L1049         "ã€å‡¡ä¾‹ã€‘ğŸ”¥=ç›´è¿‘5å–¶æ¥­æ—¥å†…ã«ã€Œãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®šã€ã¾ãŸã¯ã€ŒæŠ¼ã—ç›®åç™ºã€ã‚’æ¤œçŸ¥",
L1050         f"é¸å®š{N_G}: {', '.join(_fmt_with_fire_mark(selected12, df))}" if selected12 else f"é¸å®š{N_G}: ãªã—",
L1051         f"æ¬¡ç‚¹10: {', '.join(_fmt_with_fire_mark(near_G, df))}" if near_G else "æ¬¡ç‚¹10: ãªã—",]
L1052
L1053     if fire_recent:
L1054         fire_list = ", ".join([_label_recent_event(t, df) for t in fire_recent])
L1055         lines.append(f"éå»5å–¶æ¥­æ—¥ã®æ¤œçŸ¥: {fire_list}")
L1056     else:
L1057         lines.append("éå»5å–¶æ¥­æ—¥ã®æ¤œçŸ¥: ãªã—")
L1058
L1059     try:
L1060         webhook = os.environ.get("SLACK_WEBHOOK_URL", "")
L1061         if webhook:
L1062             requests.post(webhook, json={"text": "\n".join([s for s in lines if s != ""])}, timeout=10)
L1063     except Exception:
L1064         pass
L1065
L1066     out = Output()
L1067     # è¡¨ç¤ºå´ã‹ã‚‰é¸å®šæ™‚ã®é›†è¨ˆã¸ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†ã«ä¿æŒï¼ˆè¡¨ç¤ºå°‚ç”¨ãƒ»å‰¯ä½œç”¨ãªã—ï¼‰
L1068     try: out._sc = sc
L1069     except Exception: pass
L1070     if hasattr(sc, "_feat"):
L1071         try:
L1072             fb = sc._feat
L1073             out.miss_df = fb.missing_logs
L1074             out.display_results(
L1075                 exist=exist,
L1076                 bench=bench,
L1077                 df_z=fb.df_z,
L1078                 g_score=fb.g_score,
L1079                 d_score_all=fb.d_score_all,
L1080                 init_G=top_G,
L1081                 init_D=top_D,
L1082                 top_G=top_G,
L1083                 top_D=top_D,
L1084                 df_full_z=getattr(fb, "df_full_z", None),
L1085                 prev_G=getattr(sc, "_prev_G", exist),
L1086                 prev_D=getattr(sc, "_prev_D", exist),
L1087             )
L1088         except Exception:
L1089             pass
L1090     out.notify_slack()
L1091     sb = SelectionBundle(resG={"tickers": top_G, "avg_res_corr": avgG,
L1092               "sum_score": sumG, "objective": objG},
L1093         resD={"tickers": top_D, "avg_res_corr": avgD,
L1094               "sum_score": sumD, "objective": objD},
L1095         top_G=top_G, top_D=top_D, init_G=top_G, init_D=top_D)
L1096
L1097     # --- Low Score Candidates (GSC+DSC bottom 10) : send before debug dump ---
L1098     try:
L1099         _low_df = (pd.DataFrame({"GSC": fb.g_score, "DSC": fb.d_score_all})
L1100               .assign(G_plus_D=lambda x: x["GSC"] + x["DSC"])
L1101               .sort_values("G_plus_D")
L1102               .head(10)
L1103               .round(3))
L1104         low_msg = "Low Score Candidates (GSC+DSC bottom 10)\n" + _low_df.to_string(index=True, index_names=False)
L1105         _post_slack({"text": f"```{low_msg}```"})
L1106     except Exception as _e:
L1107         _post_slack({"text": f"```Low Score Candidates: ä½œæˆå¤±æ•—: {_e}```"})
L1108
L1109     return sb
L1110
L1111 if __name__ == "__main__":
L1112     run_pipeline()
```

## <scorer.py>
```text
L1 # scorer.py
L2 # kawatest
L3 # =============================================================================
L4 # Scorer: ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼/æŒ‡æ¨™ã®ç”Ÿæˆã¨åˆæˆã‚¹ã‚³ã‚¢ç®—å‡ºã‚’æ‹…ã†ç´”ç²‹å±¤
L5 #
L6 # ã€ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã ã‘èª­ã‚ã°åˆ†ã‹ã‚‹ãƒã‚¤ãƒ³ãƒˆã€‘
L7 # - å…¥åŠ›(InputBundle)ã¯ã€Œä¾¡æ ¼/å‡ºæ¥é«˜/ãƒ™ãƒ³ãƒ/åŸºæœ¬æƒ…å ±/EPS/FCF/ãƒªã‚¿ãƒ¼ãƒ³ã€ã‚’å«ã‚€DTO
L8 # - å‡ºåŠ›(FeatureBundle)ã¯ã€Œrawç‰¹å¾´é‡ dfã€ã€Œæ¨™æº–åŒ– df_zã€ã€ŒG/D ã‚¹ã‚³ã‚¢ã€ã€Œæ¬ æãƒ­ã‚°ã€
L9 # - é‡ã¿ç­‰ã®ã‚³ãƒ³ãƒ•ã‚£ã‚°(PipelineConfig)ã¯ factor ã‹ã‚‰æ¸¡ã™ï¼ˆcfg å¿…é ˆï¼‰
L10 # - æ—§ã‚«ãƒ©ãƒ åã¯ Scorer å†…ã§è‡ªå‹•ãƒªãƒãƒ¼ãƒ ã—ã¦å—ã‘å…¥ã‚Œï¼ˆå¾Œæ–¹äº’æ›ï¼‰
L11 #   ä¾‹) eps_ttm -> EPS_TTM, eps_q_recent -> EPS_Q_LastQ, fcf_ttm -> FCF_TTM
L12 #
L13 # ã€I/Oå¥‘ç´„ï¼ˆScorerãŒå‚ç…§ã™ã‚‹InputBundleãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰ã€‘
L14 #   - cand: List[str]    â€¦ å€™è£œéŠ˜æŸ„ï¼ˆå˜ä½“å®Ÿè¡Œã§ã¯æœªä½¿ç”¨ï¼‰
L15 #   - tickers: List[str] â€¦ å¯¾è±¡éŠ˜æŸ„ãƒªã‚¹ãƒˆ
L16 #   - bench: str         â€¦ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼ï¼ˆä¾‹ '^GSPC'ï¼‰
L17 #   - data: pd.DataFrame â€¦ yfinance downloadçµæœ ('Close','Volume' ç­‰ã®éšå±¤åˆ—)
L18 #   - px: pd.DataFrame   â€¦ data['Close'] ç›¸å½“ï¼ˆçµ‚å€¤ï¼‰
L19 #   - spx: pd.Series     â€¦ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®çµ‚å€¤
L20 #   - tickers_bulk: object         â€¦ yfinance.Tickers
L21 #   - info: Dict[str, dict]        â€¦ yfinance info per ticker
L22 #   - eps_df: pd.DataFrame         â€¦ å¿…é ˆåˆ—: EPS_TTM, EPS_Q_LastQï¼ˆæ—§åã‚‚å¯ï¼‰
L23 #   - fcf_df: pd.DataFrame         â€¦ å¿…é ˆåˆ—: FCF_TTMï¼ˆæ—§åã‚‚å¯ï¼‰
L24 #   - returns: pd.DataFrame        â€¦ px[tickers].pct_change() ç›¸å½“
L25 #
L26 # â€»å…¥å‡ºåŠ›ã®å½¢å¼ãƒ»ä¾‹å¤–æ–‡è¨€ã¯æ—¢å­˜å®Ÿè£…ã‚’å¤‰ãˆã¾ã›ã‚“ï¼ˆå®‰å…¨ãªçŸ­ç¸®ã®ã¿ï¼‰
L27 # =============================================================================
L28
L29 import os, sys, warnings
L30 import requests
L31 import numpy as np
L32 import pandas as pd
L33 import yfinance as yf
L34 from typing import Any, TYPE_CHECKING
L35 from scipy.stats import zscore
L36
L37 if TYPE_CHECKING:
L38     from factor import PipelineConfig  # type: ignore  # å®Ÿè¡Œæ™‚importãªã—ï¼ˆå¾ªç’°å›é¿ï¼‰
L39
L40 # ---- Dividend Helpers -------------------------------------------------------
L41 def _last_close(t, price_map=None):
L42     if price_map and (c := price_map.get(t)) is not None: return float(c)
L43     try:
L44         h = yf.Ticker(t).history(period="5d")["Close"].dropna()
L45         return float(h.iloc[-1]) if len(h) else np.nan
L46     except Exception:
L47         return np.nan
L48
L49 def _ttm_div_sum(t, lookback_days=400):
L50     try:
L51         div = yf.Ticker(t).dividends
L52         if div is None or len(div) == 0: return 0.0
L53         cutoff = pd.Timestamp.utcnow().tz_localize(None) - pd.Timedelta(days=lookback_days)
L54         ttm = float(div[div.index.tz_localize(None) >= cutoff].sum())
L55         return ttm if ttm > 0 else float(div.tail(4).sum())
L56     except Exception:
L57         return 0.0
L58
L59 def ttm_div_yield_portfolio(tickers, price_map=None):
L60     ys = [(lambda c, s: (s/c) if (np.isfinite(c) and c>0 and s>0) else 0.0)(_last_close(t, price_map), _ttm_div_sum(t)) for t in tickers]
L61     return float(np.mean(ys)) if ys else 0.0
L62
L63 # ---- ç°¡æ˜“ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆå®‰å…¨ãªçŸ­ç¸®ã®ã¿ï¼‰ -----------------------------------
L64 def winsorize_s(s: pd.Series, p=0.02):
L65     if s is None or s.dropna().empty: return s
L66     lo, hi = np.nanpercentile(s.astype(float), [100*p, 100*(1-p)]); return s.clip(lo, hi)
L67
L68 def robust_z(s: pd.Series, p=0.02):
L69     s2 = winsorize_s(s,p); return np.nan_to_num(zscore(s2.fillna(s2.mean())))
L70
L71 def robust_z_keepnan(s: pd.Series) -> pd.Series:
L72     """robust_z variant that preserves NaNs and falls back to rank-z when needed."""
L73     if s is None:
L74         return pd.Series(dtype=float)
L75     v = pd.to_numeric(s, errors="coerce")
L76     m = np.nanmedian(v)
L77     mad = np.nanmedian(np.abs(v - m))
L78     z = (v - m) / (1.4826 * mad + 1e-9)
L79     if np.nanstd(z) < 1e-9:
L80         r = v.rank(method="average", na_option="keep")
L81         z = (r - np.nanmean(r)) / (np.nanstd(r) + 1e-9)
L82     return pd.Series(z, index=v.index, dtype=float)
L83
L84 def _safe_div(a, b):
L85     try: return np.nan if (b is None or float(b)==0 or pd.isna(b)) else float(a)/float(b)
L86     except Exception: return np.nan
L87
L88 def _safe_last(series: pd.Series, default=np.nan):
L89     try: return float(series.iloc[-1])
L90     except Exception: return default
L91
L92 D_WEIGHTS_EFF = None  # å‡ºåŠ›è¡¨ç¤ºäº’æ›ã®ãŸã‚
L93
L94 # ---- Scorer æœ¬ä½“ -------------------------------------------------------------
L95 class Scorer:
L96     """
L97     - factor.py ã‹ã‚‰ã¯ `aggregate_scores(ib, cfg)` ã‚’å‘¼ã¶ã ã‘ã§OKã€‚
L98     - cfg ã¯å¿…é ˆï¼ˆfactor.PipelineConfig ã‚’æ¸¡ã™ï¼‰ã€‚
L99     - æ—§ã‚«ãƒ©ãƒ åã‚’è‡ªå‹•ãƒªãƒãƒ¼ãƒ ã—ã¦æ–°ã‚¹ã‚­ãƒ¼ãƒã«å¸åã—ã¾ã™ã€‚
L100     """
L101
L102     # === å…ˆé ­ã§æ—§â†’æ–°ã‚«ãƒ©ãƒ åãƒãƒƒãƒ—ï¼ˆç§»è¡Œç”¨ï¼‰ ===
L103     EPS_RENAME = {"eps_ttm":"EPS_TTM", "eps_q_recent":"EPS_Q_LastQ"}
L104     FCF_RENAME = {"fcf_ttm":"FCF_TTM"}
L105
L106     # === ã‚¹ã‚­ãƒ¼ãƒç°¡æ˜“ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€ä½é™ï¼‰ ===
L107     @staticmethod
L108     def _validate_ib_for_scorer(ib: Any):
L109         miss = [a for a in ["tickers","bench","data","px","spx","tickers_bulk","info","eps_df","fcf_df","returns"] if not hasattr(ib,a) or getattr(ib,a) is None]
L110         if miss: raise ValueError(f"InputBundle is missing required attributes for Scorer: {miss}")
L111         if any(c in ib.eps_df.columns for c in Scorer.EPS_RENAME): ib.eps_df.rename(columns=Scorer.EPS_RENAME, inplace=True)
L112         if any(c in ib.fcf_df.columns for c in Scorer.FCF_RENAME): ib.fcf_df.rename(columns=Scorer.FCF_RENAME, inplace=True)
L113         need_eps, need_fcf = {"EPS_TTM","EPS_Q_LastQ"},{"FCF_TTM"}
L114         if not need_eps.issubset(ib.eps_df.columns): raise ValueError(f"eps_df must contain columns {need_eps} (accepts old names via auto-rename). Got: {list(ib.eps_df.columns)}")
L115         if not need_fcf.issubset(ib.fcf_df.columns): raise ValueError(f"fcf_df must contain columns {need_fcf} (accepts old names via auto-rename). Got: {list(ib.fcf_df.columns)}")
L116
L117     # ----ï¼ˆScorerå°‚ç”¨ï¼‰ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ»æŒ‡æ¨™ç³» ----
L118     @staticmethod
L119     def trend(s: pd.Series):
L120         if len(s)<200: return np.nan
L121         sma50, sma150, sma200 = s.rolling(50).mean().iloc[-1], s.rolling(150).mean().iloc[-1], s.rolling(200).mean().iloc[-1]
L122         prev200, p = s.rolling(200).mean().iloc[-21], s.iloc[-1]
L123         lo_52 = s[-252:].min() if len(s)>=252 else s.min(); hi_52 = s[-252:].max() if len(s)>=252 else s.max()
L124         rng = (hi_52 - lo_52) if hi_52>lo_52 else np.nan
L125         clip = lambda x,lo,hi: (np.nan if pd.isna(x) else max(lo,min(hi,x)))
L126         a = clip(p/(s.rolling(50).mean().iloc[-1]) - 1, -0.5, 0.5)
L127         b = clip(sma50/sma150 - 1, -0.5, 0.5)
L128         c = clip(sma150/sma200 - 1, -0.5, 0.5)
L129         d = clip(sma200/prev200 - 1, -0.2, 0.2)
L130         e = clip((p - lo_52) / (rng if rng and rng>0 else np.nan) - 0.5, -0.5, 0.5)
L131         parts = [0.0 if pd.isna(x) else x for x in (a,b,c,d,e)]
L132         return 0.30*parts[0] + 0.20*parts[1] + 0.15*parts[2] + 0.15*parts[3] + 0.20*parts[4]
L133
L134     @staticmethod
L135     def rs(s, b):
L136         n, nb = len(s), len(b)
L137         if n<60 or nb<60: return np.nan
L138         L12 = 252 if n>=252 and nb>=252 else min(n,nb)-1; L1 = 22 if n>=22 and nb>=22 else max(5, min(n,nb)//3)
L139         r12, r1, br12, br1 = s.iloc[-1]/s.iloc[-L12]-1, s.iloc[-1]/s.iloc[-L1]-1, b.iloc[-1]/b.iloc[-L12]-1, b.iloc[-1]/b.iloc[-L1]-1
L140         return (r12 - br12)*0.7 + (r1 - br1)*0.3
L141
L142     @staticmethod
L143     def tr_str(s):
L144         if s is None:
L145             return np.nan
L146         s = s.ffill(limit=2).dropna()
L147         if len(s) < 50:
L148             return np.nan
L149         ma50 = s.rolling(50, min_periods=50).mean()
L150         last_ma = ma50.iloc[-1]
L151         last_px = s.iloc[-1]
L152         return float(last_px/last_ma - 1.0) if pd.notna(last_ma) and pd.notna(last_px) else np.nan
L153
L154     @staticmethod
L155     def rs_line_slope(s: pd.Series, b: pd.Series, win: int) -> float:
L156         r = (s/b).dropna()
L157         if len(r) < win: return np.nan
L158         y, x = np.log(r.iloc[-win:]), np.arange(win, dtype=float)
L159         try: return float(np.polyfit(x, y, 1)[0])
L160         except Exception: return np.nan
L161
L162     @staticmethod
L163     def ev_fallback(info_t: dict, tk: yf.Ticker) -> float:
L164         ev = info_t.get('enterpriseValue', np.nan)
L165         if pd.notna(ev) and ev>0: return float(ev)
L166         mc, debt, cash = info_t.get('marketCap', np.nan), np.nan, np.nan
L167         try:
L168             bs = tk.quarterly_balance_sheet
L169             if bs is not None and not bs.empty:
L170                 c = bs.columns[0]
L171                 for k in ("Total Debt","Long Term Debt","Short Long Term Debt"):
L172                     if k in bs.index: debt = float(bs.loc[k,c]); break
L173                 for k in ("Cash And Cash Equivalents","Cash And Cash Equivalents And Short Term Investments","Cash"):
L174                     if k in bs.index: cash = float(bs.loc[k,c]); break
L175         except Exception: pass
L176         if pd.notna(mc): return float(mc + (0 if pd.isna(debt) else debt) - (0 if pd.isna(cash) else cash))
L177         return np.nan
L178
L179     @staticmethod
L180     def dividend_status(ticker: str) -> str:
L181         t = yf.Ticker(ticker)
L182         try:
L183             if not t.dividends.empty: return "has"
L184         except Exception: return "unknown"
L185         try:
L186             a = t.actions
L187             if (a is not None and not a.empty and "Stock Splits" in a.columns and a["Stock Splits"].abs().sum()>0): return "none_confident"
L188         except Exception: pass
L189         try:
L190             fi = t.fast_info
L191             if any(getattr(fi,k,None) for k in ("last_dividend_date","dividend_rate","dividend_yield")): return "maybe_missing"
L192         except Exception: pass
L193         return "unknown"
L194
L195     @staticmethod
L196     def div_streak(t):
L197         try:
L198             divs = yf.Ticker(t).dividends.dropna(); ann = divs.groupby(divs.index.year).sum(); ann = ann[ann.index<pd.Timestamp.today().year]
L199             years, streak = sorted(ann.index), 0
L200             for i in range(len(years)-1,0,-1):
L201                 if ann[years[i]] > ann[years[i-1]]: streak += 1
L202                 else: break
L203             return streak
L204         except Exception: return 0
L205
L206     @staticmethod
L207     def fetch_finnhub_metrics(symbol):
L208         api_key = os.environ.get("FINNHUB_API_KEY")
L209         if not api_key: return {}
L210         url, params = "https://finnhub.io/api/v1/stock/metric", {"symbol":symbol,"metric":"all","token":api_key}
L211         try:
L212             r = requests.get(url, params=params, timeout=10); r.raise_for_status(); m = r.json().get("metric",{})
L213             return {'EPS':m.get('epsGrowthTTMYoy'),'REV':m.get('revenueGrowthTTMYoy'),'ROE':m.get('roeTTM'),'BETA':m.get('beta'),'DIV':m.get('dividendYieldIndicatedAnnual'),'FCF':(m.get('freeCashFlowTTM')/m.get('enterpriseValue')) if m.get('freeCashFlowTTM') and m.get('enterpriseValue') else None}
L214         except Exception: return {}
L215
L216     @staticmethod
L217     def calc_beta(series: pd.Series, market: pd.Series, lookback=252):
L218         r, m = series.pct_change().dropna(), market.pct_change().dropna()
L219         n = min(len(r), len(m), lookback)
L220         if n<60: return np.nan
L221         r, m = r.iloc[-n:], m.iloc[-n:]; cov, var = np.cov(r, m)[0,1], np.var(m)
L222         return np.nan if var==0 else cov/var
L223
L224     @staticmethod
L225     def spx_to_alpha(spx: pd.Series, bands=(0.03,0.10), w=(0.6,0.4),
L226                      span=5, q=(0.20,0.40), alphas=(0.05,0.08,0.10)) -> float:
L227         """
L228         S&P500æŒ‡æ•°ã®ã¿ã‹ã‚‰æ“¬ä¼¼breadthã‚’ä½œã‚Šã€å±¥æ­´åˆ†ä½ã§Î±ã‚’æ®µéšæ±ºå®šã€‚
L229         bands=(Â±3%, Â±10%), w=(50DMA,200DMA), åˆ†ä½q=(20%,40%), alphas=(ä½,ä¸­,é«˜)
L230         """
L231         ma50, ma200 = spx.rolling(50).mean(), spx.rolling(200).mean()
L232         b50, b200 = ((spx/ma50 - 1)+bands[0])/(2*bands[0]), ((spx/ma200 - 1)+bands[1])/(2*bands[1])
L233         hist = (w[0]*b50 + w[1]*b200).clip(0,1).ewm(span=span).mean()
L234         b, (lo, mid) = float(hist.iloc[-1]), (float(hist.quantile(q[0])), float(hist.quantile(q[1])))
L235         return alphas[0] if b < lo else alphas[1] if b < mid else alphas[2]
L236
L237     @staticmethod
L238     def soft_cap_effective_scores(scores: pd.Series|dict, sectors: dict, cap=2, alpha=0.08) -> pd.Series:
L239         """
L240         åŒä¸€ã‚»ã‚¯ã‚¿ãƒ¼capè¶…éï¼ˆ3æœ¬ç›®ä»¥é™ï¼‰ã« Î±Ã—æ®µéšæ¸›ç‚¹ã‚’èª²ã—ãŸâ€œæœ‰åŠ¹ã‚¹ã‚³ã‚¢â€Seriesã‚’è¿”ã™ã€‚
L241         æˆ»ã‚Šå€¤ã¯é™é †ã‚½ãƒ¼ãƒˆæ¸ˆã¿ã€‚
L242         """
L243         s = pd.Series(scores, dtype=float); order = s.sort_values(ascending=False).index
L244         cnt, pen = {}, {}
L245         for t in order:
L246             sec = sectors.get(t, "U"); cnt[sec] = cnt.get(sec,0) + 1; pen[t] = alpha*max(0, cnt[sec]-cap)
L247         return (s - pd.Series(pen)).sort_values(ascending=False)
L248
L249     @staticmethod
L250     def pick_top_softcap(scores: pd.Series|dict, sectors: dict, N: int, cap=2, alpha=0.08, hard: int|None=5) -> list[str]:
L251         """
L252         soft-capé©ç”¨å¾Œã®ä¸Šä½Nãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚’è¿”ã™ã€‚hard>0ãªã‚‰éå¸¸ç”¨ãƒãƒ¼ãƒ‰ä¸Šé™ã§åŒä¸€ã‚»ã‚¯ã‚¿ãƒ¼è¶…éã‚’é–“å¼•ãï¼ˆæ—¢å®š=5ï¼‰ã€‚
L253         """
L254         eff = Scorer.soft_cap_effective_scores(scores, sectors, cap, alpha)
L255         if not hard:
L256             return list(eff.head(N).index)
L257         pick, used = [], {}
L258         for t in eff.index:
L259             s = sectors.get(t, "U")
L260             if used.get(s,0) < hard:
L261                 pick.append(t); used[s] = used.get(s,0) + 1
L262             if len(pick) == N: break
L263         return pick
L264
L265     @staticmethod
L266     def trend_template_breadth_series(px: pd.DataFrame, spx: pd.Series, win_days: int | None = None) -> pd.Series:
L267         """
L268         å„å–¶æ¥­æ—¥ã® trend_template åˆæ ¼æœ¬æ•°ï¼ˆåˆæ ¼â€œæœ¬æ•°â€=Cï¼‰ã‚’è¿”ã™ã€‚
L269         - px: åˆ—=tickerï¼ˆãƒ™ãƒ³ãƒã¯å«ã‚ãªã„ï¼‰
L270         - spx: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ Seriesï¼ˆpx.index ã«æ•´åˆ—ï¼‰
L271         - win_days: æœ«å°¾ã®è¨ˆç®—å¯¾è±¡å–¶æ¥­æ—¥æ•°ï¼ˆNoneâ†’å…¨ä½“ã€æ—¢å®š600ã¯å‘¼ã³å‡ºã—å´æŒ‡å®šï¼‰
L272         ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼†rollingã®ã¿ã§è»½é‡ã€‚æ¬ æã¯ False æ‰±ã„ã€‚
L273         """
L274         import numpy as np, pandas as pd
L275         if px is None or px.empty:
L276             return pd.Series(dtype=int)
L277         px = px.dropna(how="all", axis=1)
L278         if win_days and win_days > 0:
L279             px = px.tail(win_days)
L280         if px.empty:
L281             return pd.Series(dtype=int)
L282         spx = spx.reindex(px.index).ffill()
L283
L284         ma50  = px.rolling(50).mean()
L285         ma150 = px.rolling(150).mean()
L286         ma200 = px.rolling(200).mean()
L287
L288         tt = (px > ma150)
L289         tt &= (px > ma200)
L290         tt &= (ma150 > ma200)
L291         tt &= (ma200 - ma200.shift(21) > 0)
L292         tt &= (ma50  > ma150)
L293         tt &= (ma50  > ma200)
L294         tt &= (px    > ma50)
L295
L296         lo252 = px.rolling(252).min()
L297         hi252 = px.rolling(252).max()
L298         tt &= (px.divide(lo252).sub(1.0) >= 0.30)   # P_OVER_LOW52 >= 0.30
L299         tt &= (px >= (0.75 * hi252))                # NEAR_52W_HIGH >= -0.25
L300
L301         r12  = px.divide(px.shift(252)).sub(1.0)
L302         br12 = spx.divide(spx.shift(252)).sub(1.0)
L303         r1   = px.divide(px.shift(22)).sub(1.0)
L304         br1  = spx.divide(spx.shift(22)).sub(1.0)
L305         rs   = 0.7*(r12.sub(br12, axis=0)) + 0.3*(r1.sub(br1, axis=0))
L306         tt &= (rs >= 0.10)
L307
L308         return tt.fillna(False).sum(axis=1).astype(int)
L309
L310     # ---- ã‚¹ã‚³ã‚¢é›†è¨ˆï¼ˆDTO/Configã‚’å—ã‘å–ã‚Šã€FeatureBundleã‚’è¿”ã™ï¼‰ ----
L311     def aggregate_scores(self, ib: Any, cfg):
L312         if cfg is None:
L313             raise ValueError("cfg is required; pass factor.PipelineConfig")
L314         self._validate_ib_for_scorer(ib)
L315
L316         px, spx, tickers = ib.px, ib.spx, ib.tickers
L317         tickers_bulk, info, eps_df, fcf_df = ib.tickers_bulk, ib.info, ib.eps_df, ib.fcf_df
L318
L319         df, missing_logs = pd.DataFrame(index=tickers), []
L320         for t in tickers:
L321             d, s = info[t], px[t]; ev = self.ev_fallback(d, tickers_bulk.tickers[t])
L322             # --- åŸºæœ¬ç‰¹å¾´ ---
L323             df.loc[t,'TR']   = self.trend(s)
L324             df.loc[t,'EPS']  = eps_df.loc[t,'EPS_TTM'] if t in eps_df.index else np.nan
L325             df.loc[t,'REV']  = d.get('revenueGrowth',np.nan)
L326             df.loc[t,'ROE']  = d.get('returnOnEquity',np.nan)
L327             df.loc[t,'BETA'] = self.calc_beta(s, spx, lookback=252)
L328
L329             # --- é…å½“ï¼ˆæ¬ æè£œå®Œå«ã‚€ï¼‰ ---
L330             div = d.get('dividendYield') if d.get('dividendYield') is not None else d.get('trailingAnnualDividendYield')
L331             if div is None or pd.isna(div):
L332                 try:
L333                     divs = yf.Ticker(t).dividends
L334                     if divs is not None and not divs.empty:
L335                         last_close = s.iloc[-1]; div_1y = divs[divs.index >= (divs.index.max() - pd.Timedelta(days=365))].sum()
L336                         if last_close and last_close>0: div = float(div_1y/last_close)
L337                 except Exception: pass
L338             df.loc[t,'DIV'] = 0.0 if (div is None or pd.isna(div)) else float(div)
L339
L340             # --- FCF/EV ---
L341             fcf_val = fcf_df.loc[t,'FCF_TTM'] if t in fcf_df.index else np.nan
L342             df.loc[t,'FCF'] = (fcf_val/ev) if (pd.notna(fcf_val) and pd.notna(ev) and ev>0) else np.nan
L343
L344             # --- ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ãƒ»ãƒœãƒ©é–¢é€£ ---
L345             df.loc[t,'RS'], df.loc[t,'TR_str'] = self.rs(s, spx), self.tr_str(s)
L346             r, rm = s.pct_change().dropna(), spx.pct_change().dropna()
L347             n = int(min(len(r), len(rm)))
L348
L349             DOWNSIDE_DEV = np.nan
L350             if n>=60:
L351                 r6 = r.iloc[-min(len(r),126):]; neg = r6[r6<0]
L352                 if len(neg)>=10: DOWNSIDE_DEV = float(neg.std(ddof=0)*np.sqrt(252))
L353             df.loc[t,'DOWNSIDE_DEV'] = DOWNSIDE_DEV
L354
L355             MDD_1Y = np.nan
L356             try:
L357                 w = s.iloc[-min(len(s),252):].dropna()
L358                 if len(w)>=30:
L359                     roll_max = w.cummax(); MDD_1Y = float((w/roll_max - 1.0).min())
L360             except Exception: pass
L361             df.loc[t,'MDD_1Y'] = MDD_1Y
L362
L363             RESID_VOL = np.nan
L364             if n>=120:
L365                 rr, rrm = r.iloc[-n:].align(rm.iloc[-n:], join='inner')
L366                 if len(rr)==len(rrm) and len(rr)>=120 and rrm.var()>0:
L367                     beta = float(np.cov(rr, rrm)[0,1]/np.var(rrm)); resid = rr - beta*rrm
L368                     RESID_VOL = float(resid.std(ddof=0)*np.sqrt(252))
L369             df.loc[t,'RESID_VOL'] = RESID_VOL
L370
L371             DOWN_OUTPERF = np.nan
L372             if n>=60:
L373                 m, x = rm.iloc[-n:], r.iloc[-n:]; mask = m<0
L374                 if mask.sum()>=10:
L375                     mr, sr = float(m[mask].mean()), float(x[mask].mean())
L376                     DOWN_OUTPERF = (sr - mr)/abs(mr) if mr!=0 else np.nan
L377             df.loc[t,'DOWN_OUTPERF'] = DOWN_OUTPERF
L378
L379             # --- é•·æœŸç§»å‹•å¹³å‡/ä½ç½® ---
L380             sma200 = s.rolling(200).mean(); df.loc[t,'EXT_200'] = np.nan
L381             if pd.notna(sma200.iloc[-1]) and sma200.iloc[-1]!=0: df.loc[t,'EXT_200'] = abs(float(s.iloc[-1]/sma200.iloc[-1]-1.0))
L382
L383             # --- é…å½“ã®è©³ç´°ç³» ---
L384             DIV_TTM_PS=DIV_VAR5=DIV_YOY=DIV_FCF_COVER=np.nan
L385             try:
L386                 divs = yf.Ticker(t).dividends.dropna()
L387                 if not divs.empty:
L388                     last_close = s.iloc[-1]; div_1y = float(divs[divs.index >= (divs.index.max()-pd.Timedelta(days=365))].sum())
L389                     DIV_TTM_PS = div_1y if div_1y>0 else np.nan
L390                     ann = divs.groupby(divs.index.year).sum()
L391                     if len(ann)>=2 and ann.iloc[-2]!=0: DIV_YOY = float(ann.iloc[-1]/ann.iloc[-2]-1.0)
L392                     tail = ann.iloc[-5:] if len(ann)>=5 else ann
L393                     if len(tail)>=3 and tail.mean()!=0: DIV_VAR5 = float(tail.std(ddof=1)/abs(tail.mean()))
L394                 so = d.get('sharesOutstanding',None)
L395                 if so and pd.notna(DIV_TTM_PS) and pd.notna(fcf_val) and fcf_val!=0:
L396                     DIV_FCF_COVER = float((fcf_val)/(DIV_TTM_PS*float(so)))
L397             except Exception: pass
L398             df.loc[t,'DIV_TTM_PS'], df.loc[t,'DIV_VAR5'], df.loc[t,'DIV_YOY'], df.loc[t,'DIV_FCF_COVER'] = DIV_TTM_PS, DIV_VAR5, DIV_YOY, DIV_FCF_COVER
L399
L400             # --- è²¡å‹™å®‰å®šæ€§ ---
L401             df.loc[t,'DEBT2EQ'], df.loc[t,'CURR_RATIO'] = d.get('debtToEquity',np.nan), d.get('currentRatio',np.nan)
L402
L403             # --- EPS å¤‰å‹• ---
L404             EPS_VAR_8Q = np.nan
L405             try:
L406                 qe, so = tickers_bulk.tickers[t].quarterly_earnings, d.get('sharesOutstanding',None)
L407                 if qe is not None and not qe.empty and so:
L408                     eps_q = (qe['Earnings'].dropna().astype(float)/float(so)).replace([np.inf,-np.inf],np.nan)
L409                     if len(eps_q)>=4: EPS_VAR_8Q = float(eps_q.iloc[-min(8,len(eps_q)):].std(ddof=1))
L410             except Exception: pass
L411             df.loc[t,'EPS_VAR_8Q'] = EPS_VAR_8Q
L412
L413             # --- ã‚µã‚¤ã‚º/æµå‹•æ€§ ---
L414             df.loc[t,'MARKET_CAP'] = d.get('marketCap',np.nan); adv60 = np.nan
L415             try:
L416                 vol_series = ib.data['Volume'][t].dropna()
L417                 if len(vol_series)>=5 and len(s)==len(vol_series):
L418                     dv = (vol_series*s).rolling(60).mean(); adv60 = float(dv.iloc[-1])
L419             except Exception: pass
L420             df.loc[t,'ADV60_USD'] = adv60
L421
L422             # --- å£²ä¸Š/åˆ©ç›Šã®åŠ é€Ÿåº¦ç­‰ ---
L423             REV_Q_YOY=EPS_Q_YOY=REV_YOY_ACC=REV_YOY_VAR=np.nan
L424             REV_ANNUAL_STREAK = REV_YOY = np.nan
L425             EPS_YOY = np.nan
L426             try:
L427                 qe, so = tickers_bulk.tickers[t].quarterly_earnings, d.get('sharesOutstanding',None)
L428                 if qe is not None and not qe.empty:
L429                     if 'Revenue' in qe.columns:
L430                         rev = qe['Revenue'].dropna().astype(float)
L431                         if len(rev)>=5: REV_Q_YOY = _safe_div(rev.iloc[-1]-rev.iloc[-5], rev.iloc[-5])
L432                         if len(rev)>=6:
L433                             yoy_now = _safe_div(rev.iloc[-1]-rev.iloc[-5], rev.iloc[-5]); yoy_prev = _safe_div(rev.iloc[-2]-rev.iloc[-6], rev.iloc[-6])
L434                             if pd.notna(yoy_now) and pd.notna(yoy_prev): REV_YOY_ACC = yoy_now - yoy_prev
L435                         yoy_list=[]
L436                         for k in range(1,5):
L437                             if len(rev)>=4+k:
L438                                 y = _safe_div(rev.iloc[-k]-rev.iloc[-(k+4)], rev.iloc[-(k+4)])
L439                                 if pd.notna(y): yoy_list.append(y)
L440                         if len(yoy_list)>=2: REV_YOY_VAR = float(np.std(yoy_list, ddof=1))
L441                         # NEW: å¹´æ¬¡ã®æŒç¶šæ€§ï¼ˆç›´è¿‘ã‹ã‚‰é¡ã£ã¦å‰å¹´æ¯”ãƒ—ãƒ©ã‚¹ãŒä½•å¹´é€£ç¶šã‹ã€å››åŠæœŸ4æœ¬æƒã†å®Œå…¨å¹´ã®ã¿ï¼‰
L442                         try:
L443                             g = rev.groupby(rev.index.year)
L444                             ann_sum, cnt = g.sum(), g.count()
L445                             ann_sum = ann_sum[cnt >= 4]
L446                             if len(ann_sum) >= 2:
L447                                 yoy = ann_sum.pct_change().dropna()
L448                                 if not yoy.empty:
L449                                     REV_YOY = float(yoy.iloc[-1])
L450                                 streak = 0
L451                                 for v in yoy.iloc[::-1]:
L452                                     if pd.isna(v) or v <= 0:
L453                                         break
L454                                     streak += 1
L455                                 REV_ANNUAL_STREAK = float(streak)
L456                         except Exception:
L457                             pass
L458                     if 'Earnings' in qe.columns and so:
L459                         eps_series = (qe['Earnings'].dropna().astype(float)/float(so)).replace([np.inf,-np.inf],np.nan)
L460                         if len(eps_series)>=5 and pd.notna(eps_series.iloc[-5]) and eps_series.iloc[-5]!=0:
L461                             EPS_Q_YOY = _safe_div(eps_series.iloc[-1]-eps_series.iloc[-5], eps_series.iloc[-5])
L462                         try:
L463                             g_eps = eps_series.groupby(eps_series.index.year)
L464                             ann_eps, cnt_eps = g_eps.sum(), g_eps.count()
L465                             ann_eps = ann_eps[cnt_eps >= 4]
L466                             if len(ann_eps) >= 2:
L467                                 eps_yoy = ann_eps.pct_change().dropna()
L468                                 if not eps_yoy.empty:
L469                                     EPS_YOY = float(eps_yoy.iloc[-1])
L470                         except Exception:
L471                             pass
L472             except Exception: pass
L473             df.loc[t,'REV_Q_YOY'], df.loc[t,'EPS_Q_YOY'] = REV_Q_YOY, EPS_Q_YOY
L474             df.loc[t,'REV_YOY_ACC'], df.loc[t,'REV_YOY_VAR'] = REV_YOY_ACC, REV_YOY_VAR
L475             df.loc[t,'REV_YOY'] = REV_YOY
L476             df.loc[t,'REV_ANN_STREAK'] = REV_ANNUAL_STREAK
L477             df.loc[t,'EPS_YOY'] = EPS_YOY
L478
L479             # --- Rule of 40 ã‚„å‘¨è¾º ---
L480             total_rev_ttm = d.get('totalRevenue',np.nan)
L481             FCF_MGN = _safe_div(fcf_val, total_rev_ttm)
L482             df.loc[t,'FCF_MGN'] = FCF_MGN
L483             rule40 = np.nan
L484             try:
L485                 r = df.loc[t,'REV']; rule40 = (r if pd.notna(r) else np.nan) + (FCF_MGN if pd.notna(FCF_MGN) else np.nan)
L486             except Exception: pass
L487             df.loc[t,'RULE40'] = rule40
L488
L489             # --- ãƒˆãƒ¬ãƒ³ãƒ‰è£œåŠ© ---
L490             sma50  = s.rolling(50).mean()
L491             sma150 = s.rolling(150).mean()
L492             sma200 = s.rolling(200).mean()
L493             p = _safe_last(s)
L494
L495             df.loc[t,'MA50_OVER_150'] = (_safe_last(sma50)/_safe_last(sma150) - 1
L496                 if pd.notna(_safe_last(sma50)) and pd.notna(_safe_last(sma150)) and _safe_last(sma150)!=0 else np.nan)
L497             df.loc[t,'MA150_OVER_200'] = (_safe_last(sma150)/_safe_last(sma200) - 1
L498                 if pd.notna(_safe_last(sma150)) and pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan)
L499
L500             lo52 = s[-252:].min() if len(s)>=252 else s.min()
L501             df.loc[t,'P_OVER_LOW52'] = (p/lo52 - 1) if (lo52 and lo52>0 and pd.notna(p)) else np.nan
L502
L503             df.loc[t,'MA200_SLOPE_1M'] = np.nan
L504             if len(sma200.dropna()) >= 21:
L505                 cur200 = _safe_last(sma200)
L506                 old2001 = float(sma200.iloc[-21])
L507                 if old2001:
L508                     df.loc[t,'MA200_SLOPE_1M'] = cur200/old2001 - 1
L509
L510             df.loc[t,'P_OVER_150'] = p/_safe_last(sma150)-1 if pd.notna(_safe_last(sma150)) and _safe_last(sma150)!=0 else np.nan
L511             df.loc[t,'P_OVER_200'] = p/_safe_last(sma200)-1 if pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L512             df.loc[t,'MA50_OVER_200'] = _safe_last(sma50)/_safe_last(sma200)-1 if pd.notna(_safe_last(sma50)) and pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L513             df.loc[t,'MA200_SLOPE_5M'] = np.nan
L514             if len(sma200.dropna())>=105:
L515                 cur200, old200 = _safe_last(sma200), float(sma200.iloc[-105])
L516                 if old200 and old200!=0: df.loc[t,'MA200_SLOPE_5M'] = cur200/old200 - 1
L517             # NEW: 200æ—¥ç·šãŒé€£ç¶šã§ä¸Šå‘ãã®ã€Œæ—¥æ•°ã€
L518             df.loc[t,'MA200_UP_STREAK_D'] = np.nan
L519             try:
L520                 s200 = sma200.dropna()
L521                 if len(s200) >= 2:
L522                     diff200 = s200.diff()
L523                     up = 0
L524                     for v in diff200.iloc[::-1]:
L525                         if pd.isna(v) or v <= 0:
L526                             break
L527                         up += 1
L528                     df.loc[t,'MA200_UP_STREAK_D'] = float(up)
L529             except Exception:
L530                 pass
L531             df.loc[t,'LOW52PCT25_EXCESS'] = np.nan if (lo52 is None or lo52<=0 or pd.isna(p)) else (p/(lo52*1.25)-1)
L532             hi52 = s[-252:].max() if len(s)>=252 else s.max(); df.loc[t,'NEAR_52W_HIGH'] = np.nan
L533             if hi52 and hi52>0 and pd.notna(p):
L534                 d_hi = (p/hi52)-1.0; df.loc[t,'NEAR_52W_HIGH'] = -abs(min(0.0, d_hi))
L535             df.loc[t,'RS_SLOPE_6W'] = self.rs_line_slope(s, ib.spx, 30)
L536             df.loc[t,'RS_SLOPE_13W'] = self.rs_line_slope(s, ib.spx, 65)
L537
L538             df.loc[t,'DIV_STREAK'] = self.div_streak(t)
L539
L540             # --- æ¬ æãƒ¡ãƒ¢ ---
L541             fin_cols = ['REV','ROE','BETA','DIV','FCF']
L542             need_finnhub = [col for col in fin_cols if pd.isna(df.loc[t,col])]
L543             if need_finnhub:
L544                 fin_data = self.fetch_finnhub_metrics(t)
L545                 for col in need_finnhub:
L546                     val = fin_data.get(col)
L547                     if val is not None and not pd.isna(val): df.loc[t,col] = val
L548             for col in fin_cols + ['EPS','RS','TR_str','DIV_STREAK']:
L549                 if pd.isna(df.loc[t,col]):
L550                     if col=='DIV':
L551                         status = self.dividend_status(t)
L552                         if status!='none_confident': missing_logs.append({'Ticker':t,'Column':col,'Status':status})
L553                     else:
L554                         missing_logs.append({'Ticker':t,'Column':col})
L555
L556         def _trend_template_pass(row, rs_alpha_thresh=0.10):
L557             c1 = (row.get('P_OVER_150', np.nan) > 0) and (row.get('P_OVER_200', np.nan) > 0)
L558             c2 = (row.get('MA150_OVER_200', np.nan) > 0)
L559             c3 = (row.get('MA200_SLOPE_1M', np.nan) > 0)
L560             c4 = (row.get('MA50_OVER_150', np.nan) > 0) and (row.get('MA50_OVER_200', np.nan) > 0)
L561             c5 = (row.get('TR_str', np.nan) > 0)
L562             c6 = (row.get('P_OVER_LOW52', np.nan) >= 0.30)
L563             c7 = (row.get('NEAR_52W_HIGH', np.nan) >= -0.25)
L564             c8 = (row.get('RS', np.nan) >= 0.10)
L565             return bool(c1 and c2 and c3 and c4 and c5 and c6 and c7 and c8)
L566
L567         if 'trend_template' not in df.columns: df['trend_template'] = df.apply(_trend_template_pass, axis=1).fillna(False)
L568         assert 'trend_template' in df.columns
L569
L570         # === ZåŒ–ã¨åˆæˆ ===
L571         for col in ['ROE','FCF','REV','EPS']: df[f'{col}_W'] = winsorize_s(df[col], 0.02)
L572
L573         df_z = pd.DataFrame(index=df.index)
L574         for col in ['EPS','REV','ROE','FCF','RS','TR_str','BETA','DIV','DIV_STREAK']: df_z[col] = robust_z(df[col])
L575         df_z['REV'], df_z['EPS'], df_z['TR'] = robust_z(df['REV_W']), robust_z(df['EPS_W']), robust_z(df['TR'])
L576         for col in ['P_OVER_150','P_OVER_200','MA50_OVER_200','MA200_SLOPE_5M','LOW52PCT25_EXCESS','NEAR_52W_HIGH','RS_SLOPE_6W','RS_SLOPE_13W','MA200_UP_STREAK_D']: df_z[col] = robust_z(df[col])
L577
L578         # === Growthæ·±æ˜ã‚Šç³»ï¼ˆæ¬ æä¿æŒz + RAWä½µè¼‰ï¼‰ ===
L579         grw_cols = ['REV_Q_YOY','EPS_Q_YOY','REV_YOY','EPS_YOY','REV_YOY_ACC','REV_YOY_VAR','FCF_MGN','RULE40','REV_ANN_STREAK']
L580         for col in grw_cols:
L581             if col in df.columns:
L582                 raw = pd.to_numeric(df[col], errors="coerce")
L583                 df_z[col] = robust_z_keepnan(raw)
L584                 df_z[f'{col}_RAW'] = raw
L585         for k in ("TREND_SLOPE_EPS", "TREND_SLOPE_REV"):
L586             if k in df.columns and k not in df_z.columns:
L587                 raw = pd.to_numeric(df[k], errors="coerce")
L588                 df_z[k] = robust_z_keepnan(raw)
L589                 df_z[f'{k}_RAW'] = raw
L590         for col in ['DOWNSIDE_DEV','MDD_1Y','RESID_VOL','DOWN_OUTPERF','EXT_200','DIV_TTM_PS','DIV_VAR5','DIV_YOY','DIV_FCF_COVER','DEBT2EQ','CURR_RATIO','EPS_VAR_8Q','MARKET_CAP','ADV60_USD']: df_z[col] = robust_z(df[col])
L591
L592         df_z['SIZE'], df_z['LIQ'] = robust_z(np.log1p(df['MARKET_CAP'])), robust_z(np.log1p(df['ADV60_USD']))
L593         df_z['QUALITY_F'] = robust_z(0.6*df['FCF_W'] + 0.4*df['ROE_W']).clip(-3.0,3.0)
L594         df_z['YIELD_F']   = 0.3*df_z['DIV'] + 0.7*df_z['DIV_STREAK']
L595
L596         # EPSãŒèµ¤å­—ã§ã‚‚FCFãŒé»’å­—ãªã‚‰å®Ÿè³ªé»’å­—ã¨ã¿ãªã™
L597         eps_pos_mask = (df['EPS'] > 0) | (df['FCF_MGN'] > 0)
L598         df_z['EPS_POS'] = df_z['EPS'].where(eps_pos_mask, 0.0)
L599
L600         # ===== ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ãƒ­ãƒ¼ãƒ—ç®—å‡º =====
L601         def zpos(x):
L602             arr = robust_z(x)
L603             idx = getattr(x, 'index', df_z.index)
L604             return pd.Series(arr, index=idx).fillna(0.0)
L605
L606         def relu(x):
L607             ser = x if isinstance(x, pd.Series) else pd.Series(x, index=df_z.index)
L608             return ser.clip(lower=0).fillna(0.0)
L609
L610         # å£²ä¸Šãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ãƒ­ãƒ¼ãƒ—ï¼ˆå››åŠæœŸï¼‰
L611         slope_rev = 0.70*zpos(df_z['REV_Q_YOY']) + 0.30*zpos(df_z['REV_YOY_ACC'])
L612         noise_rev = relu(robust_z(df_z['REV_YOY_VAR']) - 0.8)
L613         slope_rev_combo = slope_rev - 0.25*noise_rev
L614         df_z['TREND_SLOPE_REV_RAW'] = slope_rev_combo
L615         df_z['TREND_SLOPE_REV'] = slope_rev_combo.clip(-3.0, 3.0)
L616
L617         # EPSãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ãƒ­ãƒ¼ãƒ—ï¼ˆå››åŠæœŸï¼‰
L618         slope_eps = 0.60*zpos(df_z['EPS_Q_YOY']) + 0.40*zpos(df_z['EPS_POS'])
L619         df_z['TREND_SLOPE_EPS_RAW'] = slope_eps
L620         df_z['TREND_SLOPE_EPS'] = slope_eps.clip(-3.0, 3.0)
L621
L622         # å¹´æ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆã‚µãƒ–ï¼‰
L623         slope_rev_yr = zpos(df_z['REV_YOY'])
L624         slope_eps_yr = zpos(df_z.get('EPS_YOY', pd.Series(0.0, index=df.index)))
L625         streak_base = df['REV_ANN_STREAK'].clip(lower=0).fillna(0)
L626         streak_yr = streak_base / (streak_base.abs() + 1.0)
L627         slope_rev_yr_combo = 0.7*slope_rev_yr + 0.3*streak_yr
L628         df_z['TREND_SLOPE_REV_YR_RAW'] = slope_rev_yr_combo
L629         df_z['TREND_SLOPE_REV_YR'] = slope_rev_yr_combo.clip(-3.0, 3.0)
L630         df_z['TREND_SLOPE_EPS_YR_RAW'] = slope_eps_yr
L631         df_z['TREND_SLOPE_EPS_YR'] = slope_eps_yr.clip(-3.0, 3.0)
L632
L633         # ===== æ–°GRWåˆæˆå¼ï¼ˆSEPAå¯„ã‚Šã‚·ãƒ•ãƒˆï¼‰ =====
L634         _nz = lambda name: df_z.get(name, pd.Series(0.0, index=df_z.index)).fillna(0.0)
L635         grw_combo = (
L636               0.20*_nz('REV_Q_YOY')
L637             + 0.10*_nz('REV_YOY_ACC')
L638             + 0.10*_nz('REV_ANN_STREAK')
L639             - 0.05*_nz('REV_YOY_VAR')
L640             + 0.10*_nz('TREND_SLOPE_REV')
L641             + 0.15*_nz('EPS_Q_YOY')
L642             + 0.05*_nz('EPS_POS')
L643             + 0.20*_nz('TREND_SLOPE_EPS')
L644             + 0.05*_nz('TREND_SLOPE_REV_YR')
L645             + 0.03*_nz('TREND_SLOPE_EPS_YR')
L646             + 0.10*_nz('FCF_MGN')
L647             + 0.05*_nz('RULE40')
L648         )
L649         df_z['GROWTH_F_RAW'] = grw_combo
L650         df_z['GROWTH_F'] = robust_z(grw_combo).clip(-3.0, 3.0)
L651
L652         # Debug dump for GRW composition (console OFF by default; enable only with env)
L653         if bool(os.getenv("GRW_CONSOLE_DEBUG")):
L654             try:
L655                 i = df_z[['GROWTH_F', 'GROWTH_F_RAW']].copy()
L656                 i.sort_values('GROWTH_F', ascending=False, inplace=True)
L657                 limit = max(0, min(40, len(i)))
L658                 print("[DEBUG: GRW]")
L659                 for t in i.index[:limit]:
L660                     row = i.loc[t]
L661                     parts = [f"GROWTH_F={row['GROWTH_F']:.3f}"]
L662                     if pd.notna(row.get('GROWTH_F_RAW')):
L663                         parts.append(f"GROWTH_F_RAW={row['GROWTH_F_RAW']:.3f}")
L664                     print(f"Ticker: {t} | " + " ".join(parts))
L665                 print()
L666             except Exception as exc:
L667                 print(f"[ERR] GRW debug dump failed: {exc}")
L668
L669         df_z['MOM_F'] = robust_z(0.40*df_z['RS']
L670             + 0.15*df_z['TR_str']
L671             + 0.15*df_z['RS_SLOPE_6W']
L672             + 0.15*df_z['RS_SLOPE_13W']
L673             + 0.10*df_z['MA200_SLOPE_5M']
L674             + 0.10*df_z['MA200_UP_STREAK_D']).clip(-3.0,3.0)
L675         df_z['VOL'] = robust_z(df['BETA'])
L676         df_z['QAL'], df_z['YLD'], df_z['MOM'] = df_z['QUALITY_F'], df_z['YIELD_F'], df_z['MOM_F']
L677         df_z.drop(columns=['QUALITY_F','YIELD_F','MOM_F'], inplace=True, errors='ignore')
L678
L679         # === begin: BIO LOSS PENALTY =====================================
L680         try:
L681             penalty_z = float(os.getenv("BIO_LOSS_PENALTY_Z", "0.8"))
L682         except Exception:
L683             penalty_z = 0.8
L684
L685         def _is_bio_like(t: str) -> bool:
L686             inf = info.get(t, {}) if isinstance(info, dict) else {}
L687             sec = str(inf.get("sector", "")).lower()
L688             ind = str(inf.get("industry", "")).lower()
L689             if "health" not in sec:
L690                 return False
L691             keys = ("biotech", "biopharma", "pharma")
L692             return any(k in ind for k in keys)
L693
L694         tickers_s = pd.Index(df_z.index)
L695         is_bio = pd.Series({t: _is_bio_like(t) for t in tickers_s})
L696         is_loss = pd.Series({t: (pd.notna(df.loc[t,"EPS"]) and df.loc[t,"EPS"] <= 0) for t in tickers_s})
L697         mask_bio_loss = (is_bio & is_loss).reindex(df_z.index).fillna(False)
L698
L699         if bool(mask_bio_loss.any()) and penalty_z > 0:
L700             df_z.loc[mask_bio_loss, "GROWTH_F"] = df_z.loc[mask_bio_loss, "GROWTH_F"] - penalty_z
L701             df_z["GROWTH_F"] = df_z["GROWTH_F"].clip(-3.0, 3.0)
L702         # === end: BIO LOSS PENALTY =======================================
L703
L704         df_z['TRD'] = 0.0  # TRDã¯ã‚¹ã‚³ã‚¢å¯„ä¸ã‹ã‚‰å¤–ã—ã€ãƒ†ãƒ³ãƒ—ãƒ¬åˆ¤å®šã¯ãƒ•ã‚£ãƒ«ã‚¿ã§è¡Œã†ï¼ˆåˆ—ã¯è¡¨ç¤ºäº’æ›ã®ãŸã‚æ®‹ã™ï¼‰
L705         if 'BETA' not in df_z.columns: df_z['BETA'] = robust_z(df['BETA'])
L706
L707         df_z['D_VOL_RAW'] = robust_z(0.40*df_z['DOWNSIDE_DEV'] + 0.22*df_z['RESID_VOL'] + 0.18*df_z['MDD_1Y'] - 0.10*df_z['DOWN_OUTPERF'] - 0.05*df_z['EXT_200'] - 0.08*df_z['SIZE'] - 0.10*df_z['LIQ'] + 0.10*df_z['BETA'])
L708         df_z['D_QAL']     = robust_z(0.35*df_z['QAL'] + 0.20*df_z['FCF'] + 0.15*df_z['CURR_RATIO'] - 0.15*df_z['DEBT2EQ'] - 0.15*df_z['EPS_VAR_8Q'])
L709         df_z['D_YLD']     = robust_z(0.45*df_z['DIV'] + 0.25*df_z['DIV_STREAK'] + 0.20*df_z['DIV_FCF_COVER'] - 0.10*df_z['DIV_VAR5'])
L710         df_z['D_TRD']     = robust_z(0.40*df_z.get('MA200_SLOPE_5M',0) - 0.30*df_z.get('EXT_200',0) + 0.15*df_z.get('NEAR_52W_HIGH',0) + 0.15*df_z['TR'])
L711
L712         # --- é‡ã¿ã¯ cfg ã‚’å„ªå…ˆï¼ˆå¤–éƒ¨ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ï¼‰ ---
L713         # â‘  å…¨éŠ˜æŸ„ã§ G/D ã‚¹ã‚³ã‚¢ã‚’ç®—å‡ºï¼ˆunmaskedï¼‰
L714         g_score_all = df_z.mul(pd.Series(cfg.weights.g)).sum(axis=1)
L715
L716         d_comp = pd.concat({
L717             'QAL': df_z['D_QAL'],
L718             'YLD': df_z['D_YLD'],
L719             'VOL': df_z['D_VOL_RAW'],
L720             'TRD': df_z['D_TRD']
L721         }, axis=1)
L722         dw = pd.Series(cfg.weights.d, dtype=float).reindex(['QAL','YLD','VOL','TRD']).fillna(0.0)
L723         globals()['D_WEIGHTS_EFF'] = dw.copy()
L724         d_score_all = d_comp.mul(dw, axis=1).sum(axis=1)
L725
L726         # â‘¡ ãƒ†ãƒ³ãƒ—ãƒ¬åˆ¤å®šï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ãã®ã¾ã¾ï¼‰
L727         mask = df['trend_template']
L728         if not bool(mask.any()):
L729             mask = ((df.get('P_OVER_LOW52', np.nan) >= 0.25) &
L730                 (df.get('NEAR_52W_HIGH', np.nan) >= -0.30) &
L731                 (df.get('RS', np.nan) >= 0.08) &
L732                 (df.get('MA200_SLOPE_1M', np.nan) > 0) &
L733                 (df.get('P_OVER_150', np.nan) > 0) & (df.get('P_OVER_200', np.nan) > 0) &
L734                 (df.get('MA150_OVER_200', np.nan) > 0) &
L735                 (df.get('MA50_OVER_150', np.nan) > 0) & (df.get('MA50_OVER_200', np.nan) > 0) &
L736                 (df.get('TR_str', np.nan) > 0)).fillna(False)
L737             df['trend_template'] = mask
L738
L739         # â‘¢ æ¡ç”¨ç”¨ã¯ maskã€è¡¨ç¤º/åˆ†æç”¨ã¯åˆ—ã§å…¨éŠ˜æŸ„ä¿å­˜
L740         g_score = g_score_all.loc[mask]
L741         Scorer.g_score = g_score
L742         df_z['GSC'] = g_score_all
L743         df_z['DSC'] = d_score_all
L744
L745         try:
L746             current = (pd.read_csv("current_tickers.csv")
L747                   .iloc[:, 0]
L748                   .str.upper()
L749                   .tolist())
L750         except FileNotFoundError:
L751             warnings.warn("current_tickers.csv not found â€” bonus skipped")
L752             current = []
L753
L754         mask_bonus = g_score.index.isin(current)
L755         if mask_bonus.any():
L756             # 1) factor.BONUS_COEFF ã‹ã‚‰ k ã‚’æ±ºã‚ã€ç„¡ã‘ã‚Œã° 0.4
L757             k = float(getattr(sys.modules.get("factor"), "BONUS_COEFF", 0.4))
L758             # 2) g å´ã® Ïƒ ã‚’å–ã‚Šã€NaN ãªã‚‰ 0 ã«ä¸¸ã‚ã‚‹
L759             sigma_g = g_score.std()
L760             if pd.isna(sigma_g):
L761                 sigma_g = 0.0
L762             bonus_g = round(k * sigma_g, 3)
L763             g_score.loc[mask_bonus] += bonus_g
L764             Scorer.g_score = g_score
L765             # 3) D å´ã‚‚åŒæ§˜ã« Ïƒ ã® NaN ã‚’ã‚±ã‚¢
L766             sigma_d = d_score_all.std()
L767             if pd.isna(sigma_d):
L768                 sigma_d = 0.0
L769             bonus_d = round(k * sigma_d, 3)
L770             d_score_all.loc[d_score_all.index.isin(current)] += bonus_d
L771
L772         try:
L773             df = _apply_growth_entry_flags(df, ib, self, win_breakout=5, win_pullback=5)
L774         except Exception:
L775             pass
L776
L777         df_full = df.copy()
L778         df_full_z = df_z.copy()
L779
L780         from factor import FeatureBundle  # type: ignore  # å®Ÿè¡Œæ™‚importãªã—ï¼ˆå¾ªç’°å›é¿ï¼‰
L781         return FeatureBundle(df=df,
L782             df_z=df_z,
L783             g_score=g_score,
L784             d_score_all=d_score_all,
L785             missing_logs=pd.DataFrame(missing_logs),
L786             df_full=df_full,
L787             df_full_z=df_full_z,
L788             scaler=None)
L789
L790 def _apply_growth_entry_flags(feature_df, bundle, self_obj, win_breakout=5, win_pullback=5):
L791     """
L792     Gæ ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã«å¯¾ã—ã€ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®š/æŠ¼ã—ç›®åç™ºã®ã€Œç›´è¿‘Nå–¶æ¥­æ—¥å†…ã®ç™ºç«ã€ã‚’åˆ¤å®šã—ã€
L793     æ¬¡ã®åˆ—ã‚’ feature_df ã«è¿½åŠ ã™ã‚‹ï¼ˆindex=tickerï¼‰ã€‚
L794       - G_BREAKOUT_recent_5d : bool
L795       - G_BREAKOUT_last_date : str "YYYY-MM-DD"
L796       - G_PULLBACK_recent_5d : bool
L797       - G_PULLBACK_last_date : str "YYYY-MM-DD"
L798       - G_PIVOT_price        : float
L799     å¤±æ•—ã—ã¦ã‚‚ä¾‹å¤–ã¯æ¡ã‚Šæ½°ã—ã€æ—¢å­˜å‡¦ç†ã‚’é˜»å®³ã—ãªã„ã€‚
L800     """
L801     try:
L802         px   = bundle.px                      # çµ‚å€¤ DataFrame
L803         hi   = bundle.data['High']
L804         lo   = bundle.data['Low']
L805         vol  = bundle.data['Volume']
L806         bench= bundle.spx                     # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ Series
L807
L808         # Gãƒ¦ãƒ‹ãƒãƒ¼ã‚¹æ¨å®šï¼šself.g_universe å„ªå…ˆ â†’ feature_df['group']=='G' â†’ å…¨éŠ˜æŸ„
L809         g_universe = getattr(self_obj, "g_universe", None)
L810         if g_universe is None:
L811             try:
L812                 g_universe = feature_df.index[feature_df['group'].astype(str).str.upper().eq('G')].tolist()
L813             except Exception:
L814                 g_universe = list(feature_df.index)
L815         if not g_universe:
L816             return feature_df
L817
L818         # æŒ‡æ¨™
L819         px = px.ffill(limit=2)
L820         ema21 = px[g_universe].ewm(span=21, adjust=False).mean()
L821         ma50  = px[g_universe].rolling(50).mean()
L822         ma150 = px[g_universe].rolling(150).mean()
L823         ma200 = px[g_universe].rolling(200).mean()
L824         atr20 = (hi[g_universe] - lo[g_universe]).rolling(20).mean()
L825         vol20 = vol[g_universe].rolling(20).mean()
L826         vol50 = vol[g_universe].rolling(50).mean()
L827
L828         # ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆåˆæ ¼
L829         trend_template_ok = (px[g_universe] > ma50) & (px[g_universe] > ma150) & (px[g_universe] > ma200) \
L830                             & (ma150 > ma200) & (ma200.diff() > 0)
L831
L832         # æ±ç”¨ãƒ”ãƒœãƒƒãƒˆï¼šç›´è¿‘65å–¶æ¥­æ—¥ã®é«˜å€¤ï¼ˆå½“æ—¥é™¤å¤–ï¼‰
L833         pivot_price = hi[g_universe].rolling(65).max().shift(1)
L834
L835         # ç›¸å¯¾åŠ›ï¼šå¹´å†…é«˜å€¤æ›´æ–°
L836         bench_aligned = bench.reindex(px.index).ffill()
L837         rs = px[g_universe].div(bench_aligned, axis=0)
L838         rs_high = rs.rolling(252).max().shift(1)
L839
L840         # ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆã€Œç™ºç”Ÿæ—¥ã€ï¼šæ¡ä»¶ç«‹ã¡ä¸ŠãŒã‚Š
L841         breakout_today = trend_template_ok & (px[g_universe] > pivot_price) \
L842                          & (vol[g_universe] >= 1.5 * vol50) & (rs > rs_high)
L843         breakout_event = breakout_today & ~breakout_today.shift(1).fillna(False)
L844
L845         # æŠ¼ã—ç›®åç™ºã€Œç™ºç”Ÿæ—¥ã€ï¼šEMA21å¸¯Ã—å‡ºæ¥é«˜ãƒ‰ãƒ©ã‚¤ã‚¢ãƒƒãƒ—Ã—å‰æ—¥é«˜å€¤è¶ŠãˆÃ—çµ‚å€¤EMA21ä¸Š
L846         near_ema21_band = px[g_universe].between(ema21 - atr20, ema21 + atr20)
L847         volume_dryup = (vol20 / vol50) <= 1.0
L848         pullback_bounce_confirmed = (px[g_universe] > hi[g_universe].shift(1)) & (px[g_universe] > ema21)
L849         pullback_today = trend_template_ok & near_ema21_band & volume_dryup & pullback_bounce_confirmed
L850         pullback_event = pullback_today & ~pullback_today.shift(1).fillna(False)
L851
L852         # ç›´è¿‘Nå–¶æ¥­æ—¥å†…ã®ç™ºç« / æœ€çµ‚ç™ºç”Ÿæ—¥
L853         rows = []
L854         for t in g_universe:
L855             def _recent_and_date(s, win):
L856                 sw = s[t].iloc[-win:]
L857                 if sw.any():
L858                     d = sw[sw].index[-1]
L859                     return True, d.strftime("%Y-%m-%d")
L860                 return False, ""
L861             br_recent, br_date = _recent_and_date(breakout_event, win_breakout)
L862             pb_recent, pb_date = _recent_and_date(pullback_event, win_pullback)
L863             rows.append((t, {
L864                 "G_BREAKOUT_recent_5d": br_recent,
L865                 "G_BREAKOUT_last_date": br_date,
L866                 "G_PULLBACK_recent_5d": pb_recent,
L867                 "G_PULLBACK_last_date": pb_date,
L868                 "G_PIVOT_price": float(pivot_price[t].iloc[-1]) if t in pivot_price.columns else float('nan'),
L869             }))
L870         flags = pd.DataFrame({k: v for k, v in rows}).T
L871
L872         # åˆ—ã‚’ä½œæˆãƒ»ä¸Šæ›¸ã
L873         cols = ["G_BREAKOUT_recent_5d","G_BREAKOUT_last_date","G_PULLBACK_recent_5d","G_PULLBACK_last_date","G_PIVOT_price"]
L874         for c in cols:
L875             if c not in feature_df.columns:
L876                 feature_df[c] = np.nan
L877         feature_df.loc[flags.index, flags.columns] = flags
L878
L879     except Exception:
L880         pass
L881     return feature_df
L882
```

## <.github/workflows/weekly-report.yml>
```text
L1 name: Weekly Stock Report
L2
L3 on:
L4   push:
L5     branches: [ main ]
L6     paths-ignore:
L7       - 'CodeForChat/**'
L8   schedule:
L9     - cron: '0 0 * * 6'  # UTC 00:00 â†’ JST 09:00ï¼ˆåœŸï¼‰
L10   workflow_dispatch:
L11
L12 jobs:
L13   build-and-report:
L14     runs-on: ubuntu-latest
L15     permissions:
L16       contents: write
L17
L18     steps:
L19       - name: Debug start
L20         run: echo 'ğŸš€ DEBUGstarted'
L21               
L22       - name: Checkout repository
L23         uses: actions/checkout@v3
L24
L25       - name: Setup Python
L26         uses: actions/setup-python@v5
L27         with:
L28           python-version: '3.x'
L29           cache: 'pip'
L30           cache-dependency-path: requirements.txt
L31
L32       - name: Install dependencies
L33         run: pip install -r requirements.txt
L34
L35       - name: Prepare results directory
L36         run: mkdir -p results
L37
L38       - name: Run factor & scoring
L39         env:
L40           SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
L41           FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
L42           FIN_THREADS: "8"
L43         run: python factor.py
```

## <documents/README.md>
```text
L1 # é‹ç”¨ãƒ«ãƒ¼ãƒ«
L2
L3 ## åŸºæœ¬æ§‹æˆ
L4 - 20éŠ˜æŸ„ã‚’å‡ç­‰é…åˆ†ï¼ˆç¾é‡‘ã‚’é™¤ã1éŠ˜æŸ„ã‚ãŸã‚Š5%ï¼‰
L5 - moomooè¨¼åˆ¸ã§é‹ç”¨
L6 - **Growthæ  12éŠ˜æŸ„ / Defenseæ  8éŠ˜æŸ„**ï¼ˆNORMAL åŸºæº–ï¼‰
L7
L8 ## Barbell Growth-Defenseæ–¹é‡
L9 - Growthæ  **12éŠ˜æŸ„**ï¼šé«˜æˆé•·ã§ä¹–é›¢æºã¨ãªã‚‹æ”»ã‚ã®éŠ˜æŸ„
L10 - Defenseæ  **8éŠ˜æŸ„**ï¼šä½ãƒœãƒ©ã§å®‰å®šæˆé•·ã—é…å½“ã‚’å¢—ã‚„ã™å®ˆã‚Šã®éŠ˜æŸ„
L11 - ã€ŒçŒ›çƒˆã«ä¼¸ã³ã‚‹æ”»ã‚ Ã— ç€å®Ÿã«ç¨¼ãç›¾ã€ã®çµ„åˆã›ã§ä¹–é›¢â†’åŠæˆ»ã—ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ã‚’ç‹™ã†
L12
L13 ## ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¤å®šï¼ˆtrend_template åˆæ ¼â€œæœ¬æ•°â€ã§åˆ¤å®šï¼‰
L14 - åˆæ ¼æœ¬æ•° = current+candidate å…¨ä½“ã®ã†ã¡ã€trend_template æ¡ä»¶ã‚’æº€ãŸã—ãŸéŠ˜æŸ„ã®**æœ¬æ•°(C)**ï¼ˆåŸºæº– N_G=12ï¼‰
L15 - ã—ãã„å€¤ã¯éå»~600å–¶æ¥­æ—¥ã®åˆ†å¸ƒã‹ã‚‰**æ¯å›è‡ªå‹•æ¡ç”¨**ï¼ˆåˆ†ä½ç‚¹ã¨é‹ç”¨â€œåºŠâ€ã®maxï¼‰
L16   - ç·Šæ€¥å…¥ã‚Š: `max(q05, 12æœ¬)`ï¼ˆ= N_Gï¼‰
L17   - ç·Šæ€¥è§£é™¤: `max(q20, 18æœ¬)`ï¼ˆ= ceil(1.5Ã—12)ï¼‰
L18   - é€šå¸¸å¾©å¸°: `max(q60, 36æœ¬)`ï¼ˆ= 3Ã—N_Gï¼‰
L19 - ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹: å‰å›ãƒ¢ãƒ¼ãƒ‰ã«ä¾å­˜ï¼ˆEMERGâ†’è§£é™¤ã¯23æœ¬ä»¥ä¸Šã€CAUTIONâ†’é€šå¸¸ã¯45æœ¬ä»¥ä¸Šï¼‰
L20
L21 ## ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¥ã®ç¾é‡‘ãƒ»ãƒ‰ãƒªãƒ•ãƒˆ
L22  - **é€šå¸¸(NORMAL)** : ç¾é‡‘ **10%** / ãƒ‰ãƒªãƒ•ãƒˆé–¾å€¤ **12%**
L23  - **è­¦æˆ’(CAUTION)** : ç¾é‡‘ **12.5%** / ãƒ‰ãƒªãƒ•ãƒˆé–¾å€¤ **14%**
L24  - **ç·Šæ€¥(EMERG)** : ç¾é‡‘ **20%** / **ãƒ‰ãƒªãƒ•ãƒˆå£²è²·åœæ­¢**ï¼ˆ20Ã—5%ã«å…¨æˆ»ã—ã®ã¿ï¼‰
L25
L26 ## ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®æ¨å¥¨â€œä¿æœ‰éŠ˜æŸ„æ•°â€ï¼ˆMMFâ‰’ç¾é‡‘ï¼‰
L27 *å„æ =5%ï¼ˆ20éŠ˜æŸ„å‡ç­‰ï¼‰ã€‚ãƒ¢ãƒ¼ãƒ‰ç§»è¡Œæ™‚ã¯**Gã®æ æ•°ã®ã¿**èª¿æ•´ã—ã€å¤–ã—ãŸæ ã¯ç¾é‡‘ã¨ã—ã¦ä¿æŒã€‚*
L28
L29 - **NORMAL:** G **12** / D **8** / ç¾é‡‘åŒ–æ  **0**  
L30 - **CAUTION:** G **10** / D **8** / ç¾é‡‘åŒ–æ  **2**ï¼ˆ= 10%ï¼‰  
L31 - **EMERG:** G **8**  / D **8** / ç¾é‡‘åŒ–æ  **4**ï¼ˆ= 20%ï¼‰  
L32
L33 > å®Ÿé‹ç”¨ï¼šâ­ï¸ä½ã‚¹ã‚³ã‚¢ã®Gã‹ã‚‰é †ã«å¤–ã™ã€‚è§£é™¤æ™‚ã¯factorä¸Šä½ã‹ã‚‰è£œå……ã€‚
L34
L35 ## ãƒˆãƒ¬ãƒ¼ãƒªãƒ³ã‚°ã‚¹ãƒˆãƒƒãƒ—
L36 - **åŸºæœ¬TS (ãƒ¢ãƒ¼ãƒ‰åˆ¥):** NORMAL **15%** / CAUTION **13%** / EMERG **10%**
L37 - å«ã¿ç›ŠãŒ **+30% / +60% / +100%** åˆ°é”ã§ã€åŸºæœ¬ã‹ã‚‰ **-3pt / -6pt / -8pt** å¼•ãä¸Šã’
L38 - TSç™ºå‹•ã§æ¸›å°‘ã—ãŸéŠ˜æŸ„ã¯ç¿Œæ—¥ä»¥é™ã«è£œå……ï¼ˆâ€»ç·Šæ€¥ãƒ¢ãƒ¼ãƒ‰ä¸­ã¯è£œå……ã—ãªã„ï¼‰
L39
L40 ## åŠæˆ»ã—ï¼ˆãƒªãƒãƒ©ãƒ³ã‚¹ï¼‰æ‰‹é †
L41 ãƒ‰ãƒªãƒ•ãƒˆãƒã‚§ãƒƒã‚¯ã§**ã‚¢ãƒ©ãƒ¼ãƒˆ**ãŒå‡ºãŸå ´åˆï¼ˆåˆè¨ˆ|drift| ãŒãƒ¢ãƒ¼ãƒ‰é–¾å€¤ã‚’è¶…éã€EMERGé™¤ãï¼‰ã€ç¿Œå–¶æ¥­æ—¥ã®ç±³å›½å¯„ä»˜ãã§ä¸‹è¨˜ã‚’å®Ÿæ–½ã™ã‚‹ã€‚
L42
L43 1. **å£²å´ï¼ˆå¿…é ˆï¼‰**  
L44    Slackãƒ†ãƒ¼ãƒ–ãƒ«ã® **Î”qty ãŒãƒã‚¤ãƒŠã‚¹ã®éŠ˜æŸ„ã‚’å£²å´** ã™ã‚‹ï¼ˆå¯„ä»˜ãæˆè¡Œæ¨å¥¨ï¼‰ã€‚  
L45    ã“ã‚Œã¯ã€ŒåŠæˆ»ã—ã€è¨ˆç®—ã«åŸºã¥ãéé‡é‡ã®å‰Šæ¸›ã‚’æ„å‘³ã™ã‚‹ã€‚
L46
L47 2. **è³¼å…¥ï¼ˆä»»æ„ãƒ»åŠæˆ»ã—ç›®å®‰ï¼‰**  
L48    åŠæˆ»ã—å¾Œã®åˆè¨ˆ|drift|ã‚’**ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤ï¼ˆSlackãƒ˜ãƒƒãƒ€ã«è¡¨ç¤ºï¼‰**ã«è¿‘ã¥ã‘ã‚‹ã“ã¨ã‚’ç›®å®‰ã«ã€  
L49    **ä»»æ„ã®éŠ˜æŸ„ã‚’è²·ã„å¢—ã—**ã—ã¦ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹ï¼ˆÎ”qtyãŒãƒ—ãƒ©ã‚¹ã®éŠ˜æŸ„ã‚’å„ªå…ˆã—ã¦ã‚‚ã‚ˆã„ï¼‰ã€‚
L50
L51 3. **ãƒˆãƒ¬ãƒ¼ãƒªãƒ³ã‚°ã‚¹ãƒˆãƒƒãƒ—ã®å†è¨­å®šï¼ˆå¿…é ˆï¼‰**  
L52    ã™ã¹ã¦ã®ä¿æœ‰éŠ˜æŸ„ã«ã¤ã„ã¦ã€æœ€æ–°ã®è©•ä¾¡é¡ã«åˆã‚ã›ã¦TSã‚’**å†ç™ºæ³¨ï¼æ›´æ–°**ã™ã‚‹ã€‚  
L53    ãƒ«ãƒ¼ãƒ«ã¯ä¸‹è¨˜ï¼ˆåˆ©ç›Šåˆ°é”ã§æ®µéšçš„ã«ã‚¿ã‚¤ãƒˆåŒ–ï¼‰ï¼š  
L54    - **åŸºæœ¬TS:** -15%  
L55    - **+30% åˆ°é” â†’ TS -12%**  
L56    - **+60% åˆ°é” â†’ TS -9%**  
L57    - **+100% åˆ°é” â†’ TS -7%**  
L58    â€»ã‚¹ãƒˆãƒƒãƒ—ä¾¡æ ¼ã®å¼•ãä¸Šã’ã¯è¨±å¯ã€**å¼•ãä¸‹ã’ã¯ä¸å¯**ï¼ˆåˆ©ç›Šä¿å…¨ã®åŸå‰‡ï¼‰ã€‚
L59
L60 4. **ä¾‹å¤–ï¼ˆEMERGãƒ¢ãƒ¼ãƒ‰ï¼‰**  
L61    ç·Šæ€¥(EMERG)ã§ã¯**ãƒ‰ãƒªãƒ•ãƒˆç”±æ¥ã®å£²è²·ã¯åœæ­¢ï¼ˆâˆï¼‰**ã€‚20éŠ˜æŸ„Ã—å„5%ã¸ã®**å…¨æˆ»ã—**ã®ã¿è¨±å®¹ã€‚
L62
L63 5. **å®Ÿè¡Œã‚¿ã‚¤ãƒŸãƒ³ã‚°**
L64    - åˆ¤å®šï¼šç±³å›½å¸‚å ´çµ‚å€¤ç›´å¾Œ
L65    - åŸ·è¡Œï¼šç¿Œå–¶æ¥­æ—¥ã®ç±³å›½å¯„ä»˜ãæˆè¡Œ
L66
L67 ## ãƒ¢ãƒ¼ãƒ‰ç§»è¡Œã®å®Ÿå‹™æ‰‹é †ï¼ˆè¶…ã‚·ãƒ³ãƒ—ãƒ«ï¼‰
L68 ãƒ¢ãƒ¼ãƒ‰ãŒå¤‰ã‚ã£ãŸã‚‰ã€**MMFâ‰’ç¾é‡‘**ã¨ã—ã¦æ‰±ã„ã€**Gã®æ æ•°ã ã‘**ã‚’èª¿æ•´ã™ã‚‹ï¼š
L69 1. **Gã‚’å‰Šã‚‹**ï¼ˆCAUTION/EMERGï¼‰  
L70    - â­ï¸ä½ã‚¹ã‚³ã‚¢ã®Gã‹ã‚‰é †ã«å¤–ã™ã€‚  
L71    - **`current_tickers.csv` ã‹ã‚‰å¤–ã™GéŠ˜æŸ„ã®è¡Œã‚’å‰Šé™¤**ï¼ˆï¼ãã®æ ã¯ç¾é‡‘åŒ–ï¼‰ã€‚
L72 2. **ç¾é‡‘ã¨ã—ã¦ä¿æŒ**  
L73    - å¤–ã—ãŸæ ã¯ç¾é‡‘ï¼ˆã¾ãŸã¯MMFç›¸å½“ï¼‰ã§ãƒ—ãƒ¼ãƒ«ã€‚  
L74 3. **å¾©å¸°æ™‚ã®è£œå……**ï¼ˆNORMALã¸ï¼‰  
L75    - **`current_tickers.csv` ã«éŠ˜æŸ„ã‚’è¿½åŠ **ï¼ˆfactorä¸Šä½ã‹ã‚‰ï¼‰ã€‚  
L76    - ä»¥é™ã¯æ—¥æ¬¡ãƒ‰ãƒªãƒ•ãƒˆ/TSãƒ«ãƒ¼ãƒ«ã«å¾“ã†ã€‚
L77
L78 > driftã¯ `target_ratio = 1/éŠ˜æŸ„æ•°` ã‚’è‡ªå‹•é©ç”¨ã€‚è¡Œæ•°ã«å¿œã˜ã¦è‡ªå‹•ã§å‡ç­‰æ¯”ç‡ãŒå†è¨ˆç®—ã•ã‚Œã‚‹ã€‚
L79
L80 ## å…¥æ›¿éŠ˜æŸ„é¸å®š
L81 - Oxfordã‚­ãƒ£ãƒ”ã‚¿ãƒ«ï¼ã‚¤ãƒ³ã‚«ãƒ ã€Alpha Investorã€Motley Fool Stock Advisorã€moomooã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ç­‰ã‚’å‚è€ƒã«chatGPTã§æ¤œè¨
L82 - å¹´é–“NISAæ ã¯Growthç¾¤ã®ä¸­ã‹ã‚‰ä½ãƒœãƒ©éŠ˜æŸ„ã‚’é¸å®šã—åˆ©ç”¨ã€‚é•·æœŸä¿æŒã«ã¯ã“ã ã‚ã‚‰ãªã„ã€‚
L83
L84 ## å†ã‚¨ãƒ³ãƒˆãƒªãƒ¼ï¼ˆã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ï¼‰
L85 - TSãƒ’ãƒƒãƒˆå¾Œã®åŒéŠ˜æŸ„å†INã¯ **8å–¶æ¥­æ—¥** ã®ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ã‚’è¨­ã‘ã‚‹ï¼ˆæœŸé–“ä¸­ã¯å†INç¦æ­¢ï¼‰
L86
L87 ## å®Ÿè¡Œã‚¿ã‚¤ãƒŸãƒ³ã‚°
L88 - åˆ¤å®šï¼šç±³å›½å¸‚å ´çµ‚å€¤ç›´å¾Œ
L89 - åŸ·è¡Œï¼šç¿Œå–¶æ¥­æ—¥ã®ç±³å›½å¯„ä»˜ãæˆè¡Œ
```

## <documents/factor_design.md>
```text
L1 # factor.py è©³ç´°è¨­è¨ˆæ›¸
L2
L3 ## æ¦‚è¦
L4 - æ—¢å­˜ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®éŠ˜æŸ„ã¨æ¤œè¨ä¸­ã®éŠ˜æŸ„ç¾¤ã‚’åŒæ™‚ã«æ‰±ã†éŠ˜æŸ„é¸å®šãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€‚
L5 - ä¾¡æ ¼ãƒ»è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šè¾¼ã¿ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã¨DRRSé¸å®šã‚’è¡Œã†ã“ã¨ã§ã€ä»¥ä¸‹ã®ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆã‚’å¾—ã‚‹ã€‚
L6   - æ¡ç”¨éŠ˜æŸ„ã¨æƒœã—ãã‚‚æ¼ã‚ŒãŸéŠ˜æŸ„ã®ã‚¹ã‚³ã‚¢ä¸€è¦§
L7   - IN/OUTã®ãƒ†ã‚£ãƒƒã‚«ãƒ¼ãƒªã‚¹ãƒˆã¨OUTå´ã®ä½ã‚¹ã‚³ã‚¢éŠ˜æŸ„
L8   - æ–°æ—§ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®æ¯”è¼ƒè¡¨
L9   - æ¤œè¨ä¸­éŠ˜æŸ„ã®ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆæ•´ç†ç”¨ï¼‰
L10
L11 ## å…¨ä½“ãƒ•ãƒ­ãƒ¼
L12 1. **Input** â€“ `current_tickers.csv`ã¨`candidate_tickers.csv`ã‚’èª­ã¿è¾¼ã¿ã€yfinanceã‚„Finnhubã®APIã‹ã‚‰ä¾¡æ ¼ãƒ»è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã¦`InputBundle`ã‚’æ•´å‚™ã€‚
L13 2. **Score Calculation** â€“ ScorerãŒç‰¹å¾´é‡ã‚’è¨ˆç®—ã—å› å­ã‚¹ã‚³ã‚¢ã‚’åˆæˆã—ã¦`FeatureBundle`ã‚’ç”Ÿæˆã€‚
L14 3. **Correlation Reduction & Selection** â€“ SelectorãŒDRRSãƒ­ã‚¸ãƒƒã‚¯ã§ç›¸é–¢ã‚’æŠ‘ãˆã¤ã¤G/DéŠ˜æŸ„ã‚’é¸å®šã—`SelectionBundle`ã‚’å¾—ã‚‹ã€‚
L15 4. **Output** â€“ æ¡ç”¨çµæœã¨å‘¨è¾ºæƒ…å ±ã‚’è¡¨ãƒ»Slacké€šçŸ¥ã¨ã—ã¦å‡ºåŠ›ã€‚
L16
L17 ```mermaid
L18 flowchart LR
L19   A[Input\nAPI & å‰å‡¦ç†] --> B[Score Calculation\nç‰¹å¾´é‡ãƒ»å› å­åˆæˆ]
L20   B --> C[Correlation Reduction\nDRRSé¸å®š]
L21   C --> D[Output\nSlacké€šçŸ¥]
L22 ```
L23
L24 ## å®šæ•°ãƒ»è¨­å®š
L25 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L26 | --- | --- | --- |
L27 | `exist` / `cand` | ç¾è¡Œãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã¨æ¤œè¨ä¸­éŠ˜æŸ„ã®ãƒ†ã‚£ãƒƒã‚«ãƒ¼ãƒªã‚¹ãƒˆ | ã‚¹ã‚³ã‚¢å¯¾è±¡ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã®æ§‹æˆã€å€™è£œæ•´ç† |
L28 | `bench` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼ | ç›¸å¯¾å¼·ã•ãƒ»Î²ç®—å‡ºã€ãƒãƒ¼ãƒˆæ¯”è¼ƒ |
L29 | `CAND_PRICE_MAX` | å€™è£œéŠ˜æŸ„ã®è¨±å®¹ä¾¡æ ¼ä¸Šé™ | é«˜é¡éŠ˜æŸ„ã®äº‹å‰é™¤å¤– |
L30 | `N_G` / `N_D` | G/Dæ¡ç”¨æ ã®ä»¶æ•°ï¼ˆ**æ—¢å®š: 12 / 8**ï¼‰ | æœ€çµ‚çš„ã«é¸ã¶éŠ˜æŸ„æ•°ã®åˆ¶ç´„ |
L31 | `g_weights` / `D_weights` | å„å› å­ã®é‡ã¿dict | G/Dã‚¹ã‚³ã‚¢åˆæˆ |
L32 | `D_BETA_MAX` | Dãƒã‚±ãƒƒãƒˆã®è¨±å®¹Î²ä¸Šé™ | é«˜Î²éŠ˜æŸ„ã®é™¤å¤–ãƒ•ã‚£ãƒ«ã‚¿ |
L33 | `FILTER_SPEC` | G/Dã”ã¨ã®å‰å‡¦ç†ãƒ•ã‚£ãƒ«ã‚¿ | ãƒˆãƒ¬ãƒ³ãƒ‰ãƒã‚¹ã‚¯ã‚„Î²ä¸Šé™è¨­å®š |
L34 | `corrM` | DRRSåˆæœŸãƒ—ãƒ¼ãƒ«ã®æœ€å¤§ä»¶æ•° | ç›¸é–¢è¡Œåˆ—ã‚µã‚¤ã‚ºåˆ¶å¾¡ |
L35 | `DRRS_G` / `DRRS_D` | DRRSãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿dict | ãƒã‚±ãƒƒãƒˆåˆ¥ã®ç›¸é–¢ä½æ¸›è¨­å®š |
L36 | `DRRS_SHRINK` | æ®‹å·®ç›¸é–¢ã®å¯¾è§’ã‚·ãƒ¥ãƒªãƒ³ã‚¯ç‡ | `residual_corr`ã®å®‰å®šåŒ– |
L37 | `CROSS_MU_GD` | G-Dé–“ã‚¯ãƒ­ã‚¹ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£Î¼ | 2ãƒã‚±ãƒƒãƒˆåŒæ™‚æœ€é©åŒ–ã§ç›¸é–¢æŠ‘åˆ¶ |
L38 | `RESULTS_DIR` | é¸å®šçµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª | `_save_sel`/`_load_prev`ã®å…¥å‡ºåŠ› |
L39
L40 é¸å®šçµæœã¯`results/`é…ä¸‹ã«JSONã¨ã—ã¦ä¿å­˜ã—ã€æ¬¡å›å®Ÿè¡Œæ™‚ã«`_load_prev`ã§èª­ã¿è¾¼ã‚“ã§é¸å®šæ¡ä»¶ã«åæ˜ ã€‚
L41
L42 ## DTO/Config
L43 å„ã‚¹ãƒ†ãƒƒãƒ—é–“ã§å—ã‘æ¸¡ã™ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¨è¨­å®šå€¤ã€‚å¤‰æ•°ã®æ„å‘³åˆã„ã¨åˆ©ç”¨ç®‡æ‰€ã‚’ä»¥ä¸‹ã«ç¤ºã™ã€‚
L44
L45 ### InputBundleï¼ˆInput â†’ Scorerï¼‰
L46 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L47 | --- | --- | --- |
L48 | `cand` | å€™è£œéŠ˜æŸ„ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã®ãƒªã‚¹ãƒˆ | OUTãƒ†ãƒ¼ãƒ–ãƒ«ã‚„ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°å¯¾è±¡ã®æ¯é›†å›£ |
L49 | `tickers` | ç¾è¡Œ+å€™è£œã‚’åˆã‚ã›ãŸãƒ†ã‚£ãƒƒã‚«ãƒ¼ä¸€è¦§ | ä¾¡æ ¼ãƒ»å‡ºæ¥é«˜ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€ãƒªã‚¿ãƒ¼ãƒ³è¨ˆç®— |
L50 | `bench` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼ | ç›¸å¯¾å¼·ã•ãƒ»Î²ç®—å‡ºã€ãƒãƒ¼ãƒˆæ¯”è¼ƒ |
L51 | `data` | yfinanceã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœï¼ˆéšå±¤åˆ—ï¼‰ | `px`/`spx`/ãƒªã‚¿ãƒ¼ãƒ³ç­‰ã®åŸºç¤ãƒ‡ãƒ¼ã‚¿ |
L52 | `px` | `data['Close']`ã ã‘ã‚’æŠœãå‡ºã—ãŸä¾¡æ ¼ç³»åˆ— | æŒ‡æ¨™è¨ˆç®—ãƒ»ãƒªã‚¿ãƒ¼ãƒ³ç”Ÿæˆ |
L53 | `spx` | `data['Close'][bench]` ã®Series | `rs`ã‚„`calc_beta`ã®åŸºæº–æŒ‡æ•° |
L54 | `tickers_bulk` | `yf.Tickers`ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ | `info`ç­‰ã®ä¸€æ‹¬å–å¾— |
L55 | `info` | ãƒ†ã‚£ãƒƒã‚«ãƒ¼åˆ¥ã®yfinanceæƒ…å ±dict | ã‚»ã‚¯ã‚¿ãƒ¼åˆ¤å®šã‚„EPSè£œå®Œ |
L56 | `eps_df` | EPS TTM/ç›´è¿‘EPSç­‰ã‚’ã¾ã¨ã‚ãŸè¡¨ | æˆé•·æŒ‡æ¨™ã®ç®—å‡º |
L57 | `fcf_df` | CFOãƒ»CapExãƒ»FCF TTMã¨æƒ…å ±æºãƒ•ãƒ©ã‚° | FCF/EVã‚„é…å½“ã‚«ãƒãƒ¬ãƒƒã‚¸ |
L58 | `returns` | `px.pct_change()`ã®ãƒªã‚¿ãƒ¼ãƒ³è¡¨ | ç›¸é–¢è¡Œåˆ—ãƒ»DRRSè¨ˆç®— |
L59
L60 ### FeatureBundleï¼ˆScorer â†’ Selectorï¼‰
L61 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L62 | --- | --- | --- |
L63 | `df` | è¨ˆç®—æ¸ˆã¿æŒ‡æ¨™ã®ç”Ÿå€¤ãƒ†ãƒ¼ãƒ–ãƒ« | ãƒ‡ãƒãƒƒã‚°ãƒ»å‡ºåŠ›è¡¨ç¤º |
L64 | `df_z` | ã‚¦ã‚£ãƒ³ã‚¶ãƒ¼å¾ŒZã‚¹ã‚³ã‚¢åŒ–ã—ãŸæŒ‡æ¨™è¡¨ | å› å­ã‚¹ã‚³ã‚¢åˆæˆã€é¸å®šåŸºæº– |
L65 | `g_score` | Gãƒã‚±ãƒƒãƒˆç·åˆã‚¹ã‚³ã‚¢ | Gé¸å®šã€IN/OUTæ¯”è¼ƒ |
L66 | `d_score_all` | Dãƒã‚±ãƒƒãƒˆç·åˆã‚¹ã‚³ã‚¢ï¼ˆå…¨éŠ˜æŸ„ï¼‰ | Dé¸å®šã€ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚° |
L67 | `missing_logs` | æ¬ ææŒ‡æ¨™ã¨è£œå®ŒçŠ¶æ³ã®ãƒ­ã‚° | ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ |
L68
L69 ### SelectionBundleï¼ˆSelector â†’ Outputï¼‰
L70 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L71 | --- | --- | --- |
L72 | `resG` | Gé¸å®šçµæœã®è©³ç´°dictï¼ˆ`tickers`ã€ç›®çš„å€¤ç­‰ï¼‰ | çµæœä¿å­˜ãƒ»å¹³å‡ç›¸é–¢ãªã©ã®æŒ‡æ¨™è¡¨ç¤º |
L73 | `resD` | Dé¸å®šçµæœã®è©³ç´°dict | åŒä¸Š |
L74 | `top_G` | æœ€çµ‚æ¡ç”¨Gãƒ†ã‚£ãƒƒã‚«ãƒ¼ | æ–°ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæ§‹ç¯‰ |
L75 | `top_D` | æœ€çµ‚æ¡ç”¨Dãƒ†ã‚£ãƒƒã‚«ãƒ¼ | åŒä¸Š |
L76 | `init_G` | DRRSå‰ã®GåˆæœŸå€™è£œ | æƒœã—ãã‚‚å¤–ã‚ŒãŸéŠ˜æŸ„è¡¨ç¤º |
L77 | `init_D` | DRRSå‰ã®DåˆæœŸå€™è£œ | åŒä¸Š |
L78
L79 ### WeightsConfig
L80 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L81 | --- | --- | --- |
L82 | `g` | Gå› å­ï¼ˆGRW/MOM/VOLï¼‰ã®é‡ã¿dict | `g_score`åˆæˆ |
L83 | `d` | Då› å­ï¼ˆD_QAL/D_YLD/D_VOL_RAW/D_TRDï¼‰ã®é‡ã¿dict | `d_score_all`åˆæˆ |
L84
L85 ### DRRSParams
L86 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L87 | --- | --- | --- |
L88 | `corrM` | DRRSåˆæœŸãƒ—ãƒ¼ãƒ«ã®æœ€å¤§ä»¶æ•° | ç›¸é–¢è¡Œåˆ—ã‚µã‚¤ã‚ºåˆ¶å¾¡ |
L89 | `shrink` | æ®‹å·®ç›¸é–¢ã®ã‚·ãƒ¥ãƒªãƒ³ã‚¯ç‡ | `residual_corr`ã®å¯¾è§’å¼·èª¿ |
L90 | `G` | Gãƒã‚±ãƒƒãƒˆç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿dictï¼ˆ`lookback`ç­‰ï¼‰ | `select_bucket_drrs`è¨­å®š |
L91 | `D` | Dãƒã‚±ãƒƒãƒˆç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿dict | åŒä¸Š |
L92 | `cross_mu_gd` | G-Dã‚¯ãƒ­ã‚¹ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£ä¿‚æ•°Î¼ | `select_buckets`ã®ç›®çš„é–¢æ•° |
L93
L94 ### PipelineConfig
L95 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L96 | --- | --- | --- |
L97 | `weights` | `WeightsConfig`ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ | ã‚¹ã‚³ã‚¢åˆæˆã®é‡ã¿å‚ç…§ |
L98 | `drrs` | `DRRSParams`ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ | é¸å®šã‚¹ãƒ†ãƒƒãƒ—ã®è¨­å®šå€¤ |
L99 | `price_max` | å€™è£œéŠ˜æŸ„ã®è¨±å®¹ä¾¡æ ¼ä¸Šé™ | Inputæ®µéšã§ã®ãƒ•ã‚£ãƒ«ã‚¿ |
L100
L101 ## å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
L102 - `winsorize_s` / `robust_z` : å¤–ã‚Œå€¤å‡¦ç†ã¨Zã‚¹ã‚³ã‚¢åŒ–ã€‚
L103 - `_safe_div` / `_safe_last` : ä¾‹å¤–ã‚’æ½°ã—ãŸåˆ†å‰²ãƒ»æœ«å°¾å–å¾—ã€‚
L104 - `_load_prev` / `_save_sel` : é¸å®šçµæœã®èª­ã¿æ›¸ãã€‚
L105
L106 ## ã‚¯ãƒ©ã‚¹è¨­è¨ˆ
L107 ### Step1: Input
L108 `current_tickers.csv`ã®ç¾è¡ŒéŠ˜æŸ„ã¨`candidate_tickers.csv`ã®æ¤œè¨ä¸­éŠ˜æŸ„ã‚’èµ·ç‚¹ã«ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„ã™ã‚‹ã€‚å¤–éƒ¨I/Oã¨å‰å‡¦ç†ã‚’æ‹…å½“ã—ã€`prepare_data`ã§`InputBundle`ã‚’ç”Ÿæˆã€‚ä¾¡æ ¼ãƒ»è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã¯**yfinanceã‚’å„ªå…ˆã—ã€æ¬ æãŒã‚ã‚‹æŒ‡æ¨™ã®ã¿Finnhub APIã§è£œå®Œ**ã™ã‚‹ã€‚
L109 ä¸»ãªãƒ¡ã‚½ãƒƒãƒ‰:
L110 - `impute_eps_ttm` : å››åŠæœŸEPSÃ—4ã§TTMã‚’æ¨å®šã—æ¬ ææ™‚ã®ã¿å·®ã—æ›¿ãˆã€‚
L111 - `fetch_cfo_capex_ttm_yf` : yfinanceã®å››åŠæœŸ/å¹´æ¬¡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼ã‹ã‚‰CFOãƒ»CapExãƒ»FCF TTMã‚’ç®—å‡ºã€‚
L112 - `fetch_cfo_capex_ttm_finnhub` : yfinanceã§æ¬ ã‘ãŸéŠ˜æŸ„ã®ã¿Finnhub APIã§è£œå®Œã€‚
L113 - `compute_fcf_with_fallback` : yfinanceå€¤ã‚’åŸºæº–ã«Finnhubå€¤ã§ç©´åŸ‹ã‚ã—ã€CFO/CapEx/FCFã¨æƒ…å ±æºãƒ•ãƒ©ã‚°ã‚’è¿”ã™ã€‚
L114 - `_build_eps_df` : `info`ã‚„`quarterly_earnings`ã‹ã‚‰EPS TTMã¨ç›´è¿‘EPSã‚’è¨ˆç®—ã—ã€`impute_eps_ttm`ã§è£œå®Œã€‚
L115 - `prepare_data` :
L116     0. CSVã‹ã‚‰ç¾è¡ŒéŠ˜æŸ„ã¨å€™è£œéŠ˜æŸ„ã®ãƒ†ã‚£ãƒƒã‚«ãƒ¼ä¸€è¦§ã‚’èª­ã¿è¾¼ã‚€ã€‚
L117     1. å€™è£œéŠ˜æŸ„ã®ç¾åœ¨å€¤ã‚’å–å¾—ã—ä¾¡æ ¼ä¸Šé™ã§ãƒ•ã‚£ãƒ«ã‚¿ã€‚
L118     2. æ—¢å­˜+å€™è£œã‹ã‚‰å¯¾è±¡ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚’æ±ºå®šã—ã€ä¾¡æ ¼ãƒ»å‡ºæ¥é«˜ã‚’ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆyfinanceï¼‰ã€‚
L119     3. yfinanceå€¤ã‚’åŸºã«EPS/FCFãƒ†ãƒ¼ãƒ–ãƒ«ã‚„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç³»åˆ—ã€ãƒªã‚¿ãƒ¼ãƒ³ã‚’æ§‹ç¯‰ã—ã€æ¬ æã‚»ãƒ«ã¯Finnhubå‘¼ã³å‡ºã—ã§ç©´åŸ‹ã‚ã€‚
L120     4. ä¸Šè¨˜ã‚’`InputBundle`ã«æ ¼ç´ã—ã¦è¿”ã™ã€‚
L121
L122 ### Step2: Score Calculation (Scorer)
L123 ç‰¹å¾´é‡è¨ˆç®—ã¨ã‚¹ã‚³ã‚¢åˆæˆã‚’æ‹…å½“ã—ã€`FeatureBundle`ã‚’è¿”ã™ã€‚
L124
L125 #### è£œåŠ©é–¢æ•°
L126 - `trend(s)` : 50/150/200æ—¥ç§»å‹•å¹³å‡ã‚„52é€±ãƒ¬ãƒ³ã‚¸ã‹ã‚‰-0.5ã€œ0.5ã§æ§‹æˆã•ã‚ŒãŸãƒˆãƒ¬ãƒ³ãƒ‰æŒ‡æ¨™ã€‚
L127 - `rs(s,b)` / `tr_str(s)` / `rs_line_slope(s,b,win)` : ç›¸å¯¾å¼·ã•ã‚„çŸ­æœŸãƒˆãƒ¬ãƒ³ãƒ‰ã€RSå›å¸°å‚¾ãã‚’ç®—å‡ºã€‚
L128 - `ev_fallback` : `enterpriseValue`æ¬ ææ™‚ã«è² å‚µãƒ»ç¾é‡‘ã‹ã‚‰EVã‚’æ¨å®šã€‚
L129 - `dividend_status` / `div_streak` : é…å½“æœªè¨­å®šçŠ¶æ³ã®åˆ¤å®šã¨å¢—é…å¹´æ•°ã‚«ã‚¦ãƒ³ãƒˆã€‚
L130 - `fetch_finnhub_metrics` : Finnhub APIã‹ã‚‰EPSæˆé•·ãƒ»ROEãƒ»Î²ãªã©ä¸è¶³æŒ‡æ¨™ã‚’å–å¾—ã€‚
L131 - `calc_beta` : ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ã®å…±åˆ†æ•£ã‹ã‚‰Î²ã‚’ç®—å‡ºã€‚
L132 - `spx_to_alpha` : S&P500ã®ä½ç½®æƒ…å ±ã‹ã‚‰DRRSã§ç”¨ã„ã‚‹Î±ã‚’æ¨å®šã€‚
L133 - `soft_cap_effective_scores` / `pick_top_softcap` : ã‚»ã‚¯ã‚¿ãƒ¼ã‚½ãƒ•ãƒˆã‚­ãƒ£ãƒƒãƒ—ä»˜ãã‚¹ã‚³ã‚¢èª¿æ•´ã¨ä¸Šä½æŠ½å‡ºã€‚
L134
L135 **è£œåŠ©é–¢æ•°ã¨ç”ŸæˆæŒ‡æ¨™**
L136
L137 | è£œåŠ©é–¢æ•° | ç”ŸæˆæŒ‡æ¨™ | ç•¥ç§° |
L138 | --- | --- | --- |
L139 | `trend` | ãƒˆãƒ¬ãƒ³ãƒ‰ç·åˆå€¤ | `TR` |
L140 | `rs` | ç›¸å¯¾å¼·ã• | `RS` |
L141 | `tr_str` | ä¾¡æ ¼ã¨50æ—¥ç·šã®ä¹–é›¢ | `TR_str` |
L142 | `rs_line_slope` | RSç·šã®å›å¸°å‚¾ã | `RS_SLOPE_*` |
L143 | `calc_beta` | Î² | `BETA` |
L144 | `div_streak` | é€£ç¶šå¢—é…å¹´æ•° | `DIV_STREAK` |
L145
L146 #### `aggregate_scores` è©³ç´°
L147 1. å„éŠ˜æŸ„ã®ä¾¡æ ¼ç³»åˆ—ã‚„`info`ã‚’åŸºã«ä»¥ä¸‹ã‚’ç®—å‡ºã€‚
L148    - **ãƒˆãƒ¬ãƒ³ãƒ‰/ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ **: `TR`ã€`RS`ã€`TR_str`ã€å¤šæ§˜ãªç§»å‹•å¹³å‡æ¯”ã€`RS_SLOPE_*`ãªã©ã€‚
L149    - **ãƒªã‚¹ã‚¯**: `BETA`ã€`DOWNSIDE_DEV`ã€`MDD_1Y`ã€`RESID_VOL`ã€`DOWN_OUTPERF`ã€`EXT_200`ç­‰ã€‚
L150    - **é…å½“**: `DIV`ã€`DIV_TTM_PS`ã€`DIV_VAR5`ã€`DIV_YOY`ã€`DIV_FCF_COVER`ã€`DIV_STREAK`ã€‚
L151    - **è²¡å‹™ãƒ»æˆé•·**: `EPS`ã€`REV`ã€`ROE`ã€`FCF/EV`ã€`REV_Q_YOY`ã€`EPS_Q_YOY`ã€`REV_YOY_ACC`ã€`REV_YOY_VAR`ã€`REV_ANN_STREAK`ã€`RULE40`ã€`FCF_MGN` ç­‰ã€‚
L152    - **å®‰å®šæ€§/ã‚µã‚¤ã‚º**: `DEBT2EQ`ã€`CURR_RATIO`ã€`MARKET_CAP`ã€`ADV60_USD`ã€`EPS_VAR_8Q`ãªã©ã€‚
L153 2. æŒ‡æ¨™æ¬ æã¯Finnhub APIç­‰ã§è£œå®Œã—ã€æœªå–å¾—é …ç›®ã‚’`missing_logs`ã«è¨˜éŒ²ã€‚
L154 3. `winsorize_s`â†’`robust_z`ã§æ¨™æº–åŒ–ã—`df_z`ã¸ä¿å­˜ã€‚ã‚µã‚¤ã‚ºãƒ»æµå‹•æ€§ã¯å¯¾æ•°å¤‰æ›ã€‚
L155 4. æ­£è¦åŒ–æ¸ˆæŒ‡æ¨™ã‹ã‚‰å› å­ã‚¹ã‚³ã‚¢ã‚’åˆæˆã€‚
L156    - å„å› å­ã®æ§‹æˆã¨é‡ã¿ã¯ä»¥ä¸‹ã®é€šã‚Šã€‚
L157      - **GRW**: 0.30Ã—`REV` + 0.20Ã—`EPS_Q_YOY` + 0.15Ã—`REV_Q_YOY` + 0.15Ã—`REV_YOY_ACC` + 0.10Ã—`RULE40` + 0.10Ã—`FCF_MGN` + 0.10Ã—`REV_ANN_STREAK` âˆ’ 0.05Ã—`REV_YOY_VAR`ã€‚
L158      - **MOM**: 0.40Ã—`RS` + 0.15Ã—`TR_str` + 0.15Ã—`RS_SLOPE_6W` + 0.15Ã—`RS_SLOPE_13W` + 0.10Ã—`MA200_SLOPE_5M` + 0.10Ã—`MA200_UP_STREAK_D`ã€‚
L159      - **VOL**: `BETA`å˜ä½“ã‚’ä½¿ç”¨ã€‚
L160      - **QAL**: 0.60Ã—`FCF_W` + 0.40Ã—`ROE_W`ã§ä½œæˆã€‚
L161      - **YLD**: 0.30Ã—`DIV` + 0.70Ã—`DIV_STREAK`ã€‚
L162      - **D_QAL**: 0.35Ã—`QAL` + 0.20Ã—`FCF` + 0.15Ã—`CURR_RATIO` âˆ’ 0.15Ã—`DEBT2EQ` âˆ’ 0.15Ã—`EPS_VAR_8Q`ã€‚
L163      - **D_YLD**: 0.45Ã—`DIV` + 0.25Ã—`DIV_STREAK` + 0.20Ã—`DIV_FCF_COVER` âˆ’ 0.10Ã—`DIV_VAR5`ã€‚
L164      - **D_VOL_RAW**: 0.40Ã—`DOWNSIDE_DEV` + 0.22Ã—`RESID_VOL` + 0.18Ã—`MDD_1Y` âˆ’ 0.10Ã—`DOWN_OUTPERF` âˆ’ 0.05Ã—`EXT_200` âˆ’ 0.08Ã—`SIZE` âˆ’ 0.10Ã—`LIQ` + 0.10Ã—`BETA`ã€‚
L165      - **D_TRD**: 0.40Ã—`MA200_SLOPE_5M` âˆ’ 0.30Ã—`EXT_200` + 0.15Ã—`NEAR_52W_HIGH` + 0.15Ã—`TR`ã€‚
L166     - ä¸»ãªæŒ‡æ¨™ã®ç•¥ç§°ã¨æ„å‘³:
L167
L168       | ç•¥ç§° | è£œåŠ©é–¢æ•° | æ¦‚è¦ |
L169       | --- | --- | --- |
L170       | TR | `trend` | 50/150/200æ—¥ç§»å‹•å¹³å‡ã¨52é€±ãƒ¬ãƒ³ã‚¸ã‚’çµ„ã¿åˆã‚ã›ãŸãƒˆãƒ¬ãƒ³ãƒ‰ç·åˆå€¤ |
L171       | RS | `rs` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«å¯¾ã™ã‚‹ç›¸å¯¾å¼·ã•ï¼ˆ12M/1Mãƒªã‚¿ãƒ¼ãƒ³å·®ï¼‰ |
L172       | TR_str | `tr_str` | ä¾¡æ ¼ã¨50æ—¥ç§»å‹•å¹³å‡ã®ä¹–é›¢ |
L173       | RS_SLOPE_6W | `rs_line_slope` | ç›¸å¯¾å¼·ã•ç·šã®6é€±å›å¸°å‚¾ã |
L174       | RS_SLOPE_13W | `rs_line_slope` | ç›¸å¯¾å¼·ã•ç·šã®13é€±å›å¸°å‚¾ã |
L175       | MA200_SLOPE_5M | - | 200æ—¥ç§»å‹•å¹³å‡ã®5ã‹æœˆé¨°è½ç‡ |
L176       | MA200_UP_STREAK_D | - | 200æ—¥ç·šãŒé€£ç¶šã§ä¸Šå‘ã„ãŸæ—¥æ•° |
L177       | BETA | `calc_beta` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«å¯¾ã™ã‚‹Î² |
L178       | DOWNSIDE_DEV | - | ä¸‹æ–¹ãƒªã‚¿ãƒ¼ãƒ³ã®ã¿ã®å¹´ç‡åŒ–æ¨™æº–åå·® |
L179       | RESID_VOL | - | Î²ã§èª¿æ•´ã—ãŸæ®‹å·®ãƒªã‚¿ãƒ¼ãƒ³ã®å¹´ç‡åŒ–æ¨™æº–åå·® |
L180       | MDD_1Y | - | éå»1å¹´ã®æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³ |
L181       | DOWN_OUTPERF | - | å¸‚å ´ä¸‹è½æ—¥ã«å¯¾ã™ã‚‹å¹³å‡è¶…éãƒªã‚¿ãƒ¼ãƒ³ |
L182       | EXT_200 | - | 200æ—¥ç§»å‹•å¹³å‡ã‹ã‚‰ã®çµ¶å¯¾ä¹–é›¢ç‡ |
L183       | NEAR_52W_HIGH | - | 52é€±é«˜å€¤ã¾ã§ã®ä¸‹æ–¹è·é›¢ï¼ˆ0=é«˜å€¤ï¼‰ |
L184       | FCF_W | - | ã‚¦ã‚£ãƒ³ã‚¶ãƒ¼å‡¦ç†å¾Œã®FCF/EV |
L185       | ROE_W | - | ã‚¦ã‚£ãƒ³ã‚¶ãƒ¼å‡¦ç†å¾Œã®ROE |
L186       | FCF | - | FCF/EV |
L187       | QAL | - | FCF_Wã¨ROE_Wã‚’çµ„ã¿åˆã‚ã›ãŸå“è³ªã‚¹ã‚³ã‚¢ |
L188       | CURR_RATIO | - | æµå‹•æ¯”ç‡ |
L189       | DEBT2EQ | - | è² å‚µè³‡æœ¬å€ç‡ |
L190       | EPS_VAR_8Q | - | EPSã®8å››åŠæœŸæ¨™æº–åå·® |
L191       | DIV | - | å¹´ç‡æ›ç®—é…å½“åˆ©å›ã‚Š |
L192       | DIV_STREAK | `div_streak` | é€£ç¶šå¢—é…å¹´æ•° |
L193       | DIV_FCF_COVER | - | é…å½“ã®FCFã‚«ãƒãƒ¬ãƒƒã‚¸ |
L194       | DIV_VAR5 | - | 5å¹´é…å½“å¤‰å‹•ç‡ |
L195       | DIV_TTM_PS | - | 1æ ªå½“ãŸã‚ŠTTMé…å½“ |
L196       | DIV_YOY | - | å‰å¹´æ¯”é…å½“æˆé•·ç‡ |
L197       | REV | - | å£²ä¸Šæˆé•·ç‡TTM |
L198       | EPS_Q_YOY | - | å››åŠæœŸEPSã®å‰å¹´åŒæœŸæ¯” |
L199       | REV_Q_YOY | - | å››åŠæœŸå£²ä¸Šã®å‰å¹´åŒæœŸæ¯” |
L200       | REV_YOY_ACC | - | å£²ä¸Šæˆé•·ç‡ã®åŠ é€Ÿåˆ† |
L201       | RULE40 | - | å£²ä¸Šæˆé•·ç‡ã¨FCFãƒãƒ¼ã‚¸ãƒ³ã®åˆè¨ˆ |
L202       | FCF_MGN | - | FCFãƒãƒ¼ã‚¸ãƒ³ |
L203       | REV_ANN_STREAK | - | å¹´æ¬¡å£²ä¸Šæˆé•·ã®é€£ç¶šå¹´æ•° |
L204       | REV_YOY_VAR | - | å¹´æ¬¡å£²ä¸Šæˆé•·ç‡ã®å¤‰å‹•æ€§ |
L205       | SIZE | - | æ™‚ä¾¡ç·é¡ã®å¯¾æ•°å€¤ |
L206       | LIQ | - | 60æ—¥å¹³å‡å‡ºæ¥é«˜ãƒ‰ãƒ«ã®å¯¾æ•°å€¤ |
L207    - Gãƒã‚±ãƒƒãƒˆ: `GRW`ã€`MOM`ã€`VOL`ã‚’`cfg.weights.g`ï¼ˆ0.40/0.45/-0.15ï¼‰ã§åŠ é‡ã—`g_score`ã‚’å¾—ã‚‹ã€‚
L208    - Dãƒã‚±ãƒƒãƒˆ: `D_QAL`ã€`D_YLD`ã€`D_VOL_RAW`ã€`D_TRD`ã‚’`cfg.weights.d`ï¼ˆ0.15/0.15/-0.45/0.25ï¼‰ã§åŠ é‡ã—`d_score_all`ã‚’ç®—å‡ºã€‚
L209    - ã‚»ã‚¯ã‚¿ãƒ¼capã«ã‚ˆã‚‹`soft_cap_effective_scores`ã‚’é©ç”¨ã—ã€Gæ¡ç”¨éŠ˜æŸ„ã«ã¯ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨ã€‚
L210 5. `_apply_growth_entry_flags`ã§ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆ/æŠ¼ã—ç›®ç™ºç«çŠ¶æ³ã‚’ä»˜åŠ ã—ã€`FeatureBundle`ã‚’è¿”ã™ã€‚
L211
L212 ### Step3: Correlation Reduction & Selection (Selector)
L213 DRRSã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ç›¸é–¢ã‚’æŠ‘ãˆãŸéŠ˜æŸ„é¸å®šã‚’è¡Œã„ã€`SelectionBundle`ã‚’è¿”ã™ã€‚`results/`ã«ä¿å­˜ã•ã‚ŒãŸå‰å›é¸å®šï¼ˆ`G_selection.json` / `D_selection.json`ï¼‰ã‚’`_load_prev`ã§èª­ã¿è¾¼ã¿ã€ç›®çš„å€¤ãŒå¤§ããæ‚ªåŒ–ã—ãªã„é™ã‚Šç¶­æŒã™ã‚‹ã€‚æ–°ã—ã„æ¡ç”¨é›†åˆã¯`_save_sel`ã§JSONã«æ›¸ãå‡ºã—æ¬¡å›ä»¥é™ã®å…¥åŠ›ã«å‚™ãˆã‚‹ã€‚
L214 ä¸»ãªãƒ¡ã‚½ãƒƒãƒ‰:
L215 - `residual_corr` : åç›Šç‡è¡Œåˆ—ã‚’Zã‚¹ã‚³ã‚¢åŒ–ã—ã€ä¸Šä½ä¸»æˆåˆ†ã‚’é™¤å»ã—ãŸæ®‹å·®ã‹ã‚‰ç›¸é–¢è¡Œåˆ—ã‚’æ±‚ã‚ã€å¹³å‡ç›¸é–¢ã«å¿œã˜ã¦ã‚·ãƒ¥ãƒªãƒ³ã‚¯ã€‚
L216 - `rrqr_like_det` : ã‚¹ã‚³ã‚¢ã‚’é‡ã¿ä»˜ã‘ã—ãŸQRåˆ†è§£é¢¨ã®æ‰‹é †ã§åˆæœŸå€™è£œã‚’kä»¶æŠ½å‡ºã—ã€ã‚¹ã‚³ã‚¢ã®é«˜ã„éç›¸é–¢ãªé›†åˆã‚’å¾—ã‚‹ã€‚
L217 - `swap_local_det` / `swap_local_det_cross` : `sum(score) - Î»*within_corr - Î¼*cross_corr`ã‚’ç›®çš„é–¢æ•°ã¨ã—ã¦ã€å…¥ã‚Œæ›¿ãˆæ¢ç´¢ã§å±€æ‰€çš„ã«æœ€é©åŒ–ã€‚
L218 - `select_bucket_drrs` : ãƒ—ãƒ¼ãƒ«éŠ˜æŸ„ã¨ã‚¹ã‚³ã‚¢ã‹ã‚‰æ®‹å·®ç›¸é–¢ã‚’è¨ˆç®—ã—ã€ä¸Šè¨˜2æ®µéš(åˆæœŸé¸æŠâ†’å…¥ã‚Œæ›¿ãˆ)ã§kéŠ˜æŸ„ã‚’æ±ºå®šã€‚éå»æ¡ç”¨éŠ˜æŸ„ã¨ã®æ¯”è¼ƒã§ç›®çš„å€¤ãŒåŠ£åŒ–ã—ãªã‘ã‚Œã°ç¶­æŒã™ã‚‹ã€‚
L219 - `select_buckets` : Gãƒã‚±ãƒƒãƒˆã‚’é¸å®šå¾Œã€ãã®çµæœã‚’é™¤ã„ãŸå€™è£œã‹ã‚‰Dãƒã‚±ãƒƒãƒˆã‚’é¸ã¶ã€‚Dé¸å®šæ™‚ã¯Gã¨ã®ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£Î¼ã‚’ä»˜ä¸ã—ã€ä¸¡ãƒã‚±ãƒƒãƒˆã®åˆ†æ•£ã‚’åˆ¶å¾¡ã™ã‚‹ã€‚
L220
L221 #### ç›¸é–¢ä½æ¸›ãƒ­ã‚¸ãƒƒã‚¯è©³ç´°
L222 1. **æ®‹å·®ç›¸é–¢è¡Œåˆ—ã®æ§‹ç¯‰ (`residual_corr`)**
L223    - ãƒªã‚¿ãƒ¼ãƒ³è¡Œåˆ—`R`ã‚’Zã‚¹ã‚³ã‚¢åŒ–ã€‚
L224    - SVDã§ä¸Šä½`n_pc`ä¸»æˆåˆ†`F`ã‚’æ±‚ã‚ã€æœ€å°äºŒä¹—ã§ä¿‚æ•°`B`ã‚’ç®—å‡ºã—æ®‹å·®`E = Z - F@B`ã‚’å¾—ã‚‹ã€‚
L225    - `E`ã®ç›¸é–¢è¡Œåˆ—`C`ã‚’è¨ˆç®—ã—ã€å¹³å‡çµ¶å¯¾ç›¸é–¢ã«å¿œã˜ã¦ã‚·ãƒ¥ãƒªãƒ³ã‚¯é‡`shrink_eff`ã‚’è£œæ­£ã—ã¦å¯¾è§’ã‚’å¼·èª¿ã€‚
L226 2. **åˆæœŸå€™è£œã®æŠ½å‡º (`rrqr_like_det`)**
L227    - ã‚¹ã‚³ã‚¢ã‚’0-1æ­£è¦åŒ–ã—ãŸé‡ã¿`w`ã¨ã—ã€`Z*(1+Î³w)`ã§åˆ—ãƒãƒ«ãƒ ã‚’å¼·èª¿ã€‚
L228    - æ®‹å·®ãƒãƒ«ãƒ æœ€å¤§ã®åˆ—ã‚’é€æ¬¡é¸ã³ã€QRãƒ©ã‚¤ã‚¯ãªãƒ‡ãƒ•ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã£ã¦éç›¸é–¢ã‹ã¤é«˜ã‚¹ã‚³ã‚¢ãª`k`éŠ˜æŸ„é›†åˆ`S0`ã‚’å¾—ã‚‹ã€‚
L229 3. **å±€æ‰€æ¢ç´¢ (`swap_local_det` / `swap_local_det_cross`)**
L230    - ç›®çš„é–¢æ•°`Î£z_score âˆ’ Î»Â·within_corr âˆ’ Î¼Â·cross_corr`ã‚’æœ€å¤§åŒ–ã€‚
L231    - é¸æŠé›†åˆã®å„éŠ˜æŸ„ã‚’ä»–å€™è£œã¨å…¥ã‚Œæ›¿ãˆã€æ”¹å–„ãŒãªããªã‚‹ã¾ã§ã¾ãŸã¯`max_pass`å›ã¾ã§æ¢ç´¢ã€‚
L232    - `swap_local_det_cross`ã¯Gãƒã‚±ãƒƒãƒˆã¨ã®ã‚¯ãƒ­ã‚¹ç›¸é–¢è¡Œåˆ—`C_cross`ã‚’ä½¿ç”¨ã—ã€ãƒšãƒŠãƒ«ãƒ†ã‚£`Î¼`ã‚’ä»˜ä¸ã€‚
L233 4. **éå»æ¡ç”¨ã®ç¶­æŒã¨ã‚¯ãƒ­ã‚¹ãƒšãƒŠãƒ«ãƒ†ã‚£ (`select_bucket_drrs` / `select_buckets`)**
L234    - å±€æ‰€æ¢ç´¢çµæœ`S`ã¨éå»é›†åˆ`P`ã®ç›®çš„å€¤ã‚’æ¯”è¼ƒã—ã€`S`ãŒ`P`ã‚ˆã‚Š`Î·`æœªæº€ã®æ”¹å–„ãªã‚‰`P`ã‚’ç¶­æŒã€‚
L235    - `select_buckets`ã§ã¯Gã‚’å…ˆã«æ±ºå®šã—ã€Dé¸å®šæ™‚ã«Gã¨ã®ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£`Î¼`ã‚’åŠ ãˆã¦ã‚¯ãƒ­ã‚¹åˆ†æ•£ã‚’æŠ‘åˆ¶ã€‚
L236
L237 ### Step4: Output
L238 é¸å®šçµæœã‚’å¯è¦–åŒ–ã—å…±æœ‰ã™ã‚‹å·¥ç¨‹ã€‚ä»¥ä¸‹ã®å†…å®¹ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«åŒ–ã—ã¦æ¨™æº–å‡ºåŠ›ã¨Slackã¸é€ã‚‹ã€‚
L239 - æ¡ç”¨éŠ˜æŸ„ã¨æƒœã—ãã‚‚é¸å¤–ã¨ãªã£ãŸéŠ˜æŸ„ã®ã‚¹ã‚³ã‚¢ä¸€è¦§
L240 - IN/OUTãƒªã‚¹ãƒˆã¨OUTéŠ˜æŸ„ã®ã‚¹ã‚³ã‚¢ï¼ˆä½å¾—ç‚¹éŠ˜æŸ„ã‚’ç¢ºèªã—ã‚„ã™ãï¼‰
L241 - æ–°æ—§ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®æ¯”è¼ƒè¡¨ï¼ˆçµ„å…¥ã‚Œãƒ»é™¤å¤–ã€ã‚¹ã‚³ã‚¢å¤‰åŒ–ï¼‰
L242 - æ¤œè¨ä¸­éŠ˜æŸ„ã®ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°
L243
L244 ä¸»ãªãƒ¡ã‚½ãƒƒãƒ‰:
L245 - `display_results` : ä¸Šè¨˜ãƒ†ãƒ¼ãƒ–ãƒ«ã«åŠ ãˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã‚„åˆ†æ•£åŒ–æŒ‡æ¨™ã‚’è¡¨ç¤ºã€‚
L246 - `notify_slack` : Slack Webhookã¸åŒå†…å®¹ã‚’é€ä¿¡ã€‚
L247 - è£œåŠ©:`_avg_offdiag`ã€`_resid_avg_rho`ã€`_raw_avg_rho`ã€`_cross_block_raw_rho`ã€‚
L248
L249 ## ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
L250 1. `PipelineConfig`ã‚’æ§‹ç¯‰ã€‚
L251 2. **Step1** `Input.prepare_data`ã§`InputBundle`ã‚’ç”Ÿæˆã€‚
L252 3. **Step2** `Scorer.aggregate_scores`ã§`FeatureBundle`ã‚’å–å¾—ã€‚
L253 4. **Step3** `Selector.select_buckets`ã§`SelectionBundle`ã‚’ç®—å‡ºã€‚
L254 5. **Step4** `Output.display_results`ã¨`notify_slack`ã§çµæœã‚’å‡ºåŠ›ã€‚
```

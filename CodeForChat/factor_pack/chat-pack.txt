# === Chat Paste Pack ===
# Repo: dakara32/GPT_Code @ main
# Files: factor.py, scorer.py, .github/workflows/weekly-report.yml, documents/README.md, documents/factor_design.md
# ä½¿ã„æ–¹: ä¸‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’é †ã«è²¼ã‚Œã°ã“ã®ãƒãƒ£ãƒƒãƒˆã§å…¨ä½“æŠŠæ¡ã§ãã¾ã™ã€‚
# æ³¨è¨˜: å„ãƒ•ã‚¡ã‚¤ãƒ«ã¯å€‹åˆ¥ã« L1.. ã§è¡Œç•ªå·ä»˜ä¸ã€‚
---

## <factor.py>
```text
L1 """
L2 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
L3 â”ƒ ROLE of factor.py                                     â”ƒ
L4 â”ƒ  - Orchestration ONLYï¼ˆå¤–éƒ¨I/Oãƒ»SSOTãƒ»Slackå‡ºåŠ›ï¼‰     â”ƒ
L5 â”ƒ  - è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆæ¡ç‚¹/ãƒ•ã‚£ãƒ«ã‚¿/ç›¸é–¢ä½æ¸›ï¼‰ã¯ scorer.py â”ƒ
L6 â”ƒ  - ã“ã“ã§ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…/å¤‰æ›´ã—ãªã„                   â”ƒ
L7 â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
L8 """
L9 # === NOTE: æ©Ÿèƒ½ãƒ»å…¥å‡ºåŠ›ãƒ»ãƒ­ã‚°æ–‡è¨€ãƒ»ä¾‹å¤–æŒ™å‹•ã¯ä¸å¤‰ã€‚å®‰å…¨ãªçŸ­ç¸®ï¼ˆimportçµ±åˆ/è¤‡æ•°ä»£å…¥/å†…åŒ…è¡¨è¨˜/ãƒ¡ã‚½ãƒƒãƒ‰ãƒã‚§ãƒ¼ãƒ³/ä¸€è¡ŒåŒ–/ç©ºè¡Œåœ§ç¸®ãªã©ï¼‰ã®ã¿é©ç”¨ ===
L10 BONUS_COEFF = 0.4   # æ”»ã‚=0.3 / ä¸­åº¸=0.4 / å®ˆã‚Š=0.5
L11 import yfinance as yf, pandas as pd, numpy as np, os, requests, time, json
L12 from concurrent.futures import ThreadPoolExecutor
L13 from scipy.stats import zscore
L14 from dataclasses import dataclass
L15 from typing import Dict, List
L16 from scorer import Scorer, ttm_div_yield_portfolio
L17 from time import perf_counter
L18
L19
L20 class T:
L21     t = perf_counter()
L22
L23     @staticmethod
L24     def log(tag: str):
L25         now = perf_counter()
L26         print(f"[T] {tag}: {now - T.t:.2f}s")
L27         T.t = now
L28
L29
L30 T.log("start")
L31
L32 # ===== ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã¨å®šæ•°ï¼ˆå†’é ­ã«å›ºå®šï¼‰ =====
L33 exist, cand = [pd.read_csv(f, header=None)[0].tolist() for f in ("current_tickers.csv","candidate_tickers.csv")]
L34 T.log(f"csv loaded: exist={len(exist)} cand={len(cand)}")
L35 CAND_PRICE_MAX, bench = 450, '^GSPC'  # ä¾¡æ ¼ä¸Šé™ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
L36 N_G, N_D = 12, 13  # G/Dæ ã‚µã‚¤ã‚º
L37 g_weights = {'GRW':0.40,'MOM':0.45,'VOL':-0.15}
L38 D_BETA_MAX = float(os.environ.get("D_BETA_MAX", "0.8"))
L39 FILTER_SPEC = {"G":{"pre_mask":["trend_template"]},"D":{"pre_filter":{"beta_max":D_BETA_MAX}}}
L40 D_weights = {'QAL':0.15,'YLD':0.15,'VOL':-0.45,'TRD':0.25}
L41 def _fmt_w(w): return " ".join(f"{k}{int(v*100)}" for k,v in w.items())
L42
L43 # DRRS åˆæœŸãƒ—ãƒ¼ãƒ«ãƒ»å„ç¨®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
L44 corrM = 45
L45 DRRS_G, DRRS_D = dict(lookback=252,n_pc=3,gamma=1.2,lam=0.68,eta=0.8), dict(lookback=504,n_pc=4,gamma=0.8,lam=0.85,eta=0.5)
L46 DRRS_SHRINK = 0.10  # æ®‹å·®ç›¸é–¢ã®å¯¾è§’ã‚·ãƒ¥ãƒªãƒ³ã‚¯ï¼ˆåŸºç¤ï¼‰
L47
L48 # ã‚¯ãƒ­ã‚¹ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆæœªå®šç¾©ãªã‚‰è¨­å®šï¼‰
L49 try: CROSS_MU_GD
L50 except NameError: CROSS_MU_GD = 0.40  # æ¨å¥¨ 0.35â€“0.45ï¼ˆlam=0.85æƒ³å®šï¼‰
L51
L52 # å‡ºåŠ›é–¢é€£
L53 RESULTS_DIR = "results"
L54 os.makedirs(RESULTS_DIR, exist_ok=True)
L55
L56 # ãã®ä»–
L57 debug_mode, FINNHUB_API_KEY = False, os.environ.get("FINNHUB_API_KEY")
L58
L59
L60 # ===== å…±æœ‰DTOï¼ˆã‚¯ãƒ©ã‚¹é–“I/Oå¥‘ç´„ï¼‰ï¼‹ Config =====
L61 @dataclass(frozen=True)
L62 class InputBundle:
L63     # Input â†’ Scorer ã§å—ã‘æ¸¡ã™ç´ æï¼ˆI/Oç¦æ­¢ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ï¼‰
L64     cand: List[str]
L65     tickers: List[str]
L66     bench: str
L67     data: pd.DataFrame              # yfinance downloadçµæœï¼ˆ'Close','Volume'ç­‰ã®éšå±¤åˆ—ï¼‰
L68     px: pd.DataFrame                # data['Close']
L69     spx: pd.Series                  # data['Close'][bench]
L70     tickers_bulk: object            # yfinance.Tickers
L71     info: Dict[str, dict]           # yfinance info per ticker
L72     eps_df: pd.DataFrame            # ['eps_ttm','eps_q_recent',...]
L73     fcf_df: pd.DataFrame            # ['fcf_ttm', ...]
L74     returns: pd.DataFrame           # px[tickers].pct_change()
L75
L76 @dataclass(frozen=True)
L77 class FeatureBundle:
L78     df: pd.DataFrame
L79     df_z: pd.DataFrame
L80     g_score: pd.Series
L81     d_score_all: pd.Series
L82     missing_logs: pd.DataFrame
L83
L84 @dataclass(frozen=True)
L85 class SelectionBundle:
L86     resG: dict
L87     resD: dict
L88     top_G: List[str]
L89     top_D: List[str]
L90     init_G: List[str]
L91     init_D: List[str]
L92
L93 @dataclass(frozen=True)
L94 class WeightsConfig:
L95     g: Dict[str,float]
L96     d: Dict[str,float]
L97
L98 @dataclass(frozen=True)
L99 class DRRSParams:
L100     corrM: int
L101     shrink: float
L102     G: Dict[str,float]   # lookback, n_pc, gamma, lam, eta
L103     D: Dict[str,float]
L104     cross_mu_gd: float
L105
L106 @dataclass(frozen=True)
L107 class PipelineConfig:
L108     weights: WeightsConfig
L109     drrs: DRRSParams
L110     price_max: float
L111
L112
L113 # ===== å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆè¤‡æ•°ã‚¯ãƒ©ã‚¹ã§ä½¿ç”¨ï¼‰ =====
L114 # (unused local utils removed â€“ use scorer.py versions if needed)
L115
L116 def _env_true(name: str, default=False):
L117     v = os.getenv(name)
L118     return default if v is None else v.strip().lower() == "true"
L119
L120 def _slack(message, code=False):
L121     url = os.getenv("SLACK_WEBHOOK_URL")
L122     if not url:
L123         print("âš ï¸ SLACK_WEBHOOK_URL æœªè¨­å®š"); return
L124     try:
L125         requests.post(url, json={"text": f"```{message}```" if code else message}).raise_for_status()
L126     except Exception as e:
L127         print(f"âš ï¸ Slacké€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
L128
L129 def _slack_debug(text: str, chunk=2800):
L130     url=os.getenv("SLACK_WEBHOOK_URL")
L131     if not url: print("âš ï¸ SLACK_WEBHOOK_URL æœªè¨­å®š"); return
L132     i=0
L133     while i<len(text):
L134         j=min(len(text), i+chunk); k=text.rfind("\n", i, j); j=k if k>i+100 else j
L135         blk={"type":"section","text":{"type":"mrkdwn","text":f"```{text[i:j]}```"}}
L136         try: requests.post(url, json={"blocks":[blk]}).raise_for_status()
L137         except Exception as e: print(f"âš ï¸ Slacké€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
L138         i=j
L139
L140 def _compact_debug(fb, sb, prevG, prevD, max_rows=140):
L141     # ---- åˆ—é¸æŠï¼šæ—¢å®šã¯æœ€å°åˆ—ã€DEBUG_ALL_COLS=True ã§å…¨åˆ—ã« ----
L142     want=["TR","EPS","REV","ROE","BETA_RAW","FCF","RS","TR_str","DIV_STREAK","DSC"]
L143     all_cols = _env_true("DEBUG_ALL_COLS", False)
L144     cols = list(fb.df_z.columns if all_cols else [c for c in want if c in fb.df_z.columns])
L145
L146     # ---- å·®åˆ†ï¼ˆå…¥æ›¿ï¼‰----
L147     Gp, Dp = set(prevG or []), set(prevD or [])
L148     g_new=[t for t in (sb.top_G or []) if t not in Gp]; g_out=[t for t in Gp if t not in (sb.top_G or [])]
L149     d_new=[t for t in (sb.top_D or []) if t not in Dp]; d_out=[t for t in Dp if t not in (sb.top_D or [])]
L150
L151     # ---- æ¬¡ç‚¹10ï¼ˆãƒ•ãƒ©ã‚°ã§æœ‰ç„¡åˆ‡æ›¿ï¼‰----
L152     show_near = _env_true("DEBUG_NEAR5", True)
L153     gs = getattr(fb,"g_score",None); ds = getattr(fb,"d_score_all",None)
L154     gs = gs.sort_values(ascending=False) if show_near and hasattr(gs,"sort_values") else None
L155     ds = ds.sort_values(ascending=False) if show_near and hasattr(ds,"sort_values") else None
L156     g_miss = ([t for t in gs.index if t not in (sb.top_G or [])][:10]) if gs is not None else []
L157     d_excl = set((sb.top_G or [])+(sb.top_D or []))
L158     d_miss = ([t for t in ds.index if t not in d_excl][:10]) if ds is not None else []
L159
L160     # ---- è¡Œé¸æŠï¼šæ—¢å®šã¯å…¥æ›¿+æ¡ç”¨+æ¬¡ç‚¹ã€DEBUG_ALL_ROWS=True ã§å…¨éŠ˜æŸ„ ----
L161     all_rows = _env_true("DEBUG_ALL_ROWS", False)
L162     focus = list(fb.df_z.index) if all_rows else sorted(set(g_new+g_out+d_new+d_out+(sb.top_G or [])+(sb.top_D or [])+g_miss+d_miss))
L163     focus = focus[:max_rows]
L164
L165     # ---- ãƒ˜ãƒƒãƒ€ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã‚’æ˜ç¤ºï¼‰----
L166     def _fmt_near(lbl, ser, lst):
L167         if ser is None: return f"{lbl}: off"
L168         parts=[]
L169         for t in lst:
L170             x=ser.get(t, float("nan"))
L171             parts.append(f"{t}:{x:.3f}" if pd.notna(x) else f"{t}:nan")
L172         return f"{lbl}: "+(", ".join(parts) if parts else "-")
L173     head=[f"G new/out: {len(g_new)}/{len(g_out)}  D new/out: {len(d_new)}/{len(d_out)}",
L174           _fmt_near("G near10", gs, g_miss),
L175           _fmt_near("D near10", ds, d_miss),
L176           f"Filters: G pre_mask=['trend_template'], D pre_filter={{'beta_max': {D_BETA_MAX}}}",
L177           f"Cols={'ALL' if all_cols else 'MIN'}  Rows={'ALL' if all_rows else 'SUBSET'}"]
L178
L179     # ---- ãƒ†ãƒ¼ãƒ–ãƒ« ----
L180     if fb.df_z.empty or not cols:
L181         tbl="(df_z or columns not available)"
L182     else:
L183         idx=[t for t in focus if t in fb.df_z.index]
L184         tbl=fb.df_z.loc[idx, cols].round(3).to_string(max_rows=None, max_cols=None)
L185
L186     # ---- æ¬ æãƒ­ã‚°ï¼ˆãƒ•ãƒ©ã‚°ã§æœ‰ç„¡åˆ‡æ›¿ï¼‰----
L187     miss_txt=""
L188     if _env_true("DEBUG_MISSING_LOGS", False):
L189         miss=getattr(fb,"missing_logs",None)
L190         if miss is not None and not miss.empty:
L191             miss_txt="\nMissing data (head)\n"+miss.head(10).to_string(index=False)
L192
L193     return "\n".join(head+["\nChanged/Selected (+ Near Miss)", tbl])+miss_txt
L194
L195 def _disjoint_keepG(top_G, top_D, poolD):
L196     """
L197     Gã«å«ã¾ã‚Œã‚‹éŠ˜æŸ„ã‚’Dã‹ã‚‰é™¤å»ã—ã€Dã¯poolDï¼ˆæ¬¡ç‚¹ï¼‰ã§è£œå……ã™ã‚‹ã€‚
L198     - å¼•æ•°:
L199         top_G: List[str]  â€¦ Gæœ€çµ‚12éŠ˜æŸ„
L200         top_D: List[str]  â€¦ Dæœ€çµ‚13éŠ˜æŸ„ï¼ˆé‡è¤‡ã‚’å«ã‚€å¯èƒ½æ€§ã‚ã‚Šï¼‰
L201         poolD: List[str]  â€¦ Då€™è£œã®é †ä½ãƒªã‚¹ãƒˆï¼ˆtop_Dã‚’å«ã‚€ä¸Šä½æ‹¡å¼µï¼‰
L202     - æˆ»ã‚Šå€¤: (top_G, top_D_disjoint)
L203     - æŒ™å‹•:
L204         1) Dã«Gé‡è¤‡ãŒã‚ã‚Œã°é †ã«ç½®æ›
L205         2) ç½®æ›å€™è£œã¯ poolD ã‹ã‚‰ã€æ—¢ä½¿ç”¨(GâˆªD)ã‚’é¿ã‘ã¦å‰ã‹ã‚‰æ¡ç”¨
L206         3) è£œå……åˆ†ãŒå°½ããŸå ´åˆã¯å…ƒã®éŠ˜æŸ„ã‚’æ®‹ã™ï¼ˆå®‰å…¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
L207     """
L208     used, D = set(top_G), list(top_D)
L209     i = 0
L210     for j, t in enumerate(D):
L211         if t in used:
L212             while i < len(poolD) and (poolD[i] in used or poolD[i] in D):
L213                 i += 1
L214             if i < len(poolD):
L215                 D[j] = poolD[i]; used.add(D[j]); i += 1
L216     return top_G, D
L217
L218 # --- Breadth mode state I/Oï¼ˆmode ã®ã¿æ°¸ç¶šï¼‰ ---
L219 def _state_path():
L220     return os.path.join(RESULTS_DIR, "breadth_state.json")
L221
L222 def load_mode(default: str="NORMAL") -> str:
L223     try:
L224         with open(_state_path(), "r") as f:
L225             m = json.loads(f.read()).get("mode", default)
L226             return m if m in ("EMERG","CAUTION","NORMAL") else default
L227     except Exception:
L228         return default
L229
L230 def save_mode(mode: str):
L231     try:
L232         with open(_state_path(), "w") as f:
L233             f.write(json.dumps({"mode": mode}))
L234     except Exception:
L235         pass
L236
L237 # --- Breadthâ†’è‡ªå‹•ã—ãã„å€¤â†’ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹â†’Slackå…ˆé ­è¡Œã‚’ä½œæˆ ---
L238 def _build_breadth_lead_lines(inb) -> tuple[list[str], str]:
L239     """
L240     è¿”ã‚Šå€¤: (lead_lines, mode)
L241     - lead_lines: Slackå†’é ­3è¡Œ
L242     - mode: "EMERG" / "CAUTION" / "NORMAL"
L243     ä¾‹å¤–ã¯ä¸Šä½ã§æ¡ã‚‹ï¼ˆæ—¢å­˜å‡ºåŠ›ã¯ç¶™ç¶šï¼‰
L244     """
L245     win = int(os.getenv("BREADTH_CALIB_WIN_DAYS", "600"))
L246     C_ts = Scorer.trend_template_breadth_series(inb.px[inb.tickers], inb.spx, win_days=win)
L247     if C_ts.empty:
L248         raise RuntimeError("breadth series empty")
L249     C_full = int(C_ts.iloc[-1])
L250     q05 = int(np.nan_to_num(C_ts.quantile(float(os.getenv("BREADTH_Q_EMERG_IN",  "0.05"))), nan=0.0))
L251     q20 = int(np.nan_to_num(C_ts.quantile(float(os.getenv("BREADTH_Q_EMERG_OUT", "0.20"))), nan=0.0))
L252     q60 = int(np.nan_to_num(C_ts.quantile(float(os.getenv("BREADTH_Q_WARN_OUT",  "0.60"))), nan=0.0))
L253     # é‹ç”¨â€œåºŠâ€ï¼ˆN_G, 1.5*N_G, 3*N_Gï¼‰ã¨ã®max
L254     th_in_rec   = max(N_G, q05)
L255     th_out_rec  = max(int(np.ceil(1.5*N_G)), q20)
L256     th_norm_rec = max(3*N_G, q60)
L257     # æ¡ç”¨ï¼ˆè‡ªå‹• or æ‰‹å‹•ï¼‰
L258     use_calib = os.getenv("BREADTH_USE_CALIB", "true").strip().lower() == "true"
L259     if use_calib:
L260         th_in, th_out, th_norm, th_src = th_in_rec, th_out_rec, th_norm_rec, "è‡ªå‹•"
L261     else:
L262         th_in   = int(os.getenv("GTT_EMERG_IN",    str(N_G)))
L263         th_out  = int(os.getenv("GTT_EMERG_OUT",   str(int(1.5*N_G))))
L264         th_norm = int(os.getenv("GTT_CAUTION_OUT", str(3*N_G)))
L265         th_src  = "æ‰‹å‹•"
L266     # ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹
L267     prev = load_mode("NORMAL")
L268     if prev == "EMERG":
L269         mode = "EMERG" if (C_full < th_out) else ("CAUTION" if (C_full < th_norm) else "NORMAL")
L270     elif prev == "CAUTION":
L271         mode = "CAUTION" if (C_full < th_norm) else "NORMAL"
L272     else:
L273         mode = "EMERG" if (C_full < th_in) else ("CAUTION" if (C_full < th_norm) else "NORMAL")
L274     save_mode(mode)
L275     _MODE_JA = {"EMERG":"ç·Šæ€¥", "CAUTION":"è­¦æˆ’", "NORMAL":"é€šå¸¸"}
L276     mode_ja = _MODE_JA.get(mode, mode)
L277     lead_lines = [
L278         f"ãƒ†ãƒ³ãƒ—ãƒ¬åˆæ ¼æœ¬æ•°: {C_full}æœ¬ â†’ ãƒ¢ãƒ¼ãƒ‰ {mode_ja}",
L279         f"ç¾åœ¨ã®ã—ãã„å€¤ï¼ˆ{th_src}ï¼‰: ç·Šæ€¥å…¥ã‚Š<{th_in}æœ¬ / è§£é™¤â‰¥{th_out}æœ¬ / é€šå¸¸å¾©å¸°â‰¥{th_norm}æœ¬",
L280         f"å‚è€ƒæŒ‡æ¨™ï¼ˆéå»~{win}å–¶æ¥­æ—¥ï¼‰: ä¸‹ä½5%={q05}æœ¬ / ä¸‹ä½20%={q20}æœ¬ / 60%åˆ†ä½={q60}æœ¬",
L281     ]
L282     return lead_lines, mode
L283
L284
L285 # ===== Inputï¼šå¤–éƒ¨I/Oã¨å‰å‡¦ç†ï¼ˆCSV/APIãƒ»æ¬ æè£œå®Œï¼‰ =====
L286 class Input:
L287     def __init__(self, cand, exist, bench, price_max, finnhub_api_key=None):
L288         self.cand, self.exist, self.bench, self.price_max = cand, exist, bench, price_max
L289         self.api_key = finnhub_api_key or os.environ.get("FINNHUB_API_KEY")
L290
L291     # ---- ï¼ˆInputå°‚ç”¨ï¼‰EPSè£œå®Œãƒ»FCFç®—å‡ºç³» ----
L292     @staticmethod
L293     def impute_eps_ttm(df: pd.DataFrame, ttm_col: str="eps_ttm", q_col: str="eps_q_recent", out_col: str|None=None) -> pd.DataFrame:
L294         out_col = out_col or ttm_col; df = df.copy(); df["eps_imputed"] = False
L295         cand = df[q_col]*4; ok = df[ttm_col].isna() & cand.replace([np.inf,-np.inf], np.nan).notna()
L296         df.loc[ok, out_col], df.loc[ok,"eps_imputed"] = cand[ok], True; return df
L297
L298     _CF_ALIASES = {"cfo":["Operating Cash Flow","Total Cash From Operating Activities"], "capex":["Capital Expenditure","Capital Expenditures"]}
L299
L300     @staticmethod
L301     def _pick_row(df: pd.DataFrame, names: list[str]) -> pd.Series|None:
L302         if df is None or df.empty: return None
L303         idx_lower = {str(i).lower(): i for i in df.index}
L304         for name in names:
L305             key = name.lower()
L306             if key in idx_lower: return df.loc[idx_lower[key]]
L307         return None
L308
L309     @staticmethod
L310     def _sum_last_n(s: pd.Series|None, n: int) -> float|None:
L311         if s is None or s.empty: return None
L312         vals = s.dropna().astype(float); return None if vals.empty else vals.iloc[:n].sum()
L313
L314     @staticmethod
L315     def _latest(s: pd.Series|None) -> float|None:
L316         if s is None or s.empty: return None
L317         vals = s.dropna().astype(float); return vals.iloc[0] if not vals.empty else None
L318
L319     def fetch_cfo_capex_ttm_yf(self, tickers: list[str]) -> pd.DataFrame:
L320         from concurrent.futures import ThreadPoolExecutor, as_completed
L321         pick, sumn, latest, aliases = self._pick_row, self._sum_last_n, self._latest, self._CF_ALIASES
L322
L323         def one(t: str):
L324             try:
L325                 tk = yf.Ticker(t)  # â˜… ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¯æ¸¡ã•ãªã„ï¼ˆYFãŒcurl_cffiã§ç®¡ç†ï¼‰
L326                 qcf = tk.quarterly_cashflow
L327                 cfo_q, capex_q = pick(qcf, aliases["cfo"]), pick(qcf, aliases["capex"])
L328                 fcf_q = pick(qcf, ["Free Cash Flow","FreeCashFlow","Free cash flow"])
L329                 cfo, capex, fcf = sumn(cfo_q,4), sumn(capex_q,4), sumn(fcf_q,4)
L330                 if any(v is None for v in (cfo, capex, fcf)):
L331                     acf = tk.cashflow
L332                     if cfo   is None: cfo   = latest(pick(acf, aliases["cfo"]))
L333                     if capex is None: capex = latest(pick(acf, aliases["capex"]))
L334                     if fcf   is None: fcf   = latest(pick(acf, ["Free Cash Flow","FreeCashFlow","Free cash flow"]))
L335             except Exception as e:
L336                 print(f"[warn] yf financials error: {t}: {e}"); cfo=capex=fcf=None
L337             n=np.nan
L338             return {"ticker":t,
L339                     "cfo_ttm_yf":   n if cfo   is None else cfo,
L340                     "capex_ttm_yf": n if capex is None else capex,
L341                     "fcf_ttm_yf_direct": n if fcf is None else fcf}
L342
L343         rows, mw = [], int(os.getenv("FIN_THREADS","8"))
L344         with ThreadPoolExecutor(max_workers=mw) as ex:
L345             for f in as_completed(ex.submit(one,t) for t in tickers): rows.append(f.result())
L346         return pd.DataFrame(rows).set_index("ticker")
L347
L348     _FINN_CFO_KEYS = ["netCashProvidedByOperatingActivities","netCashFromOperatingActivities","cashFlowFromOperatingActivities","operatingCashFlow"]
L349     _FINN_CAPEX_KEYS = ["capitalExpenditure","capitalExpenditures","purchaseOfPPE","investmentsInPropertyPlantAndEquipment"]
L350
L351     @staticmethod
L352     def _first_key(d: dict, keys: list[str]):
L353         for k in keys:
L354             if k in d and d[k] is not None: return d[k]
L355         return None
L356
L357     @staticmethod
L358     def _finn_get(session: requests.Session, url: str, params: dict, retries: int=3, sleep_s: float=0.5):
L359         for i in range(retries):
L360             r = session.get(url, params=params, timeout=15)
L361             if r.status_code==429: time.sleep(min(2**i*sleep_s,4.0)); continue
L362             r.raise_for_status(); return r.json()
L363         r.raise_for_status()
L364
L365     def fetch_cfo_capex_ttm_finnhub(self, tickers: list[str], api_key: str|None=None) -> pd.DataFrame:
L366         api_key = api_key or os.getenv("FINNHUB_API_KEY")
L367         if not api_key: raise ValueError("Finnhub API key not provided. Set FINNHUB_API_KEY or pass api_key=")
L368         base, s, rows = "https://finnhub.io/api/v1", requests.Session(), []
L369         for sym in tickers:
L370             cfo_ttm = capex_ttm = None
L371             try:
L372                 j = self._finn_get(s, f"{base}/stock/cash-flow", {"symbol":sym,"frequency":"quarterly","limit":8,"token":api_key})
L373                 arr = j.get("cashFlow") or []; cfo_vals, capex_vals = [], []
L374                 for item in arr[:4]:
L375                     cfo_vals.append(self._first_key(item,self._FINN_CFO_KEYS)); capex_vals.append(self._first_key(item,self._FINN_CAPEX_KEYS))
L376                 if any(v is not None for v in cfo_vals): cfo_ttm = float(np.nansum([np.nan if v is None else float(v) for v in cfo_vals]))
L377                 if any(v is not None for v in capex_vals): capex_ttm = float(np.nansum([np.nan if v is None else float(v) for v in capex_vals]))
L378             except Exception: pass
L379             if cfo_ttm is None or capex_ttm is None:
L380                 try:
L381                     j = self._finn_get(s, f"{base}/stock/cash-flow", {"symbol":sym,"frequency":"annual","limit":1,"token":api_key})
L382                     arr = j.get("cashFlow") or []
L383                     if arr:
L384                         item0 = arr[0]
L385                         if cfo_ttm is None:
L386                             v = self._first_key(item0,self._FINN_CFO_KEYS)
L387                             if v is not None: cfo_ttm = float(v)
L388                         if capex_ttm is None:
L389                             v = self._first_key(item0,self._FINN_CAPEX_KEYS)
L390                             if v is not None: capex_ttm = float(v)
L391                 except Exception: pass
L392             rows.append({"ticker":sym,"cfo_ttm_fh":np.nan if cfo_ttm is None else cfo_ttm,"capex_ttm_fh":np.nan if capex_ttm is None else capex_ttm})
L393         return pd.DataFrame(rows).set_index("ticker")
L394
L395     def compute_fcf_with_fallback(self, tickers: list[str], finnhub_api_key: str|None=None) -> pd.DataFrame:
L396         yf_df = self.fetch_cfo_capex_ttm_yf(tickers)
L397         T.log("financials (yf) done")
L398         miss_mask = yf_df[["cfo_ttm_yf","capex_ttm_yf","fcf_ttm_yf_direct"]].isna().any(axis=1)
L399         need = yf_df.index[miss_mask].tolist(); print(f"[T] yf financials missing: {len(need)} {need[:10]}{'...' if len(need)>10 else ''}")
L400         if need:
L401             fh_df = self.fetch_cfo_capex_ttm_finnhub(need, api_key=finnhub_api_key)
L402             df = yf_df.join(fh_df, how="left")
L403             for col_yf, col_fh in [("cfo_ttm_yf","cfo_ttm_fh"),("capex_ttm_yf","capex_ttm_fh")]:
L404                 df[col_yf] = df[col_yf].fillna(df[col_fh])
L405             print("[T] financials (finnhub) done (fallback only)")
L406         else:
L407             df = yf_df.assign(cfo_ttm_fh=np.nan, capex_ttm_fh=np.nan)
L408             print("[T] financials (finnhub) skipped (no missing)")
L409         df["cfo_ttm"]  = df["cfo_ttm_yf"].where(df["cfo_ttm_yf"].notna(), df["cfo_ttm_fh"])
L410         df["capex_ttm"] = df["capex_ttm_yf"].where(df["capex_ttm_yf"].notna(), df["capex_ttm_fh"])
L411         cfo, capex = pd.to_numeric(df["cfo_ttm"], errors="coerce"), pd.to_numeric(df["capex_ttm"], errors="coerce").abs()
L412         fcf_calc = cfo - capex
L413         fcf_direct = pd.to_numeric(df.get("fcf_ttm_yf_direct"), errors="coerce")
L414         df["fcf_ttm"] = fcf_calc.where(fcf_calc.notna(), fcf_direct)
L415         df["cfo_source"]  = np.where(df["cfo_ttm_yf"].notna(),"yfinance",np.where(df["cfo_ttm_fh"].notna(),"finnhub",""))
L416         df["capex_source"] = np.where(df["capex_ttm_yf"].notna(),"yfinance",np.where(df["capex_ttm_fh"].notna(),"finnhub",""))
L417         df["fcf_imputed"] = df[["cfo_ttm","capex_ttm"]].isna().any(axis=1) & df["fcf_ttm"].notna()
L418         cols = ["cfo_ttm_yf","capex_ttm_yf","cfo_ttm_fh","capex_ttm_fh","cfo_ttm","capex_ttm","fcf_ttm","fcf_ttm_yf_direct","cfo_source","capex_source","fcf_imputed"]
L419         return df[cols].sort_index()
L420
L421     def _build_eps_df(self, tickers, tickers_bulk, info):
L422         eps_rows=[]
L423         for t in tickers:
L424             info_t, eps_ttm, eps_q = info[t], info[t].get("trailingEps", np.nan), np.nan
L425             try:
L426                 qearn, so = tickers_bulk.tickers[t].quarterly_earnings, info_t.get("sharesOutstanding")
L427                 if so and qearn is not None and not qearn.empty and "Earnings" in qearn.columns:
L428                     eps_ttm_q = qearn["Earnings"].head(4).sum()/so
L429                     if pd.notna(eps_ttm_q) and (pd.isna(eps_ttm) or (abs(eps_ttm)>0 and abs(eps_ttm/eps_ttm_q)>3)): eps_ttm = eps_ttm_q
L430                     eps_q = qearn["Earnings"].iloc[-1]/so
L431             except Exception: pass
L432             eps_rows.append({"ticker":t,"eps_ttm":eps_ttm,"eps_q_recent":eps_q})
L433         return self.impute_eps_ttm(pd.DataFrame(eps_rows).set_index("ticker"))
L434
L435     def prepare_data(self):
L436         """Fetch price and fundamental data for all tickers."""
L437         cand_info = yf.Tickers(" ".join(self.cand)); cand_prices = {}
L438         for t in self.cand:
L439             try: cand_prices[t] = cand_info.tickers[t].fast_info.get("lastPrice", np.inf)
L440             except Exception as e: print(f"{t}: price fetch failed ({e})"); cand_prices[t] = np.inf
L441         cand_f = [t for t,p in cand_prices.items() if p<=self.price_max]
L442         T.log("price cap filter done (CAND_PRICE_MAX)")
L443         tickers = sorted(set(self.exist + cand_f))
L444         T.log(f"universe prepared: unique={len(tickers)} bench={self.bench}")
L445         data = yf.download(tickers + [self.bench], period="600d", auto_adjust=True, progress=False)
L446         T.log("yf.download done")
L447         px, spx = data["Close"], data["Close"][self.bench]
L448         clip_days = int(os.getenv("PRICE_CLIP_DAYS", "0"))   # 0ãªã‚‰ç„¡åŠ¹ï¼ˆæ—¢å®šï¼‰
L449         if clip_days > 0:
L450             px  = px.tail(clip_days + 1)
L451             spx = spx.tail(clip_days + 1)
L452             print(f"[T] price window clipped by env: {len(px)} rows (PRICE_CLIP_DAYS={clip_days})")
L453         else:
L454             print(f"[T] price window clip skipped; rows={len(px)}")
L455         tickers_bulk, info = yf.Tickers(" ".join(tickers)), {}
L456         for t in tickers:
L457             try: info[t] = tickers_bulk.tickers[t].info
L458             except Exception as e: print(f"{t}: info fetch failed ({e})"); info[t] = {}
L459         eps_df = self._build_eps_df(tickers, tickers_bulk, info)
L460         fcf_df = self.compute_fcf_with_fallback(tickers, finnhub_api_key=self.api_key)
L461         T.log("eps/fcf prep done")
L462         returns = px[tickers].pct_change()
L463         T.log("price prep/returns done")
L464         return dict(cand=cand_f, tickers=tickers, data=data, px=px, spx=spx, tickers_bulk=tickers_bulk, info=info, eps_df=eps_df, fcf_df=fcf_df, returns=returns)
L465
L466
L467 # ===== Selectorï¼šç›¸é–¢ä½æ¸›ãƒ»é¸å®šï¼ˆã‚¹ã‚³ã‚¢ï¼†ãƒªã‚¿ãƒ¼ãƒ³ã ã‘èª­ã‚€ï¼‰ =====
L468 class Selector:
L469     # ---- DRRS helpersï¼ˆSelectorå°‚ç”¨ï¼‰ ----
L470     @staticmethod
L471     def _z_np(X: np.ndarray) -> np.ndarray:
L472         X = np.asarray(X, dtype=np.float32); m = np.nanmean(X, axis=0, keepdims=True); s = np.nanstd(X, axis=0, keepdims=True)+1e-9
L473         return (np.nan_to_num(X)-m)/s
L474
L475     @classmethod
L476     def residual_corr(cls, R: np.ndarray, n_pc: int=3, shrink: float=0.1) -> np.ndarray:
L477         Z = cls._z_np(R); U,S,_ = np.linalg.svd(Z, full_matrices=False); F = U[:,:n_pc]*S[:n_pc]; B = np.linalg.lstsq(F, Z, rcond=None)[0]
L478         E = Z - F@B; C = np.corrcoef(E, rowvar=False)
L479         off = C - np.diag(np.diag(C)); iu = np.triu_indices_from(off,1); avg_abs = np.nanmean(np.abs(off[iu])) if iu[0].size else 0.0
L480         shrink_eff = float(np.clip(shrink + 0.5*avg_abs, 0.1, 0.6)); N = C.shape[0]
L481         return (1.0 - shrink_eff)*C + shrink_eff*np.eye(N, dtype=C.dtype)
L482
L483     @classmethod
L484     def rrqr_like_det(cls, R: np.ndarray, score: np.ndarray, k: int, gamma: float=1.0):
L485         Z, w = cls._z_np(R), (score-score.min())/(np.ptp(score)+1e-12); X = Z*(1.0 + gamma*w)
L486         N, k = X.shape[1], max(0, min(k, X.shape[1]))
L487         if k==0: return []
L488         S, selected, Rres = [], np.zeros(N, dtype=bool), X.copy()
L489         for _ in range(k):
L490             norms = (Rres*Rres).sum(axis=0); cand = np.where(~selected)[0]
L491             j = sorted(cand, key=lambda c:(-norms[c], -w[c], c))[0]
L492             S.append(j); selected[j]=True; u = X[:,j:j+1]; u/=(np.linalg.norm(u)+1e-12); Rres = Rres - u @ (u.T @ Rres)
L493         return sorted(S)
L494
L495     @staticmethod
L496     def _obj(corrM: np.ndarray, score: np.ndarray, idx, lam: float) -> float:
L497         idx = list(idx); P = corrM[np.ix_(idx, idx)]; s = (score-score.mean())/(score.std()+1e-9)
L498         return float(s[idx].sum() - lam*((P.sum()-np.trace(P))/2.0))
L499
L500     @classmethod
L501     def swap_local_det(cls, corrM: np.ndarray, score: np.ndarray, idx, lam: float=0.6, max_pass: int=15):
L502         S, best, improved, passes = sorted(idx), cls._obj(corrM, score, idx, lam), True, 0
L503         while improved and passes<max_pass:
L504             improved, passes = False, passes+1
L505             for i,out in enumerate(list(S)):
L506                 for inn in range(len(score)):
L507                     if inn in S: continue
L508                     cand = sorted(S[:i]+[inn]+S[i+1:]); v = cls._obj(corrM, score, cand, lam)
L509                     if v>best+1e-10: S, best, improved = cand, v, True; break
L510                 if improved: break
L511         return S, best
L512
L513     @staticmethod
L514     def _obj_with_cross(C_within: np.ndarray, C_cross: np.ndarray|None, score: np.ndarray, idx, lam: float, mu: float) -> float:
L515         idx = list(idx); P = C_within[np.ix_(idx, idx)]; s = (score-score.mean())/(score.std()+1e-9)
L516         within = (P.sum()-np.trace(P))/2.0; cross = 0.0
L517         if C_cross is not None and C_cross.size>0: cross = C_cross[idx,:].sum()
L518         return float(s[idx].sum() - lam*within - mu*cross)
L519
L520     @classmethod
L521     def swap_local_det_cross(cls, C_within: np.ndarray, C_cross: np.ndarray|None, score: np.ndarray, idx, lam: float=0.6, mu: float=0.3, max_pass: int=15):
L522         S, best, improved, passes, N = sorted(idx), cls._obj_with_cross(C_within,C_cross,score,idx,lam,mu), True, 0, len(score)
L523         while improved and passes<max_pass:
L524             improved, passes = False, passes+1
L525             for i,out in enumerate(list(S)):
L526                 for inn in range(N):
L527                     if inn in S: continue
L528                     cand = sorted(S[:i]+[inn]+S[i+1:]); v = cls._obj_with_cross(C_within,C_cross,score,cand,lam,mu)
L529                     if v>best+1e-10: S, best, improved = cand, v, True; break
L530                 if improved: break
L531         return S, best
L532
L533     @staticmethod
L534     def avg_corr(C: np.ndarray, idx) -> float:
L535         k = len(idx); P = C[np.ix_(idx, idx)]
L536         return float((P.sum()-np.trace(P))/(k*(k-1)+1e-12))
L537
L538     @classmethod
L539     def select_bucket_drrs(cls, returns_df: pd.DataFrame, score_ser: pd.Series, pool_tickers: list[str], k: int, *, n_pc: int, gamma: float, lam: float, lookback: int, shrink: float=0.10, g_fixed_tickers: list[str]|None=None, mu: float=0.0):
L540         g_fixed = [t for t in (g_fixed_tickers or []) if t in returns_df.columns]
L541         union = [t for t in pool_tickers if t in returns_df.columns]
L542         for t in g_fixed:
L543             if t not in union: union.append(t)
L544         Rdf_all = returns_df[union]; Rdf_all = Rdf_all.iloc[-lookback:] if len(Rdf_all)>=lookback else Rdf_all; Rdf_all = Rdf_all.dropna()
L545         pool_eff, g_eff = [t for t in pool_tickers if t in Rdf_all.columns], [t for t in g_fixed if t in Rdf_all.columns]
L546         if len(pool_eff)==0: return dict(idx=[], tickers=[], avg_res_corr=np.nan, sum_score=0.0, objective=-np.inf)
L547         score = score_ser.reindex(pool_eff).to_numpy(dtype=np.float32)
L548         C_all = cls.residual_corr(Rdf_all.to_numpy(), n_pc=n_pc, shrink=shrink)
L549         col_pos = {c:i for i,c in enumerate(Rdf_all.columns)}; pool_pos = [col_pos[t] for t in pool_eff]
L550         C_within, C_cross = C_all[np.ix_(pool_pos,pool_pos)], None
L551         if len(g_eff)>0 and mu>0.0:
L552             g_pos = [col_pos[t] for t in g_eff]; C_cross = C_all[np.ix_(pool_pos,g_pos)]
L553         R_pool = Rdf_all[pool_eff].to_numpy(); S0 = cls.rrqr_like_det(R_pool, score, k, gamma=gamma)
L554         S, Jn = (cls.swap_local_det_cross(C_within, C_cross, score, S0, lam=lam, mu=mu, max_pass=15) if C_cross is not None else cls.swap_local_det(C_within, score, S0, lam=lam, max_pass=15))
L555         selected_tickers = [pool_eff[i] for i in S]
L556         return dict(idx=S, tickers=selected_tickers, avg_res_corr=cls.avg_corr(C_within,S), sum_score=float(score[S].sum()), objective=float(Jn))
L557
L558     # ---- é¸å®šï¼ˆã‚¹ã‚³ã‚¢ Series / returns ã ã‘ã‚’å—ã‘ã‚‹ï¼‰----
L559 # ===== Outputï¼šå‡ºåŠ›æ•´å½¢ã¨é€ä¿¡ï¼ˆè¡¨ç¤ºãƒ»Slackï¼‰ =====
L560 class Output:
L561
L562     def __init__(self, debug=False):
L563         self.debug = debug
L564         self.miss_df = self.g_table = self.d_table = self.io_table = self.df_metrics_fmt = self.debug_table = None
L565         self.g_title = self.d_title = ""
L566         self.g_formatters = self.d_formatters = {}
L567         # ä½ã‚¹ã‚³ã‚¢ï¼ˆGSC+DSCï¼‰Top10 è¡¨ç¤º/é€ä¿¡ç”¨
L568         self.low10_table = None
L569
L570     # --- è¡¨ç¤ºï¼ˆå…ƒ display_results ã®ãƒ­ã‚¸ãƒƒã‚¯ãã®ã¾ã¾ï¼‰ ---
L571     def display_results(self, *, exist, bench, df_z, g_score, d_score_all,
L572                         init_G, init_D, top_G, top_D, **kwargs):
L573         pd.set_option('display.float_format','{:.3f}'.format)
L574         print("ğŸ“ˆ ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼åˆ†æ•£æœ€é©åŒ–ã®çµæœ")
L575         if self.miss_df is not None and not self.miss_df.empty:
L576             print("Missing Data:")
L577             print(self.miss_df.to_string(index=False))
L578
L579         # ---- è¡¨ç¤ºç”¨ï¼šChanges/Near-Miss ã®ã‚¹ã‚³ã‚¢æºã‚’â€œæœ€çµ‚é›†è¨ˆâ€ã«çµ±ä¸€ã™ã‚‹ãƒ—ãƒ­ã‚­ã‚· ----
L580         try:
L581             sc = getattr(self, "_sc", None)
L582             agg_G = getattr(sc, "_agg_G", None)
L583             agg_D = getattr(sc, "_agg_D", None)
L584         except Exception:
L585             sc = agg_G = agg_D = None
L586         class _SeriesProxy:
L587             __slots__ = ("primary", "fallback")
L588             def __init__(self, primary, fallback): self.primary, self.fallback = primary, fallback
L589             def get(self, key, default=None):
L590                 try:
L591                     v = self.primary.get(key) if hasattr(self.primary, "get") else None
L592                     if v is not None and not (isinstance(v, float) and v != v):
L593                         return v
L594                 except Exception:
L595                     pass
L596                 try:
L597                     return self.fallback.get(key) if hasattr(self.fallback, "get") else default
L598                 except Exception:
L599                     return default
L600         g_score = _SeriesProxy(agg_G, g_score)
L601         d_score_all = _SeriesProxy(agg_D, d_score_all)
L602         near_G = getattr(sc, "_near_G", []) if sc else []
L603         near_D = getattr(sc, "_near_D", []) if sc else []
L604
L605         extra_G = [t for t in init_G if t not in top_G][:5]; G_UNI = top_G + extra_G
L606         gsc_series = pd.Series({t: g_score.get(t) for t in G_UNI}, name='GSC')
L607         self.g_table = pd.concat([df_z.loc[G_UNI,['GRW','MOM','TRD','VOL']], gsc_series], axis=1)
L608         self.g_table.index = [t + ("â­ï¸" if t in top_G else "") for t in G_UNI]
L609         self.g_formatters = {col:"{:.2f}".format for col in ['GRW','MOM','TRD','VOL']}; self.g_formatters['GSC'] = "{:.3f}".format
L610         self.g_title = (f"[Gæ  / {N_G} / {_fmt_w(g_weights)} / corrM={corrM} / "
L611                         f"LB={DRRS_G['lookback']} nPC={DRRS_G['n_pc']} Î³={DRRS_G['gamma']} Î»={DRRS_G['lam']} Î·={DRRS_G['eta']} shrink={DRRS_SHRINK}]")
L612         if near_G:
L613             add = [t for t in near_G if t not in set(G_UNI)][:10]
L614             if len(add) < 10:
L615                 try:
L616                     aggG = getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False)
L617                     out_now = sorted(set(exist) - set(top_G + top_D))  # ä»Šå› OUT
L618                     used = set(G_UNI + add)
L619                     def _push(lst):
L620                         nonlocal add, used
L621                         for t in lst:
L622                             if len(add) == 10: break
L623                             if t in aggG.index and t not in used:
L624                                 add.append(t); used.add(t)
L625                     _push(out_now)           # â‘  ä»Šå› OUT ã‚’å„ªå…ˆ
L626                     _push(list(aggG.index))  # â‘¡ ã¾ã è¶³ã‚Šãªã‘ã‚Œã°ä¸Šä½ã§å……å¡«
L627                 except Exception:
L628                     pass
L629             if add:
L630                 near_tbl = pd.concat([df_z.loc[add,['GRW','MOM','TRD','VOL']], pd.Series({t: g_score.get(t) for t in add}, name='GSC')], axis=1)
L631                 self.g_table = pd.concat([self.g_table, near_tbl], axis=0)
L632         print(self.g_title); print(self.g_table.to_string(formatters=self.g_formatters))
L633
L634         extra_D = [t for t in init_D if t not in top_D][:5]; D_UNI = top_D + extra_D
L635         cols_D = ['QAL','YLD','VOL','TRD']; d_disp = pd.DataFrame(index=D_UNI)
L636         d_disp['QAL'], d_disp['YLD'], d_disp['VOL'], d_disp['TRD'] = df_z.loc[D_UNI,'D_QAL'], df_z.loc[D_UNI,'D_YLD'], df_z.loc[D_UNI,'D_VOL_RAW'], df_z.loc[D_UNI,'D_TRD']
L637         dsc_series = pd.Series({t: d_score_all.get(t) for t in D_UNI}, name='DSC')
L638         self.d_table = pd.concat([d_disp, dsc_series], axis=1); self.d_table.index = [t + ("â­ï¸" if t in top_D else "") for t in D_UNI]
L639         self.d_formatters = {col:"{:.2f}".format for col in cols_D}; self.d_formatters['DSC']="{:.3f}".format
L640         import scorer
L641         dw_eff = scorer.D_WEIGHTS_EFF
L642         self.d_title = (f"[Dæ  / {N_D} / {_fmt_w(dw_eff)} / corrM={corrM} / "
L643                         f"LB={DRRS_D['lookback']} nPC={DRRS_D['n_pc']} Î³={DRRS_D['gamma']} Î»={DRRS_D['lam']} Î¼={CROSS_MU_GD} Î·={DRRS_D['eta']} shrink={DRRS_SHRINK}]")
L644         if near_D:
L645             add = [t for t in near_D if t not in set(D_UNI)][:10]
L646             if add:
L647                 d_disp2 = pd.DataFrame(index=add)
L648                 d_disp2['QAL'], d_disp2['YLD'], d_disp2['VOL'], d_disp2['TRD'] = df_z.loc[add,'D_QAL'], df_z.loc[add,'D_YLD'], df_z.loc[add,'D_VOL_RAW'], df_z.loc[add,'D_TRD']
L649                 near_tbl = pd.concat([d_disp2, pd.Series({t: d_score_all.get(t) for t in add}, name='DSC')], axis=1)
L650                 self.d_table = pd.concat([self.d_table, near_tbl], axis=0)
L651         print(self.d_title); print(self.d_table.to_string(formatters=self.d_formatters))
L652
L653         # === Changesï¼ˆIN ã® GSC/DSC ã‚’è¡¨ç¤ºã€‚OUT ã¯éŠ˜æŸ„åã®ã¿ï¼‰ ===
L654         in_list = sorted(set(list(top_G)+list(top_D)) - set(exist))
L655         out_list = sorted(set(exist) - set(list(top_G)+list(top_D)))
L656
L657         self.io_table = pd.DataFrame({
L658             'IN': pd.Series(in_list),
L659             '/ OUT': pd.Series(out_list)
L660         })
L661         g_list = [f"{g_score.get(t):.3f}" if pd.notna(g_score.get(t)) else 'â€”' for t in out_list]
L662         d_list = [f"{d_score_all.get(t):.3f}" if pd.notna(d_score_all.get(t)) else 'â€”' for t in out_list]
L663         self.io_table['GSC'] = pd.Series(g_list)
L664         self.io_table['DSC'] = pd.Series(d_list)
L665
L666         print("Changes:")
L667         print(self.io_table.to_string(index=False))
L668
L669         all_tickers = list(set(exist + list(top_G) + list(top_D) + [bench])); prices = yf.download(all_tickers, period='1y', auto_adjust=True, progress=False)['Close']
L670         ret = prices.pct_change(); portfolios = {'CUR':exist,'NEW':list(top_G)+list(top_D)}; metrics={}
L671         for name,ticks in portfolios.items():
L672             pr = ret[ticks].mean(axis=1, skipna=True).dropna(); cum = (1+pr).cumprod()-1; n = len(pr)
L673             if n>=252: ann_ret, ann_vol = (1+cum.iloc[-1])**(252/n)-1, pr.std()*np.sqrt(252)
L674             else: ann_ret, ann_vol = cum.iloc[-1], pr.std()*np.sqrt(n)
L675             sharpe, drawdown = ann_ret/ann_vol, (cum - cum.cummax()).min()
L676             if len(ticks)>=2:
L677                 C_raw = ret[ticks].corr(); RAW_rho = C_raw.mask(np.eye(len(ticks), dtype=bool)).stack().mean()
L678                 R = ret[ticks].dropna().to_numpy(); C_resid = Selector.residual_corr(R, n_pc=3, shrink=DRRS_SHRINK)
L679                 RESID_rho = float((C_resid.sum()-np.trace(C_resid))/(C_resid.shape[0]*(C_resid.shape[0]-1)))
L680             else: RAW_rho = RESID_rho = np.nan
L681             divy = ttm_div_yield_portfolio(ticks); metrics[name] = {'RET':ann_ret,'VOL':ann_vol,'SHP':sharpe,'MDD':drawdown,'RAWÏ':RAW_rho,'RESIDÏ':RESID_rho,'DIVY':divy}
L682         df_metrics = pd.DataFrame(metrics).T; df_metrics_pct = df_metrics.copy(); self.df_metrics = df_metrics
L683         for col in ['RET','VOL','MDD','DIVY']: df_metrics_pct[col] = df_metrics_pct[col]*100
L684         cols_order = ['RET','VOL','SHP','MDD','RAWÏ','RESIDÏ','DIVY']; df_metrics_pct = df_metrics_pct.reindex(columns=cols_order)
L685         def _fmt_row(s):
L686             return pd.Series({'RET':f"{s['RET']:.1f}%",'VOL':f"{s['VOL']:.1f}%",'SHP':f"{s['SHP']:.1f}",'MDD':f"{s['MDD']:.1f}%",'RAWÏ':(f"{s['RAWÏ']:.2f}" if pd.notna(s['RAWÏ']) else "NaN"),'RESIDÏ':(f"{s['RESIDÏ']:.2f}" if pd.notna(s['RESIDÏ']) else "NaN"),'DIVY':f"{s['DIVY']:.1f}%"})
L687         self.df_metrics_fmt = df_metrics_pct.apply(_fmt_row, axis=1); print("Performance Comparison:"); print(self.df_metrics_fmt.to_string())
L688         if self.debug:
L689             self.debug_table = pd.concat([df_z[['TR','EPS','REV','ROE','BETA','DIV','FCF','RS','TR_str','DIV_STREAK']], g_score.rename('GSC'), d_score_all.rename('DSC')], axis=1).round(3)
L690             print("Debug Data:"); print(self.debug_table.to_string())
L691
L692         # === è¿½åŠ : GSC+DSC ãŒä½ã„é † TOP10 ===
L693         try:
L694             all_scores = pd.DataFrame({'GSC': df_z['GSC'], 'DSC': df_z['DSC']}).copy()
L695             all_scores['G_plus_D'] = all_scores['GSC'] + all_scores['DSC']
L696             all_scores = all_scores.dropna(subset=['G_plus_D'])
L697             self.low10_table = all_scores.sort_values('G_plus_D', ascending=True).head(10).round(3)
L698             print("Low Score Candidates (GSC+DSC bottom 10):")
L699             print(self.low10_table.to_string())
L700         except Exception as e:
L701             print(f"[warn] low-score ranking failed: {e}")
L702             self.low10_table = None
L703
L704     # --- Slacké€ä¿¡ï¼ˆå…ƒ notify_slack ã®ãƒ­ã‚¸ãƒƒã‚¯ãã®ã¾ã¾ï¼‰ ---
L705     def notify_slack(self):
L706         SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")
L707         if not SLACK_WEBHOOK_URL: raise ValueError("SLACK_WEBHOOK_URL not set (ç’°å¢ƒå¤‰æ•°ãŒæœªè¨­å®šã§ã™)")
L708         def _filter_suffix_from(spec: dict, group: str) -> str:
L709             g = spec.get(group, {})
L710             parts = [str(m) for m in g.get("pre_mask", [])]
L711             for k, v in (g.get("pre_filter", {}) or {}).items():
L712                 base, op = (k[:-4], "<") if k.endswith("_max") else ((k[:-4], ">") if k.endswith("_min") else (k, "="))
L713                 name = {"beta": "Î²"}.get(base, base)
L714                 try: val = f"{float(v):g}"
L715                 except: val = str(v)
L716                 parts.append(f"{name}{op}{val}")
L717             return "" if not parts else " / filter:" + " & ".join(parts)
L718         def _inject_filter_suffix(title: str, group: str) -> str:
L719             suf = _filter_suffix_from(FILTER_SPEC, group)
L720             return f"{title[:-1]}{suf}]" if suf and title.endswith("]") else (title + suf)
L721         def _blk(title, tbl, fmt=None, drop=()):
L722             if tbl is None or getattr(tbl,'empty',False): return f"{title}\n(é¸å®šãªã—)\n"
L723             if drop and hasattr(tbl,'columns'):
L724                 keep = [c for c in tbl.columns if c not in drop]
L725                 tbl, fmt = tbl[keep], {k:v for k,v in (fmt or {}).items() if k in keep}
L726             return f"{title}\n```{tbl.to_string(formatters=fmt)}```\n"
L727
L728         g_title = _inject_filter_suffix(self.g_title, "G")
L729         d_title = _inject_filter_suffix(self.d_title, "D")
L730         message  = "ğŸ“ˆ ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼åˆ†æ•£æœ€é©åŒ–ã®çµæœ\n"
L731         if self.miss_df is not None and not self.miss_df.empty:
L732             message += "Missing Data\n```" + self.miss_df.to_string(index=False) + "```\n"
L733         message += _blk(g_title, self.g_table, self.g_formatters, drop=("TRD",))
L734         message += _blk(d_title, self.d_table, self.d_formatters)
L735         message += "Changes\n" + ("(å¤‰æ›´ãªã—)\n" if self.io_table is None or getattr(self.io_table,'empty',False) else f"```{self.io_table.to_string(index=False)}```\n")
L736         message += "Performance Comparison:\n```" + self.df_metrics_fmt.to_string() + "```"
L737         if self.debug and self.debug_table is not None:
L738             message += "\nDebug Data\n```" + self.debug_table.to_string() + "```"
L739         payload = {"text": message}
L740         try:
L741             resp = requests.post(SLACK_WEBHOOK_URL, json=payload); resp.raise_for_status(); print("âœ… Slackï¼ˆWebhookï¼‰ã¸é€ä¿¡ã—ã¾ã—ãŸ")
L742         except Exception as e: print(f"âš ï¸ Slacké€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
L743
L744
L745 def _infer_g_universe(feature_df, selected12=None, near5=None):
L746     try:
L747         out = feature_df.index[feature_df['group'].astype(str).str.upper().eq('G')].tolist()
L748         if out: return out
L749     except Exception:
L750         pass
L751     base = set()
L752     for lst in (selected12 or []), (near5 or []):
L753         for x in (lst or []): base.add(x)
L754     return list(base) if base else list(feature_df.index)
L755
L756
L757 def _fmt_with_fire_mark(tickers, feature_df):
L758     out = []
L759     for t in tickers or []:
L760         try:
L761             br = bool(feature_df.at[t, "G_BREAKOUT_recent_5d"])
L762             pb = bool(feature_df.at[t, "G_PULLBACK_recent_5d"])
L763             out.append(f"{t}{' ğŸ”¥' if (br or pb) else ''}")
L764         except Exception:
L765             out.append(t)
L766     return out
L767
L768
L769 def _label_recent_event(t, feature_df):
L770     try:
L771         br = bool(feature_df.at[t, "G_BREAKOUT_recent_5d"]); dbr = str(feature_df.at[t, "G_BREAKOUT_last_date"]) if br else ""
L772         pb = bool(feature_df.at[t, "G_PULLBACK_recent_5d"]); dpb = str(feature_df.at[t, "G_PULLBACK_last_date"]) if pb else ""
L773         if   br and not pb: return f"{t}ï¼ˆãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®š {dbr}ï¼‰"
L774         elif pb and not br: return f"{t}ï¼ˆæŠ¼ã—ç›®åç™º {dpb}ï¼‰"
L775         elif br and pb:     return f"{t}ï¼ˆãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®š {dbr}ï¼æŠ¼ã—ç›®åç™º {dpb}ï¼‰"
L776     except Exception:
L777         pass
L778     return t
L779
L780
L781 # ===== ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å¯è¦–åŒ–ï¼šG/Då…±é€šãƒ•ãƒ­ãƒ¼ï¼ˆå‡ºåŠ›ã¯ä¸å¤‰ï¼‰ ==============================
L782
L783 def io_build_input_bundle() -> InputBundle:
L784     """
L785     æ—¢å­˜ã®ã€ãƒ‡ãƒ¼ã‚¿å–å¾—â†’å‰å‡¦ç†ã€ã‚’å®Ÿè¡Œã—ã€InputBundle ã‚’è¿”ã™ã€‚
L786     å‡¦ç†å†…å®¹ãƒ»åˆ—åãƒ»ä¸¸ã‚ãƒ»ä¾‹å¤–ãƒ»ãƒ­ã‚°æ–‡è¨€ã¯ç¾è¡Œã©ãŠã‚Šï¼ˆå¤‰æ›´ç¦æ­¢ï¼‰ã€‚
L787     """
L788     inp = Input(cand=cand, exist=exist, bench=bench,
L789                 price_max=CAND_PRICE_MAX, finnhub_api_key=FINNHUB_API_KEY)
L790     state = inp.prepare_data()
L791     return InputBundle(
L792         cand=state["cand"], tickers=state["tickers"], bench=bench,
L793         data=state["data"], px=state["px"], spx=state["spx"],
L794         tickers_bulk=state["tickers_bulk"], info=state["info"],
L795         eps_df=state["eps_df"], fcf_df=state["fcf_df"],
L796         returns=state["returns"]
L797     )
L798
L799 def run_group(sc: Scorer, group: str, inb: InputBundle, cfg: PipelineConfig,
L800               n_target: int) -> tuple[list, float, float, float]:
L801     """
L802     G/Dã‚’åŒä¸€æ‰‹é †ã§å‡¦ç†ï¼šæ¡ç‚¹â†’ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼â†’é¸å®šï¼ˆç›¸é–¢ä½æ¸›è¾¼ã¿ï¼‰ã€‚
L803     æˆ»ã‚Šå€¤ï¼š(pick, avg_res_corr, sum_score, objective)
L804     JSONä¿å­˜ã¯æ—¢å­˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆã‚­ãƒ¼åãƒ»ä¸¸ã‚æ¡ãƒ»é †åºï¼‰ã‚’è¸è¥²ã€‚
L805     """
L806     sc.cfg = cfg
L807
L808     if hasattr(sc, "score_build_features"):
L809         feat = sc.score_build_features(inb)
L810         if not hasattr(sc, "_feat_logged"):
L811             T.log("features built (scorer)")
L812             sc._feat_logged = True
L813         agg = sc.score_aggregate(feat, group, cfg) if hasattr(sc, "score_aggregate") else feat
L814     else:
L815         fb = sc.aggregate_scores(inb, cfg)
L816         if not hasattr(sc, "_feat_logged"):
L817             T.log("features built (scorer)")
L818             sc._feat_logged = True
L819         sc._feat = fb
L820         agg = fb.g_score if group == "G" else fb.d_score_all
L821         if group == "D" and hasattr(fb, "df"):
L822             agg = agg[fb.df['BETA'] < D_BETA_MAX]
L823
L824     if hasattr(sc, "filter_candidates"):
L825         mask = sc.filter_candidates(inb, agg, group, cfg)
L826         agg = agg[mask]
L827
L828     selector = Selector()
L829     if hasattr(sc, "select_diversified"):
L830         pick, avg_r, sum_sc, obj = sc.select_diversified(
L831             agg, group, cfg, n_target,
L832             selector=selector, prev_tickers=None,
L833             corrM=cfg.drrs.corrM, shrink=cfg.drrs.shrink,
L834             cross_mu=cfg.drrs.cross_mu_gd
L835         )
L836     else:
L837         if group == "G":
L838             init = agg.nlargest(min(cfg.drrs.corrM, len(agg))).index.tolist()
L839             res = selector.select_bucket_drrs(
L840                 returns_df=inb.returns, score_ser=agg, pool_tickers=init, k=n_target,
L841                 n_pc=cfg.drrs.G.get("n_pc", 3), gamma=cfg.drrs.G.get("gamma", 1.2),
L842                 lam=cfg.drrs.G.get("lam", 0.68),
L843                 lookback=cfg.drrs.G.get("lookback", 252),
L844                 shrink=cfg.drrs.shrink, g_fixed_tickers=None, mu=0.0
L845             )
L846         else:
L847             init = agg.nlargest(min(cfg.drrs.corrM, len(agg))).index.tolist()
L848             g_fixed = getattr(sc, "_top_G", None)
L849             res = selector.select_bucket_drrs(
L850                 returns_df=inb.returns, score_ser=agg, pool_tickers=init, k=n_target,
L851                 n_pc=cfg.drrs.D.get("n_pc", 4), gamma=cfg.drrs.D.get("gamma", 0.8),
L852                 lam=cfg.drrs.D.get("lam", 0.85),
L853                 lookback=cfg.drrs.D.get("lookback", 504),
L854                 shrink=cfg.drrs.shrink, g_fixed_tickers=g_fixed,
L855                 mu=cfg.drrs.cross_mu_gd
L856             )
L857         pick = res["tickers"]; avg_r = res["avg_res_corr"]
L858         sum_sc = res["sum_score"]; obj = res["objective"]
L859         if group == "D":
L860             _, pick = _disjoint_keepG(getattr(sc, "_top_G", []), pick, init)
L861             T.log("selection finalized (G/D)")
L862     # --- Near-Miss: æƒœã—ãã‚‚é¸ã°ã‚Œãªã‹ã£ãŸä¸Šä½10ã‚’ä¿æŒï¼ˆSlackè¡¨ç¤ºç”¨ï¼‰ ---
L863     # 5) Near-Miss ã¨æœ€çµ‚é›†è¨ˆSeriesã‚’ä¿æŒï¼ˆè¡¨ç¤ºå°‚ç”¨ã€‚è¨ˆç®—ã¸å½±éŸ¿ãªã—ï¼‰
L864     try:
L865         pool = agg.drop(index=[t for t in pick if t in agg.index], errors="ignore")
L866         near10 = list(pool.sort_values(ascending=False).head(10).index)
L867         setattr(sc, f"_near_{group}", near10)
L868         setattr(sc, f"_agg_{group}", agg)
L869     except Exception:
L870         pass
L871
L872     if group == "D":
L873         T.log("save done")
L874     if group == "G":
L875         sc._top_G = pick
L876     return pick, avg_r, sum_sc, obj
L877
L878 def run_pipeline() -> SelectionBundle:
L879     """
L880     G/Då…±é€šãƒ•ãƒ­ãƒ¼ã®å…¥å£ã€‚I/Oã¯ã“ã“ã ã‘ã§å®Ÿæ–½ã—ã€è¨ˆç®—ã¯Scorerã«å§”è­²ã€‚
L881     Slackæ–‡è¨€ãƒ»ä¸¸ã‚ãƒ»é †åºã¯æ—¢å­˜ã® Output ã‚’ç”¨ã„ã¦å¤‰æ›´ã—ãªã„ã€‚
L882     """
L883     inb = io_build_input_bundle()
L884     cfg = PipelineConfig(
L885         weights=WeightsConfig(g=g_weights, d=D_weights),
L886         drrs=DRRSParams(corrM=corrM, shrink=DRRS_SHRINK,
L887                          G=DRRS_G, D=DRRS_D, cross_mu_gd=CROSS_MU_GD),
L888         price_max=CAND_PRICE_MAX
L889     )
L890     sc = Scorer()
L891     top_G, avgG, sumG, objG = run_group(sc, "G", inb, cfg, N_G)
L892     poolG = list(getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False).index)
L893     alpha = Scorer.spx_to_alpha(inb.spx)
L894     sectors = {t: (inb.info.get(t, {}).get("sector") or "U") for t in poolG}
L895     scores = {t: Scorer.g_score.get(t, 0.0) for t in poolG}
L896     top_G = Scorer.pick_top_softcap(scores, sectors, N=N_G, cap=2, alpha=alpha, hard=5)
L897     sc._top_G = top_G
L898     try:
L899         aggG = getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False)
L900         sc._near_G = [t for t in aggG.index if t not in set(top_G)][:10]
L901     except Exception:
L902         pass
L903     base = sum(Scorer.g_score.get(t,0.0) for t in poolG[:N_G])
L904     effs = sum(Scorer.g_score.get(t,0.0) for t in top_G)
L905     print(f"[soft_cap2] score_cost={(base-effs)/max(1e-9,abs(base)):.2%}, alpha={alpha:.3f}")
L906     top_D, avgD, sumD, objD = run_group(sc, "D", inb, cfg, N_D)
L907     fb = getattr(sc, "_feat", None)
L908     near_G = getattr(sc, "_near_G", [])
L909     selected12 = list(top_G)
L910     df = fb.df if fb is not None else pd.DataFrame()
L911     guni = _infer_g_universe(df, selected12, near_G)
L912     try:
L913         fire_recent = [t for t in guni
L914                        if (str(df.at[t, "G_BREAKOUT_recent_5d"]) == "True") or
L915                           (str(df.at[t, "G_PULLBACK_recent_5d"]) == "True")]
L916     except Exception:
L917         fire_recent = []
L918
L919     # --- Breadthè¡Œã‚’ä¸¦åˆ—ã§å…ˆè¡Œè¨ˆç®—ï¼ˆInputBundleã®ã¿ä¾å­˜ï¼‰ ---
L920     breadth_fut = None
L921     try:
L922         ex = ThreadPoolExecutor(max_workers=2)
L923         breadth_fut = ex.submit(_build_breadth_lead_lines, inb)
L924     except Exception:
L925         breadth_fut = None
L926
L927     lines = [
L928         "ã€Gæ ãƒ¬ãƒãƒ¼ãƒˆï½œé€±æ¬¡ãƒ¢ãƒ‹ã‚¿ï¼ˆç›´è¿‘5å–¶æ¥­æ—¥ï¼‰ã€‘",
L929         "ã€å‡¡ä¾‹ã€‘ğŸ”¥=ç›´è¿‘5å–¶æ¥­æ—¥å†…ã«ã€Œãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®šã€ã¾ãŸã¯ã€ŒæŠ¼ã—ç›®åç™ºã€ã‚’æ¤œçŸ¥",
L930         f"é¸å®š12: {', '.join(_fmt_with_fire_mark(selected12, df))}" if selected12 else "é¸å®š12: ãªã—",
L931         f"æ¬¡ç‚¹10: {', '.join(_fmt_with_fire_mark(near_G, df))}" if near_G else "æ¬¡ç‚¹10: ãªã—",
L932     ]
L933     # --- ä¸¦åˆ—çµæœã‚’ã“ã“ã§åˆæµï¼ˆå¤±æ•—ã—ã¦ã‚‚æ—¢å­˜ã®å‡ºåŠ›ã¯ç¶™ç¶šï¼‰ ---
L934     if breadth_fut is not None:
L935         try:
L936             lead_lines, _mode = breadth_fut.result()
L937             lines = lead_lines + lines
L938         except Exception as _e:
L939             lines = [f"Breadthè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {_e}"] + lines
L940
L941     if fire_recent:
L942         fire_list = ", ".join([_label_recent_event(t, df) for t in fire_recent])
L943         lines.append(f"éå»5å–¶æ¥­æ—¥ã®æ¤œçŸ¥: {fire_list}")
L944     else:
L945         lines.append("éå»5å–¶æ¥­æ—¥ã®æ¤œçŸ¥: ãªã—")
L946
L947     try:
L948         webhook = os.environ.get("SLACK_WEBHOOK_URL", "")
L949         if webhook:
L950             requests.post(webhook, json={"text": "\n".join(lines)}, timeout=10)
L951     except Exception:
L952         pass
L953
L954     out = Output(debug=debug_mode)
L955     # è¡¨ç¤ºå´ã‹ã‚‰é¸å®šæ™‚ã®é›†è¨ˆã¸ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†ã«ä¿æŒï¼ˆè¡¨ç¤ºå°‚ç”¨ãƒ»å‰¯ä½œç”¨ãªã—ï¼‰
L956     try: out._sc = sc
L957     except Exception: pass
L958     if hasattr(sc, "_feat"):
L959         try:
L960             out.miss_df = sc._feat.missing_logs
L961             out.display_results(
L962                 exist=exist, bench=bench, df_z=sc._feat.df_z,
L963                 g_score=sc._feat.g_score, d_score_all=sc._feat.d_score_all,
L964                 init_G=top_G, init_D=top_D, top_G=top_G, top_D=top_D
L965             )
L966         except Exception:
L967             pass
L968     out.notify_slack()
L969     sb = SelectionBundle(
L970         resG={"tickers": top_G, "avg_res_corr": avgG,
L971               "sum_score": sumG, "objective": objG},
L972         resD={"tickers": top_D, "avg_res_corr": avgD,
L973               "sum_score": sumD, "objective": objD},
L974         top_G=top_G, top_D=top_D, init_G=top_G, init_D=top_D
L975     )
L976
L977     # --- Low Score Candidates (GSC+DSC bottom 10) : send before debug dump ---
L978     try:
L979         _low_df = (
L980             pd.DataFrame({"GSC": fb.g_score, "DSC": fb.d_score_all})
L981               .assign(G_plus_D=lambda x: x["GSC"] + x["DSC"])
L982               .sort_values("G_plus_D")
L983               .head(10)
L984               .round(3)
L985         )
L986         _slack("Low Score Candidates (GSC+DSC bottom 10)\n"
L987                "```"
L988                + _low_df.to_string(index=True, index_names=False)
L989                + "\n```")
L990     except Exception as _e:
L991         _slack(f"Low Score Candidates: ä½œæˆå¤±æ•—: {_e}")
L992
L993     if debug_mode:
L994         try:
L995             _slack_debug(_compact_debug(fb, sb, [], []))
L996         except Exception as e:
L997             print(f"[debug skipped] {e}")
L998
L999     return sb
L1000
L1001 if __name__ == "__main__":
L1002     run_pipeline()
```

## <scorer.py>
```text
L1 # scorer.py 
L2 # kawatest
L3 # =============================================================================
L4 # Scorer: ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼/æŒ‡æ¨™ã®ç”Ÿæˆã¨åˆæˆã‚¹ã‚³ã‚¢ç®—å‡ºã‚’æ‹…ã†ç´”ç²‹å±¤
L5 #
L6 # ã€ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã ã‘èª­ã‚ã°åˆ†ã‹ã‚‹ãƒã‚¤ãƒ³ãƒˆã€‘
L7 # - å…¥åŠ›(InputBundle)ã¯ã€Œä¾¡æ ¼/å‡ºæ¥é«˜/ãƒ™ãƒ³ãƒ/åŸºæœ¬æƒ…å ±/EPS/FCF/ãƒªã‚¿ãƒ¼ãƒ³ã€ã‚’å«ã‚€DTO
L8 # - å‡ºåŠ›(FeatureBundle)ã¯ã€Œrawç‰¹å¾´é‡ dfã€ã€Œæ¨™æº–åŒ– df_zã€ã€ŒG/D ã‚¹ã‚³ã‚¢ã€ã€Œæ¬ æãƒ­ã‚°ã€
L9 # - é‡ã¿ç­‰ã®ã‚³ãƒ³ãƒ•ã‚£ã‚°(PipelineConfig)ã¯ factor ã‹ã‚‰æ¸¡ã™ï¼ˆcfg å¿…é ˆï¼‰
L10 # - æ—§ã‚«ãƒ©ãƒ åã¯ Scorer å†…ã§è‡ªå‹•ãƒªãƒãƒ¼ãƒ ã—ã¦å—ã‘å…¥ã‚Œï¼ˆå¾Œæ–¹äº’æ›ï¼‰
L11 #   ä¾‹) eps_ttm -> EPS_TTM, eps_q_recent -> EPS_Q_LastQ, fcf_ttm -> FCF_TTM
L12 #
L13 # ã€I/Oå¥‘ç´„ï¼ˆScorerãŒå‚ç…§ã™ã‚‹InputBundleãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰ã€‘
L14 #   - cand: List[str]    â€¦ å€™è£œéŠ˜æŸ„ï¼ˆå˜ä½“å®Ÿè¡Œã§ã¯æœªä½¿ç”¨ï¼‰
L15 #   - tickers: List[str] â€¦ å¯¾è±¡éŠ˜æŸ„ãƒªã‚¹ãƒˆ
L16 #   - bench: str         â€¦ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼ï¼ˆä¾‹ '^GSPC'ï¼‰
L17 #   - data: pd.DataFrame â€¦ yfinance downloadçµæœ ('Close','Volume' ç­‰ã®éšå±¤åˆ—)
L18 #   - px: pd.DataFrame   â€¦ data['Close'] ç›¸å½“ï¼ˆçµ‚å€¤ï¼‰
L19 #   - spx: pd.Series     â€¦ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®çµ‚å€¤
L20 #   - tickers_bulk: object         â€¦ yfinance.Tickers
L21 #   - info: Dict[str, dict]        â€¦ yfinance info per ticker
L22 #   - eps_df: pd.DataFrame         â€¦ å¿…é ˆåˆ—: EPS_TTM, EPS_Q_LastQï¼ˆæ—§åã‚‚å¯ï¼‰
L23 #   - fcf_df: pd.DataFrame         â€¦ å¿…é ˆåˆ—: FCF_TTMï¼ˆæ—§åã‚‚å¯ï¼‰
L24 #   - returns: pd.DataFrame        â€¦ px[tickers].pct_change() ç›¸å½“
L25 #
L26 # â€»å…¥å‡ºåŠ›ã®å½¢å¼ãƒ»ä¾‹å¤–æ–‡è¨€ã¯æ—¢å­˜å®Ÿè£…ã‚’å¤‰ãˆã¾ã›ã‚“ï¼ˆå®‰å…¨ãªçŸ­ç¸®ã®ã¿ï¼‰
L27 # =============================================================================
L28
L29 import os, sys, warnings
L30 import requests
L31 import numpy as np
L32 import pandas as pd
L33 import yfinance as yf
L34 from typing import Any, TYPE_CHECKING
L35 from scipy.stats import zscore
L36
L37 if TYPE_CHECKING:
L38     from factor import PipelineConfig  # type: ignore  # å®Ÿè¡Œæ™‚importãªã—ï¼ˆå¾ªç’°å›é¿ï¼‰
L39
L40 # ---- Dividend Helpers -------------------------------------------------------
L41 def _last_close(t, price_map=None):
L42     if price_map and (c := price_map.get(t)) is not None:
L43         return float(c)
L44     try:
L45         h = yf.Ticker(t).history(period="5d")["Close"].dropna()
L46         return float(h.iloc[-1]) if len(h) else np.nan
L47     except Exception:
L48         return np.nan
L49
L50 def _ttm_div_sum(t, lookback_days=400):
L51     try:
L52         div = yf.Ticker(t).dividends
L53         if div is None or len(div) == 0:
L54             return 0.0
L55         cutoff = pd.Timestamp.utcnow().tz_localize(None) - pd.Timedelta(days=lookback_days)
L56         ttm = float(div[div.index.tz_localize(None) >= cutoff].sum())
L57         return ttm if ttm > 0 else float(div.tail(4).sum())
L58     except Exception:
L59         return 0.0
L60
L61 def ttm_div_yield_portfolio(tickers, price_map=None):
L62     ys = []
L63     for t in tickers:
L64         c = _last_close(t, price_map)
L65         if not np.isfinite(c) or c <= 0:
L66             ys.append(0.0)
L67             continue
L68         s = _ttm_div_sum(t)
L69         ys.append(s / c if s > 0 else 0.0)
L70     return float(np.mean(ys)) if ys else 0.0
L71
L72 # ---- ç°¡æ˜“ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆå®‰å…¨ãªçŸ­ç¸®ã®ã¿ï¼‰ -----------------------------------
L73 def winsorize_s(s: pd.Series, p=0.02):
L74     if s is None or s.dropna().empty: return s
L75     lo, hi = np.nanpercentile(s.astype(float), [100*p, 100*(1-p)]); return s.clip(lo, hi)
L76
L77 def robust_z(s: pd.Series, p=0.02):
L78     s2 = winsorize_s(s, p); return np.nan_to_num(zscore(s2.fillna(s2.mean())))
L79
L80 def _safe_div(a, b):
L81     try:
L82         if b is None or float(b)==0 or pd.isna(b): return np.nan
L83         return float(a)/float(b)
L84     except Exception: return np.nan
L85
L86 def _safe_last(series: pd.Series, default=np.nan):
L87     try: return float(series.iloc[-1])
L88     except Exception: return default
L89
L90 D_WEIGHTS_EFF = None  # å‡ºåŠ›è¡¨ç¤ºäº’æ›ã®ãŸã‚
L91
L92 # ---- Scorer æœ¬ä½“ -------------------------------------------------------------
L93 class Scorer:
L94     """
L95     - factor.py ã‹ã‚‰ã¯ `aggregate_scores(ib, cfg)` ã‚’å‘¼ã¶ã ã‘ã§OKã€‚
L96     - cfg ã¯å¿…é ˆï¼ˆfactor.PipelineConfig ã‚’æ¸¡ã™ï¼‰ã€‚
L97     - æ—§ã‚«ãƒ©ãƒ åã‚’è‡ªå‹•ãƒªãƒãƒ¼ãƒ ã—ã¦æ–°ã‚¹ã‚­ãƒ¼ãƒã«å¸åã—ã¾ã™ã€‚
L98     """
L99
L100     # === å…ˆé ­ã§æ—§â†’æ–°ã‚«ãƒ©ãƒ åãƒãƒƒãƒ—ï¼ˆç§»è¡Œç”¨ï¼‰ ===
L101     EPS_RENAME = {"eps_ttm":"EPS_TTM", "eps_q_recent":"EPS_Q_LastQ"}
L102     FCF_RENAME = {"fcf_ttm":"FCF_TTM"}
L103
L104     # === ã‚¹ã‚­ãƒ¼ãƒç°¡æ˜“ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€ä½é™ï¼‰ ===
L105     @staticmethod
L106     def _validate_ib_for_scorer(ib: Any):
L107         must_attrs = ["tickers","bench","data","px","spx","tickers_bulk","info","eps_df","fcf_df","returns"]
L108         miss = [a for a in must_attrs if not hasattr(ib, a) or getattr(ib, a) is None]
L109         if miss: raise ValueError(f"InputBundle is missing required attributes for Scorer: {miss}")
L110
L111         # å¾Œæ–¹äº’æ›ã®ãŸã‚ã€ã¾ãš rename ã‚’è©¦ã¿ã‚‹
L112         if any(c in ib.eps_df.columns for c in Scorer.EPS_RENAME.keys()):
L113             ib.eps_df.rename(columns=Scorer.EPS_RENAME, inplace=True)
L114         if any(c in ib.fcf_df.columns for c in Scorer.FCF_RENAME.keys()):
L115             ib.fcf_df.rename(columns=Scorer.FCF_RENAME, inplace=True)
L116
L117         # å¿…é ˆåˆ—ã®å­˜åœ¨ç¢ºèª
L118         need_eps = {"EPS_TTM","EPS_Q_LastQ"}
L119         need_fcf = {"FCF_TTM"}
L120         if not need_eps.issubset(set(ib.eps_df.columns)):
L121             raise ValueError(f"eps_df must contain columns {need_eps} (accepts old names via auto-rename). Got: {list(ib.eps_df.columns)}")
L122         if not need_fcf.issubset(set(ib.fcf_df.columns)):
L123             raise ValueError(f"fcf_df must contain columns {need_fcf} (accepts old names via auto-rename). Got: {list(ib.fcf_df.columns)}")
L124
L125     # ----ï¼ˆScorerå°‚ç”¨ï¼‰ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ»æŒ‡æ¨™ç³» ----
L126     @staticmethod
L127     def trend(s: pd.Series):
L128         if len(s)<200: return np.nan
L129         sma50, sma150, sma200 = s.rolling(50).mean().iloc[-1], s.rolling(150).mean().iloc[-1], s.rolling(200).mean().iloc[-1]
L130         prev200, p = s.rolling(200).mean().iloc[-21], s.iloc[-1]
L131         lo_52 = s[-252:].min() if len(s)>=252 else s.min(); hi_52 = s[-252:].max() if len(s)>=252 else s.max()
L132         rng = (hi_52 - lo_52) if hi_52>lo_52 else np.nan
L133         clip = lambda x,lo,hi: (np.nan if pd.isna(x) else max(lo,min(hi,x)))
L134         a = clip(p/(s.rolling(50).mean().iloc[-1]) - 1, -0.5, 0.5)
L135         b = clip(sma50/sma150 - 1, -0.5, 0.5)
L136         c = clip(sma150/sma200 - 1, -0.5, 0.5)
L137         d = clip(sma200/prev200 - 1, -0.2, 0.2)
L138         e = clip((p - lo_52) / (rng if rng and rng>0 else np.nan) - 0.5, -0.5, 0.5)
L139         parts = [0.0 if pd.isna(x) else x for x in (a,b,c,d,e)]
L140         return 0.30*parts[0] + 0.20*parts[1] + 0.15*parts[2] + 0.15*parts[3] + 0.20*parts[4]
L141
L142     @staticmethod
L143     def rs(s, b):
L144         n, nb = len(s), len(b)
L145         if n<60 or nb<60: return np.nan
L146         L12 = 252 if n>=252 and nb>=252 else min(n,nb)-1; L1 = 22 if n>=22 and nb>=22 else max(5, min(n,nb)//3)
L147         r12, r1, br12, br1 = s.iloc[-1]/s.iloc[-L12]-1, s.iloc[-1]/s.iloc[-L1]-1, b.iloc[-1]/b.iloc[-L12]-1, b.iloc[-1]/b.iloc[-L1]-1
L148         return (r12 - br12)*0.7 + (r1 - br1)*0.3
L149
L150     @staticmethod
L151     def tr_str(s):
L152         if len(s)<50: return np.nan
L153         return s.iloc[-1]/s.rolling(50).mean().iloc[-1] - 1
L154
L155     @staticmethod
L156     def rs_line_slope(s: pd.Series, b: pd.Series, win: int) -> float:
L157         r = (s/b).dropna()
L158         if len(r)<win: return np.nan
L159         y, x = np.log(r.iloc[-win:]), np.arange(win, dtype=float)
L160         try: return float(np.polyfit(x, y, 1)[0])
L161         except Exception: return np.nan
L162
L163     @staticmethod
L164     def ev_fallback(info_t: dict, tk: yf.Ticker) -> float:
L165         ev = info_t.get('enterpriseValue', np.nan)
L166         if pd.notna(ev) and ev>0: return float(ev)
L167         mc, debt, cash = info_t.get('marketCap', np.nan), np.nan, np.nan
L168         try:
L169             bs = tk.quarterly_balance_sheet
L170             if bs is not None and not bs.empty:
L171                 c = bs.columns[0]
L172                 for k in ("Total Debt","Long Term Debt","Short Long Term Debt"):
L173                     if k in bs.index: debt = float(bs.loc[k,c]); break
L174                 for k in ("Cash And Cash Equivalents","Cash And Cash Equivalents And Short Term Investments","Cash"):
L175                     if k in bs.index: cash = float(bs.loc[k,c]); break
L176         except Exception: pass
L177         if pd.notna(mc): return float(mc + (0 if pd.isna(debt) else debt) - (0 if pd.isna(cash) else cash))
L178         return np.nan
L179
L180     @staticmethod
L181     def dividend_status(ticker: str) -> str:
L182         t = yf.Ticker(ticker)
L183         try:
L184             if not t.dividends.empty: return "has"
L185         except Exception: return "unknown"
L186         try:
L187             a = t.actions
L188             if a is not None and not a.empty and "Stock Splits" in a.columns and a["Stock Splits"].abs().sum()>0: return "none_confident"
L189         except Exception: pass
L190         try:
L191             fi = t.fast_info
L192             if any(getattr(fi,k,None) for k in ("last_dividend_date","dividend_rate","dividend_yield")): return "maybe_missing"
L193         except Exception: pass
L194         return "unknown"
L195
L196     @staticmethod
L197     def div_streak(t):
L198         try:
L199             divs = yf.Ticker(t).dividends.dropna(); ann = divs.groupby(divs.index.year).sum(); ann = ann[ann.index<pd.Timestamp.today().year]
L200             years, streak = sorted(ann.index), 0
L201             for i in range(len(years)-1,0,-1):
L202                 if ann[years[i]] > ann[years[i-1]]: streak += 1
L203                 else: break
L204             return streak
L205         except Exception: return 0
L206
L207     @staticmethod
L208     def fetch_finnhub_metrics(symbol):
L209         api_key = os.environ.get("FINNHUB_API_KEY")
L210         if not api_key: return {}
L211         url, params = "https://finnhub.io/api/v1/stock/metric", {"symbol":symbol,"metric":"all","token":api_key}
L212         try:
L213             r = requests.get(url, params=params, timeout=10); r.raise_for_status(); m = r.json().get("metric",{})
L214             return {'EPS':m.get('epsGrowthTTMYoy'),'REV':m.get('revenueGrowthTTMYoy'),'ROE':m.get('roeTTM'),'BETA':m.get('beta'),'DIV':m.get('dividendYieldIndicatedAnnual'),'FCF':(m.get('freeCashFlowTTM')/m.get('enterpriseValue')) if m.get('freeCashFlowTTM') and m.get('enterpriseValue') else None}
L215         except Exception: return {}
L216
L217     @staticmethod
L218     def calc_beta(series: pd.Series, market: pd.Series, lookback=252):
L219         r, m = series.pct_change().dropna(), market.pct_change().dropna()
L220         n = min(len(r), len(m), lookback)
L221         if n<60: return np.nan
L222         r, m = r.iloc[-n:], m.iloc[-n:]; cov, var = np.cov(r, m)[0,1], np.var(m)
L223         return np.nan if var==0 else cov/var
L224
L225     @staticmethod
L226     def spx_to_alpha(spx: pd.Series, bands=(0.03,0.10), w=(0.6,0.4),
L227                      span=5, q=(0.20,0.40), alphas=(0.05,0.08,0.10)) -> float:
L228         """
L229         S&P500æŒ‡æ•°ã®ã¿ã‹ã‚‰æ“¬ä¼¼breadthã‚’ä½œã‚Šã€å±¥æ­´åˆ†ä½ã§Î±ã‚’æ®µéšæ±ºå®šã€‚
L230         bands=(Â±3%, Â±10%), w=(50DMA,200DMA), åˆ†ä½q=(20%,40%), alphas=(ä½,ä¸­,é«˜)
L231         """
L232         ma50, ma200 = spx.rolling(50).mean(), spx.rolling(200).mean()
L233         b50  = ((spx/ma50 - 1) + bands[0])/(2*bands[0])
L234         b200 = ((spx/ma200 - 1) + bands[1])/(2*bands[1])
L235         hist = (w[0]*b50 + w[1]*b200).clip(0,1).ewm(span=span).mean()
L236         b = float(hist.iloc[-1])
L237         lo, mid = float(hist.quantile(q[0])), float(hist.quantile(q[1]))
L238         return alphas[0] if b < lo else alphas[1] if b < mid else alphas[2]
L239
L240     @staticmethod
L241     def soft_cap_effective_scores(scores: pd.Series|dict, sectors: dict, cap=2, alpha=0.08) -> pd.Series:
L242         """
L243         åŒä¸€ã‚»ã‚¯ã‚¿ãƒ¼capè¶…éï¼ˆ3æœ¬ç›®ä»¥é™ï¼‰ã« Î±Ã—æ®µéšæ¸›ç‚¹ã‚’èª²ã—ãŸâ€œæœ‰åŠ¹ã‚¹ã‚³ã‚¢â€Seriesã‚’è¿”ã™ã€‚
L244         æˆ»ã‚Šå€¤ã¯é™é †ã‚½ãƒ¼ãƒˆæ¸ˆã¿ã€‚
L245         """
L246         s = pd.Series(scores, dtype=float); order = s.sort_values(ascending=False).index
L247         cnt, pen = {}, {}
L248         for t in order:
L249             sec = sectors.get(t, "U")
L250             k = cnt.get(sec, 0) + 1
L251             pen[t] = alpha * max(0, k - cap)
L252             cnt[sec] = k
L253         return (s - pd.Series(pen)).sort_values(ascending=False)
L254
L255     @staticmethod
L256     def pick_top_softcap(scores: pd.Series|dict, sectors: dict, N: int, cap=2, alpha=0.08, hard: int|None=5) -> list[str]:
L257         """
L258         soft-capé©ç”¨å¾Œã®ä¸Šä½Nãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚’è¿”ã™ã€‚hard>0ãªã‚‰éå¸¸ç”¨ãƒãƒ¼ãƒ‰ä¸Šé™ã§åŒä¸€ã‚»ã‚¯ã‚¿ãƒ¼è¶…éã‚’é–“å¼•ãï¼ˆæ—¢å®š=5ï¼‰ã€‚
L259         """
L260         eff = Scorer.soft_cap_effective_scores(scores, sectors, cap, alpha)
L261         if not hard:
L262             return list(eff.head(N).index)
L263         pick, used = [], {}
L264         for t in eff.index:
L265             s = sectors.get(t, "U")
L266             if used.get(s, 0) < hard:
L267                 pick.append(t)
L268                 used[s] = used.get(s, 0) + 1
L269             if len(pick) == N:
L270                 break
L271         return pick
L272
L273     @staticmethod
L274     def trend_template_breadth_series(px: pd.DataFrame, spx: pd.Series, win_days: int | None = None) -> pd.Series:
L275         """
L276         å„å–¶æ¥­æ—¥ã® trend_template åˆæ ¼æœ¬æ•°ï¼ˆåˆæ ¼â€œæœ¬æ•°â€=Cï¼‰ã‚’è¿”ã™ã€‚
L277         - px: åˆ—=tickerï¼ˆãƒ™ãƒ³ãƒã¯å«ã‚ãªã„ï¼‰
L278         - spx: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ Seriesï¼ˆpx.index ã«æ•´åˆ—ï¼‰
L279         - win_days: æœ«å°¾ã®è¨ˆç®—å¯¾è±¡å–¶æ¥­æ—¥æ•°ï¼ˆNoneâ†’å…¨ä½“ã€æ—¢å®š600ã¯å‘¼ã³å‡ºã—å´æŒ‡å®šï¼‰
L280         ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼†rollingã®ã¿ã§è»½é‡ã€‚æ¬ æã¯ False æ‰±ã„ã€‚
L281         """
L282         import numpy as np, pandas as pd
L283         if px is None or px.empty:
L284             return pd.Series(dtype=int)
L285         px = px.dropna(how="all", axis=1)
L286         if win_days and win_days > 0:
L287             px = px.tail(win_days)
L288         if px.empty:
L289             return pd.Series(dtype=int)
L290         spx = spx.reindex(px.index).ffill()
L291
L292         ma50  = px.rolling(50).mean()
L293         ma150 = px.rolling(150).mean()
L294         ma200 = px.rolling(200).mean()
L295
L296         tt = (px > ma150)
L297         tt &= (px > ma200)
L298         tt &= (ma150 > ma200)
L299         tt &= (ma200 - ma200.shift(21) > 0)
L300         tt &= (ma50  > ma150)
L301         tt &= (ma50  > ma200)
L302         tt &= (px    > ma50)
L303
L304         lo252 = px.rolling(252).min()
L305         hi252 = px.rolling(252).max()
L306         tt &= (px.divide(lo252).sub(1.0) >= 0.30)   # P_OVER_LOW52 >= 0.30
L307         tt &= (px >= (0.75 * hi252))                # NEAR_52W_HIGH >= -0.25
L308
L309         r12  = px.divide(px.shift(252)).sub(1.0)
L310         br12 = spx.divide(spx.shift(252)).sub(1.0)
L311         r1   = px.divide(px.shift(22)).sub(1.0)
L312         br1  = spx.divide(spx.shift(22)).sub(1.0)
L313         rs   = 0.7*(r12.sub(br12, axis=0)) + 0.3*(r1.sub(br1, axis=0))
L314         tt &= (rs >= 0.10)
L315
L316         return tt.fillna(False).sum(axis=1).astype(int)
L317
L318     # ---- ã‚¹ã‚³ã‚¢é›†è¨ˆï¼ˆDTO/Configã‚’å—ã‘å–ã‚Šã€FeatureBundleã‚’è¿”ã™ï¼‰ ----
L319     def aggregate_scores(self, ib: Any, cfg):
L320         if cfg is None:
L321             raise ValueError("cfg is required; pass factor.PipelineConfig")
L322         self._validate_ib_for_scorer(ib)
L323
L324         px, spx, tickers = ib.px, ib.spx, ib.tickers
L325         tickers_bulk, info, eps_df, fcf_df = ib.tickers_bulk, ib.info, ib.eps_df, ib.fcf_df
L326
L327         df, missing_logs = pd.DataFrame(index=tickers), []
L328         for t in tickers:
L329             d, s = info[t], px[t]; ev = self.ev_fallback(d, tickers_bulk.tickers[t])
L330             # --- åŸºæœ¬ç‰¹å¾´ ---
L331             df.loc[t,'TR']   = self.trend(s)
L332             df.loc[t,'EPS']  = eps_df.loc[t,'EPS_TTM'] if t in eps_df.index else np.nan
L333             df.loc[t,'REV']  = d.get('revenueGrowth',np.nan)
L334             df.loc[t,'ROE']  = d.get('returnOnEquity',np.nan)
L335             df.loc[t,'BETA'] = self.calc_beta(s, spx, lookback=252)
L336
L337             # --- é…å½“ï¼ˆæ¬ æè£œå®Œå«ã‚€ï¼‰ ---
L338             div = d.get('dividendYield') if d.get('dividendYield') is not None else d.get('trailingAnnualDividendYield')
L339             if div is None or pd.isna(div):
L340                 try:
L341                     divs = yf.Ticker(t).dividends
L342                     if divs is not None and not divs.empty:
L343                         last_close = s.iloc[-1]; div_1y = divs[divs.index >= (divs.index.max() - pd.Timedelta(days=365))].sum()
L344                         if last_close and last_close>0: div = float(div_1y/last_close)
L345                 except Exception: pass
L346             df.loc[t,'DIV'] = 0.0 if (div is None or pd.isna(div)) else float(div)
L347
L348             # --- FCF/EV ---
L349             fcf_val = fcf_df.loc[t,'FCF_TTM'] if t in fcf_df.index else np.nan
L350             df.loc[t,'FCF'] = (fcf_val/ev) if (pd.notna(fcf_val) and pd.notna(ev) and ev>0) else np.nan
L351
L352             # --- ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ãƒ»ãƒœãƒ©é–¢é€£ ---
L353             df.loc[t,'RS'], df.loc[t,'TR_str'] = self.rs(s, spx), self.tr_str(s)
L354             r, rm = s.pct_change().dropna(), spx.pct_change().dropna()
L355             n = int(min(len(r), len(rm)))
L356
L357             DOWNSIDE_DEV = np.nan
L358             if n>=60:
L359                 r6 = r.iloc[-min(len(r),126):]; neg = r6[r6<0]
L360                 if len(neg)>=10: DOWNSIDE_DEV = float(neg.std(ddof=0)*np.sqrt(252))
L361             df.loc[t,'DOWNSIDE_DEV'] = DOWNSIDE_DEV
L362
L363             MDD_1Y = np.nan
L364             try:
L365                 w = s.iloc[-min(len(s),252):].dropna()
L366                 if len(w)>=30:
L367                     roll_max = w.cummax(); MDD_1Y = float((w/roll_max - 1.0).min())
L368             except Exception: pass
L369             df.loc[t,'MDD_1Y'] = MDD_1Y
L370
L371             RESID_VOL = np.nan
L372             if n>=120:
L373                 rr, rrm = r.iloc[-n:].align(rm.iloc[-n:], join='inner')
L374                 if len(rr)==len(rrm) and len(rr)>=120 and rrm.var()>0:
L375                     beta = float(np.cov(rr, rrm)[0,1]/np.var(rrm)); resid = rr - beta*rrm
L376                     RESID_VOL = float(resid.std(ddof=0)*np.sqrt(252))
L377             df.loc[t,'RESID_VOL'] = RESID_VOL
L378
L379             DOWN_OUTPERF = np.nan
L380             if n>=60:
L381                 m, x = rm.iloc[-n:], r.iloc[-n:]; mask = m<0
L382                 if mask.sum()>=10:
L383                     mr, sr = float(m[mask].mean()), float(x[mask].mean())
L384                     DOWN_OUTPERF = (sr - mr)/abs(mr) if mr!=0 else np.nan
L385             df.loc[t,'DOWN_OUTPERF'] = DOWN_OUTPERF
L386
L387             # --- é•·æœŸç§»å‹•å¹³å‡/ä½ç½® ---
L388             sma200 = s.rolling(200).mean(); df.loc[t,'EXT_200'] = np.nan
L389             if pd.notna(sma200.iloc[-1]) and sma200.iloc[-1]!=0: df.loc[t,'EXT_200'] = abs(float(s.iloc[-1]/sma200.iloc[-1]-1.0))
L390
L391             # --- é…å½“ã®è©³ç´°ç³» ---
L392             DIV_TTM_PS=DIV_VAR5=DIV_YOY=DIV_FCF_COVER=np.nan
L393             try:
L394                 divs = yf.Ticker(t).dividends.dropna()
L395                 if not divs.empty:
L396                     last_close = s.iloc[-1]; div_1y = float(divs[divs.index >= (divs.index.max()-pd.Timedelta(days=365))].sum())
L397                     DIV_TTM_PS = div_1y if div_1y>0 else np.nan
L398                     ann = divs.groupby(divs.index.year).sum()
L399                     if len(ann)>=2 and ann.iloc[-2]!=0: DIV_YOY = float(ann.iloc[-1]/ann.iloc[-2]-1.0)
L400                     tail = ann.iloc[-5:] if len(ann)>=5 else ann
L401                     if len(tail)>=3 and tail.mean()!=0: DIV_VAR5 = float(tail.std(ddof=1)/abs(tail.mean()))
L402                 so = d.get('sharesOutstanding',None)
L403                 if so and pd.notna(DIV_TTM_PS) and pd.notna(fcf_val) and fcf_val!=0:
L404                     DIV_FCF_COVER = float((fcf_val)/(DIV_TTM_PS*float(so)))
L405             except Exception: pass
L406             df.loc[t,'DIV_TTM_PS'], df.loc[t,'DIV_VAR5'], df.loc[t,'DIV_YOY'], df.loc[t,'DIV_FCF_COVER'] = DIV_TTM_PS, DIV_VAR5, DIV_YOY, DIV_FCF_COVER
L407
L408             # --- è²¡å‹™å®‰å®šæ€§ ---
L409             df.loc[t,'DEBT2EQ'], df.loc[t,'CURR_RATIO'] = d.get('debtToEquity',np.nan), d.get('currentRatio',np.nan)
L410
L411             # --- EPS å¤‰å‹• ---
L412             EPS_VAR_8Q = np.nan
L413             try:
L414                 qe, so = tickers_bulk.tickers[t].quarterly_earnings, d.get('sharesOutstanding',None)
L415                 if qe is not None and not qe.empty and so:
L416                     eps_q = (qe['Earnings'].dropna().astype(float)/float(so)).replace([np.inf,-np.inf],np.nan)
L417                     if len(eps_q)>=4: EPS_VAR_8Q = float(eps_q.iloc[-min(8,len(eps_q)):].std(ddof=1))
L418             except Exception: pass
L419             df.loc[t,'EPS_VAR_8Q'] = EPS_VAR_8Q
L420
L421             # --- ã‚µã‚¤ã‚º/æµå‹•æ€§ ---
L422             df.loc[t,'MARKET_CAP'] = d.get('marketCap',np.nan); adv60 = np.nan
L423             try:
L424                 vol_series = ib.data['Volume'][t].dropna()
L425                 if len(vol_series)>=5 and len(s)==len(vol_series):
L426                     dv = (vol_series*s).rolling(60).mean(); adv60 = float(dv.iloc[-1])
L427             except Exception: pass
L428             df.loc[t,'ADV60_USD'] = adv60
L429
L430             # --- å£²ä¸Š/åˆ©ç›Šã®åŠ é€Ÿåº¦ç­‰ ---
L431             REV_Q_YOY=EPS_Q_YOY=REV_YOY_ACC=REV_YOY_VAR=np.nan
L432             REV_ANNUAL_STREAK = np.nan
L433             try:
L434                 qe, so = tickers_bulk.tickers[t].quarterly_earnings, d.get('sharesOutstanding',None)
L435                 if qe is not None and not qe.empty:
L436                     if 'Revenue' in qe.columns:
L437                         rev = qe['Revenue'].dropna().astype(float)
L438                         if len(rev)>=5: REV_Q_YOY = _safe_div(rev.iloc[-1]-rev.iloc[-5], rev.iloc[-5])
L439                         if len(rev)>=6:
L440                             yoy_now = _safe_div(rev.iloc[-1]-rev.iloc[-5], rev.iloc[-5]); yoy_prev = _safe_div(rev.iloc[-2]-rev.iloc[-6], rev.iloc[-6])
L441                             if pd.notna(yoy_now) and pd.notna(yoy_prev): REV_YOY_ACC = yoy_now - yoy_prev
L442                         yoy_list=[]
L443                         for k in range(1,5):
L444                             if len(rev)>=4+k:
L445                                 y = _safe_div(rev.iloc[-k]-rev.iloc[-(k+4)], rev.iloc[-(k+4)])
L446                                 if pd.notna(y): yoy_list.append(y)
L447                         if len(yoy_list)>=2: REV_YOY_VAR = float(np.std(yoy_list, ddof=1))
L448                         # NEW: å¹´æ¬¡ã®æŒç¶šæ€§ï¼ˆç›´è¿‘ã‹ã‚‰é¡ã£ã¦å‰å¹´æ¯”ãƒ—ãƒ©ã‚¹ãŒä½•å¹´é€£ç¶šã‹ã€å››åŠæœŸ4æœ¬æƒã†å®Œå…¨å¹´ã®ã¿ï¼‰
L449                         try:
L450                             g = rev.groupby(rev.index.year)
L451                             ann_sum, cnt = g.sum(), g.count()
L452                             ann_sum = ann_sum[cnt >= 4]
L453                             if len(ann_sum) >= 3:
L454                                 yoy = ann_sum.pct_change().dropna()
L455                                 streak = 0
L456                                 for v in yoy.iloc[::-1]:
L457                                     if pd.isna(v) or v <= 0:
L458                                         break
L459                                     streak += 1
L460                                 REV_ANNUAL_STREAK = float(streak)
L461                         except Exception:
L462                             pass
L463                     if 'Earnings' in qe.columns and so:
L464                         eps_series = (qe['Earnings'].dropna().astype(float)/float(so)).replace([np.inf,-np.inf],np.nan)
L465                         if len(eps_series)>=5 and pd.notna(eps_series.iloc[-5]) and eps_series.iloc[-5]!=0:
L466                             EPS_Q_YOY = _safe_div(eps_series.iloc[-1]-eps_series.iloc[-5], eps_series.iloc[-5])
L467             except Exception: pass
L468             df.loc[t,'REV_Q_YOY'], df.loc[t,'EPS_Q_YOY'], df.loc[t,'REV_YOY_ACC'], df.loc[t,'REV_YOY_VAR'] = REV_Q_YOY, EPS_Q_YOY, REV_YOY_ACC, REV_YOY_VAR
L469             df.loc[t,'REV_ANN_STREAK'] = REV_ANNUAL_STREAK
L470
L471             # --- Rule of 40 ã‚„å‘¨è¾º ---
L472             total_rev_ttm = d.get('totalRevenue',np.nan)
L473             FCF_MGN = _safe_div(fcf_val, total_rev_ttm)
L474             df.loc[t,'FCF_MGN'] = FCF_MGN
L475             rule40 = np.nan
L476             try:
L477                 r = df.loc[t,'REV']; rule40 = (r if pd.notna(r) else np.nan) + (FCF_MGN if pd.notna(FCF_MGN) else np.nan)
L478             except Exception: pass
L479             df.loc[t,'RULE40'] = rule40
L480
L481             # --- ãƒˆãƒ¬ãƒ³ãƒ‰è£œåŠ© ---
L482             sma50  = s.rolling(50).mean()
L483             sma150 = s.rolling(150).mean()
L484             sma200 = s.rolling(200).mean()
L485             p = _safe_last(s)
L486
L487             df.loc[t,'MA50_OVER_150'] = (
L488                 _safe_last(sma50)/_safe_last(sma150) - 1
L489                 if pd.notna(_safe_last(sma50)) and pd.notna(_safe_last(sma150)) and _safe_last(sma150)!=0 else np.nan
L490             )
L491             df.loc[t,'MA150_OVER_200'] = (
L492                 _safe_last(sma150)/_safe_last(sma200) - 1
L493                 if pd.notna(_safe_last(sma150)) and pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L494             )
L495
L496             lo52 = s[-252:].min() if len(s)>=252 else s.min()
L497             df.loc[t,'P_OVER_LOW52'] = (p/lo52 - 1) if (lo52 and lo52>0 and pd.notna(p)) else np.nan
L498
L499             df.loc[t,'MA200_SLOPE_1M'] = np.nan
L500             if len(sma200.dropna()) >= 21:
L501                 cur200 = _safe_last(sma200)
L502                 old2001 = float(sma200.iloc[-21])
L503                 if old2001:
L504                     df.loc[t,'MA200_SLOPE_1M'] = cur200/old2001 - 1
L505
L506             df.loc[t,'P_OVER_150'] = p/_safe_last(sma150)-1 if pd.notna(_safe_last(sma150)) and _safe_last(sma150)!=0 else np.nan
L507             df.loc[t,'P_OVER_200'] = p/_safe_last(sma200)-1 if pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L508             df.loc[t,'MA50_OVER_200'] = _safe_last(sma50)/_safe_last(sma200)-1 if pd.notna(_safe_last(sma50)) and pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L509             df.loc[t,'MA200_SLOPE_5M'] = np.nan
L510             if len(sma200.dropna())>=105:
L511                 cur200, old200 = _safe_last(sma200), float(sma200.iloc[-105])
L512                 if old200 and old200!=0: df.loc[t,'MA200_SLOPE_5M'] = cur200/old200 - 1
L513             # NEW: 200æ—¥ç·šãŒé€£ç¶šã§ä¸Šå‘ãã®ã€Œæ—¥æ•°ã€
L514             df.loc[t,'MA200_UP_STREAK_D'] = np.nan
L515             try:
L516                 s200 = sma200.dropna()
L517                 if len(s200) >= 2:
L518                     diff200 = s200.diff()
L519                     up = 0
L520                     for v in diff200.iloc[::-1]:
L521                         if pd.isna(v) or v <= 0:
L522                             break
L523                         up += 1
L524                     df.loc[t,'MA200_UP_STREAK_D'] = float(up)
L525             except Exception:
L526                 pass
L527             df.loc[t,'LOW52PCT25_EXCESS'] = np.nan if (lo52 is None or lo52<=0 or pd.isna(p)) else (p/(lo52*1.25)-1)
L528             hi52 = s[-252:].max() if len(s)>=252 else s.max(); df.loc[t,'NEAR_52W_HIGH'] = np.nan
L529             if hi52 and hi52>0 and pd.notna(p):
L530                 d_hi = (p/hi52)-1.0; df.loc[t,'NEAR_52W_HIGH'] = -abs(min(0.0, d_hi))
L531             df.loc[t,'RS_SLOPE_6W'] = self.rs_line_slope(s, ib.spx, 30)
L532             df.loc[t,'RS_SLOPE_13W'] = self.rs_line_slope(s, ib.spx, 65)
L533
L534             df.loc[t,'DIV_STREAK'] = self.div_streak(t)
L535
L536             # --- æ¬ æãƒ¡ãƒ¢ ---
L537             fin_cols = ['REV','ROE','BETA','DIV','FCF']
L538             need_finnhub = [col for col in fin_cols if pd.isna(df.loc[t,col])]
L539             if need_finnhub:
L540                 fin_data = self.fetch_finnhub_metrics(t)
L541                 for col in need_finnhub:
L542                     val = fin_data.get(col)
L543                     if val is not None and not pd.isna(val): df.loc[t,col] = val
L544             for col in fin_cols + ['EPS','RS','TR_str','DIV_STREAK']:
L545                 if pd.isna(df.loc[t,col]):
L546                     if col=='DIV':
L547                         status = self.dividend_status(t)
L548                         if status!='none_confident': missing_logs.append({'Ticker':t,'Column':col,'Status':status})
L549                     else:
L550                         missing_logs.append({'Ticker':t,'Column':col})
L551
L552         def _trend_template_pass(row, rs_alpha_thresh=0.10):
L553             c1 = (row.get('P_OVER_150', np.nan) > 0) and (row.get('P_OVER_200', np.nan) > 0)
L554             c2 = (row.get('MA150_OVER_200', np.nan) > 0)
L555             c3 = (row.get('MA200_SLOPE_1M', np.nan) > 0)
L556             c4 = (row.get('MA50_OVER_150', np.nan) > 0) and (row.get('MA50_OVER_200', np.nan) > 0)
L557             c5 = (row.get('TR_str', np.nan) > 0)
L558             c6 = (row.get('P_OVER_LOW52', np.nan) >= 0.30)
L559             c7 = (row.get('NEAR_52W_HIGH', np.nan) >= -0.25)
L560             c8 = (row.get('RS', np.nan) >= 0.10)
L561             return bool(c1 and c2 and c3 and c4 and c5 and c6 and c7 and c8)
L562
L563         if 'trend_template' not in df.columns: df['trend_template'] = df.apply(_trend_template_pass, axis=1).fillna(False)
L564         assert 'trend_template' in df.columns
L565
L566         # === ZåŒ–ã¨åˆæˆ ===
L567         for col in ['ROE','FCF','REV','EPS']: df[f'{col}_W'] = winsorize_s(df[col], 0.02)
L568
L569         df_z = pd.DataFrame(index=df.index)
L570         for col in ['EPS','REV','ROE','FCF','RS','TR_str','BETA','DIV','DIV_STREAK']: df_z[col] = robust_z(df[col])
L571         df_z['REV'], df_z['EPS'], df_z['TR'] = robust_z(df['REV_W']), robust_z(df['EPS_W']), robust_z(df['TR'])
L572         for col in ['P_OVER_150','P_OVER_200','MA50_OVER_200','MA200_SLOPE_5M','LOW52PCT25_EXCESS','NEAR_52W_HIGH','RS_SLOPE_6W','RS_SLOPE_13W','MA200_UP_STREAK_D']: df_z[col] = robust_z(df[col])
L573         for col in ['REV_Q_YOY','EPS_Q_YOY','REV_YOY_ACC','REV_YOY_VAR','FCF_MGN','RULE40','REV_ANN_STREAK']: df_z[col] = robust_z(df[col])
L574         for col in ['DOWNSIDE_DEV','MDD_1Y','RESID_VOL','DOWN_OUTPERF','EXT_200','DIV_TTM_PS','DIV_VAR5','DIV_YOY','DIV_FCF_COVER','DEBT2EQ','CURR_RATIO','EPS_VAR_8Q','MARKET_CAP','ADV60_USD']: df_z[col] = robust_z(df[col])
L575
L576         df_z['SIZE'], df_z['LIQ'] = robust_z(np.log1p(df['MARKET_CAP'])), robust_z(np.log1p(df['ADV60_USD']))
L577         df_z['QUALITY_F'] = robust_z(0.6*df['FCF_W'] + 0.4*df['ROE_W']).clip(-3.0,3.0)
L578         df_z['YIELD_F']   = 0.3*df_z['DIV'] + 0.7*df_z['DIV_STREAK']
L579         df_z['GROWTH_F']  = robust_z(
L580               0.25*df_z['REV']          # â†“0.30â†’0.25
L581             + 0.20*df_z['EPS_Q_YOY']
L582             + 0.15*df_z['REV_Q_YOY']
L583             + 0.15*df_z['REV_YOY_ACC']
L584             + 0.10*df_z['RULE40']
L585             + 0.10*df_z['FCF_MGN']
L586             + 0.10*df_z['EPS']          # â˜…è¿½åŠ ï¼šé»’å­—å„ªé‡ï¼èµ¤å­—æ¸›ç‚¹
L587             + 0.05*df_z['REV_ANN_STREAK']
L588             - 0.05*df_z['REV_YOY_VAR']
L589         ).clip(-3.0,3.0)
L590         df_z['MOM_F'] = robust_z(
L591               0.40*df_z['RS']
L592             + 0.15*df_z['TR_str']
L593             + 0.15*df_z['RS_SLOPE_6W']
L594             + 0.15*df_z['RS_SLOPE_13W']
L595             + 0.10*df_z['MA200_SLOPE_5M']
L596             + 0.10*df_z['MA200_UP_STREAK_D']
L597         ).clip(-3.0,3.0)
L598         df_z['VOL'] = robust_z(df['BETA'])
L599         df_z.rename(columns={'GROWTH_F':'GRW','MOM_F':'MOM','QUALITY_F':'QAL','YIELD_F':'YLD'}, inplace=True)
L600
L601         # === begin: BIO LOSS PENALTY =====================================
L602         try:
L603             penalty_z = float(os.getenv("BIO_LOSS_PENALTY_Z", "0.8"))
L604         except Exception:
L605             penalty_z = 0.8
L606
L607         def _is_bio_like(t: str) -> bool:
L608             inf = info.get(t, {}) if isinstance(info, dict) else {}
L609             sec = str(inf.get("sector", "")).lower()
L610             ind = str(inf.get("industry", "")).lower()
L611             if "health" not in sec:
L612                 return False
L613             keys = ("biotech", "biopharma", "pharma")
L614             return any(k in ind for k in keys)
L615
L616         tickers_s = pd.Index(df_z.index)
L617         is_bio = pd.Series({t: _is_bio_like(t) for t in tickers_s})
L618         is_loss = pd.Series({t: (pd.notna(df.loc[t,"EPS"]) and df.loc[t,"EPS"] <= 0) for t in tickers_s})
L619         mask_bio_loss = (is_bio & is_loss).reindex(df_z.index).fillna(False)
L620
L621         if bool(mask_bio_loss.any()) and penalty_z > 0:
L622             df_z.loc[mask_bio_loss, "GRW"] = df_z.loc[mask_bio_loss, "GRW"] - penalty_z
L623             df_z["GRW"] = df_z["GRW"].clip(-3.0, 3.0)
L624         # === end: BIO LOSS PENALTY =======================================
L625
L626         df_z['TRD'] = 0.0  # TRDã¯ã‚¹ã‚³ã‚¢å¯„ä¸ã‹ã‚‰å¤–ã—ã€ãƒ†ãƒ³ãƒ—ãƒ¬åˆ¤å®šã¯ãƒ•ã‚£ãƒ«ã‚¿ã§è¡Œã†ï¼ˆåˆ—ã¯è¡¨ç¤ºäº’æ›ã®ãŸã‚æ®‹ã™ï¼‰
L627         if 'BETA' not in df_z.columns: df_z['BETA'] = robust_z(df['BETA'])
L628
L629         df_z['D_VOL_RAW'] = robust_z(0.40*df_z['DOWNSIDE_DEV'] + 0.22*df_z['RESID_VOL'] + 0.18*df_z['MDD_1Y'] - 0.10*df_z['DOWN_OUTPERF'] - 0.05*df_z['EXT_200'] - 0.08*df_z['SIZE'] - 0.10*df_z['LIQ'] + 0.10*df_z['BETA'])
L630         df_z['D_QAL']     = robust_z(0.35*df_z['QAL'] + 0.20*df_z['FCF'] + 0.15*df_z['CURR_RATIO'] - 0.15*df_z['DEBT2EQ'] - 0.15*df_z['EPS_VAR_8Q'])
L631         df_z['D_YLD']     = robust_z(0.45*df_z['DIV'] + 0.25*df_z['DIV_STREAK'] + 0.20*df_z['DIV_FCF_COVER'] - 0.10*df_z['DIV_VAR5'])
L632         df_z['D_TRD']     = robust_z(0.40*df_z.get('MA200_SLOPE_5M',0) - 0.30*df_z.get('EXT_200',0) + 0.15*df_z.get('NEAR_52W_HIGH',0) + 0.15*df_z['TR'])
L633
L634         # --- é‡ã¿ã¯ cfg ã‚’å„ªå…ˆï¼ˆå¤–éƒ¨ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ï¼‰ ---
L635         # â‘  å…¨éŠ˜æŸ„ã§ G/D ã‚¹ã‚³ã‚¢ã‚’ç®—å‡ºï¼ˆunmaskedï¼‰
L636         g_score_all = df_z.mul(pd.Series(cfg.weights.g)).sum(axis=1)
L637
L638         d_comp = pd.concat({
L639             'QAL': df_z['D_QAL'],
L640             'YLD': df_z['D_YLD'],
L641             'VOL': df_z['D_VOL_RAW'],
L642             'TRD': df_z['D_TRD']
L643         }, axis=1)
L644         dw = pd.Series(cfg.weights.d, dtype=float).reindex(['QAL','YLD','VOL','TRD']).fillna(0.0)
L645         globals()['D_WEIGHTS_EFF'] = dw.copy()
L646         d_score_all = d_comp.mul(dw, axis=1).sum(axis=1)
L647
L648         # â‘¡ ãƒ†ãƒ³ãƒ—ãƒ¬åˆ¤å®šï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ãã®ã¾ã¾ï¼‰
L649         mask = df['trend_template']
L650         if not bool(mask.any()):
L651             mask = (
L652                 (df.get('P_OVER_LOW52', np.nan) >= 0.25) &
L653                 (df.get('NEAR_52W_HIGH', np.nan) >= -0.30) &
L654                 (df.get('RS', np.nan) >= 0.08) &
L655                 (df.get('MA200_SLOPE_1M', np.nan) > 0) &
L656                 (df.get('P_OVER_150', np.nan) > 0) & (df.get('P_OVER_200', np.nan) > 0) &
L657                 (df.get('MA150_OVER_200', np.nan) > 0) &
L658                 (df.get('MA50_OVER_150', np.nan) > 0) & (df.get('MA50_OVER_200', np.nan) > 0) &
L659                 (df.get('TR_str', np.nan) > 0)
L660             ).fillna(False)
L661             df['trend_template'] = mask
L662
L663         # â‘¢ æ¡ç”¨ç”¨ã¯ maskã€è¡¨ç¤º/åˆ†æç”¨ã¯åˆ—ã§å…¨éŠ˜æŸ„ä¿å­˜
L664         g_score = g_score_all.loc[mask]
L665         Scorer.g_score = g_score
L666         df_z['GSC'] = g_score_all
L667         df_z['DSC'] = d_score_all
L668
L669         try:
L670             current = (
L671                 pd.read_csv("current_tickers.csv")
L672                   .iloc[:, 0]
L673                   .str.upper()
L674                   .tolist()
L675             )
L676         except FileNotFoundError:
L677             warnings.warn("current_tickers.csv not found â€” bonus skipped")
L678             current = []
L679
L680         mask_bonus = g_score.index.isin(current)
L681         if mask_bonus.any():
L682             # 1) factor.BONUS_COEFF ã‹ã‚‰ k ã‚’æ±ºã‚ã€ç„¡ã‘ã‚Œã° 0.4
L683             k = float(getattr(sys.modules.get("factor"), "BONUS_COEFF", 0.4))
L684             # 2) g å´ã® Ïƒ ã‚’å–ã‚Šã€NaN ãªã‚‰ 0 ã«ä¸¸ã‚ã‚‹
L685             sigma_g = g_score.std()
L686             if pd.isna(sigma_g):
L687                 sigma_g = 0.0
L688             bonus_g = round(k * sigma_g, 3)
L689             g_score.loc[mask_bonus] += bonus_g
L690             Scorer.g_score = g_score
L691             # 3) D å´ã‚‚åŒæ§˜ã« Ïƒ ã® NaN ã‚’ã‚±ã‚¢
L692             sigma_d = d_score_all.std()
L693             if pd.isna(sigma_d):
L694                 sigma_d = 0.0
L695             bonus_d = round(k * sigma_d, 3)
L696             d_score_all.loc[d_score_all.index.isin(current)] += bonus_d
L697
L698         try:
L699             df = _apply_growth_entry_flags(df, ib, self, win_breakout=5, win_pullback=5)
L700         except Exception:
L701             pass
L702
L703         from factor import FeatureBundle  # type: ignore  # å®Ÿè¡Œæ™‚importãªã—ï¼ˆå¾ªç’°å›é¿ï¼‰
L704         return FeatureBundle(
L705             df=df,
L706             df_z=df_z,
L707             g_score=g_score,
L708             d_score_all=d_score_all,
L709             missing_logs=pd.DataFrame(missing_logs)
L710         )
L711
L712
L713 def _apply_growth_entry_flags(feature_df, bundle, self_obj, win_breakout=5, win_pullback=5):
L714     """
L715     Gæ ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã«å¯¾ã—ã€ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®š/æŠ¼ã—ç›®åç™ºã®ã€Œç›´è¿‘Nå–¶æ¥­æ—¥å†…ã®ç™ºç«ã€ã‚’åˆ¤å®šã—ã€
L716     æ¬¡ã®åˆ—ã‚’ feature_df ã«è¿½åŠ ã™ã‚‹ï¼ˆindex=tickerï¼‰ã€‚
L717       - G_BREAKOUT_recent_5d : bool
L718       - G_BREAKOUT_last_date : str "YYYY-MM-DD"
L719       - G_PULLBACK_recent_5d : bool
L720       - G_PULLBACK_last_date : str "YYYY-MM-DD"
L721       - G_PIVOT_price        : float
L722     å¤±æ•—ã—ã¦ã‚‚ä¾‹å¤–ã¯æ¡ã‚Šæ½°ã—ã€æ—¢å­˜å‡¦ç†ã‚’é˜»å®³ã—ãªã„ã€‚
L723     """
L724     try:
L725         px   = bundle.px                      # çµ‚å€¤ DataFrame
L726         hi   = bundle.data['High']
L727         lo   = bundle.data['Low']
L728         vol  = bundle.data['Volume']
L729         bench= bundle.spx                     # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ Series
L730
L731         # Gãƒ¦ãƒ‹ãƒãƒ¼ã‚¹æ¨å®šï¼šself.g_universe å„ªå…ˆ â†’ feature_df['group']=='G' â†’ å…¨éŠ˜æŸ„
L732         g_universe = getattr(self_obj, "g_universe", None)
L733         if g_universe is None:
L734             try:
L735                 g_universe = feature_df.index[feature_df['group'].astype(str).str.upper().eq('G')].tolist()
L736             except Exception:
L737                 g_universe = list(feature_df.index)
L738         if not g_universe:
L739             return feature_df
L740
L741         # æŒ‡æ¨™
L742         ema21 = px[g_universe].ewm(span=21, adjust=False).mean()
L743         ma50  = px[g_universe].rolling(50).mean()
L744         ma150 = px[g_universe].rolling(150).mean()
L745         ma200 = px[g_universe].rolling(200).mean()
L746         atr20 = (hi[g_universe] - lo[g_universe]).rolling(20).mean()
L747         vol20 = vol[g_universe].rolling(20).mean()
L748         vol50 = vol[g_universe].rolling(50).mean()
L749
L750         # ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆåˆæ ¼
L751         trend_template_ok = (px[g_universe] > ma50) & (px[g_universe] > ma150) & (px[g_universe] > ma200) \
L752                             & (ma150 > ma200) & (ma200.diff() > 0)
L753
L754         # æ±ç”¨ãƒ”ãƒœãƒƒãƒˆï¼šç›´è¿‘65å–¶æ¥­æ—¥ã®é«˜å€¤ï¼ˆå½“æ—¥é™¤å¤–ï¼‰
L755         pivot_price = hi[g_universe].rolling(65).max().shift(1)
L756
L757         # ç›¸å¯¾åŠ›ï¼šå¹´å†…é«˜å€¤æ›´æ–°
L758         bench_aligned = bench.reindex(px.index).ffill()
L759         rs = px[g_universe].div(bench_aligned, axis=0)
L760         rs_high = rs.rolling(252).max().shift(1)
L761
L762         # ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆã€Œç™ºç”Ÿæ—¥ã€ï¼šæ¡ä»¶ç«‹ã¡ä¸ŠãŒã‚Š
L763         breakout_today = trend_template_ok & (px[g_universe] > pivot_price) \
L764                          & (vol[g_universe] >= 1.5 * vol50) & (rs > rs_high)
L765         breakout_event = breakout_today & ~breakout_today.shift(1).fillna(False)
L766
L767         # æŠ¼ã—ç›®åç™ºã€Œç™ºç”Ÿæ—¥ã€ï¼šEMA21å¸¯Ã—å‡ºæ¥é«˜ãƒ‰ãƒ©ã‚¤ã‚¢ãƒƒãƒ—Ã—å‰æ—¥é«˜å€¤è¶ŠãˆÃ—çµ‚å€¤EMA21ä¸Š
L768         near_ema21_band = px[g_universe].between(ema21 - atr20, ema21 + atr20)
L769         volume_dryup = (vol20 / vol50) <= 1.0
L770         pullback_bounce_confirmed = (px[g_universe] > hi[g_universe].shift(1)) & (px[g_universe] > ema21)
L771         pullback_today = trend_template_ok & near_ema21_band & volume_dryup & pullback_bounce_confirmed
L772         pullback_event = pullback_today & ~pullback_today.shift(1).fillna(False)
L773
L774         # ç›´è¿‘Nå–¶æ¥­æ—¥å†…ã®ç™ºç« / æœ€çµ‚ç™ºç”Ÿæ—¥
L775         rows = []
L776         for t in g_universe:
L777             def _recent_and_date(s, win):
L778                 sw = s[t].iloc[-win:]
L779                 if sw.any():
L780                     d = sw[sw].index[-1]
L781                     return True, d.strftime("%Y-%m-%d")
L782                 return False, ""
L783             br_recent, br_date = _recent_and_date(breakout_event, win_breakout)
L784             pb_recent, pb_date = _recent_and_date(pullback_event, win_pullback)
L785             rows.append((t, {
L786                 "G_BREAKOUT_recent_5d": br_recent,
L787                 "G_BREAKOUT_last_date": br_date,
L788                 "G_PULLBACK_recent_5d": pb_recent,
L789                 "G_PULLBACK_last_date": pb_date,
L790                 "G_PIVOT_price": float(pivot_price[t].iloc[-1]) if t in pivot_price.columns else float('nan'),
L791             }))
L792         flags = pd.DataFrame({k: v for k, v in rows}).T
L793
L794         # åˆ—ã‚’ä½œæˆãƒ»ä¸Šæ›¸ã
L795         cols = ["G_BREAKOUT_recent_5d","G_BREAKOUT_last_date","G_PULLBACK_recent_5d","G_PULLBACK_last_date","G_PIVOT_price"]
L796         for c in cols:
L797             if c not in feature_df.columns:
L798                 feature_df[c] = np.nan
L799         feature_df.loc[flags.index, flags.columns] = flags
L800
L801     except Exception:
L802         pass
L803     return feature_df
L804
L805
L806
```

## <.github/workflows/weekly-report.yml>
```text
L1 name: Weekly Stock Report
L2
L3 on:
L4   push:
L5     branches: [ main ]
L6     paths-ignore:
L7       - 'CodeForChat/**'
L8   schedule:
L9     - cron: '0 0 * * 6'  # UTC 00:00 â†’ JST 09:00ï¼ˆåœŸï¼‰
L10   workflow_dispatch:
L11
L12 jobs:
L13   build-and-report:
L14     runs-on: ubuntu-latest
L15     permissions:
L16       contents: write
L17
L18     steps:
L19       - name: Debug start
L20         run: echo 'ğŸš€ DEBUGstarted'
L21               
L22       - name: Checkout repository
L23         uses: actions/checkout@v3
L24
L25       - name: Setup Python
L26         uses: actions/setup-python@v5
L27         with:
L28           python-version: '3.x'                # ï¼ˆå¿…è¦æœ€å°é™ã®ã¾ã¾ã€‚å›ºå®šã—ãŸã‘ã‚Œã° '3.13'ï¼‰
L29           cache: 'pip'                         # â˜… pipã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æœ‰åŠ¹åŒ–
L30           cache-dependency-path: requirements.txt  # â˜… ä¾å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã«
L31
L32       - name: Install dependencies
L33         run: pip install -r requirements.txt
L34
L35       - name: Prepare results directory
L36         run: mkdir -p results
L37
L38       - name: Run factor & scoring
L39         env:
L40           SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
L41           FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
L42           FIN_THREADS: "8"
L43         run: python factor.py
L44       - name: Persist breadth mode (if changed)
L45         run: |
L46           git config user.name "github-actions[bot]"
L47           git config user.email "github-actions[bot]@users.noreply.github.com"
L48           git add results/breadth_state.json || true
L49           if ! git diff --cached --quiet; then
L50             git commit -m "chore: update breadth_state.json [skip ci]" || true
L51             git push || true
L52           else
L53             echo "No breadth_state.json changes."
L54           fi
```

## <documents/README.md>
```text
L1 # é‹ç”¨ãƒ«ãƒ¼ãƒ«
L2
L3 ## åŸºæœ¬æ§‹æˆ
L4 - 25éŠ˜æŸ„ã‚’å‡ç­‰é…åˆ†ï¼ˆç¾é‡‘ã‚’é™¤ã1éŠ˜æŸ„ã‚ãŸã‚Š4%ï¼‰
L5 - moomooè¨¼åˆ¸ã§é‹ç”¨
L6
L7 ## Barbell Growth-Defenseæ–¹é‡
L8 - Growthæ 12éŠ˜æŸ„ï¼šé«˜æˆé•·ã§ä¹–é›¢æºã¨ãªã‚‹æ”»ã‚ã®éŠ˜æŸ„
L9 - Defenseæ 13éŠ˜æŸ„ï¼šä½ãƒœãƒ©ã§å®‰å®šæˆé•·ã—é…å½“ã‚’å¢—ã‚„ã™å®ˆã‚Šã®éŠ˜æŸ„
L10 - ã€ŒçŒ›çƒˆã«ä¼¸ã³ã‚‹æ”»ã‚ Ã— ç€å®Ÿã«ç¨¼ãç›¾ã€ã®çµ„åˆã›ã§ä¹–é›¢â†’åŠæˆ»ã—ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ã‚’ç‹™ã†
L11
L12 ## ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¤å®šï¼ˆtrend_template åˆæ ¼â€œæœ¬æ•°â€ã§åˆ¤å®šï¼‰
L13 - åˆæ ¼æœ¬æ•° = current+candidate å…¨ä½“ã®ã†ã¡ã€trend_template æ¡ä»¶ã‚’æº€ãŸã—ãŸéŠ˜æŸ„ã®**æœ¬æ•°(C)**
L14 - ã—ãã„å€¤ã¯éå»~600å–¶æ¥­æ—¥ã®åˆ†å¸ƒã‹ã‚‰**æ¯å›è‡ªå‹•æ¡ç”¨**ï¼ˆåˆ†ä½ç‚¹ã¨é‹ç”¨â€œåºŠâ€ã®maxï¼‰
L15   - ç·Šæ€¥å…¥ã‚Š: `max(q05, 12æœ¬)`ï¼ˆ= N_Gï¼‰
L16   - ç·Šæ€¥è§£é™¤: `max(q20, 18æœ¬)`ï¼ˆ= 1.5Ã—N_Gï¼‰
L17   - é€šå¸¸å¾©å¸°: `max(q60, 36æœ¬)`ï¼ˆ= 3Ã—N_Gï¼‰
L18 - ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹: å‰å›ãƒ¢ãƒ¼ãƒ‰ã«ä¾å­˜ï¼ˆEMERGâ†’è§£é™¤ã¯18æœ¬ä»¥ä¸Šã€CAUTIONâ†’é€šå¸¸ã¯36æœ¬ä»¥ä¸Šï¼‰
L19
L20 ## ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¥ã®ç¾é‡‘ãƒ»ãƒ‰ãƒªãƒ•ãƒˆ
L21 - **é€šå¸¸(NORMAL)** : ç¾é‡‘ **10%** / ãƒ‰ãƒªãƒ•ãƒˆé–¾å€¤ **10%**
L22 - **è­¦æˆ’(CAUTION)** : ç¾é‡‘ **12.5%** / ãƒ‰ãƒªãƒ•ãƒˆé–¾å€¤ **12%**
L23 - **ç·Šæ€¥(EMERG)** : ç¾é‡‘ **20%** / **ãƒ‰ãƒªãƒ•ãƒˆå£²è²·åœæ­¢**ï¼ˆ25Ã—4%ã«å…¨æˆ»ã—ã®ã¿ï¼‰
L24
L25 ## ãƒˆãƒ¬ãƒ¼ãƒªãƒ³ã‚°ã‚¹ãƒˆãƒƒãƒ—ï¼ˆçµ±ä¸€ï¼‰
L26 - G/D å…±é€šã® **åŸºæœ¬TS=15%**
L27 - å«ã¿ç›ŠãŒ **+20% / +40% / +60%** åˆ°é”ã§ TS ã‚’ **12% / 9% / 7%** ã«æ®µéšå¼•ãä¸Šã’
L28 - TSç™ºå‹•ã§æ¸›å°‘ã—ãŸéŠ˜æŸ„ã¯ç¿Œæ—¥ä»¥é™ã«è£œå……ï¼ˆâ€»ç·Šæ€¥ãƒ¢ãƒ¼ãƒ‰ä¸­ã¯è£œå……ã—ãªã„ï¼‰
L29
L30 ## å…¥æ›¿éŠ˜æŸ„é¸å®š
L31 - Oxfordã‚­ãƒ£ãƒ”ã‚¿ãƒ«ï¼ã‚¤ãƒ³ã‚«ãƒ ã€Alpha Investorã€Motley Fool Stock Advisorã€moomooã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ç­‰ã‚’å‚è€ƒã«chatGPTã§æ¤œè¨
L32 - å¹´é–“NISAæ ã¯Growthç¾¤ã®ä¸­ã‹ã‚‰ä½ãƒœãƒ©éŠ˜æŸ„ã‚’é¸å®šã—åˆ©ç”¨ã€‚é•·æœŸä¿æŒã«ã¯ã“ã ã‚ã‚‰ãªã„ã€‚
L33
L34 ## å†ã‚¨ãƒ³ãƒˆãƒªãƒ¼ï¼ˆã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ï¼‰
L35 - TSãƒ’ãƒƒãƒˆå¾Œã®åŒéŠ˜æŸ„å†INã¯ **8å–¶æ¥­æ—¥** ã®ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ã‚’è¨­ã‘ã‚‹ï¼ˆæœŸé–“ä¸­ã¯å†INç¦æ­¢ï¼‰
L36
L37 ## å®Ÿè¡Œã‚¿ã‚¤ãƒŸãƒ³ã‚°
L38 - åˆ¤å®šï¼šç±³å›½å¸‚å ´çµ‚å€¤ç›´å¾Œ
L39 - åŸ·è¡Œï¼šç¿Œå–¶æ¥­æ—¥ã®ç±³å›½å¯„ä»˜ãæˆè¡Œ
```

## <documents/factor_design.md>
```text
L1 # factor.py è©³ç´°è¨­è¨ˆæ›¸
L2
L3 ## æ¦‚è¦
L4 - æ—¢å­˜ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®éŠ˜æŸ„ã¨æ¤œè¨ä¸­ã®éŠ˜æŸ„ç¾¤ã‚’åŒæ™‚ã«æ‰±ã†éŠ˜æŸ„é¸å®šãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€‚
L5 - ä¾¡æ ¼ãƒ»è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šè¾¼ã¿ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã¨DRRSé¸å®šã‚’è¡Œã†ã“ã¨ã§ã€ä»¥ä¸‹ã®ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆã‚’å¾—ã‚‹ã€‚
L6   - æ¡ç”¨éŠ˜æŸ„ã¨æƒœã—ãã‚‚æ¼ã‚ŒãŸéŠ˜æŸ„ã®ã‚¹ã‚³ã‚¢ä¸€è¦§
L7   - IN/OUTã®ãƒ†ã‚£ãƒƒã‚«ãƒ¼ãƒªã‚¹ãƒˆã¨OUTå´ã®ä½ã‚¹ã‚³ã‚¢éŠ˜æŸ„
L8   - æ–°æ—§ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®æ¯”è¼ƒè¡¨
L9   - æ¤œè¨ä¸­éŠ˜æŸ„ã®ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆæ•´ç†ç”¨ï¼‰
L10
L11 ## å…¨ä½“ãƒ•ãƒ­ãƒ¼
L12 1. **Input** â€“ `current_tickers.csv`ã¨`candidate_tickers.csv`ã‚’èª­ã¿è¾¼ã¿ã€yfinanceã‚„Finnhubã®APIã‹ã‚‰ä¾¡æ ¼ãƒ»è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã¦`InputBundle`ã‚’æ•´å‚™ã€‚
L13 2. **Score Calculation** â€“ ScorerãŒç‰¹å¾´é‡ã‚’è¨ˆç®—ã—å› å­ã‚¹ã‚³ã‚¢ã‚’åˆæˆã—ã¦`FeatureBundle`ã‚’ç”Ÿæˆã€‚
L14 3. **Correlation Reduction & Selection** â€“ SelectorãŒDRRSãƒ­ã‚¸ãƒƒã‚¯ã§ç›¸é–¢ã‚’æŠ‘ãˆã¤ã¤G/DéŠ˜æŸ„ã‚’é¸å®šã—`SelectionBundle`ã‚’å¾—ã‚‹ã€‚
L15 4. **Output** â€“ æ¡ç”¨çµæœã¨å‘¨è¾ºæƒ…å ±ã‚’è¡¨ãƒ»Slacké€šçŸ¥ã¨ã—ã¦å‡ºåŠ›ã€‚
L16
L17 ```mermaid
L18 flowchart LR
L19   A[Input\nAPI & å‰å‡¦ç†] --> B[Score Calculation\nç‰¹å¾´é‡ãƒ»å› å­åˆæˆ]
L20   B --> C[Correlation Reduction\nDRRSé¸å®š]
L21   C --> D[Output\nSlacké€šçŸ¥]
L22 ```
L23
L24 ## å®šæ•°ãƒ»è¨­å®š
L25 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L26 | --- | --- | --- |
L27 | `exist` / `cand` | ç¾è¡Œãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã¨æ¤œè¨ä¸­éŠ˜æŸ„ã®ãƒ†ã‚£ãƒƒã‚«ãƒ¼ãƒªã‚¹ãƒˆ | ã‚¹ã‚³ã‚¢å¯¾è±¡ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã®æ§‹æˆã€å€™è£œæ•´ç† |
L28 | `bench` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼ | ç›¸å¯¾å¼·ã•ãƒ»Î²ç®—å‡ºã€ãƒãƒ¼ãƒˆæ¯”è¼ƒ |
L29 | `CAND_PRICE_MAX` | å€™è£œéŠ˜æŸ„ã®è¨±å®¹ä¾¡æ ¼ä¸Šé™ | é«˜é¡éŠ˜æŸ„ã®äº‹å‰é™¤å¤– |
L30 | `N_G` / `N_D` | G/Dæ¡ç”¨æ ã®ä»¶æ•° | æœ€çµ‚çš„ã«é¸ã¶éŠ˜æŸ„æ•°ã®åˆ¶ç´„ |
L31 | `g_weights` / `D_weights` | å„å› å­ã®é‡ã¿dict | G/Dã‚¹ã‚³ã‚¢åˆæˆ |
L32 | `D_BETA_MAX` | Dãƒã‚±ãƒƒãƒˆã®è¨±å®¹Î²ä¸Šé™ | é«˜Î²éŠ˜æŸ„ã®é™¤å¤–ãƒ•ã‚£ãƒ«ã‚¿ |
L33 | `FILTER_SPEC` | G/Dã”ã¨ã®å‰å‡¦ç†ãƒ•ã‚£ãƒ«ã‚¿ | ãƒˆãƒ¬ãƒ³ãƒ‰ãƒã‚¹ã‚¯ã‚„Î²ä¸Šé™è¨­å®š |
L34 | `corrM` | DRRSåˆæœŸãƒ—ãƒ¼ãƒ«ã®æœ€å¤§ä»¶æ•° | ç›¸é–¢è¡Œåˆ—ã‚µã‚¤ã‚ºåˆ¶å¾¡ |
L35 | `DRRS_G` / `DRRS_D` | DRRSãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿dict | ãƒã‚±ãƒƒãƒˆåˆ¥ã®ç›¸é–¢ä½æ¸›è¨­å®š |
L36 | `DRRS_SHRINK` | æ®‹å·®ç›¸é–¢ã®å¯¾è§’ã‚·ãƒ¥ãƒªãƒ³ã‚¯ç‡ | `residual_corr`ã®å®‰å®šåŒ– |
L37 | `CROSS_MU_GD` | G-Dé–“ã‚¯ãƒ­ã‚¹ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£Î¼ | 2ãƒã‚±ãƒƒãƒˆåŒæ™‚æœ€é©åŒ–ã§ç›¸é–¢æŠ‘åˆ¶ |
L38 | `RESULTS_DIR` | é¸å®šçµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª | `_save_sel`/`_load_prev`ã®å…¥å‡ºåŠ› |
L39
L40 é¸å®šçµæœã¯`results/`é…ä¸‹ã«JSONã¨ã—ã¦ä¿å­˜ã—ã€æ¬¡å›å®Ÿè¡Œæ™‚ã«`_load_prev`ã§èª­ã¿è¾¼ã‚“ã§é¸å®šæ¡ä»¶ã«åæ˜ ã€‚
L41
L42 ## DTO/Config
L43 å„ã‚¹ãƒ†ãƒƒãƒ—é–“ã§å—ã‘æ¸¡ã™ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¨è¨­å®šå€¤ã€‚å¤‰æ•°ã®æ„å‘³åˆã„ã¨åˆ©ç”¨ç®‡æ‰€ã‚’ä»¥ä¸‹ã«ç¤ºã™ã€‚
L44
L45 ### InputBundleï¼ˆInput â†’ Scorerï¼‰
L46 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L47 | --- | --- | --- |
L48 | `cand` | å€™è£œéŠ˜æŸ„ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã®ãƒªã‚¹ãƒˆ | OUTãƒ†ãƒ¼ãƒ–ãƒ«ã‚„ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°å¯¾è±¡ã®æ¯é›†å›£ |
L49 | `tickers` | ç¾è¡Œ+å€™è£œã‚’åˆã‚ã›ãŸãƒ†ã‚£ãƒƒã‚«ãƒ¼ä¸€è¦§ | ä¾¡æ ¼ãƒ»å‡ºæ¥é«˜ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€ãƒªã‚¿ãƒ¼ãƒ³è¨ˆç®— |
L50 | `bench` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼ | ç›¸å¯¾å¼·ã•ãƒ»Î²ç®—å‡ºã€ãƒãƒ¼ãƒˆæ¯”è¼ƒ |
L51 | `data` | yfinanceã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœï¼ˆéšå±¤åˆ—ï¼‰ | `px`/`spx`/ãƒªã‚¿ãƒ¼ãƒ³ç­‰ã®åŸºç¤ãƒ‡ãƒ¼ã‚¿ |
L52 | `px` | `data['Close']`ã ã‘ã‚’æŠœãå‡ºã—ãŸä¾¡æ ¼ç³»åˆ— | æŒ‡æ¨™è¨ˆç®—ãƒ»ãƒªã‚¿ãƒ¼ãƒ³ç”Ÿæˆ |
L53 | `spx` | `data['Close'][bench]` ã®Series | `rs`ã‚„`calc_beta`ã®åŸºæº–æŒ‡æ•° |
L54 | `tickers_bulk` | `yf.Tickers`ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ | `info`ç­‰ã®ä¸€æ‹¬å–å¾— |
L55 | `info` | ãƒ†ã‚£ãƒƒã‚«ãƒ¼åˆ¥ã®yfinanceæƒ…å ±dict | ã‚»ã‚¯ã‚¿ãƒ¼åˆ¤å®šã‚„EPSè£œå®Œ |
L56 | `eps_df` | EPS TTM/ç›´è¿‘EPSç­‰ã‚’ã¾ã¨ã‚ãŸè¡¨ | æˆé•·æŒ‡æ¨™ã®ç®—å‡º |
L57 | `fcf_df` | CFOãƒ»CapExãƒ»FCF TTMã¨æƒ…å ±æºãƒ•ãƒ©ã‚° | FCF/EVã‚„é…å½“ã‚«ãƒãƒ¬ãƒƒã‚¸ |
L58 | `returns` | `px.pct_change()`ã®ãƒªã‚¿ãƒ¼ãƒ³è¡¨ | ç›¸é–¢è¡Œåˆ—ãƒ»DRRSè¨ˆç®— |
L59
L60 ### FeatureBundleï¼ˆScorer â†’ Selectorï¼‰
L61 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L62 | --- | --- | --- |
L63 | `df` | è¨ˆç®—æ¸ˆã¿æŒ‡æ¨™ã®ç”Ÿå€¤ãƒ†ãƒ¼ãƒ–ãƒ« | ãƒ‡ãƒãƒƒã‚°ãƒ»å‡ºåŠ›è¡¨ç¤º |
L64 | `df_z` | ã‚¦ã‚£ãƒ³ã‚¶ãƒ¼å¾ŒZã‚¹ã‚³ã‚¢åŒ–ã—ãŸæŒ‡æ¨™è¡¨ | å› å­ã‚¹ã‚³ã‚¢åˆæˆã€é¸å®šåŸºæº– |
L65 | `g_score` | Gãƒã‚±ãƒƒãƒˆç·åˆã‚¹ã‚³ã‚¢ | Gé¸å®šã€IN/OUTæ¯”è¼ƒ |
L66 | `d_score_all` | Dãƒã‚±ãƒƒãƒˆç·åˆã‚¹ã‚³ã‚¢ï¼ˆå…¨éŠ˜æŸ„ï¼‰ | Dé¸å®šã€ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚° |
L67 | `missing_logs` | æ¬ ææŒ‡æ¨™ã¨è£œå®ŒçŠ¶æ³ã®ãƒ­ã‚° | ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ |
L68
L69 ### SelectionBundleï¼ˆSelector â†’ Outputï¼‰
L70 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L71 | --- | --- | --- |
L72 | `resG` | Gé¸å®šçµæœã®è©³ç´°dictï¼ˆ`tickers`ã€ç›®çš„å€¤ç­‰ï¼‰ | çµæœä¿å­˜ãƒ»å¹³å‡ç›¸é–¢ãªã©ã®æŒ‡æ¨™è¡¨ç¤º |
L73 | `resD` | Dé¸å®šçµæœã®è©³ç´°dict | åŒä¸Š |
L74 | `top_G` | æœ€çµ‚æ¡ç”¨Gãƒ†ã‚£ãƒƒã‚«ãƒ¼ | æ–°ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæ§‹ç¯‰ |
L75 | `top_D` | æœ€çµ‚æ¡ç”¨Dãƒ†ã‚£ãƒƒã‚«ãƒ¼ | åŒä¸Š |
L76 | `init_G` | DRRSå‰ã®GåˆæœŸå€™è£œ | æƒœã—ãã‚‚å¤–ã‚ŒãŸéŠ˜æŸ„è¡¨ç¤º |
L77 | `init_D` | DRRSå‰ã®DåˆæœŸå€™è£œ | åŒä¸Š |
L78
L79 ### WeightsConfig
L80 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L81 | --- | --- | --- |
L82 | `g` | Gå› å­ï¼ˆGRW/MOM/VOLï¼‰ã®é‡ã¿dict | `g_score`åˆæˆ |
L83 | `d` | Då› å­ï¼ˆD_QAL/D_YLD/D_VOL_RAW/D_TRDï¼‰ã®é‡ã¿dict | `d_score_all`åˆæˆ |
L84
L85 ### DRRSParams
L86 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L87 | --- | --- | --- |
L88 | `corrM` | DRRSåˆæœŸãƒ—ãƒ¼ãƒ«ã®æœ€å¤§ä»¶æ•° | ç›¸é–¢è¡Œåˆ—ã‚µã‚¤ã‚ºåˆ¶å¾¡ |
L89 | `shrink` | æ®‹å·®ç›¸é–¢ã®ã‚·ãƒ¥ãƒªãƒ³ã‚¯ç‡ | `residual_corr`ã®å¯¾è§’å¼·èª¿ |
L90 | `G` | Gãƒã‚±ãƒƒãƒˆç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿dictï¼ˆ`lookback`ç­‰ï¼‰ | `select_bucket_drrs`è¨­å®š |
L91 | `D` | Dãƒã‚±ãƒƒãƒˆç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿dict | åŒä¸Š |
L92 | `cross_mu_gd` | G-Dã‚¯ãƒ­ã‚¹ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£ä¿‚æ•°Î¼ | `select_buckets`ã®ç›®çš„é–¢æ•° |
L93
L94 ### PipelineConfig
L95 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L96 | --- | --- | --- |
L97 | `weights` | `WeightsConfig`ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ | ã‚¹ã‚³ã‚¢åˆæˆã®é‡ã¿å‚ç…§ |
L98 | `drrs` | `DRRSParams`ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ | é¸å®šã‚¹ãƒ†ãƒƒãƒ—ã®è¨­å®šå€¤ |
L99 | `price_max` | å€™è£œéŠ˜æŸ„ã®è¨±å®¹ä¾¡æ ¼ä¸Šé™ | Inputæ®µéšã§ã®ãƒ•ã‚£ãƒ«ã‚¿ |
L100
L101 ## å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
L102 - `winsorize_s` / `robust_z` : å¤–ã‚Œå€¤å‡¦ç†ã¨Zã‚¹ã‚³ã‚¢åŒ–ã€‚
L103 - `_safe_div` / `_safe_last` : ä¾‹å¤–ã‚’æ½°ã—ãŸåˆ†å‰²ãƒ»æœ«å°¾å–å¾—ã€‚
L104 - `_load_prev` / `_save_sel` : é¸å®šçµæœã®èª­ã¿æ›¸ãã€‚
L105
L106 ## ã‚¯ãƒ©ã‚¹è¨­è¨ˆ
L107 ### Step1: Input
L108 `current_tickers.csv`ã®ç¾è¡ŒéŠ˜æŸ„ã¨`candidate_tickers.csv`ã®æ¤œè¨ä¸­éŠ˜æŸ„ã‚’èµ·ç‚¹ã«ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„ã™ã‚‹ã€‚å¤–éƒ¨I/Oã¨å‰å‡¦ç†ã‚’æ‹…å½“ã—ã€`prepare_data`ã§`InputBundle`ã‚’ç”Ÿæˆã€‚ä¾¡æ ¼ãƒ»è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã¯**yfinanceã‚’å„ªå…ˆã—ã€æ¬ æãŒã‚ã‚‹æŒ‡æ¨™ã®ã¿Finnhub APIã§è£œå®Œ**ã™ã‚‹ã€‚
L109 ä¸»ãªãƒ¡ã‚½ãƒƒãƒ‰:
L110 - `impute_eps_ttm` : å››åŠæœŸEPSÃ—4ã§TTMã‚’æ¨å®šã—æ¬ ææ™‚ã®ã¿å·®ã—æ›¿ãˆã€‚
L111 - `fetch_cfo_capex_ttm_yf` : yfinanceã®å››åŠæœŸ/å¹´æ¬¡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼ã‹ã‚‰CFOãƒ»CapExãƒ»FCF TTMã‚’ç®—å‡ºã€‚
L112 - `fetch_cfo_capex_ttm_finnhub` : yfinanceã§æ¬ ã‘ãŸéŠ˜æŸ„ã®ã¿Finnhub APIã§è£œå®Œã€‚
L113 - `compute_fcf_with_fallback` : yfinanceå€¤ã‚’åŸºæº–ã«Finnhubå€¤ã§ç©´åŸ‹ã‚ã—ã€CFO/CapEx/FCFã¨æƒ…å ±æºãƒ•ãƒ©ã‚°ã‚’è¿”ã™ã€‚
L114 - `_build_eps_df` : `info`ã‚„`quarterly_earnings`ã‹ã‚‰EPS TTMã¨ç›´è¿‘EPSã‚’è¨ˆç®—ã—ã€`impute_eps_ttm`ã§è£œå®Œã€‚
L115 - `prepare_data` :
L116     0. CSVã‹ã‚‰ç¾è¡ŒéŠ˜æŸ„ã¨å€™è£œéŠ˜æŸ„ã®ãƒ†ã‚£ãƒƒã‚«ãƒ¼ä¸€è¦§ã‚’èª­ã¿è¾¼ã‚€ã€‚
L117     1. å€™è£œéŠ˜æŸ„ã®ç¾åœ¨å€¤ã‚’å–å¾—ã—ä¾¡æ ¼ä¸Šé™ã§ãƒ•ã‚£ãƒ«ã‚¿ã€‚
L118     2. æ—¢å­˜+å€™è£œã‹ã‚‰å¯¾è±¡ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚’æ±ºå®šã—ã€ä¾¡æ ¼ãƒ»å‡ºæ¥é«˜ã‚’ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆyfinanceï¼‰ã€‚
L119     3. yfinanceå€¤ã‚’åŸºã«EPS/FCFãƒ†ãƒ¼ãƒ–ãƒ«ã‚„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç³»åˆ—ã€ãƒªã‚¿ãƒ¼ãƒ³ã‚’æ§‹ç¯‰ã—ã€æ¬ æã‚»ãƒ«ã¯Finnhubå‘¼ã³å‡ºã—ã§ç©´åŸ‹ã‚ã€‚
L120     4. ä¸Šè¨˜ã‚’`InputBundle`ã«æ ¼ç´ã—ã¦è¿”ã™ã€‚
L121
L122 ### Step2: Score Calculation (Scorer)
L123 ç‰¹å¾´é‡è¨ˆç®—ã¨ã‚¹ã‚³ã‚¢åˆæˆã‚’æ‹…å½“ã—ã€`FeatureBundle`ã‚’è¿”ã™ã€‚
L124
L125 #### è£œåŠ©é–¢æ•°
L126 - `trend(s)` : 50/150/200æ—¥ç§»å‹•å¹³å‡ã‚„52é€±ãƒ¬ãƒ³ã‚¸ã‹ã‚‰-0.5ã€œ0.5ã§æ§‹æˆã•ã‚ŒãŸãƒˆãƒ¬ãƒ³ãƒ‰æŒ‡æ¨™ã€‚
L127 - `rs(s,b)` / `tr_str(s)` / `rs_line_slope(s,b,win)` : ç›¸å¯¾å¼·ã•ã‚„çŸ­æœŸãƒˆãƒ¬ãƒ³ãƒ‰ã€RSå›å¸°å‚¾ãã‚’ç®—å‡ºã€‚
L128 - `ev_fallback` : `enterpriseValue`æ¬ ææ™‚ã«è² å‚µãƒ»ç¾é‡‘ã‹ã‚‰EVã‚’æ¨å®šã€‚
L129 - `dividend_status` / `div_streak` : é…å½“æœªè¨­å®šçŠ¶æ³ã®åˆ¤å®šã¨å¢—é…å¹´æ•°ã‚«ã‚¦ãƒ³ãƒˆã€‚
L130 - `fetch_finnhub_metrics` : Finnhub APIã‹ã‚‰EPSæˆé•·ãƒ»ROEãƒ»Î²ãªã©ä¸è¶³æŒ‡æ¨™ã‚’å–å¾—ã€‚
L131 - `calc_beta` : ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ã®å…±åˆ†æ•£ã‹ã‚‰Î²ã‚’ç®—å‡ºã€‚
L132 - `spx_to_alpha` : S&P500ã®ä½ç½®æƒ…å ±ã‹ã‚‰DRRSã§ç”¨ã„ã‚‹Î±ã‚’æ¨å®šã€‚
L133 - `soft_cap_effective_scores` / `pick_top_softcap` : ã‚»ã‚¯ã‚¿ãƒ¼ã‚½ãƒ•ãƒˆã‚­ãƒ£ãƒƒãƒ—ä»˜ãã‚¹ã‚³ã‚¢èª¿æ•´ã¨ä¸Šä½æŠ½å‡ºã€‚
L134
L135 **è£œåŠ©é–¢æ•°ã¨ç”ŸæˆæŒ‡æ¨™**
L136
L137 | è£œåŠ©é–¢æ•° | ç”ŸæˆæŒ‡æ¨™ | ç•¥ç§° |
L138 | --- | --- | --- |
L139 | `trend` | ãƒˆãƒ¬ãƒ³ãƒ‰ç·åˆå€¤ | `TR` |
L140 | `rs` | ç›¸å¯¾å¼·ã• | `RS` |
L141 | `tr_str` | ä¾¡æ ¼ã¨50æ—¥ç·šã®ä¹–é›¢ | `TR_str` |
L142 | `rs_line_slope` | RSç·šã®å›å¸°å‚¾ã | `RS_SLOPE_*` |
L143 | `calc_beta` | Î² | `BETA` |
L144 | `div_streak` | é€£ç¶šå¢—é…å¹´æ•° | `DIV_STREAK` |
L145
L146 #### `aggregate_scores` è©³ç´°
L147 1. å„éŠ˜æŸ„ã®ä¾¡æ ¼ç³»åˆ—ã‚„`info`ã‚’åŸºã«ä»¥ä¸‹ã‚’ç®—å‡ºã€‚
L148    - **ãƒˆãƒ¬ãƒ³ãƒ‰/ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ **: `TR`ã€`RS`ã€`TR_str`ã€å¤šæ§˜ãªç§»å‹•å¹³å‡æ¯”ã€`RS_SLOPE_*`ãªã©ã€‚
L149    - **ãƒªã‚¹ã‚¯**: `BETA`ã€`DOWNSIDE_DEV`ã€`MDD_1Y`ã€`RESID_VOL`ã€`DOWN_OUTPERF`ã€`EXT_200`ç­‰ã€‚
L150    - **é…å½“**: `DIV`ã€`DIV_TTM_PS`ã€`DIV_VAR5`ã€`DIV_YOY`ã€`DIV_FCF_COVER`ã€`DIV_STREAK`ã€‚
L151    - **è²¡å‹™ãƒ»æˆé•·**: `EPS`ã€`REV`ã€`ROE`ã€`FCF/EV`ã€`REV_Q_YOY`ã€`EPS_Q_YOY`ã€`REV_YOY_ACC`ã€`REV_YOY_VAR`ã€`REV_ANN_STREAK`ã€`RULE40`ã€`FCF_MGN` ç­‰ã€‚
L152    - **å®‰å®šæ€§/ã‚µã‚¤ã‚º**: `DEBT2EQ`ã€`CURR_RATIO`ã€`MARKET_CAP`ã€`ADV60_USD`ã€`EPS_VAR_8Q`ãªã©ã€‚
L153 2. æŒ‡æ¨™æ¬ æã¯Finnhub APIç­‰ã§è£œå®Œã—ã€æœªå–å¾—é …ç›®ã‚’`missing_logs`ã«è¨˜éŒ²ã€‚
L154 3. `winsorize_s`â†’`robust_z`ã§æ¨™æº–åŒ–ã—`df_z`ã¸ä¿å­˜ã€‚ã‚µã‚¤ã‚ºãƒ»æµå‹•æ€§ã¯å¯¾æ•°å¤‰æ›ã€‚
L155 4. æ­£è¦åŒ–æ¸ˆæŒ‡æ¨™ã‹ã‚‰å› å­ã‚¹ã‚³ã‚¢ã‚’åˆæˆã€‚
L156    - å„å› å­ã®æ§‹æˆã¨é‡ã¿ã¯ä»¥ä¸‹ã®é€šã‚Šã€‚
L157      - **GRW**: 0.30Ã—`REV` + 0.20Ã—`EPS_Q_YOY` + 0.15Ã—`REV_Q_YOY` + 0.15Ã—`REV_YOY_ACC` + 0.10Ã—`RULE40` + 0.10Ã—`FCF_MGN` + 0.10Ã—`REV_ANN_STREAK` âˆ’ 0.05Ã—`REV_YOY_VAR`ã€‚
L158      - **MOM**: 0.40Ã—`RS` + 0.15Ã—`TR_str` + 0.15Ã—`RS_SLOPE_6W` + 0.15Ã—`RS_SLOPE_13W` + 0.10Ã—`MA200_SLOPE_5M` + 0.10Ã—`MA200_UP_STREAK_D`ã€‚
L159      - **VOL**: `BETA`å˜ä½“ã‚’ä½¿ç”¨ã€‚
L160      - **QAL**: 0.60Ã—`FCF_W` + 0.40Ã—`ROE_W`ã§ä½œæˆã€‚
L161      - **YLD**: 0.30Ã—`DIV` + 0.70Ã—`DIV_STREAK`ã€‚
L162      - **D_QAL**: 0.35Ã—`QAL` + 0.20Ã—`FCF` + 0.15Ã—`CURR_RATIO` âˆ’ 0.15Ã—`DEBT2EQ` âˆ’ 0.15Ã—`EPS_VAR_8Q`ã€‚
L163      - **D_YLD**: 0.45Ã—`DIV` + 0.25Ã—`DIV_STREAK` + 0.20Ã—`DIV_FCF_COVER` âˆ’ 0.10Ã—`DIV_VAR5`ã€‚
L164      - **D_VOL_RAW**: 0.40Ã—`DOWNSIDE_DEV` + 0.22Ã—`RESID_VOL` + 0.18Ã—`MDD_1Y` âˆ’ 0.10Ã—`DOWN_OUTPERF` âˆ’ 0.05Ã—`EXT_200` âˆ’ 0.08Ã—`SIZE` âˆ’ 0.10Ã—`LIQ` + 0.10Ã—`BETA`ã€‚
L165      - **D_TRD**: 0.40Ã—`MA200_SLOPE_5M` âˆ’ 0.30Ã—`EXT_200` + 0.15Ã—`NEAR_52W_HIGH` + 0.15Ã—`TR`ã€‚
L166     - ä¸»ãªæŒ‡æ¨™ã®ç•¥ç§°ã¨æ„å‘³:
L167
L168       | ç•¥ç§° | è£œåŠ©é–¢æ•° | æ¦‚è¦ |
L169       | --- | --- | --- |
L170       | TR | `trend` | 50/150/200æ—¥ç§»å‹•å¹³å‡ã¨52é€±ãƒ¬ãƒ³ã‚¸ã‚’çµ„ã¿åˆã‚ã›ãŸãƒˆãƒ¬ãƒ³ãƒ‰ç·åˆå€¤ |
L171       | RS | `rs` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«å¯¾ã™ã‚‹ç›¸å¯¾å¼·ã•ï¼ˆ12M/1Mãƒªã‚¿ãƒ¼ãƒ³å·®ï¼‰ |
L172       | TR_str | `tr_str` | ä¾¡æ ¼ã¨50æ—¥ç§»å‹•å¹³å‡ã®ä¹–é›¢ |
L173       | RS_SLOPE_6W | `rs_line_slope` | ç›¸å¯¾å¼·ã•ç·šã®6é€±å›å¸°å‚¾ã |
L174       | RS_SLOPE_13W | `rs_line_slope` | ç›¸å¯¾å¼·ã•ç·šã®13é€±å›å¸°å‚¾ã |
L175       | MA200_SLOPE_5M | - | 200æ—¥ç§»å‹•å¹³å‡ã®5ã‹æœˆé¨°è½ç‡ |
L176       | MA200_UP_STREAK_D | - | 200æ—¥ç·šãŒé€£ç¶šã§ä¸Šå‘ã„ãŸæ—¥æ•° |
L177       | BETA | `calc_beta` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«å¯¾ã™ã‚‹Î² |
L178       | DOWNSIDE_DEV | - | ä¸‹æ–¹ãƒªã‚¿ãƒ¼ãƒ³ã®ã¿ã®å¹´ç‡åŒ–æ¨™æº–åå·® |
L179       | RESID_VOL | - | Î²ã§èª¿æ•´ã—ãŸæ®‹å·®ãƒªã‚¿ãƒ¼ãƒ³ã®å¹´ç‡åŒ–æ¨™æº–åå·® |
L180       | MDD_1Y | - | éå»1å¹´ã®æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³ |
L181       | DOWN_OUTPERF | - | å¸‚å ´ä¸‹è½æ—¥ã«å¯¾ã™ã‚‹å¹³å‡è¶…éãƒªã‚¿ãƒ¼ãƒ³ |
L182       | EXT_200 | - | 200æ—¥ç§»å‹•å¹³å‡ã‹ã‚‰ã®çµ¶å¯¾ä¹–é›¢ç‡ |
L183       | NEAR_52W_HIGH | - | 52é€±é«˜å€¤ã¾ã§ã®ä¸‹æ–¹è·é›¢ï¼ˆ0=é«˜å€¤ï¼‰ |
L184       | FCF_W | - | ã‚¦ã‚£ãƒ³ã‚¶ãƒ¼å‡¦ç†å¾Œã®FCF/EV |
L185       | ROE_W | - | ã‚¦ã‚£ãƒ³ã‚¶ãƒ¼å‡¦ç†å¾Œã®ROE |
L186       | FCF | - | FCF/EV |
L187       | QAL | - | FCF_Wã¨ROE_Wã‚’çµ„ã¿åˆã‚ã›ãŸå“è³ªã‚¹ã‚³ã‚¢ |
L188       | CURR_RATIO | - | æµå‹•æ¯”ç‡ |
L189       | DEBT2EQ | - | è² å‚µè³‡æœ¬å€ç‡ |
L190       | EPS_VAR_8Q | - | EPSã®8å››åŠæœŸæ¨™æº–åå·® |
L191       | DIV | - | å¹´ç‡æ›ç®—é…å½“åˆ©å›ã‚Š |
L192       | DIV_STREAK | `div_streak` | é€£ç¶šå¢—é…å¹´æ•° |
L193       | DIV_FCF_COVER | - | é…å½“ã®FCFã‚«ãƒãƒ¬ãƒƒã‚¸ |
L194       | DIV_VAR5 | - | 5å¹´é…å½“å¤‰å‹•ç‡ |
L195       | DIV_TTM_PS | - | 1æ ªå½“ãŸã‚ŠTTMé…å½“ |
L196       | DIV_YOY | - | å‰å¹´æ¯”é…å½“æˆé•·ç‡ |
L197       | REV | - | å£²ä¸Šæˆé•·ç‡TTM |
L198       | EPS_Q_YOY | - | å››åŠæœŸEPSã®å‰å¹´åŒæœŸæ¯” |
L199       | REV_Q_YOY | - | å››åŠæœŸå£²ä¸Šã®å‰å¹´åŒæœŸæ¯” |
L200       | REV_YOY_ACC | - | å£²ä¸Šæˆé•·ç‡ã®åŠ é€Ÿåˆ† |
L201       | RULE40 | - | å£²ä¸Šæˆé•·ç‡ã¨FCFãƒãƒ¼ã‚¸ãƒ³ã®åˆè¨ˆ |
L202       | FCF_MGN | - | FCFãƒãƒ¼ã‚¸ãƒ³ |
L203       | REV_ANN_STREAK | - | å¹´æ¬¡å£²ä¸Šæˆé•·ã®é€£ç¶šå¹´æ•° |
L204       | REV_YOY_VAR | - | å¹´æ¬¡å£²ä¸Šæˆé•·ç‡ã®å¤‰å‹•æ€§ |
L205       | SIZE | - | æ™‚ä¾¡ç·é¡ã®å¯¾æ•°å€¤ |
L206       | LIQ | - | 60æ—¥å¹³å‡å‡ºæ¥é«˜ãƒ‰ãƒ«ã®å¯¾æ•°å€¤ |
L207    - Gãƒã‚±ãƒƒãƒˆ: `GRW`ã€`MOM`ã€`VOL`ã‚’`cfg.weights.g`ï¼ˆ0.40/0.45/-0.15ï¼‰ã§åŠ é‡ã—`g_score`ã‚’å¾—ã‚‹ã€‚
L208    - Dãƒã‚±ãƒƒãƒˆ: `D_QAL`ã€`D_YLD`ã€`D_VOL_RAW`ã€`D_TRD`ã‚’`cfg.weights.d`ï¼ˆ0.15/0.15/-0.45/0.25ï¼‰ã§åŠ é‡ã—`d_score_all`ã‚’ç®—å‡ºã€‚
L209    - ã‚»ã‚¯ã‚¿ãƒ¼capã«ã‚ˆã‚‹`soft_cap_effective_scores`ã‚’é©ç”¨ã—ã€Gæ¡ç”¨éŠ˜æŸ„ã«ã¯ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨ã€‚
L210 5. `_apply_growth_entry_flags`ã§ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆ/æŠ¼ã—ç›®ç™ºç«çŠ¶æ³ã‚’ä»˜åŠ ã—ã€`FeatureBundle`ã‚’è¿”ã™ã€‚
L211
L212 ### Step3: Correlation Reduction & Selection (Selector)
L213 DRRSã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ç›¸é–¢ã‚’æŠ‘ãˆãŸéŠ˜æŸ„é¸å®šã‚’è¡Œã„ã€`SelectionBundle`ã‚’è¿”ã™ã€‚`results/`ã«ä¿å­˜ã•ã‚ŒãŸå‰å›é¸å®šï¼ˆ`G_selection.json` / `D_selection.json`ï¼‰ã‚’`_load_prev`ã§èª­ã¿è¾¼ã¿ã€ç›®çš„å€¤ãŒå¤§ããæ‚ªåŒ–ã—ãªã„é™ã‚Šç¶­æŒã™ã‚‹ã€‚æ–°ã—ã„æ¡ç”¨é›†åˆã¯`_save_sel`ã§JSONã«æ›¸ãå‡ºã—æ¬¡å›ä»¥é™ã®å…¥åŠ›ã«å‚™ãˆã‚‹ã€‚
L214 ä¸»ãªãƒ¡ã‚½ãƒƒãƒ‰:
L215 - `residual_corr` : åç›Šç‡è¡Œåˆ—ã‚’Zã‚¹ã‚³ã‚¢åŒ–ã—ã€ä¸Šä½ä¸»æˆåˆ†ã‚’é™¤å»ã—ãŸæ®‹å·®ã‹ã‚‰ç›¸é–¢è¡Œåˆ—ã‚’æ±‚ã‚ã€å¹³å‡ç›¸é–¢ã«å¿œã˜ã¦ã‚·ãƒ¥ãƒªãƒ³ã‚¯ã€‚
L216 - `rrqr_like_det` : ã‚¹ã‚³ã‚¢ã‚’é‡ã¿ä»˜ã‘ã—ãŸQRåˆ†è§£é¢¨ã®æ‰‹é †ã§åˆæœŸå€™è£œã‚’kä»¶æŠ½å‡ºã—ã€ã‚¹ã‚³ã‚¢ã®é«˜ã„éç›¸é–¢ãªé›†åˆã‚’å¾—ã‚‹ã€‚
L217 - `swap_local_det` / `swap_local_det_cross` : `sum(score) - Î»*within_corr - Î¼*cross_corr`ã‚’ç›®çš„é–¢æ•°ã¨ã—ã¦ã€å…¥ã‚Œæ›¿ãˆæ¢ç´¢ã§å±€æ‰€çš„ã«æœ€é©åŒ–ã€‚
L218 - `select_bucket_drrs` : ãƒ—ãƒ¼ãƒ«éŠ˜æŸ„ã¨ã‚¹ã‚³ã‚¢ã‹ã‚‰æ®‹å·®ç›¸é–¢ã‚’è¨ˆç®—ã—ã€ä¸Šè¨˜2æ®µéš(åˆæœŸé¸æŠâ†’å…¥ã‚Œæ›¿ãˆ)ã§kéŠ˜æŸ„ã‚’æ±ºå®šã€‚éå»æ¡ç”¨éŠ˜æŸ„ã¨ã®æ¯”è¼ƒã§ç›®çš„å€¤ãŒåŠ£åŒ–ã—ãªã‘ã‚Œã°ç¶­æŒã™ã‚‹ã€‚
L219 - `select_buckets` : Gãƒã‚±ãƒƒãƒˆã‚’é¸å®šå¾Œã€ãã®çµæœã‚’é™¤ã„ãŸå€™è£œã‹ã‚‰Dãƒã‚±ãƒƒãƒˆã‚’é¸ã¶ã€‚Dé¸å®šæ™‚ã¯Gã¨ã®ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£Î¼ã‚’ä»˜ä¸ã—ã€ä¸¡ãƒã‚±ãƒƒãƒˆã®åˆ†æ•£ã‚’åˆ¶å¾¡ã™ã‚‹ã€‚
L220
L221 #### ç›¸é–¢ä½æ¸›ãƒ­ã‚¸ãƒƒã‚¯è©³ç´°
L222 1. **æ®‹å·®ç›¸é–¢è¡Œåˆ—ã®æ§‹ç¯‰ (`residual_corr`)**
L223    - ãƒªã‚¿ãƒ¼ãƒ³è¡Œåˆ—`R`ã‚’Zã‚¹ã‚³ã‚¢åŒ–ã€‚
L224    - SVDã§ä¸Šä½`n_pc`ä¸»æˆåˆ†`F`ã‚’æ±‚ã‚ã€æœ€å°äºŒä¹—ã§ä¿‚æ•°`B`ã‚’ç®—å‡ºã—æ®‹å·®`E = Z - F@B`ã‚’å¾—ã‚‹ã€‚
L225    - `E`ã®ç›¸é–¢è¡Œåˆ—`C`ã‚’è¨ˆç®—ã—ã€å¹³å‡çµ¶å¯¾ç›¸é–¢ã«å¿œã˜ã¦ã‚·ãƒ¥ãƒªãƒ³ã‚¯é‡`shrink_eff`ã‚’è£œæ­£ã—ã¦å¯¾è§’ã‚’å¼·èª¿ã€‚
L226 2. **åˆæœŸå€™è£œã®æŠ½å‡º (`rrqr_like_det`)**
L227    - ã‚¹ã‚³ã‚¢ã‚’0-1æ­£è¦åŒ–ã—ãŸé‡ã¿`w`ã¨ã—ã€`Z*(1+Î³w)`ã§åˆ—ãƒãƒ«ãƒ ã‚’å¼·èª¿ã€‚
L228    - æ®‹å·®ãƒãƒ«ãƒ æœ€å¤§ã®åˆ—ã‚’é€æ¬¡é¸ã³ã€QRãƒ©ã‚¤ã‚¯ãªãƒ‡ãƒ•ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã£ã¦éç›¸é–¢ã‹ã¤é«˜ã‚¹ã‚³ã‚¢ãª`k`éŠ˜æŸ„é›†åˆ`S0`ã‚’å¾—ã‚‹ã€‚
L229 3. **å±€æ‰€æ¢ç´¢ (`swap_local_det` / `swap_local_det_cross`)**
L230    - ç›®çš„é–¢æ•°`Î£z_score âˆ’ Î»Â·within_corr âˆ’ Î¼Â·cross_corr`ã‚’æœ€å¤§åŒ–ã€‚
L231    - é¸æŠé›†åˆã®å„éŠ˜æŸ„ã‚’ä»–å€™è£œã¨å…¥ã‚Œæ›¿ãˆã€æ”¹å–„ãŒãªããªã‚‹ã¾ã§ã¾ãŸã¯`max_pass`å›ã¾ã§æ¢ç´¢ã€‚
L232    - `swap_local_det_cross`ã¯Gãƒã‚±ãƒƒãƒˆã¨ã®ã‚¯ãƒ­ã‚¹ç›¸é–¢è¡Œåˆ—`C_cross`ã‚’ä½¿ç”¨ã—ã€ãƒšãƒŠãƒ«ãƒ†ã‚£`Î¼`ã‚’ä»˜ä¸ã€‚
L233 4. **éå»æ¡ç”¨ã®ç¶­æŒã¨ã‚¯ãƒ­ã‚¹ãƒšãƒŠãƒ«ãƒ†ã‚£ (`select_bucket_drrs` / `select_buckets`)**
L234    - å±€æ‰€æ¢ç´¢çµæœ`S`ã¨éå»é›†åˆ`P`ã®ç›®çš„å€¤ã‚’æ¯”è¼ƒã—ã€`S`ãŒ`P`ã‚ˆã‚Š`Î·`æœªæº€ã®æ”¹å–„ãªã‚‰`P`ã‚’ç¶­æŒã€‚
L235    - `select_buckets`ã§ã¯Gã‚’å…ˆã«æ±ºå®šã—ã€Dé¸å®šæ™‚ã«Gã¨ã®ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£`Î¼`ã‚’åŠ ãˆã¦ã‚¯ãƒ­ã‚¹åˆ†æ•£ã‚’æŠ‘åˆ¶ã€‚
L236
L237 ### Step4: Output
L238 é¸å®šçµæœã‚’å¯è¦–åŒ–ã—å…±æœ‰ã™ã‚‹å·¥ç¨‹ã€‚ä»¥ä¸‹ã®å†…å®¹ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«åŒ–ã—ã¦æ¨™æº–å‡ºåŠ›ã¨Slackã¸é€ã‚‹ã€‚
L239 - æ¡ç”¨éŠ˜æŸ„ã¨æƒœã—ãã‚‚é¸å¤–ã¨ãªã£ãŸéŠ˜æŸ„ã®ã‚¹ã‚³ã‚¢ä¸€è¦§
L240 - IN/OUTãƒªã‚¹ãƒˆã¨OUTéŠ˜æŸ„ã®ã‚¹ã‚³ã‚¢ï¼ˆä½å¾—ç‚¹éŠ˜æŸ„ã‚’ç¢ºèªã—ã‚„ã™ãï¼‰
L241 - æ–°æ—§ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®æ¯”è¼ƒè¡¨ï¼ˆçµ„å…¥ã‚Œãƒ»é™¤å¤–ã€ã‚¹ã‚³ã‚¢å¤‰åŒ–ï¼‰
L242 - æ¤œè¨ä¸­éŠ˜æŸ„ã®ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°
L243
L244 ä¸»ãªãƒ¡ã‚½ãƒƒãƒ‰:
L245 - `display_results` : ä¸Šè¨˜ãƒ†ãƒ¼ãƒ–ãƒ«ã«åŠ ãˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã‚„åˆ†æ•£åŒ–æŒ‡æ¨™ã‚’è¡¨ç¤ºã€‚
L246 - `notify_slack` : Slack Webhookã¸åŒå†…å®¹ã‚’é€ä¿¡ã€‚
L247 - è£œåŠ©:`_avg_offdiag`ã€`_resid_avg_rho`ã€`_raw_avg_rho`ã€`_cross_block_raw_rho`ã€‚
L248
L249 ## ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
L250 1. `PipelineConfig`ã‚’æ§‹ç¯‰ã€‚
L251 2. **Step1** `Input.prepare_data`ã§`InputBundle`ã‚’ç”Ÿæˆã€‚
L252 3. **Step2** `Scorer.aggregate_scores`ã§`FeatureBundle`ã‚’å–å¾—ã€‚
L253 4. **Step3** `Selector.select_buckets`ã§`SelectionBundle`ã‚’ç®—å‡ºã€‚
L254 5. **Step4** `Output.display_results`ã¨`notify_slack`ã§çµæœã‚’å‡ºåŠ›ã€‚
```

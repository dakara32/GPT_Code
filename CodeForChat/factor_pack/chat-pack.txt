# === Chat Paste Pack ===
# Repo: dakara32/GPT_Code @ main
# Files: factor.py, scorer.py, .github/workflows/weekly-report.yml, documents/README.md, documents/factor_design.md
# ä½¿ã„æ–¹: ä¸‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’é †ã«è²¼ã‚Œã°ã“ã®ãƒãƒ£ãƒƒãƒˆã§å…¨ä½“æŠŠæ¡ã§ãã¾ã™ã€‚
# æ³¨è¨˜: å„ãƒ•ã‚¡ã‚¤ãƒ«ã¯å€‹åˆ¥ã« L1.. ã§è¡Œç•ªå·ä»˜ä¸ã€‚
---

## <factor.py>
```text
L1 '''ROLE: Orchestration ONLYï¼ˆå¤–éƒ¨I/Oãƒ»SSOTãƒ»Slackå‡ºåŠ›ï¼‰, è¨ˆç®—ã¯ scorer.py'''
L2 # === NOTE: æ©Ÿèƒ½ãƒ»å…¥å‡ºåŠ›ãƒ»ãƒ­ã‚°æ–‡è¨€ãƒ»ä¾‹å¤–æŒ™å‹•ã¯ä¸å¤‰ã€‚å®‰å…¨ãªçŸ­ç¸®ï¼ˆimportçµ±åˆ/è¤‡æ•°ä»£å…¥/å†…åŒ…è¡¨è¨˜/ãƒ¡ã‚½ãƒƒãƒ‰ãƒã‚§ãƒ¼ãƒ³/ä¸€è¡ŒåŒ–/ç©ºè¡Œåœ§ç¸®ãªã©ï¼‰ã®ã¿é©ç”¨ ===
L3 BONUS_COEFF = 0.4   # æ”»ã‚=0.3 / ä¸­åº¸=0.4 / å®ˆã‚Š=0.5
L4 import os, json, time, requests
L5 from time import perf_counter
L6 from dataclasses import dataclass
L7 from typing import Dict, List
L8 from concurrent.futures import ThreadPoolExecutor
L9 import numpy as np
L10 import pandas as pd
L11 import yfinance as yf
L12 from scipy.stats import zscore  # used via scorer
L13 from scorer import Scorer, ttm_div_yield_portfolio
L14
L15
L16 class T:
L17     t = perf_counter()
L18     log = staticmethod(lambda tag: (lambda now=perf_counter(): (print(f"[T] {tag}: {now - T.t:.2f}s"), setattr(T, "t", now))[-1])())
L19
L20
L21 T.log("start")
L22
L23 # === ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã¨å®šæ•°ï¼ˆå†’é ­ã«å›ºå®šï¼‰ ===
L24 exist, cand = [pd.read_csv(f, header=None)[0].tolist() for f in ("current_tickers.csv","candidate_tickers.csv")]
L25 T.log(f"csv loaded: exist={len(exist)} cand={len(cand)}")
L26 CAND_PRICE_MAX, bench = 450, '^GSPC'  # ä¾¡æ ¼ä¸Šé™ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
L27 N_G, N_D = 12, 13  # G/Dæ ã‚µã‚¤ã‚º
L28 g_weights = {'GRW':0.40,'MOM':0.45,'VOL':-0.15}
L29 D_BETA_MAX = float(os.environ.get("D_BETA_MAX", "0.8"))
L30 FILTER_SPEC = {"G":{"pre_mask":["trend_template"]},"D":{"pre_filter":{"beta_max":D_BETA_MAX}}}
L31 D_weights = {'QAL':0.15,'YLD':0.15,'VOL':-0.45,'TRD':0.25}
L32 _fmt_w = lambda w: " ".join(f"{k}{int(v*100)}" for k, v in w.items())
L33
L34 # DRRS åˆæœŸãƒ—ãƒ¼ãƒ«ãƒ»å„ç¨®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
L35 corrM = 45
L36 DRRS_G, DRRS_D = dict(lookback=252,n_pc=3,gamma=1.2,lam=0.68,eta=0.8), dict(lookback=504,n_pc=4,gamma=0.8,lam=0.85,eta=0.5)
L37 DRRS_SHRINK = 0.10  # æ®‹å·®ç›¸é–¢ã®å¯¾è§’ã‚·ãƒ¥ãƒªãƒ³ã‚¯ï¼ˆåŸºç¤ï¼‰
L38
L39 # ã‚¯ãƒ­ã‚¹ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆæœªå®šç¾©ãªã‚‰è¨­å®šï¼‰
L40 try: CROSS_MU_GD
L41 except NameError: CROSS_MU_GD = 0.40  # æ¨å¥¨ 0.35â€“0.45ï¼ˆlam=0.85æƒ³å®šï¼‰
L42
L43 # å‡ºåŠ›é–¢é€£
L44 RESULTS_DIR = "results"
L45 os.makedirs(RESULTS_DIR, exist_ok=True)
L46
L47 # ãã®ä»–
L48 debug_mode, FINNHUB_API_KEY = False, os.environ.get("FINNHUB_API_KEY")
L49
L50
L51 # === å…±æœ‰DTOï¼ˆã‚¯ãƒ©ã‚¹é–“I/Oå¥‘ç´„ï¼‰ï¼‹ Config ===
L52 @dataclass(frozen=True)
L53 class InputBundle:
L54     # Input â†’ Scorer ã§å—ã‘æ¸¡ã™ç´ æï¼ˆI/Oç¦æ­¢ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ï¼‰
L55     cand: List[str]
L56     tickers: List[str]
L57     bench: str
L58     data: pd.DataFrame              # yfinance downloadçµæœï¼ˆ'Close','Volume'ç­‰ã®éšå±¤åˆ—ï¼‰
L59     px: pd.DataFrame                # data['Close']
L60     spx: pd.Series                  # data['Close'][bench]
L61     tickers_bulk: object            # yfinance.Tickers
L62     info: Dict[str, dict]           # yfinance info per ticker
L63     eps_df: pd.DataFrame            # ['eps_ttm','eps_q_recent',...]
L64     fcf_df: pd.DataFrame            # ['fcf_ttm', ...]
L65     returns: pd.DataFrame           # px[tickers].pct_change()
L66
L67 @dataclass(frozen=True)
L68 class FeatureBundle:
L69     df: pd.DataFrame
L70     df_z: pd.DataFrame
L71     g_score: pd.Series
L72     d_score_all: pd.Series
L73     missing_logs: pd.DataFrame
L74
L75 @dataclass(frozen=True)
L76 class SelectionBundle:
L77     resG: dict
L78     resD: dict
L79     top_G: List[str]
L80     top_D: List[str]
L81     init_G: List[str]
L82     init_D: List[str]
L83
L84 @dataclass(frozen=True)
L85 class WeightsConfig:
L86     g: Dict[str,float]
L87     d: Dict[str,float]
L88
L89 @dataclass(frozen=True)
L90 class DRRSParams:
L91     corrM: int
L92     shrink: float
L93     G: Dict[str,float]   # lookback, n_pc, gamma, lam, eta
L94     D: Dict[str,float]
L95     cross_mu_gd: float
L96
L97 @dataclass(frozen=True)
L98 class PipelineConfig:
L99     weights: WeightsConfig
L100     drrs: DRRSParams
L101     price_max: float
L102
L103
L104 # === å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆè¤‡æ•°ã‚¯ãƒ©ã‚¹ã§ä½¿ç”¨ï¼‰ ===
L105 # (unused local utils removed â€“ use scorer.py versions if needed)
L106
L107 _env_true = lambda name, default=False: (os.getenv(name) or str(default)).strip().lower() == "true"
L108
L109 def _post_slack(payload: dict):
L110     url = os.getenv("SLACK_WEBHOOK_URL")
L111     if not url: print("âš ï¸ SLACK_WEBHOOK_URL æœªè¨­å®š"); return
L112     try:
L113         requests.post(url, json=payload).raise_for_status()
L114     except Exception as e:
L115         print(f"âš ï¸ Slacké€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
L116
L117 _slack = lambda message, code=False: _post_slack({"text": f"```{message}```" if code else message})
L118
L119 def _slack_debug(text: str, chunk=2800):
L120     i=0
L121     while i<len(text):
L122         j=min(len(text), i+chunk); k=text.rfind("\n", i, j); j=k if k>i+100 else j
L123         _post_slack({"blocks":[{"type":"section","text":{"type":"mrkdwn","text":f"```{text[i:j]}```"}}]}); i=j
L124
L125 def _compact_debug(fb, sb, prevG, prevD, max_rows=140):
L126     want=["TR","EPS","REV","ROE","BETA_RAW","FCF","RS","TR_str","DIV_STREAK","DSC"]
L127     all_cols = _env_true("DEBUG_ALL_COLS", False)
L128     cols = list(fb.df_z.columns if all_cols else [c for c in want if c in fb.df_z.columns])
L129
L130     Gp, Dp = set(prevG or []), set(prevD or [])
L131     g_new=[t for t in (sb.top_G or []) if t not in Gp]; g_out=[t for t in Gp if t not in (sb.top_G or [])]
L132     d_new=[t for t in (sb.top_D or []) if t not in Dp]; d_out=[t for t in Dp if t not in (sb.top_D or [])]
L133
L134     show_near = _env_true("DEBUG_NEAR5", True)
L135     gs, ds = getattr(fb,"g_score",None), getattr(fb,"d_score_all",None)
L136     gs = (gs.sort_values(ascending=False) if show_near and hasattr(gs,"sort_values") else None)
L137     ds = (ds.sort_values(ascending=False) if show_near and hasattr(ds,"sort_values") else None)
L138     g_miss = ([t for t in gs.index if t not in (sb.top_G or [])][:10]) if gs is not None else []
L139     d_excl = set((sb.top_G or [])+(sb.top_D or []))
L140     d_miss = ([t for t in ds.index if t not in d_excl][:10]) if ds is not None else []
L141
L142     all_rows = _env_true("DEBUG_ALL_ROWS", False)
L143     focus = list(fb.df_z.index) if all_rows else sorted(set(g_new+g_out+d_new+d_out+(sb.top_G or [])+(sb.top_D or [])+g_miss+d_miss))[:max_rows]
L144
L145     def _fmt_near(lbl, ser, lst):
L146         if ser is None: return f"{lbl}: off"
L147         g = ser.get
L148         parts=[f"{t}:{g(t,float('nan')):.3f}" if pd.notna(g(t)) else f"{t}:nan" for t in lst]
L149         return f"{lbl}: " + (", ".join(parts) if parts else "-")
L150
L151     head=[f"G new/out: {len(g_new)}/{len(g_out)}  D new/out: {len(d_new)}/{len(d_out)}",
L152           _fmt_near("G near10", gs, g_miss),
L153           _fmt_near("D near10", ds, d_miss),
L154           f"Filters: G pre_mask=['trend_template'], D pre_filter={{'beta_max': {D_BETA_MAX}}}",
L155           f"Cols={'ALL' if all_cols else 'MIN'}  Rows={'ALL' if all_rows else 'SUBSET'}"]
L156
L157     tbl="(df_z or columns not available)"
L158     if not fb.df_z.empty and cols:
L159         idx=[t for t in focus if t in fb.df_z.index]
L160         tbl=fb.df_z.loc[idx, cols].round(3).to_string(max_rows=None, max_cols=None)
L161
L162     miss_txt=""
L163     if _env_true("DEBUG_MISSING_LOGS", False):
L164         miss=getattr(fb,"missing_logs",None)
L165         if miss is not None and not miss.empty:
L166             miss_txt="\nMissing data (head)\n"+miss.head(10).to_string(index=False)
L167
L168     return "\n".join(head+["\nChanged/Selected (+ Near Miss)", tbl])+miss_txt
L169
L170 def _disjoint_keepG(top_G, top_D, poolD):
L171     """
L172     Gã«å«ã¾ã‚Œã‚‹éŠ˜æŸ„ã‚’Dã‹ã‚‰é™¤å»ã—ã€Dã¯poolDï¼ˆæ¬¡ç‚¹ï¼‰ã§è£œå……ã™ã‚‹ã€‚
L173     - å¼•æ•°:
L174         top_G: List[str]  â€¦ Gæœ€çµ‚12éŠ˜æŸ„
L175         top_D: List[str]  â€¦ Dæœ€çµ‚13éŠ˜æŸ„ï¼ˆé‡è¤‡ã‚’å«ã‚€å¯èƒ½æ€§ã‚ã‚Šï¼‰
L176         poolD: List[str]  â€¦ Då€™è£œã®é †ä½ãƒªã‚¹ãƒˆï¼ˆtop_Dã‚’å«ã‚€ä¸Šä½æ‹¡å¼µï¼‰
L177     - æˆ»ã‚Šå€¤: (top_G, top_D_disjoint)
L178     - æŒ™å‹•:
L179         1) Dã«Gé‡è¤‡ãŒã‚ã‚Œã°é †ã«ç½®æ›
L180         2) ç½®æ›å€™è£œã¯ poolD ã‹ã‚‰ã€æ—¢ä½¿ç”¨(GâˆªD)ã‚’é¿ã‘ã¦å‰ã‹ã‚‰æ¡ç”¨
L181         3) è£œå……åˆ†ãŒå°½ããŸå ´åˆã¯å…ƒã®éŠ˜æŸ„ã‚’æ®‹ã™ï¼ˆå®‰å…¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
L182     """
L183     used, D, i = set(top_G), list(top_D), 0
L184     for j, t in enumerate(D):
L185         if t in used:
L186             while i<len(poolD) and (poolD[i] in used or poolD[i] in D): i+=1
L187             if i < len(poolD): D[j] = poolD[i]; used.add(D[j]); i += 1
L188     return top_G, D
L189
L190 _state_file = lambda: os.path.join(RESULTS_DIR, "breadth_state.json")
L191 def load_mode(default: str="NORMAL") -> str:
L192     try: m=json.loads(open(_state_file()).read()).get("mode", default); return m if m in ("EMERG","CAUTION","NORMAL") else default
L193     except Exception: return default
L194 def save_mode(mode: str):
L195     try: open(_state_file(),"w").write(json.dumps({"mode": mode}))
L196     except Exception: pass
L197
L198 # --- Breadthâ†’è‡ªå‹•ã—ãã„å€¤â†’ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹â†’Slackå…ˆé ­è¡Œã‚’ä½œæˆ ---
L199 def _build_breadth_lead_lines(inb) -> tuple[list[str], str]:
L200     win = int(os.getenv("BREADTH_CALIB_WIN_DAYS", "600"))
L201     C_ts = Scorer.trend_template_breadth_series(inb.px[inb.tickers], inb.spx, win_days=win)
L202     if C_ts.empty: raise RuntimeError("breadth series empty")
L203     warmup=int(os.getenv("BREADTH_WARMUP_DAYS","252")); base=C_ts.iloc[warmup:] if len(C_ts)>warmup else C_ts; C_full=int(C_ts.iloc[-1])
L204     q05 = int(np.nan_to_num(base.quantile(float(os.getenv("BREADTH_Q_EMERG_IN",  "0.05"))), nan=0.0))
L205     q20 = int(np.nan_to_num(base.quantile(float(os.getenv("BREADTH_Q_EMERG_OUT", "0.20"))), nan=0.0))
L206     q60 = int(np.nan_to_num(base.quantile(float(os.getenv("BREADTH_Q_WARN_OUT",  "0.60"))), nan=0.0))
L207     th_in_rec, th_out_rec, th_norm_rec = max(N_G, q05), max(int(np.ceil(1.5*N_G)), q20), max(3*N_G, q60)
L208     use_calib = os.getenv("BREADTH_USE_CALIB", "true").strip().lower() == "true"
L209     th_in, th_out, th_norm, th_src = (th_in_rec, th_out_rec, th_norm_rec, "è‡ªå‹•") if use_calib else (int(os.getenv("GTT_EMERG_IN",str(N_G))), int(os.getenv("GTT_EMERG_OUT",str(int(1.5*N_G)))), int(os.getenv("GTT_CAUTION_OUT",str(3*N_G))), "æ‰‹å‹•")
L210     prev = load_mode("NORMAL")
L211     if   prev == "EMERG":  mode = "EMERG" if (C_full < th_out) else ("CAUTION" if (C_full < th_norm) else "NORMAL")
L212     elif prev == "CAUTION": mode = "CAUTION" if (C_full < th_norm) else "NORMAL"
L213     else:                   mode = "EMERG" if (C_full < th_in) else ("CAUTION" if (C_full < th_norm) else "NORMAL")
L214     save_mode(mode)
L215     _MODE_JA={"EMERG":"ç·Šæ€¥","CAUTION":"è­¦æˆ’","NORMAL":"é€šå¸¸"}; _MODE_EMOJI={"EMERG":"ğŸš¨","CAUTION":"âš ï¸","NORMAL":"ğŸŸ¢"}
L216     mode_ja,emoji,eff_days=_MODE_JA.get(mode,mode),_MODE_EMOJI.get(mode,"â„¹ï¸"),len(base)
L217     lead_lines = [
L218         f"{emoji} *ç¾åœ¨ãƒ¢ãƒ¼ãƒ‰: {mode_ja}*", f"ãƒ†ãƒ³ãƒ—ãƒ¬åˆæ ¼æœ¬æ•°: *{C_full}æœ¬*", "ã—ãã„å€¤ï¼ˆ{0}ï¼‰".format(th_src),
L219         f"  ãƒ»ç·Šæ€¥å…¥ã‚Š: <{th_in}æœ¬", f"  ãƒ»ç·Šæ€¥è§£é™¤: â‰¥{th_out}æœ¬", f"  ãƒ»é€šå¸¸å¾©å¸°: â‰¥{th_norm}æœ¬",
L220         f"å‚è€ƒæŒ‡æ¨™ï¼ˆéå»~{win}å–¶æ¥­æ—¥, æœ‰åŠ¹={eff_days}æ—¥ï¼‰",
L221         f"  ãƒ»ä¸‹ä½5%: {q05}æœ¬", f"  ãƒ»ä¸‹ä½20%: {q20}æœ¬", f"  ãƒ»60%åˆ†ä½: {q60}æœ¬",
L222     ]
L223     return lead_lines, mode
L224
L225
L226 # === Inputï¼šå¤–éƒ¨I/Oã¨å‰å‡¦ç†ï¼ˆCSV/APIãƒ»æ¬ æè£œå®Œï¼‰ ===
L227 class Input:
L228     def __init__(self, cand, exist, bench, price_max, finnhub_api_key=None):
L229         self.cand, self.exist, self.bench, self.price_max = cand, exist, bench, price_max
L230         self.api_key = finnhub_api_key or os.environ.get("FINNHUB_API_KEY")
L231
L232     # ---- ï¼ˆInputå°‚ç”¨ï¼‰EPSè£œå®Œãƒ»FCFç®—å‡ºç³» ----
L233     @staticmethod
L234     def impute_eps_ttm(df: pd.DataFrame, ttm_col: str="eps_ttm", q_col: str="eps_q_recent", out_col: str|None=None) -> pd.DataFrame:
L235         out_col = out_col or ttm_col; df = df.copy(); df["eps_imputed"] = False
L236         cand = df[q_col]*4; ok = df[ttm_col].isna() & cand.replace([np.inf,-np.inf], np.nan).notna()
L237         df.loc[ok, out_col], df.loc[ok,"eps_imputed"] = cand[ok], True; return df
L238
L239     _CF_ALIASES = {"cfo":["Operating Cash Flow","Total Cash From Operating Activities"], "capex":["Capital Expenditure","Capital Expenditures"]}
L240
L241     @staticmethod
L242     def _pick_row(df: pd.DataFrame, names: list[str]) -> pd.Series|None:
L243         if df is None or df.empty: return None
L244         idx_lower={str(i).lower():i for i in df.index}
L245         for n in names:
L246             k=n.lower()
L247             if k in idx_lower: return df.loc[idx_lower[k]]
L248         return None
L249
L250     @staticmethod
L251     def _sum_last_n(s: pd.Series|None, n: int) -> float|None:
L252         if s is None or s.empty: return None
L253         v=s.dropna().astype(float); return None if v.empty else v.iloc[:n].sum()
L254
L255     @staticmethod
L256     def _latest(s: pd.Series|None) -> float|None:
L257         if s is None or s.empty: return None
L258         v=s.dropna().astype(float); return v.iloc[0] if not v.empty else None
L259
L260     def fetch_cfo_capex_ttm_yf(self, tickers: list[str]) -> pd.DataFrame:
L261         from concurrent.futures import ThreadPoolExecutor, as_completed
L262         pick, sumn, latest, aliases = self._pick_row, self._sum_last_n, self._latest, self._CF_ALIASES
L263
L264         def one(t: str):
L265             try:
L266                 tk = yf.Ticker(t)  # â˜… ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¯æ¸¡ã•ãªã„ï¼ˆYFãŒcurl_cffiã§ç®¡ç†ï¼‰
L267                 qcf = tk.quarterly_cashflow
L268                 cfo_q, capex_q = pick(qcf, aliases["cfo"]), pick(qcf, aliases["capex"])
L269                 fcf_q = pick(qcf, ["Free Cash Flow","FreeCashFlow","Free cash flow"])
L270                 cfo, capex, fcf = sumn(cfo_q,4), sumn(capex_q,4), sumn(fcf_q,4)
L271                 if any(v is None for v in (cfo, capex, fcf)):
L272                     acf = tk.cashflow
L273                     if cfo   is None: cfo   = latest(pick(acf, aliases["cfo"]))
L274                     if capex is None: capex = latest(pick(acf, aliases["capex"]))
L275                     if fcf   is None: fcf   = latest(pick(acf, ["Free Cash Flow","FreeCashFlow","Free cash flow"]))
L276             except Exception as e:
L277                 print(f"[warn] yf financials error: {t}: {e}"); cfo=capex=fcf=None
L278             n=np.nan
L279             return {"ticker":t,
L280                     "cfo_ttm_yf":   n if cfo   is None else cfo,
L281                     "capex_ttm_yf": n if capex is None else capex,
L282                     "fcf_ttm_yf_direct": n if fcf is None else fcf}
L283
L284         rows, mw = [], int(os.getenv("FIN_THREADS","8"))
L285         with ThreadPoolExecutor(max_workers=mw) as ex:
L286             rows=[f.result() for f in as_completed(ex.submit(one,t) for t in tickers)]
L287         return pd.DataFrame(rows).set_index("ticker")
L288
L289     _FINN_CFO_KEYS = ["netCashProvidedByOperatingActivities","netCashFromOperatingActivities","cashFlowFromOperatingActivities","operatingCashFlow"]
L290     _FINN_CAPEX_KEYS = ["capitalExpenditure","capitalExpenditures","purchaseOfPPE","investmentsInPropertyPlantAndEquipment"]
L291
L292     @staticmethod
L293     def _first_key(d: dict, keys: list[str]):
L294         for k in keys:
L295             if k in d and d[k] is not None: return d[k]
L296         return None
L297
L298     @staticmethod
L299     def _finn_get(session: requests.Session, url: str, params: dict, retries: int=3, sleep_s: float=0.5):
L300         for i in range(retries):
L301             r = session.get(url, params=params, timeout=15)
L302             if r.status_code==429: time.sleep(min(2**i*sleep_s,4.0)); continue
L303             r.raise_for_status(); return r.json()
L304         r.raise_for_status()
L305
L306     def fetch_cfo_capex_ttm_finnhub(self, tickers: list[str], api_key: str|None=None) -> pd.DataFrame:
L307         api_key = api_key or os.getenv("FINNHUB_API_KEY")
L308         if not api_key: raise ValueError("Finnhub API key not provided. Set FINNHUB_API_KEY or pass api_key=")
L309         base, s, rows = "https://finnhub.io/api/v1", requests.Session(), []
L310         for sym in tickers:
L311             cfo_ttm = capex_ttm = None
L312             try:
L313                 j = self._finn_get(s, f"{base}/stock/cash-flow", {"symbol":sym,"frequency":"quarterly","limit":8,"token":api_key})
L314                 arr = j.get("cashFlow") or []; cfo_vals, capex_vals = [], []
L315                 for item in arr[:4]:
L316                     cfo_vals.append(self._first_key(item,self._FINN_CFO_KEYS)); capex_vals.append(self._first_key(item,self._FINN_CAPEX_KEYS))
L317                 if any(v is not None for v in cfo_vals): cfo_ttm = float(np.nansum([np.nan if v is None else float(v) for v in cfo_vals]))
L318                 if any(v is not None for v in capex_vals): capex_ttm = float(np.nansum([np.nan if v is None else float(v) for v in capex_vals]))
L319             except Exception: pass
L320             if cfo_ttm is None or capex_ttm is None:
L321                 try:
L322                     j = self._finn_get(s, f"{base}/stock/cash-flow", {"symbol":sym,"frequency":"annual","limit":1,"token":api_key})
L323                     arr = j.get("cashFlow") or []
L324                     if arr:
L325                         item0 = arr[0]
L326                         if cfo_ttm is None:
L327                             v = self._first_key(item0,self._FINN_CFO_KEYS)
L328                             if v is not None: cfo_ttm = float(v)
L329                         if capex_ttm is None:
L330                             v = self._first_key(item0,self._FINN_CAPEX_KEYS)
L331                             if v is not None: capex_ttm = float(v)
L332                 except Exception: pass
L333             rows.append({"ticker":sym,"cfo_ttm_fh":np.nan if cfo_ttm is None else cfo_ttm,"capex_ttm_fh":np.nan if capex_ttm is None else capex_ttm})
L334         return pd.DataFrame(rows).set_index("ticker")
L335
L336     def compute_fcf_with_fallback(self, tickers: list[str], finnhub_api_key: str|None=None) -> pd.DataFrame:
L337         yf_df = self.fetch_cfo_capex_ttm_yf(tickers)
L338         T.log("financials (yf) done")
L339         miss_mask = yf_df[["cfo_ttm_yf","capex_ttm_yf","fcf_ttm_yf_direct"]].isna().any(axis=1)
L340         need = yf_df.index[miss_mask].tolist(); print(f"[T] yf financials missing: {len(need)} {need[:10]}{'...' if len(need)>10 else ''}")
L341         if need:
L342             fh_df = self.fetch_cfo_capex_ttm_finnhub(need, api_key=finnhub_api_key)
L343             df = yf_df.join(fh_df, how="left")
L344             for col_yf, col_fh in [("cfo_ttm_yf","cfo_ttm_fh"),("capex_ttm_yf","capex_ttm_fh")]:
L345                 df[col_yf] = df[col_yf].fillna(df[col_fh])
L346             print("[T] financials (finnhub) done (fallback only)")
L347         else:
L348             df = yf_df.assign(cfo_ttm_fh=np.nan, capex_ttm_fh=np.nan)
L349             print("[T] financials (finnhub) skipped (no missing)")
L350         df["cfo_ttm"]  = df["cfo_ttm_yf"].where(df["cfo_ttm_yf"].notna(), df["cfo_ttm_fh"])
L351         df["capex_ttm"] = df["capex_ttm_yf"].where(df["capex_ttm_yf"].notna(), df["capex_ttm_fh"])
L352         cfo, capex = pd.to_numeric(df["cfo_ttm"], errors="coerce"), pd.to_numeric(df["capex_ttm"], errors="coerce").abs()
L353         fcf_calc = cfo - capex
L354         fcf_direct = pd.to_numeric(df.get("fcf_ttm_yf_direct"), errors="coerce")
L355         df["fcf_ttm"] = fcf_calc.where(fcf_calc.notna(), fcf_direct)
L356         df["cfo_source"]  = np.where(df["cfo_ttm_yf"].notna(),"yfinance",np.where(df["cfo_ttm_fh"].notna(),"finnhub",""))
L357         df["capex_source"] = np.where(df["capex_ttm_yf"].notna(),"yfinance",np.where(df["capex_ttm_fh"].notna(),"finnhub",""))
L358         df["fcf_imputed"] = df[["cfo_ttm","capex_ttm"]].isna().any(axis=1) & df["fcf_ttm"].notna()
L359         cols = ["cfo_ttm_yf","capex_ttm_yf","cfo_ttm_fh","capex_ttm_fh","cfo_ttm","capex_ttm","fcf_ttm","fcf_ttm_yf_direct","cfo_source","capex_source","fcf_imputed"]
L360         return df[cols].sort_index()
L361
L362     def _build_eps_df(self, tickers, tickers_bulk, info):
L363         eps_rows=[]
L364         for t in tickers:
L365             info_t, eps_ttm, eps_q = info[t], info[t].get("trailingEps", np.nan), np.nan
L366             try:
L367                 qearn, so = tickers_bulk.tickers[t].quarterly_earnings, info_t.get("sharesOutstanding")
L368                 if so and qearn is not None and not qearn.empty and "Earnings" in qearn.columns:
L369                     eps_ttm_q = qearn["Earnings"].head(4).sum()/so
L370                     if pd.notna(eps_ttm_q) and (pd.isna(eps_ttm) or (abs(eps_ttm)>0 and abs(eps_ttm/eps_ttm_q)>3)): eps_ttm = eps_ttm_q
L371                     eps_q = qearn["Earnings"].iloc[-1]/so
L372             except Exception: pass
L373             eps_rows.append({"ticker":t,"eps_ttm":eps_ttm,"eps_q_recent":eps_q})
L374         return self.impute_eps_ttm(pd.DataFrame(eps_rows).set_index("ticker"))
L375
L376     def prepare_data(self):
L377         """Fetch price and fundamental data for all tickers."""
L378         cand_info = yf.Tickers(" ".join(self.cand)); cand_prices = {}
L379         for t in self.cand:
L380             try: cand_prices[t] = cand_info.tickers[t].fast_info.get("lastPrice", np.inf)
L381             except Exception as e: print(f"{t}: price fetch failed ({e})"); cand_prices[t] = np.inf
L382         cand_f = [t for t,p in cand_prices.items() if p<=self.price_max]
L383         T.log("price cap filter done (CAND_PRICE_MAX)")
L384         tickers = sorted(set(self.exist + cand_f))
L385         T.log(f"universe prepared: unique={len(tickers)} bench={self.bench}")
L386         data = yf.download(tickers + [self.bench], period="600d", auto_adjust=True, progress=False)
L387         T.log("yf.download done")
L388         px, spx = data["Close"], data["Close"][self.bench]
L389         clip_days = int(os.getenv("PRICE_CLIP_DAYS", "0"))   # 0ãªã‚‰ç„¡åŠ¹ï¼ˆæ—¢å®šï¼‰
L390         if clip_days > 0:
L391             px  = px.tail(clip_days + 1)
L392             spx = spx.tail(clip_days + 1)
L393             print(f"[T] price window clipped by env: {len(px)} rows (PRICE_CLIP_DAYS={clip_days})")
L394         else:
L395             print(f"[T] price window clip skipped; rows={len(px)}")
L396         tickers_bulk, info = yf.Tickers(" ".join(tickers)), {}
L397         for t in tickers:
L398             try: info[t] = tickers_bulk.tickers[t].info
L399             except Exception as e: print(f"{t}: info fetch failed ({e})"); info[t] = {}
L400         eps_df = self._build_eps_df(tickers, tickers_bulk, info)
L401         fcf_df = self.compute_fcf_with_fallback(tickers, finnhub_api_key=self.api_key)
L402         T.log("eps/fcf prep done")
L403         returns = px[tickers].pct_change()
L404         T.log("price prep/returns done")
L405         return dict(cand=cand_f, tickers=tickers, data=data, px=px, spx=spx, tickers_bulk=tickers_bulk, info=info, eps_df=eps_df, fcf_df=fcf_df, returns=returns)
L406
L407
L408 # === Selectorï¼šç›¸é–¢ä½æ¸›ãƒ»é¸å®šï¼ˆã‚¹ã‚³ã‚¢ï¼†ãƒªã‚¿ãƒ¼ãƒ³ã ã‘èª­ã‚€ï¼‰ ===
L409 class Selector:
L410     # ---- DRRS helpersï¼ˆSelectorå°‚ç”¨ï¼‰ ----
L411     @staticmethod
L412     def _z_np(X: np.ndarray) -> np.ndarray:
L413         X = np.asarray(X, dtype=np.float32); m = np.nanmean(X, axis=0, keepdims=True); s = np.nanstd(X, axis=0, keepdims=True)+1e-9
L414         return (np.nan_to_num(X)-m)/s
L415
L416     @classmethod
L417     def residual_corr(cls, R: np.ndarray, n_pc: int=3, shrink: float=0.1) -> np.ndarray:
L418         Z = cls._z_np(R); U,S,_ = np.linalg.svd(Z, full_matrices=False); F = U[:,:n_pc]*S[:n_pc]; B = np.linalg.lstsq(F, Z, rcond=None)[0]
L419         E = Z - F@B; C = np.corrcoef(E, rowvar=False)
L420         off = C - np.diag(np.diag(C)); iu = np.triu_indices_from(off,1); avg_abs = np.nanmean(np.abs(off[iu])) if iu[0].size else 0.0
L421         shrink_eff = float(np.clip(shrink + 0.5*avg_abs, 0.1, 0.6)); N = C.shape[0]
L422         return (1.0 - shrink_eff)*C + shrink_eff*np.eye(N, dtype=C.dtype)
L423
L424     @classmethod
L425     def rrqr_like_det(cls, R: np.ndarray, score: np.ndarray, k: int, gamma: float=1.0):
L426         Z, w = cls._z_np(R), (score-score.min())/(np.ptp(score)+1e-12); X = Z*(1.0 + gamma*w)
L427         N, k = X.shape[1], max(0, min(k, X.shape[1]))
L428         if k==0: return []
L429         S, selected, Rres = [], np.zeros(N, dtype=bool), X.copy()
L430         for _ in range(k):
L431             norms = (Rres*Rres).sum(axis=0); cand = np.where(~selected)[0]
L432             j = sorted(cand, key=lambda c:(-norms[c], -w[c], c))[0]
L433             S.append(j); selected[j]=True; u = X[:,j:j+1]; u/=(np.linalg.norm(u)+1e-12); Rres = Rres - u @ (u.T @ Rres)
L434         return sorted(S)
L435
L436     @staticmethod
L437     def _obj(corrM: np.ndarray, score: np.ndarray, idx, lam: float) -> float:
L438         idx = list(idx); P = corrM[np.ix_(idx, idx)]; s = (score-score.mean())/(score.std()+1e-9)
L439         return float(s[idx].sum() - lam*((P.sum()-np.trace(P))/2.0))
L440
L441     @classmethod
L442     def swap_local_det(cls, corrM: np.ndarray, score: np.ndarray, idx, lam: float=0.6, max_pass: int=15):
L443         S, best, improved, passes = sorted(idx), cls._obj(corrM, score, idx, lam), True, 0
L444         while improved and passes<max_pass:
L445             improved, passes = False, passes+1
L446             for i,out in enumerate(list(S)):
L447                 for inn in range(len(score)):
L448                     if inn in S: continue
L449                     cand = sorted(S[:i]+[inn]+S[i+1:]); v = cls._obj(corrM, score, cand, lam)
L450                     if v>best+1e-10: S, best, improved = cand, v, True; break
L451                 if improved: break
L452         return S, best
L453
L454     @staticmethod
L455     def _obj_with_cross(C_within: np.ndarray, C_cross: np.ndarray|None, score: np.ndarray, idx, lam: float, mu: float) -> float:
L456         idx = list(idx); P = C_within[np.ix_(idx, idx)]; s = (score-score.mean())/(score.std()+1e-9)
L457         within = (P.sum()-np.trace(P))/2.0; cross = 0.0
L458         if C_cross is not None and C_cross.size>0: cross = C_cross[idx,:].sum()
L459         return float(s[idx].sum() - lam*within - mu*cross)
L460
L461     @classmethod
L462     def swap_local_det_cross(cls, C_within: np.ndarray, C_cross: np.ndarray|None, score: np.ndarray, idx, lam: float=0.6, mu: float=0.3, max_pass: int=15):
L463         S, best, improved, passes, N = sorted(idx), cls._obj_with_cross(C_within,C_cross,score,idx,lam,mu), True, 0, len(score)
L464         while improved and passes<max_pass:
L465             improved, passes = False, passes+1
L466             for i,out in enumerate(list(S)):
L467                 for inn in range(N):
L468                     if inn in S: continue
L469                     cand = sorted(S[:i]+[inn]+S[i+1:]); v = cls._obj_with_cross(C_within,C_cross,score,cand,lam,mu)
L470                     if v>best+1e-10: S, best, improved = cand, v, True; break
L471                 if improved: break
L472         return S, best
L473
L474     @staticmethod
L475     def avg_corr(C: np.ndarray, idx) -> float:
L476         k = len(idx); P = C[np.ix_(idx, idx)]
L477         return float((P.sum()-np.trace(P))/(k*(k-1)+1e-12))
L478
L479     @classmethod
L480     def select_bucket_drrs(cls, returns_df: pd.DataFrame, score_ser: pd.Series, pool_tickers: list[str], k: int, *, n_pc: int, gamma: float, lam: float, lookback: int, shrink: float=0.10, g_fixed_tickers: list[str]|None=None, mu: float=0.0):
L481         g_fixed = [t for t in (g_fixed_tickers or []) if t in returns_df.columns]
L482         union = [t for t in pool_tickers if t in returns_df.columns]
L483         for t in g_fixed:
L484             if t not in union: union.append(t)
L485         Rdf_all = returns_df[union]; Rdf_all = Rdf_all.iloc[-lookback:] if len(Rdf_all)>=lookback else Rdf_all; Rdf_all = Rdf_all.dropna()
L486         pool_eff, g_eff = [t for t in pool_tickers if t in Rdf_all.columns], [t for t in g_fixed if t in Rdf_all.columns]
L487         if len(pool_eff)==0: return dict(idx=[], tickers=[], avg_res_corr=np.nan, sum_score=0.0, objective=-np.inf)
L488         score = score_ser.reindex(pool_eff).to_numpy(dtype=np.float32)
L489         C_all = cls.residual_corr(Rdf_all.to_numpy(), n_pc=n_pc, shrink=shrink)
L490         col_pos = {c:i for i,c in enumerate(Rdf_all.columns)}; pool_pos = [col_pos[t] for t in pool_eff]
L491         C_within, C_cross = C_all[np.ix_(pool_pos,pool_pos)], None
L492         if len(g_eff)>0 and mu>0.0:
L493             g_pos = [col_pos[t] for t in g_eff]; C_cross = C_all[np.ix_(pool_pos,g_pos)]
L494         R_pool = Rdf_all[pool_eff].to_numpy(); S0 = cls.rrqr_like_det(R_pool, score, k, gamma=gamma)
L495         S, Jn = (cls.swap_local_det_cross(C_within, C_cross, score, S0, lam=lam, mu=mu, max_pass=15) if C_cross is not None else cls.swap_local_det(C_within, score, S0, lam=lam, max_pass=15))
L496         selected_tickers = [pool_eff[i] for i in S]
L497         return dict(idx=S, tickers=selected_tickers, avg_res_corr=cls.avg_corr(C_within,S), sum_score=float(score[S].sum()), objective=float(Jn))
L498
L499     # ---- é¸å®šï¼ˆã‚¹ã‚³ã‚¢ Series / returns ã ã‘ã‚’å—ã‘ã‚‹ï¼‰----
L500 # === Outputï¼šå‡ºåŠ›æ•´å½¢ã¨é€ä¿¡ï¼ˆè¡¨ç¤ºãƒ»Slackï¼‰ ===
L501 class Output:
L502
L503     def __init__(self, debug=False):
L504         self.debug = debug
L505         self.miss_df = self.g_table = self.d_table = self.io_table = self.df_metrics_fmt = self.debug_table = None
L506         self.g_title = self.d_title = ""
L507         self.g_formatters = self.d_formatters = {}
L508         # ä½ã‚¹ã‚³ã‚¢ï¼ˆGSC+DSCï¼‰Top10 è¡¨ç¤º/é€ä¿¡ç”¨
L509         self.low10_table = None
L510
L511     # --- è¡¨ç¤ºï¼ˆå…ƒ display_results ã®ãƒ­ã‚¸ãƒƒã‚¯ãã®ã¾ã¾ï¼‰ ---
L512     def display_results(self, *, exist, bench, df_z, g_score, d_score_all,
L513                         init_G, init_D, top_G, top_D, **kwargs):
L514         pd.set_option('display.float_format','{:.3f}'.format)
L515         print("ğŸ“ˆ ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼åˆ†æ•£æœ€é©åŒ–ã®çµæœ")
L516         if self.miss_df is not None and not self.miss_df.empty:
L517             print("Missing Data:")
L518             print(self.miss_df.to_string(index=False))
L519
L520         # ---- è¡¨ç¤ºç”¨ï¼šChanges/Near-Miss ã®ã‚¹ã‚³ã‚¢æºã‚’â€œæœ€çµ‚é›†è¨ˆâ€ã«çµ±ä¸€ã™ã‚‹ãƒ—ãƒ­ã‚­ã‚· ----
L521         try:
L522             sc = getattr(self, "_sc", None)
L523             agg_G = getattr(sc, "_agg_G", None)
L524             agg_D = getattr(sc, "_agg_D", None)
L525         except Exception:
L526             sc = agg_G = agg_D = None
L527         class _SeriesProxy:
L528             __slots__ = ("primary", "fallback")
L529             def __init__(self, primary, fallback): self.primary, self.fallback = primary, fallback
L530             def get(self, key, default=None):
L531                 try:
L532                     v = self.primary.get(key) if hasattr(self.primary, "get") else None
L533                     if v is not None and not (isinstance(v, float) and v != v):
L534                         return v
L535                 except Exception:
L536                     pass
L537                 try:
L538                     return self.fallback.get(key) if hasattr(self.fallback, "get") else default
L539                 except Exception:
L540                     return default
L541         g_score = _SeriesProxy(agg_G, g_score)
L542         d_score_all = _SeriesProxy(agg_D, d_score_all)
L543         near_G = getattr(sc, "_near_G", []) if sc else []
L544         near_D = getattr(sc, "_near_D", []) if sc else []
L545
L546         extra_G = [t for t in init_G if t not in top_G][:5]; G_UNI = top_G + extra_G
L547         gsc_series = pd.Series({t: g_score.get(t) for t in G_UNI}, name='GSC')
L548         self.g_table = pd.concat([df_z.loc[G_UNI,['GRW','MOM','TRD','VOL']], gsc_series], axis=1)
L549         self.g_table.index = [t + ("â­ï¸" if t in top_G else "") for t in G_UNI]
L550         self.g_formatters = {col:"{:.2f}".format for col in ['GRW','MOM','TRD','VOL']}; self.g_formatters['GSC'] = "{:.3f}".format
L551         self.g_title = (f"[Gæ  / {N_G} / {_fmt_w(g_weights)} / corrM={corrM} / "
L552                         f"LB={DRRS_G['lookback']} nPC={DRRS_G['n_pc']} Î³={DRRS_G['gamma']} Î»={DRRS_G['lam']} Î·={DRRS_G['eta']} shrink={DRRS_SHRINK}]")
L553         if near_G:
L554             add = [t for t in near_G if t not in set(G_UNI)][:10]
L555             if len(add) < 10:
L556                 try:
L557                     aggG = getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False)
L558                     out_now = sorted(set(exist) - set(top_G + top_D))  # ä»Šå› OUT
L559                     used = set(G_UNI + add)
L560                     def _push(lst):
L561                         nonlocal add, used
L562                         for t in lst:
L563                             if len(add) == 10: break
L564                             if t in aggG.index and t not in used:
L565                                 add.append(t); used.add(t)
L566                     _push(out_now)           # â‘  ä»Šå› OUT ã‚’å„ªå…ˆ
L567                     _push(list(aggG.index))  # â‘¡ ã¾ã è¶³ã‚Šãªã‘ã‚Œã°ä¸Šä½ã§å……å¡«
L568                 except Exception:
L569                     pass
L570             if add:
L571                 near_tbl = pd.concat([df_z.loc[add,['GRW','MOM','TRD','VOL']], pd.Series({t: g_score.get(t) for t in add}, name='GSC')], axis=1)
L572                 self.g_table = pd.concat([self.g_table, near_tbl], axis=0)
L573         print(self.g_title); print(self.g_table.to_string(formatters=self.g_formatters))
L574
L575         extra_D = [t for t in init_D if t not in top_D][:5]; D_UNI = top_D + extra_D
L576         cols_D = ['QAL','YLD','VOL','TRD']; d_disp = pd.DataFrame(index=D_UNI)
L577         d_disp['QAL'], d_disp['YLD'], d_disp['VOL'], d_disp['TRD'] = df_z.loc[D_UNI,'D_QAL'], df_z.loc[D_UNI,'D_YLD'], df_z.loc[D_UNI,'D_VOL_RAW'], df_z.loc[D_UNI,'D_TRD']
L578         dsc_series = pd.Series({t: d_score_all.get(t) for t in D_UNI}, name='DSC')
L579         self.d_table = pd.concat([d_disp, dsc_series], axis=1); self.d_table.index = [t + ("â­ï¸" if t in top_D else "") for t in D_UNI]
L580         self.d_formatters = {col:"{:.2f}".format for col in cols_D}; self.d_formatters['DSC']="{:.3f}".format
L581         import scorer
L582         dw_eff = scorer.D_WEIGHTS_EFF
L583         self.d_title = (f"[Dæ  / {N_D} / {_fmt_w(dw_eff)} / corrM={corrM} / "
L584                         f"LB={DRRS_D['lookback']} nPC={DRRS_D['n_pc']} Î³={DRRS_D['gamma']} Î»={DRRS_D['lam']} Î¼={CROSS_MU_GD} Î·={DRRS_D['eta']} shrink={DRRS_SHRINK}]")
L585         if near_D:
L586             add = [t for t in near_D if t not in set(D_UNI)][:10]
L587             if add:
L588                 d_disp2 = pd.DataFrame(index=add)
L589                 d_disp2['QAL'], d_disp2['YLD'], d_disp2['VOL'], d_disp2['TRD'] = df_z.loc[add,'D_QAL'], df_z.loc[add,'D_YLD'], df_z.loc[add,'D_VOL_RAW'], df_z.loc[add,'D_TRD']
L590                 near_tbl = pd.concat([d_disp2, pd.Series({t: d_score_all.get(t) for t in add}, name='DSC')], axis=1)
L591                 self.d_table = pd.concat([self.d_table, near_tbl], axis=0)
L592         print(self.d_title); print(self.d_table.to_string(formatters=self.d_formatters))
L593
L594         # === Changesï¼ˆIN ã® GSC/DSC ã‚’è¡¨ç¤ºã€‚OUT ã¯éŠ˜æŸ„åã®ã¿ï¼‰ ===
L595         in_list = sorted(set(list(top_G)+list(top_D)) - set(exist))
L596         out_list = sorted(set(exist) - set(list(top_G)+list(top_D)))
L597
L598         self.io_table = pd.DataFrame({
L599             'IN': pd.Series(in_list),
L600             '/ OUT': pd.Series(out_list)
L601         })
L602         g_list = [f"{g_score.get(t):.3f}" if pd.notna(g_score.get(t)) else 'â€”' for t in out_list]
L603         d_list = [f"{d_score_all.get(t):.3f}" if pd.notna(d_score_all.get(t)) else 'â€”' for t in out_list]
L604         self.io_table['GSC'] = pd.Series(g_list)
L605         self.io_table['DSC'] = pd.Series(d_list)
L606
L607         print("Changes:")
L608         print(self.io_table.to_string(index=False))
L609
L610         all_tickers = list(set(exist + list(top_G) + list(top_D) + [bench])); prices = yf.download(all_tickers, period='1y', auto_adjust=True, progress=False)['Close']
L611         ret = prices.pct_change(); portfolios = {'CUR':exist,'NEW':list(top_G)+list(top_D)}; metrics={}
L612         for name,ticks in portfolios.items():
L613             pr = ret[ticks].mean(axis=1, skipna=True).dropna(); cum = (1+pr).cumprod()-1; n = len(pr)
L614             if n>=252: ann_ret, ann_vol = (1+cum.iloc[-1])**(252/n)-1, pr.std()*np.sqrt(252)
L615             else: ann_ret, ann_vol = cum.iloc[-1], pr.std()*np.sqrt(n)
L616             sharpe, drawdown = ann_ret/ann_vol, (cum - cum.cummax()).min()
L617             if len(ticks)>=2:
L618                 C_raw = ret[ticks].corr(); RAW_rho = C_raw.mask(np.eye(len(ticks), dtype=bool)).stack().mean()
L619                 R = ret[ticks].dropna().to_numpy(); C_resid = Selector.residual_corr(R, n_pc=3, shrink=DRRS_SHRINK)
L620                 RESID_rho = float((C_resid.sum()-np.trace(C_resid))/(C_resid.shape[0]*(C_resid.shape[0]-1)))
L621             else: RAW_rho = RESID_rho = np.nan
L622             divy = ttm_div_yield_portfolio(ticks); metrics[name] = {'RET':ann_ret,'VOL':ann_vol,'SHP':sharpe,'MDD':drawdown,'RAWÏ':RAW_rho,'RESIDÏ':RESID_rho,'DIVY':divy}
L623         df_metrics = pd.DataFrame(metrics).T; df_metrics_pct = df_metrics.copy(); self.df_metrics = df_metrics
L624         for col in ['RET','VOL','MDD','DIVY']: df_metrics_pct[col] = df_metrics_pct[col]*100
L625         cols_order = ['RET','VOL','SHP','MDD','RAWÏ','RESIDÏ','DIVY']; df_metrics_pct = df_metrics_pct.reindex(columns=cols_order)
L626         def _fmt_row(s):
L627             return pd.Series({'RET':f"{s['RET']:.1f}%",'VOL':f"{s['VOL']:.1f}%",'SHP':f"{s['SHP']:.1f}",'MDD':f"{s['MDD']:.1f}%",'RAWÏ':(f"{s['RAWÏ']:.2f}" if pd.notna(s['RAWÏ']) else "NaN"),'RESIDÏ':(f"{s['RESIDÏ']:.2f}" if pd.notna(s['RESIDÏ']) else "NaN"),'DIVY':f"{s['DIVY']:.1f}%"})
L628         self.df_metrics_fmt = df_metrics_pct.apply(_fmt_row, axis=1); print("Performance Comparison:"); print(self.df_metrics_fmt.to_string())
L629         if self.debug:
L630             self.debug_table = pd.concat([df_z[['TR','EPS','REV','ROE','BETA','DIV','FCF','RS','TR_str','DIV_STREAK']], g_score.rename('GSC'), d_score_all.rename('DSC')], axis=1).round(3)
L631             print("Debug Data:"); print(self.debug_table.to_string())
L632
L633         # === è¿½åŠ : GSC+DSC ãŒä½ã„é † TOP10 ===
L634         try:
L635             all_scores = pd.DataFrame({'GSC': df_z['GSC'], 'DSC': df_z['DSC']}).copy()
L636             all_scores['G_plus_D'] = all_scores['GSC'] + all_scores['DSC']
L637             all_scores = all_scores.dropna(subset=['G_plus_D'])
L638             self.low10_table = all_scores.sort_values('G_plus_D', ascending=True).head(10).round(3)
L639             print("Low Score Candidates (GSC+DSC bottom 10):")
L640             print(self.low10_table.to_string())
L641         except Exception as e:
L642             print(f"[warn] low-score ranking failed: {e}")
L643             self.low10_table = None
L644
L645     # --- Slacké€ä¿¡ï¼ˆå…ƒ notify_slack ã®ãƒ­ã‚¸ãƒƒã‚¯ãã®ã¾ã¾ï¼‰ ---
L646     def notify_slack(self):
L647         SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")
L648         if not SLACK_WEBHOOK_URL: raise ValueError("SLACK_WEBHOOK_URL not set (ç’°å¢ƒå¤‰æ•°ãŒæœªè¨­å®šã§ã™)")
L649         def _filter_suffix_from(spec: dict, group: str) -> str:
L650             g = spec.get(group, {})
L651             parts = [str(m) for m in g.get("pre_mask", [])]
L652             for k, v in (g.get("pre_filter", {}) or {}).items():
L653                 base, op = (k[:-4], "<") if k.endswith("_max") else ((k[:-4], ">") if k.endswith("_min") else (k, "="))
L654                 name = {"beta": "Î²"}.get(base, base)
L655                 try: val = f"{float(v):g}"
L656                 except: val = str(v)
L657                 parts.append(f"{name}{op}{val}")
L658             return "" if not parts else " / filter:" + " & ".join(parts)
L659         def _inject_filter_suffix(title: str, group: str) -> str:
L660             suf = _filter_suffix_from(FILTER_SPEC, group)
L661             return f"{title[:-1]}{suf}]" if suf and title.endswith("]") else (title + suf)
L662         def _blk(title, tbl, fmt=None, drop=()):
L663             if tbl is None or getattr(tbl,'empty',False): return f"{title}\n(é¸å®šãªã—)\n"
L664             if drop and hasattr(tbl,'columns'):
L665                 keep = [c for c in tbl.columns if c not in drop]
L666                 tbl, fmt = tbl[keep], {k:v for k,v in (fmt or {}).items() if k in keep}
L667             return f"{title}\n```{tbl.to_string(formatters=fmt)}```\n"
L668
L669         g_title = _inject_filter_suffix(self.g_title, "G")
L670         d_title = _inject_filter_suffix(self.d_title, "D")
L671         message  = "ğŸ“ˆ ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼åˆ†æ•£æœ€é©åŒ–ã®çµæœ\n"
L672         if self.miss_df is not None and not self.miss_df.empty:
L673             message += "Missing Data\n```" + self.miss_df.to_string(index=False) + "```\n"
L674         message += _blk(g_title, self.g_table, self.g_formatters, drop=("TRD",))
L675         message += _blk(d_title, self.d_table, self.d_formatters)
L676         message += "Changes\n" + ("(å¤‰æ›´ãªã—)\n" if self.io_table is None or getattr(self.io_table,'empty',False) else f"```{self.io_table.to_string(index=False)}```\n")
L677         message += "Performance Comparison:\n```" + self.df_metrics_fmt.to_string() + "```"
L678         if self.debug and self.debug_table is not None:
L679             message += "\nDebug Data\n```" + self.debug_table.to_string() + "```"
L680         payload = {"text": message}
L681         try:
L682             resp = requests.post(SLACK_WEBHOOK_URL, json=payload); resp.raise_for_status(); print("âœ… Slackï¼ˆWebhookï¼‰ã¸é€ä¿¡ã—ã¾ã—ãŸ")
L683         except Exception as e: print(f"âš ï¸ Slacké€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
L684
L685
L686 def _infer_g_universe(feature_df, selected12=None, near5=None):
L687     try:
L688         out = feature_df.index[feature_df['group'].astype(str).str.upper().eq('G')].tolist()
L689         if out: return out
L690     except Exception:
L691         pass
L692     base = set()
L693     for lst in (selected12 or []), (near5 or []):
L694         for x in (lst or []): base.add(x)
L695     return list(base) if base else list(feature_df.index)
L696
L697
L698 def _fmt_with_fire_mark(tickers, feature_df):
L699     out = []
L700     for t in tickers or []:
L701         try:
L702             br = bool(feature_df.at[t, "G_BREAKOUT_recent_5d"])
L703             pb = bool(feature_df.at[t, "G_PULLBACK_recent_5d"])
L704             out.append(f"{t}{' ğŸ”¥' if (br or pb) else ''}")
L705         except Exception:
L706             out.append(t)
L707     return out
L708
L709
L710 def _label_recent_event(t, feature_df):
L711     try:
L712         br = bool(feature_df.at[t, "G_BREAKOUT_recent_5d"]); dbr = str(feature_df.at[t, "G_BREAKOUT_last_date"]) if br else ""
L713         pb = bool(feature_df.at[t, "G_PULLBACK_recent_5d"]); dpb = str(feature_df.at[t, "G_PULLBACK_last_date"]) if pb else ""
L714         if   br and not pb: return f"{t}ï¼ˆãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®š {dbr}ï¼‰"
L715         elif pb and not br: return f"{t}ï¼ˆæŠ¼ã—ç›®åç™º {dpb}ï¼‰"
L716         elif br and pb:     return f"{t}ï¼ˆãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®š {dbr}ï¼æŠ¼ã—ç›®åç™º {dpb}ï¼‰"
L717     except Exception:
L718         pass
L719     return t
L720
L721
L722 # === ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å¯è¦–åŒ–ï¼šG/Då…±é€šãƒ•ãƒ­ãƒ¼ï¼ˆå‡ºåŠ›ã¯ä¸å¤‰ï¼‰ ===
L723
L724 def io_build_input_bundle() -> InputBundle:
L725     """
L726     æ—¢å­˜ã®ã€ãƒ‡ãƒ¼ã‚¿å–å¾—â†’å‰å‡¦ç†ã€ã‚’å®Ÿè¡Œã—ã€InputBundle ã‚’è¿”ã™ã€‚
L727     å‡¦ç†å†…å®¹ãƒ»åˆ—åãƒ»ä¸¸ã‚ãƒ»ä¾‹å¤–ãƒ»ãƒ­ã‚°æ–‡è¨€ã¯ç¾è¡Œã©ãŠã‚Šï¼ˆå¤‰æ›´ç¦æ­¢ï¼‰ã€‚
L728     """
L729     inp = Input(cand=cand, exist=exist, bench=bench,
L730                 price_max=CAND_PRICE_MAX, finnhub_api_key=FINNHUB_API_KEY)
L731     state = inp.prepare_data()
L732     return InputBundle(
L733         cand=state["cand"], tickers=state["tickers"], bench=bench,
L734         data=state["data"], px=state["px"], spx=state["spx"],
L735         tickers_bulk=state["tickers_bulk"], info=state["info"],
L736         eps_df=state["eps_df"], fcf_df=state["fcf_df"],
L737         returns=state["returns"]
L738     )
L739
L740 def run_group(sc: Scorer, group: str, inb: InputBundle, cfg: PipelineConfig,
L741               n_target: int) -> tuple[list, float, float, float]:
L742     """
L743     G/Dã‚’åŒä¸€æ‰‹é †ã§å‡¦ç†ï¼šæ¡ç‚¹â†’ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼â†’é¸å®šï¼ˆç›¸é–¢ä½æ¸›è¾¼ã¿ï¼‰ã€‚
L744     æˆ»ã‚Šå€¤ï¼š(pick, avg_res_corr, sum_score, objective)
L745     JSONä¿å­˜ã¯æ—¢å­˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆã‚­ãƒ¼åãƒ»ä¸¸ã‚æ¡ãƒ»é †åºï¼‰ã‚’è¸è¥²ã€‚
L746     """
L747     sc.cfg = cfg
L748
L749     if hasattr(sc, "score_build_features"):
L750         feat = sc.score_build_features(inb)
L751         if not hasattr(sc, "_feat_logged"):
L752             T.log("features built (scorer)")
L753             sc._feat_logged = True
L754         agg = sc.score_aggregate(feat, group, cfg) if hasattr(sc, "score_aggregate") else feat
L755     else:
L756         fb = sc.aggregate_scores(inb, cfg)
L757         if not hasattr(sc, "_feat_logged"):
L758             T.log("features built (scorer)")
L759             sc._feat_logged = True
L760         sc._feat = fb
L761         agg = fb.g_score if group == "G" else fb.d_score_all
L762         if group == "D" and hasattr(fb, "df"):
L763             agg = agg[fb.df['BETA'] < D_BETA_MAX]
L764
L765     if hasattr(sc, "filter_candidates"):
L766         mask = sc.filter_candidates(inb, agg, group, cfg)
L767         agg = agg[mask]
L768
L769     selector = Selector()
L770     if hasattr(sc, "select_diversified"):
L771         pick, avg_r, sum_sc, obj = sc.select_diversified(
L772             agg, group, cfg, n_target,
L773             selector=selector, prev_tickers=None,
L774             corrM=cfg.drrs.corrM, shrink=cfg.drrs.shrink,
L775             cross_mu=cfg.drrs.cross_mu_gd
L776         )
L777     else:
L778         if group == "G":
L779             init = agg.nlargest(min(cfg.drrs.corrM, len(agg))).index.tolist()
L780             res = selector.select_bucket_drrs(
L781                 returns_df=inb.returns, score_ser=agg, pool_tickers=init, k=n_target,
L782                 n_pc=cfg.drrs.G.get("n_pc", 3), gamma=cfg.drrs.G.get("gamma", 1.2),
L783                 lam=cfg.drrs.G.get("lam", 0.68),
L784                 lookback=cfg.drrs.G.get("lookback", 252),
L785                 shrink=cfg.drrs.shrink, g_fixed_tickers=None, mu=0.0
L786             )
L787         else:
L788             init = agg.nlargest(min(cfg.drrs.corrM, len(agg))).index.tolist()
L789             g_fixed = getattr(sc, "_top_G", None)
L790             res = selector.select_bucket_drrs(
L791                 returns_df=inb.returns, score_ser=agg, pool_tickers=init, k=n_target,
L792                 n_pc=cfg.drrs.D.get("n_pc", 4), gamma=cfg.drrs.D.get("gamma", 0.8),
L793                 lam=cfg.drrs.D.get("lam", 0.85),
L794                 lookback=cfg.drrs.D.get("lookback", 504),
L795                 shrink=cfg.drrs.shrink, g_fixed_tickers=g_fixed,
L796                 mu=cfg.drrs.cross_mu_gd
L797             )
L798         pick = res["tickers"]; avg_r = res["avg_res_corr"]
L799         sum_sc = res["sum_score"]; obj = res["objective"]
L800         if group == "D":
L801             _, pick = _disjoint_keepG(getattr(sc, "_top_G", []), pick, init)
L802             T.log("selection finalized (G/D)")
L803     # --- Near-Miss: æƒœã—ãã‚‚é¸ã°ã‚Œãªã‹ã£ãŸä¸Šä½10ã‚’ä¿æŒï¼ˆSlackè¡¨ç¤ºç”¨ï¼‰ ---
L804     # 5) Near-Miss ã¨æœ€çµ‚é›†è¨ˆSeriesã‚’ä¿æŒï¼ˆè¡¨ç¤ºå°‚ç”¨ã€‚è¨ˆç®—ã¸å½±éŸ¿ãªã—ï¼‰
L805     try:
L806         pool = agg.drop(index=[t for t in pick if t in agg.index], errors="ignore")
L807         near10 = list(pool.sort_values(ascending=False).head(10).index)
L808         setattr(sc, f"_near_{group}", near10)
L809         setattr(sc, f"_agg_{group}", agg)
L810     except Exception:
L811         pass
L812
L813     if group == "D":
L814         T.log("save done")
L815     if group == "G":
L816         sc._top_G = pick
L817     return pick, avg_r, sum_sc, obj
L818
L819 def run_pipeline() -> SelectionBundle:
L820     """
L821     G/Då…±é€šãƒ•ãƒ­ãƒ¼ã®å…¥å£ã€‚I/Oã¯ã“ã“ã ã‘ã§å®Ÿæ–½ã—ã€è¨ˆç®—ã¯Scorerã«å§”è­²ã€‚
L822     Slackæ–‡è¨€ãƒ»ä¸¸ã‚ãƒ»é †åºã¯æ—¢å­˜ã® Output ã‚’ç”¨ã„ã¦å¤‰æ›´ã—ãªã„ã€‚
L823     """
L824     inb = io_build_input_bundle()
L825     cfg = PipelineConfig(
L826         weights=WeightsConfig(g=g_weights, d=D_weights),
L827         drrs=DRRSParams(corrM=corrM, shrink=DRRS_SHRINK,
L828                          G=DRRS_G, D=DRRS_D, cross_mu_gd=CROSS_MU_GD),
L829         price_max=CAND_PRICE_MAX
L830     )
L831     sc = Scorer()
L832     top_G, avgG, sumG, objG = run_group(sc, "G", inb, cfg, N_G)
L833     poolG = list(getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False).index)
L834     alpha = Scorer.spx_to_alpha(inb.spx)
L835     sectors = {t: (inb.info.get(t, {}).get("sector") or "U") for t in poolG}
L836     scores = {t: Scorer.g_score.get(t, 0.0) for t in poolG}
L837     top_G = Scorer.pick_top_softcap(scores, sectors, N=N_G, cap=2, alpha=alpha, hard=5)
L838     sc._top_G = top_G
L839     try:
L840         aggG = getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False)
L841         sc._near_G = [t for t in aggG.index if t not in set(top_G)][:10]
L842     except Exception:
L843         pass
L844     base = sum(Scorer.g_score.get(t,0.0) for t in poolG[:N_G])
L845     effs = sum(Scorer.g_score.get(t,0.0) for t in top_G)
L846     print(f"[soft_cap2] score_cost={(base-effs)/max(1e-9,abs(base)):.2%}, alpha={alpha:.3f}")
L847     top_D, avgD, sumD, objD = run_group(sc, "D", inb, cfg, N_D)
L848     fb = getattr(sc, "_feat", None)
L849     near_G = getattr(sc, "_near_G", [])
L850     selected12 = list(top_G)
L851     df = fb.df if fb is not None else pd.DataFrame()
L852     guni = _infer_g_universe(df, selected12, near_G)
L853     try:
L854         fire_recent = [t for t in guni
L855                        if (str(df.at[t, "G_BREAKOUT_recent_5d"]) == "True") or
L856                           (str(df.at[t, "G_PULLBACK_recent_5d"]) == "True")]
L857     except Exception: fire_recent = []
L858
L859     # === å…ˆé ­ãƒ˜ãƒƒãƒ€ï¼ˆãƒ¢ãƒ¼ãƒ‰ãƒ»ã—ãã„å€¤ãƒ»åˆ†ä½ï¼‰ã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ–ãƒ­ãƒƒã‚¯åŒ–ã—ã¦å·®ã—è¾¼ã¿ ===
L860     try:
L861         lead_lines, _mode = _build_breadth_lead_lines(inb)  # æ—¢å­˜ã®é–¢æ•°ï¼ˆä»¥å‰ã®æ”¹ä¿®ã§è¿½åŠ æ¸ˆã¿ï¼‰
L862         head_block = "```" + "\n".join(lead_lines) + "```"
L863     except Exception: head_block = ""  # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ï¼ˆãƒ˜ãƒƒãƒ€ãªã—ã§ã‚‚å¾Œç¶šã¯ç¶™ç¶šï¼‰
L864
L865     lines = [
L866         head_block,
L867         "ã€Gæ ãƒ¬ãƒãƒ¼ãƒˆï½œé€±æ¬¡ãƒ¢ãƒ‹ã‚¿ï¼ˆç›´è¿‘5å–¶æ¥­æ—¥ï¼‰ã€‘",
L868         "ã€å‡¡ä¾‹ã€‘ğŸ”¥=ç›´è¿‘5å–¶æ¥­æ—¥å†…ã«ã€Œãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®šã€ã¾ãŸã¯ã€ŒæŠ¼ã—ç›®åç™ºã€ã‚’æ¤œçŸ¥",
L869         f"é¸å®š12: {', '.join(_fmt_with_fire_mark(selected12, df))}" if selected12 else "é¸å®š12: ãªã—",
L870         f"æ¬¡ç‚¹10: {', '.join(_fmt_with_fire_mark(near_G, df))}" if near_G else "æ¬¡ç‚¹10: ãªã—",
L871     ]
L872
L873     if fire_recent:
L874         fire_list = ", ".join([_label_recent_event(t, df) for t in fire_recent])
L875         lines.append(f"éå»5å–¶æ¥­æ—¥ã®æ¤œçŸ¥: {fire_list}")
L876     else:
L877         lines.append("éå»5å–¶æ¥­æ—¥ã®æ¤œçŸ¥: ãªã—")
L878
L879     try:
L880         webhook = os.environ.get("SLACK_WEBHOOK_URL", "")
L881         if webhook:
L882             # å…ˆé ­ã® head_block ã‚’å«ã‚€è¤‡æ•°è¡Œã‚’ãã®ã¾ã¾é€ä¿¡ï¼ˆSlackå´ã§```ãŒã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã¨ã—ã¦æç”»ï¼‰
L883             requests.post(webhook, json={"text": "\n".join([s for s in lines if s != ""] )}, timeout=10)
L884     except Exception:
L885         pass
L886
L887     out = Output(debug=debug_mode)
L888     # è¡¨ç¤ºå´ã‹ã‚‰é¸å®šæ™‚ã®é›†è¨ˆã¸ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†ã«ä¿æŒï¼ˆè¡¨ç¤ºå°‚ç”¨ãƒ»å‰¯ä½œç”¨ãªã—ï¼‰
L889     try: out._sc = sc
L890     except Exception: pass
L891     if hasattr(sc, "_feat"):
L892         try:
L893             out.miss_df = sc._feat.missing_logs
L894             out.display_results(
L895                 exist=exist, bench=bench, df_z=sc._feat.df_z,
L896                 g_score=sc._feat.g_score, d_score_all=sc._feat.d_score_all,
L897                 init_G=top_G, init_D=top_D, top_G=top_G, top_D=top_D
L898             )
L899         except Exception:
L900             pass
L901     out.notify_slack()
L902     sb = SelectionBundle(
L903         resG={"tickers": top_G, "avg_res_corr": avgG,
L904               "sum_score": sumG, "objective": objG},
L905         resD={"tickers": top_D, "avg_res_corr": avgD,
L906               "sum_score": sumD, "objective": objD},
L907         top_G=top_G, top_D=top_D, init_G=top_G, init_D=top_D
L908     )
L909
L910     # --- Low Score Candidates (GSC+DSC bottom 10) : send before debug dump ---
L911     try:
L912         _low_df = (
L913             pd.DataFrame({"GSC": fb.g_score, "DSC": fb.d_score_all})
L914               .assign(G_plus_D=lambda x: x["GSC"] + x["DSC"])
L915               .sort_values("G_plus_D")
L916               .head(10)
L917               .round(3)
L918         )
L919         _slack("Low Score Candidates (GSC+DSC bottom 10)\n"
L920                "```"
L921                + _low_df.to_string(index=True, index_names=False)
L922                + "\n```")
L923     except Exception as _e:
L924         _slack(f"Low Score Candidates: ä½œæˆå¤±æ•—: {_e}")
L925
L926     if debug_mode:
L927         try:
L928             _slack_debug(_compact_debug(fb, sb, [], []))
L929         except Exception as e:
L930             print(f"[debug skipped] {e}")
L931
L932     return sb
L933
L934 if __name__ == "__main__":
L935     run_pipeline()
```

## <scorer.py>
```text
L1 # scorer.py 
L2 # kawatest
L3 # =============================================================================
L4 # Scorer: ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼/æŒ‡æ¨™ã®ç”Ÿæˆã¨åˆæˆã‚¹ã‚³ã‚¢ç®—å‡ºã‚’æ‹…ã†ç´”ç²‹å±¤
L5 #
L6 # ã€ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã ã‘èª­ã‚ã°åˆ†ã‹ã‚‹ãƒã‚¤ãƒ³ãƒˆã€‘
L7 # - å…¥åŠ›(InputBundle)ã¯ã€Œä¾¡æ ¼/å‡ºæ¥é«˜/ãƒ™ãƒ³ãƒ/åŸºæœ¬æƒ…å ±/EPS/FCF/ãƒªã‚¿ãƒ¼ãƒ³ã€ã‚’å«ã‚€DTO
L8 # - å‡ºåŠ›(FeatureBundle)ã¯ã€Œrawç‰¹å¾´é‡ dfã€ã€Œæ¨™æº–åŒ– df_zã€ã€ŒG/D ã‚¹ã‚³ã‚¢ã€ã€Œæ¬ æãƒ­ã‚°ã€
L9 # - é‡ã¿ç­‰ã®ã‚³ãƒ³ãƒ•ã‚£ã‚°(PipelineConfig)ã¯ factor ã‹ã‚‰æ¸¡ã™ï¼ˆcfg å¿…é ˆï¼‰
L10 # - æ—§ã‚«ãƒ©ãƒ åã¯ Scorer å†…ã§è‡ªå‹•ãƒªãƒãƒ¼ãƒ ã—ã¦å—ã‘å…¥ã‚Œï¼ˆå¾Œæ–¹äº’æ›ï¼‰
L11 #   ä¾‹) eps_ttm -> EPS_TTM, eps_q_recent -> EPS_Q_LastQ, fcf_ttm -> FCF_TTM
L12 #
L13 # ã€I/Oå¥‘ç´„ï¼ˆScorerãŒå‚ç…§ã™ã‚‹InputBundleãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰ã€‘
L14 #   - cand: List[str]    â€¦ å€™è£œéŠ˜æŸ„ï¼ˆå˜ä½“å®Ÿè¡Œã§ã¯æœªä½¿ç”¨ï¼‰
L15 #   - tickers: List[str] â€¦ å¯¾è±¡éŠ˜æŸ„ãƒªã‚¹ãƒˆ
L16 #   - bench: str         â€¦ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼ï¼ˆä¾‹ '^GSPC'ï¼‰
L17 #   - data: pd.DataFrame â€¦ yfinance downloadçµæœ ('Close','Volume' ç­‰ã®éšå±¤åˆ—)
L18 #   - px: pd.DataFrame   â€¦ data['Close'] ç›¸å½“ï¼ˆçµ‚å€¤ï¼‰
L19 #   - spx: pd.Series     â€¦ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®çµ‚å€¤
L20 #   - tickers_bulk: object         â€¦ yfinance.Tickers
L21 #   - info: Dict[str, dict]        â€¦ yfinance info per ticker
L22 #   - eps_df: pd.DataFrame         â€¦ å¿…é ˆåˆ—: EPS_TTM, EPS_Q_LastQï¼ˆæ—§åã‚‚å¯ï¼‰
L23 #   - fcf_df: pd.DataFrame         â€¦ å¿…é ˆåˆ—: FCF_TTMï¼ˆæ—§åã‚‚å¯ï¼‰
L24 #   - returns: pd.DataFrame        â€¦ px[tickers].pct_change() ç›¸å½“
L25 #
L26 # â€»å…¥å‡ºåŠ›ã®å½¢å¼ãƒ»ä¾‹å¤–æ–‡è¨€ã¯æ—¢å­˜å®Ÿè£…ã‚’å¤‰ãˆã¾ã›ã‚“ï¼ˆå®‰å…¨ãªçŸ­ç¸®ã®ã¿ï¼‰
L27 # =============================================================================
L28
L29 import os, sys, warnings
L30 import requests
L31 import numpy as np
L32 import pandas as pd
L33 import yfinance as yf
L34 from typing import Any, TYPE_CHECKING
L35 from scipy.stats import zscore
L36
L37 if TYPE_CHECKING:
L38     from factor import PipelineConfig  # type: ignore  # å®Ÿè¡Œæ™‚importãªã—ï¼ˆå¾ªç’°å›é¿ï¼‰
L39
L40 # ---- Dividend Helpers -------------------------------------------------------
L41 def _last_close(t, price_map=None):
L42     if price_map and (c := price_map.get(t)) is not None:
L43         return float(c)
L44     try:
L45         h = yf.Ticker(t).history(period="5d")["Close"].dropna()
L46         return float(h.iloc[-1]) if len(h) else np.nan
L47     except Exception:
L48         return np.nan
L49
L50 def _ttm_div_sum(t, lookback_days=400):
L51     try:
L52         div = yf.Ticker(t).dividends
L53         if div is None or len(div) == 0:
L54             return 0.0
L55         cutoff = pd.Timestamp.utcnow().tz_localize(None) - pd.Timedelta(days=lookback_days)
L56         ttm = float(div[div.index.tz_localize(None) >= cutoff].sum())
L57         return ttm if ttm > 0 else float(div.tail(4).sum())
L58     except Exception:
L59         return 0.0
L60
L61 def ttm_div_yield_portfolio(tickers, price_map=None):
L62     ys = [(lambda c, s: (s/c) if (np.isfinite(c) and c>0 and s>0) else 0.0)(_last_close(t, price_map), _ttm_div_sum(t)) for t in tickers]
L63     return float(np.mean(ys)) if ys else 0.0
L64
L65 # ---- ç°¡æ˜“ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆå®‰å…¨ãªçŸ­ç¸®ã®ã¿ï¼‰ -----------------------------------
L66 def winsorize_s(s: pd.Series, p=0.02):
L67     if s is None or s.dropna().empty: return s
L68     lo, hi = np.nanpercentile(s.astype(float), [100*p, 100*(1-p)]); return s.clip(lo, hi)
L69
L70 def robust_z(s: pd.Series, p=0.02):
L71     s2=winsorize_s(s,p); return np.nan_to_num(zscore(s2.fillna(s2.mean())))
L72
L73 def _safe_div(a, b):
L74     try: return np.nan if (b is None or float(b)==0 or pd.isna(b)) else float(a)/float(b)
L75     except Exception: return np.nan
L76
L77 def _safe_last(series: pd.Series, default=np.nan):
L78     try: return float(series.iloc[-1])
L79     except Exception: return default
L80
L81 D_WEIGHTS_EFF = None  # å‡ºåŠ›è¡¨ç¤ºäº’æ›ã®ãŸã‚
L82
L83 # ---- Scorer æœ¬ä½“ -------------------------------------------------------------
L84 class Scorer:
L85     """
L86     - factor.py ã‹ã‚‰ã¯ `aggregate_scores(ib, cfg)` ã‚’å‘¼ã¶ã ã‘ã§OKã€‚
L87     - cfg ã¯å¿…é ˆï¼ˆfactor.PipelineConfig ã‚’æ¸¡ã™ï¼‰ã€‚
L88     - æ—§ã‚«ãƒ©ãƒ åã‚’è‡ªå‹•ãƒªãƒãƒ¼ãƒ ã—ã¦æ–°ã‚¹ã‚­ãƒ¼ãƒã«å¸åã—ã¾ã™ã€‚
L89     """
L90
L91     # === å…ˆé ­ã§æ—§â†’æ–°ã‚«ãƒ©ãƒ åãƒãƒƒãƒ—ï¼ˆç§»è¡Œç”¨ï¼‰ ===
L92     EPS_RENAME = {"eps_ttm":"EPS_TTM", "eps_q_recent":"EPS_Q_LastQ"}
L93     FCF_RENAME = {"fcf_ttm":"FCF_TTM"}
L94
L95     # === ã‚¹ã‚­ãƒ¼ãƒç°¡æ˜“ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€ä½é™ï¼‰ ===
L96     @staticmethod
L97     def _validate_ib_for_scorer(ib: Any):
L98         miss = [a for a in ["tickers","bench","data","px","spx","tickers_bulk","info","eps_df","fcf_df","returns"] if not hasattr(ib,a) or getattr(ib,a) is None]
L99         if miss: raise ValueError(f"InputBundle is missing required attributes for Scorer: {miss}")
L100         if any(c in ib.eps_df.columns for c in Scorer.EPS_RENAME): ib.eps_df.rename(columns=Scorer.EPS_RENAME, inplace=True)
L101         if any(c in ib.fcf_df.columns for c in Scorer.FCF_RENAME): ib.fcf_df.rename(columns=Scorer.FCF_RENAME, inplace=True)
L102         need_eps, need_fcf = {"EPS_TTM","EPS_Q_LastQ"},{"FCF_TTM"}
L103         if not need_eps.issubset(ib.eps_df.columns): raise ValueError(f"eps_df must contain columns {need_eps} (accepts old names via auto-rename). Got: {list(ib.eps_df.columns)}")
L104         if not need_fcf.issubset(ib.fcf_df.columns): raise ValueError(f"fcf_df must contain columns {need_fcf} (accepts old names via auto-rename). Got: {list(ib.fcf_df.columns)}")
L105
L106     # ----ï¼ˆScorerå°‚ç”¨ï¼‰ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ»æŒ‡æ¨™ç³» ----
L107     @staticmethod
L108     def trend(s: pd.Series):
L109         if len(s)<200: return np.nan
L110         sma50, sma150, sma200 = s.rolling(50).mean().iloc[-1], s.rolling(150).mean().iloc[-1], s.rolling(200).mean().iloc[-1]
L111         prev200, p = s.rolling(200).mean().iloc[-21], s.iloc[-1]
L112         lo_52 = s[-252:].min() if len(s)>=252 else s.min(); hi_52 = s[-252:].max() if len(s)>=252 else s.max()
L113         rng = (hi_52 - lo_52) if hi_52>lo_52 else np.nan
L114         clip = lambda x,lo,hi: (np.nan if pd.isna(x) else max(lo,min(hi,x)))
L115         a = clip(p/(s.rolling(50).mean().iloc[-1]) - 1, -0.5, 0.5)
L116         b = clip(sma50/sma150 - 1, -0.5, 0.5)
L117         c = clip(sma150/sma200 - 1, -0.5, 0.5)
L118         d = clip(sma200/prev200 - 1, -0.2, 0.2)
L119         e = clip((p - lo_52) / (rng if rng and rng>0 else np.nan) - 0.5, -0.5, 0.5)
L120         parts = [0.0 if pd.isna(x) else x for x in (a,b,c,d,e)]
L121         return 0.30*parts[0] + 0.20*parts[1] + 0.15*parts[2] + 0.15*parts[3] + 0.20*parts[4]
L122
L123     @staticmethod
L124     def rs(s, b):
L125         n, nb = len(s), len(b)
L126         if n<60 or nb<60: return np.nan
L127         L12 = 252 if n>=252 and nb>=252 else min(n,nb)-1; L1 = 22 if n>=22 and nb>=22 else max(5, min(n,nb)//3)
L128         r12, r1, br12, br1 = s.iloc[-1]/s.iloc[-L12]-1, s.iloc[-1]/s.iloc[-L1]-1, b.iloc[-1]/b.iloc[-L12]-1, b.iloc[-1]/b.iloc[-L1]-1
L129         return (r12 - br12)*0.7 + (r1 - br1)*0.3
L130
L131     @staticmethod
L132     def tr_str(s):
L133         if len(s)<50: return np.nan
L134         return s.iloc[-1]/s.rolling(50).mean().iloc[-1] - 1
L135
L136     @staticmethod
L137     def rs_line_slope(s: pd.Series, b: pd.Series, win: int) -> float:
L138         r = (s/b).dropna()
L139         if len(r) < win: return np.nan
L140         y, x = np.log(r.iloc[-win:]), np.arange(win, dtype=float)
L141         try: return float(np.polyfit(x, y, 1)[0])
L142         except Exception: return np.nan
L143
L144     @staticmethod
L145     def ev_fallback(info_t: dict, tk: yf.Ticker) -> float:
L146         ev = info_t.get('enterpriseValue', np.nan)
L147         if pd.notna(ev) and ev>0: return float(ev)
L148         mc, debt, cash = info_t.get('marketCap', np.nan), np.nan, np.nan
L149         try:
L150             bs = tk.quarterly_balance_sheet
L151             if bs is not None and not bs.empty:
L152                 c = bs.columns[0]
L153                 for k in ("Total Debt","Long Term Debt","Short Long Term Debt"):
L154                     if k in bs.index: debt = float(bs.loc[k,c]); break
L155                 for k in ("Cash And Cash Equivalents","Cash And Cash Equivalents And Short Term Investments","Cash"):
L156                     if k in bs.index: cash = float(bs.loc[k,c]); break
L157         except Exception: pass
L158         if pd.notna(mc): return float(mc + (0 if pd.isna(debt) else debt) - (0 if pd.isna(cash) else cash))
L159         return np.nan
L160
L161     @staticmethod
L162     def dividend_status(ticker: str) -> str:
L163         t = yf.Ticker(ticker)
L164         try:
L165             if not t.dividends.empty: return "has"
L166         except Exception: return "unknown"
L167         try:
L168             a = t.actions
L169             if (a is not None and not a.empty and "Stock Splits" in a.columns and a["Stock Splits"].abs().sum()>0): return "none_confident"
L170         except Exception: pass
L171         try:
L172             fi = t.fast_info
L173             if any(getattr(fi,k,None) for k in ("last_dividend_date","dividend_rate","dividend_yield")): return "maybe_missing"
L174         except Exception: pass
L175         return "unknown"
L176
L177     @staticmethod
L178     def div_streak(t):
L179         try:
L180             divs = yf.Ticker(t).dividends.dropna(); ann = divs.groupby(divs.index.year).sum(); ann = ann[ann.index<pd.Timestamp.today().year]
L181             years, streak = sorted(ann.index), 0
L182             for i in range(len(years)-1,0,-1):
L183                 if ann[years[i]] > ann[years[i-1]]: streak += 1
L184                 else: break
L185             return streak
L186         except Exception: return 0
L187
L188     @staticmethod
L189     def fetch_finnhub_metrics(symbol):
L190         api_key = os.environ.get("FINNHUB_API_KEY")
L191         if not api_key: return {}
L192         url, params = "https://finnhub.io/api/v1/stock/metric", {"symbol":symbol,"metric":"all","token":api_key}
L193         try:
L194             r = requests.get(url, params=params, timeout=10); r.raise_for_status(); m = r.json().get("metric",{})
L195             return {'EPS':m.get('epsGrowthTTMYoy'),'REV':m.get('revenueGrowthTTMYoy'),'ROE':m.get('roeTTM'),'BETA':m.get('beta'),'DIV':m.get('dividendYieldIndicatedAnnual'),'FCF':(m.get('freeCashFlowTTM')/m.get('enterpriseValue')) if m.get('freeCashFlowTTM') and m.get('enterpriseValue') else None}
L196         except Exception: return {}
L197
L198     @staticmethod
L199     def calc_beta(series: pd.Series, market: pd.Series, lookback=252):
L200         r, m = series.pct_change().dropna(), market.pct_change().dropna()
L201         n = min(len(r), len(m), lookback)
L202         if n<60: return np.nan
L203         r, m = r.iloc[-n:], m.iloc[-n:]; cov, var = np.cov(r, m)[0,1], np.var(m)
L204         return np.nan if var==0 else cov/var
L205
L206     @staticmethod
L207     def spx_to_alpha(spx: pd.Series, bands=(0.03,0.10), w=(0.6,0.4),
L208                      span=5, q=(0.20,0.40), alphas=(0.05,0.08,0.10)) -> float:
L209         """
L210         S&P500æŒ‡æ•°ã®ã¿ã‹ã‚‰æ“¬ä¼¼breadthã‚’ä½œã‚Šã€å±¥æ­´åˆ†ä½ã§Î±ã‚’æ®µéšæ±ºå®šã€‚
L211         bands=(Â±3%, Â±10%), w=(50DMA,200DMA), åˆ†ä½q=(20%,40%), alphas=(ä½,ä¸­,é«˜)
L212         """
L213         ma50, ma200 = spx.rolling(50).mean(), spx.rolling(200).mean()
L214         b50  = ((spx/ma50 - 1) + bands[0])/(2*bands[0])
L215         b200 = ((spx/ma200 - 1) + bands[1])/(2*bands[1])
L216         hist = (w[0]*b50 + w[1]*b200).clip(0,1).ewm(span=span).mean()
L217         b = float(hist.iloc[-1])
L218         lo, mid = float(hist.quantile(q[0])), float(hist.quantile(q[1]))
L219         return alphas[0] if b < lo else alphas[1] if b < mid else alphas[2]
L220
L221     @staticmethod
L222     def soft_cap_effective_scores(scores: pd.Series|dict, sectors: dict, cap=2, alpha=0.08) -> pd.Series:
L223         """
L224         åŒä¸€ã‚»ã‚¯ã‚¿ãƒ¼capè¶…éï¼ˆ3æœ¬ç›®ä»¥é™ï¼‰ã« Î±Ã—æ®µéšæ¸›ç‚¹ã‚’èª²ã—ãŸâ€œæœ‰åŠ¹ã‚¹ã‚³ã‚¢â€Seriesã‚’è¿”ã™ã€‚
L225         æˆ»ã‚Šå€¤ã¯é™é †ã‚½ãƒ¼ãƒˆæ¸ˆã¿ã€‚
L226         """
L227         s = pd.Series(scores, dtype=float); order = s.sort_values(ascending=False).index
L228         cnt, pen = {}, {}
L229         for t in order:
L230             sec = sectors.get(t, "U")
L231             k = cnt.get(sec, 0) + 1
L232             pen[t] = alpha * max(0, k - cap)
L233             cnt[sec] = k
L234         return (s - pd.Series(pen)).sort_values(ascending=False)
L235
L236     @staticmethod
L237     def pick_top_softcap(scores: pd.Series|dict, sectors: dict, N: int, cap=2, alpha=0.08, hard: int|None=5) -> list[str]:
L238         """
L239         soft-capé©ç”¨å¾Œã®ä¸Šä½Nãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚’è¿”ã™ã€‚hard>0ãªã‚‰éå¸¸ç”¨ãƒãƒ¼ãƒ‰ä¸Šé™ã§åŒä¸€ã‚»ã‚¯ã‚¿ãƒ¼è¶…éã‚’é–“å¼•ãï¼ˆæ—¢å®š=5ï¼‰ã€‚
L240         """
L241         eff = Scorer.soft_cap_effective_scores(scores, sectors, cap, alpha)
L242         if not hard:
L243             return list(eff.head(N).index)
L244         pick, used = [], {}
L245         for t in eff.index:
L246             s = sectors.get(t, "U")
L247             if used.get(s, 0) < hard:
L248                 pick.append(t)
L249                 used[s] = used.get(s, 0) + 1
L250             if len(pick) == N:
L251                 break
L252         return pick
L253
L254     @staticmethod
L255     def trend_template_breadth_series(px: pd.DataFrame, spx: pd.Series, win_days: int | None = None) -> pd.Series:
L256         """
L257         å„å–¶æ¥­æ—¥ã® trend_template åˆæ ¼æœ¬æ•°ï¼ˆåˆæ ¼â€œæœ¬æ•°â€=Cï¼‰ã‚’è¿”ã™ã€‚
L258         - px: åˆ—=tickerï¼ˆãƒ™ãƒ³ãƒã¯å«ã‚ãªã„ï¼‰
L259         - spx: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ Seriesï¼ˆpx.index ã«æ•´åˆ—ï¼‰
L260         - win_days: æœ«å°¾ã®è¨ˆç®—å¯¾è±¡å–¶æ¥­æ—¥æ•°ï¼ˆNoneâ†’å…¨ä½“ã€æ—¢å®š600ã¯å‘¼ã³å‡ºã—å´æŒ‡å®šï¼‰
L261         ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼†rollingã®ã¿ã§è»½é‡ã€‚æ¬ æã¯ False æ‰±ã„ã€‚
L262         """
L263         import numpy as np, pandas as pd
L264         if px is None or px.empty:
L265             return pd.Series(dtype=int)
L266         px = px.dropna(how="all", axis=1)
L267         if win_days and win_days > 0:
L268             px = px.tail(win_days)
L269         if px.empty:
L270             return pd.Series(dtype=int)
L271         spx = spx.reindex(px.index).ffill()
L272
L273         ma50  = px.rolling(50).mean()
L274         ma150 = px.rolling(150).mean()
L275         ma200 = px.rolling(200).mean()
L276
L277         tt = (px > ma150)
L278         tt &= (px > ma200)
L279         tt &= (ma150 > ma200)
L280         tt &= (ma200 - ma200.shift(21) > 0)
L281         tt &= (ma50  > ma150)
L282         tt &= (ma50  > ma200)
L283         tt &= (px    > ma50)
L284
L285         lo252 = px.rolling(252).min()
L286         hi252 = px.rolling(252).max()
L287         tt &= (px.divide(lo252).sub(1.0) >= 0.30)   # P_OVER_LOW52 >= 0.30
L288         tt &= (px >= (0.75 * hi252))                # NEAR_52W_HIGH >= -0.25
L289
L290         r12  = px.divide(px.shift(252)).sub(1.0)
L291         br12 = spx.divide(spx.shift(252)).sub(1.0)
L292         r1   = px.divide(px.shift(22)).sub(1.0)
L293         br1  = spx.divide(spx.shift(22)).sub(1.0)
L294         rs   = 0.7*(r12.sub(br12, axis=0)) + 0.3*(r1.sub(br1, axis=0))
L295         tt &= (rs >= 0.10)
L296
L297         return tt.fillna(False).sum(axis=1).astype(int)
L298
L299     # ---- ã‚¹ã‚³ã‚¢é›†è¨ˆï¼ˆDTO/Configã‚’å—ã‘å–ã‚Šã€FeatureBundleã‚’è¿”ã™ï¼‰ ----
L300     def aggregate_scores(self, ib: Any, cfg):
L301         if cfg is None:
L302             raise ValueError("cfg is required; pass factor.PipelineConfig")
L303         self._validate_ib_for_scorer(ib)
L304
L305         px, spx, tickers = ib.px, ib.spx, ib.tickers
L306         tickers_bulk, info, eps_df, fcf_df = ib.tickers_bulk, ib.info, ib.eps_df, ib.fcf_df
L307
L308         df, missing_logs = pd.DataFrame(index=tickers), []
L309         for t in tickers:
L310             d, s = info[t], px[t]; ev = self.ev_fallback(d, tickers_bulk.tickers[t])
L311             # --- åŸºæœ¬ç‰¹å¾´ ---
L312             df.loc[t,'TR']   = self.trend(s)
L313             df.loc[t,'EPS']  = eps_df.loc[t,'EPS_TTM'] if t in eps_df.index else np.nan
L314             df.loc[t,'REV']  = d.get('revenueGrowth',np.nan)
L315             df.loc[t,'ROE']  = d.get('returnOnEquity',np.nan)
L316             df.loc[t,'BETA'] = self.calc_beta(s, spx, lookback=252)
L317
L318             # --- é…å½“ï¼ˆæ¬ æè£œå®Œå«ã‚€ï¼‰ ---
L319             div = d.get('dividendYield') if d.get('dividendYield') is not None else d.get('trailingAnnualDividendYield')
L320             if div is None or pd.isna(div):
L321                 try:
L322                     divs = yf.Ticker(t).dividends
L323                     if divs is not None and not divs.empty:
L324                         last_close = s.iloc[-1]; div_1y = divs[divs.index >= (divs.index.max() - pd.Timedelta(days=365))].sum()
L325                         if last_close and last_close>0: div = float(div_1y/last_close)
L326                 except Exception: pass
L327             df.loc[t,'DIV'] = 0.0 if (div is None or pd.isna(div)) else float(div)
L328
L329             # --- FCF/EV ---
L330             fcf_val = fcf_df.loc[t,'FCF_TTM'] if t in fcf_df.index else np.nan
L331             df.loc[t,'FCF'] = (fcf_val/ev) if (pd.notna(fcf_val) and pd.notna(ev) and ev>0) else np.nan
L332
L333             # --- ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ãƒ»ãƒœãƒ©é–¢é€£ ---
L334             df.loc[t,'RS'], df.loc[t,'TR_str'] = self.rs(s, spx), self.tr_str(s)
L335             r, rm = s.pct_change().dropna(), spx.pct_change().dropna()
L336             n = int(min(len(r), len(rm)))
L337
L338             DOWNSIDE_DEV = np.nan
L339             if n>=60:
L340                 r6 = r.iloc[-min(len(r),126):]; neg = r6[r6<0]
L341                 if len(neg)>=10: DOWNSIDE_DEV = float(neg.std(ddof=0)*np.sqrt(252))
L342             df.loc[t,'DOWNSIDE_DEV'] = DOWNSIDE_DEV
L343
L344             MDD_1Y = np.nan
L345             try:
L346                 w = s.iloc[-min(len(s),252):].dropna()
L347                 if len(w)>=30:
L348                     roll_max = w.cummax(); MDD_1Y = float((w/roll_max - 1.0).min())
L349             except Exception: pass
L350             df.loc[t,'MDD_1Y'] = MDD_1Y
L351
L352             RESID_VOL = np.nan
L353             if n>=120:
L354                 rr, rrm = r.iloc[-n:].align(rm.iloc[-n:], join='inner')
L355                 if len(rr)==len(rrm) and len(rr)>=120 and rrm.var()>0:
L356                     beta = float(np.cov(rr, rrm)[0,1]/np.var(rrm)); resid = rr - beta*rrm
L357                     RESID_VOL = float(resid.std(ddof=0)*np.sqrt(252))
L358             df.loc[t,'RESID_VOL'] = RESID_VOL
L359
L360             DOWN_OUTPERF = np.nan
L361             if n>=60:
L362                 m, x = rm.iloc[-n:], r.iloc[-n:]; mask = m<0
L363                 if mask.sum()>=10:
L364                     mr, sr = float(m[mask].mean()), float(x[mask].mean())
L365                     DOWN_OUTPERF = (sr - mr)/abs(mr) if mr!=0 else np.nan
L366             df.loc[t,'DOWN_OUTPERF'] = DOWN_OUTPERF
L367
L368             # --- é•·æœŸç§»å‹•å¹³å‡/ä½ç½® ---
L369             sma200 = s.rolling(200).mean(); df.loc[t,'EXT_200'] = np.nan
L370             if pd.notna(sma200.iloc[-1]) and sma200.iloc[-1]!=0: df.loc[t,'EXT_200'] = abs(float(s.iloc[-1]/sma200.iloc[-1]-1.0))
L371
L372             # --- é…å½“ã®è©³ç´°ç³» ---
L373             DIV_TTM_PS=DIV_VAR5=DIV_YOY=DIV_FCF_COVER=np.nan
L374             try:
L375                 divs = yf.Ticker(t).dividends.dropna()
L376                 if not divs.empty:
L377                     last_close = s.iloc[-1]; div_1y = float(divs[divs.index >= (divs.index.max()-pd.Timedelta(days=365))].sum())
L378                     DIV_TTM_PS = div_1y if div_1y>0 else np.nan
L379                     ann = divs.groupby(divs.index.year).sum()
L380                     if len(ann)>=2 and ann.iloc[-2]!=0: DIV_YOY = float(ann.iloc[-1]/ann.iloc[-2]-1.0)
L381                     tail = ann.iloc[-5:] if len(ann)>=5 else ann
L382                     if len(tail)>=3 and tail.mean()!=0: DIV_VAR5 = float(tail.std(ddof=1)/abs(tail.mean()))
L383                 so = d.get('sharesOutstanding',None)
L384                 if so and pd.notna(DIV_TTM_PS) and pd.notna(fcf_val) and fcf_val!=0:
L385                     DIV_FCF_COVER = float((fcf_val)/(DIV_TTM_PS*float(so)))
L386             except Exception: pass
L387             df.loc[t,'DIV_TTM_PS'], df.loc[t,'DIV_VAR5'], df.loc[t,'DIV_YOY'], df.loc[t,'DIV_FCF_COVER'] = DIV_TTM_PS, DIV_VAR5, DIV_YOY, DIV_FCF_COVER
L388
L389             # --- è²¡å‹™å®‰å®šæ€§ ---
L390             df.loc[t,'DEBT2EQ'], df.loc[t,'CURR_RATIO'] = d.get('debtToEquity',np.nan), d.get('currentRatio',np.nan)
L391
L392             # --- EPS å¤‰å‹• ---
L393             EPS_VAR_8Q = np.nan
L394             try:
L395                 qe, so = tickers_bulk.tickers[t].quarterly_earnings, d.get('sharesOutstanding',None)
L396                 if qe is not None and not qe.empty and so:
L397                     eps_q = (qe['Earnings'].dropna().astype(float)/float(so)).replace([np.inf,-np.inf],np.nan)
L398                     if len(eps_q)>=4: EPS_VAR_8Q = float(eps_q.iloc[-min(8,len(eps_q)):].std(ddof=1))
L399             except Exception: pass
L400             df.loc[t,'EPS_VAR_8Q'] = EPS_VAR_8Q
L401
L402             # --- ã‚µã‚¤ã‚º/æµå‹•æ€§ ---
L403             df.loc[t,'MARKET_CAP'] = d.get('marketCap',np.nan); adv60 = np.nan
L404             try:
L405                 vol_series = ib.data['Volume'][t].dropna()
L406                 if len(vol_series)>=5 and len(s)==len(vol_series):
L407                     dv = (vol_series*s).rolling(60).mean(); adv60 = float(dv.iloc[-1])
L408             except Exception: pass
L409             df.loc[t,'ADV60_USD'] = adv60
L410
L411             # --- å£²ä¸Š/åˆ©ç›Šã®åŠ é€Ÿåº¦ç­‰ ---
L412             REV_Q_YOY=EPS_Q_YOY=REV_YOY_ACC=REV_YOY_VAR=np.nan
L413             REV_ANNUAL_STREAK = np.nan
L414             try:
L415                 qe, so = tickers_bulk.tickers[t].quarterly_earnings, d.get('sharesOutstanding',None)
L416                 if qe is not None and not qe.empty:
L417                     if 'Revenue' in qe.columns:
L418                         rev = qe['Revenue'].dropna().astype(float)
L419                         if len(rev)>=5: REV_Q_YOY = _safe_div(rev.iloc[-1]-rev.iloc[-5], rev.iloc[-5])
L420                         if len(rev)>=6:
L421                             yoy_now = _safe_div(rev.iloc[-1]-rev.iloc[-5], rev.iloc[-5]); yoy_prev = _safe_div(rev.iloc[-2]-rev.iloc[-6], rev.iloc[-6])
L422                             if pd.notna(yoy_now) and pd.notna(yoy_prev): REV_YOY_ACC = yoy_now - yoy_prev
L423                         yoy_list=[]
L424                         for k in range(1,5):
L425                             if len(rev)>=4+k:
L426                                 y = _safe_div(rev.iloc[-k]-rev.iloc[-(k+4)], rev.iloc[-(k+4)])
L427                                 if pd.notna(y): yoy_list.append(y)
L428                         if len(yoy_list)>=2: REV_YOY_VAR = float(np.std(yoy_list, ddof=1))
L429                         # NEW: å¹´æ¬¡ã®æŒç¶šæ€§ï¼ˆç›´è¿‘ã‹ã‚‰é¡ã£ã¦å‰å¹´æ¯”ãƒ—ãƒ©ã‚¹ãŒä½•å¹´é€£ç¶šã‹ã€å››åŠæœŸ4æœ¬æƒã†å®Œå…¨å¹´ã®ã¿ï¼‰
L430                         try:
L431                             g = rev.groupby(rev.index.year)
L432                             ann_sum, cnt = g.sum(), g.count()
L433                             ann_sum = ann_sum[cnt >= 4]
L434                             if len(ann_sum) >= 3:
L435                                 yoy = ann_sum.pct_change().dropna()
L436                                 streak = 0
L437                                 for v in yoy.iloc[::-1]:
L438                                     if pd.isna(v) or v <= 0:
L439                                         break
L440                                     streak += 1
L441                                 REV_ANNUAL_STREAK = float(streak)
L442                         except Exception:
L443                             pass
L444                     if 'Earnings' in qe.columns and so:
L445                         eps_series = (qe['Earnings'].dropna().astype(float)/float(so)).replace([np.inf,-np.inf],np.nan)
L446                         if len(eps_series)>=5 and pd.notna(eps_series.iloc[-5]) and eps_series.iloc[-5]!=0:
L447                             EPS_Q_YOY = _safe_div(eps_series.iloc[-1]-eps_series.iloc[-5], eps_series.iloc[-5])
L448             except Exception: pass
L449             df.loc[t,'REV_Q_YOY'], df.loc[t,'EPS_Q_YOY'], df.loc[t,'REV_YOY_ACC'], df.loc[t,'REV_YOY_VAR'] = REV_Q_YOY, EPS_Q_YOY, REV_YOY_ACC, REV_YOY_VAR
L450             df.loc[t,'REV_ANN_STREAK'] = REV_ANNUAL_STREAK
L451
L452             # --- Rule of 40 ã‚„å‘¨è¾º ---
L453             total_rev_ttm = d.get('totalRevenue',np.nan)
L454             FCF_MGN = _safe_div(fcf_val, total_rev_ttm)
L455             df.loc[t,'FCF_MGN'] = FCF_MGN
L456             rule40 = np.nan
L457             try:
L458                 r = df.loc[t,'REV']; rule40 = (r if pd.notna(r) else np.nan) + (FCF_MGN if pd.notna(FCF_MGN) else np.nan)
L459             except Exception: pass
L460             df.loc[t,'RULE40'] = rule40
L461
L462             # --- ãƒˆãƒ¬ãƒ³ãƒ‰è£œåŠ© ---
L463             sma50  = s.rolling(50).mean()
L464             sma150 = s.rolling(150).mean()
L465             sma200 = s.rolling(200).mean()
L466             p = _safe_last(s)
L467
L468             df.loc[t,'MA50_OVER_150'] = (
L469                 _safe_last(sma50)/_safe_last(sma150) - 1
L470                 if pd.notna(_safe_last(sma50)) and pd.notna(_safe_last(sma150)) and _safe_last(sma150)!=0 else np.nan
L471             )
L472             df.loc[t,'MA150_OVER_200'] = (
L473                 _safe_last(sma150)/_safe_last(sma200) - 1
L474                 if pd.notna(_safe_last(sma150)) and pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L475             )
L476
L477             lo52 = s[-252:].min() if len(s)>=252 else s.min()
L478             df.loc[t,'P_OVER_LOW52'] = (p/lo52 - 1) if (lo52 and lo52>0 and pd.notna(p)) else np.nan
L479
L480             df.loc[t,'MA200_SLOPE_1M'] = np.nan
L481             if len(sma200.dropna()) >= 21:
L482                 cur200 = _safe_last(sma200)
L483                 old2001 = float(sma200.iloc[-21])
L484                 if old2001:
L485                     df.loc[t,'MA200_SLOPE_1M'] = cur200/old2001 - 1
L486
L487             df.loc[t,'P_OVER_150'] = p/_safe_last(sma150)-1 if pd.notna(_safe_last(sma150)) and _safe_last(sma150)!=0 else np.nan
L488             df.loc[t,'P_OVER_200'] = p/_safe_last(sma200)-1 if pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L489             df.loc[t,'MA50_OVER_200'] = _safe_last(sma50)/_safe_last(sma200)-1 if pd.notna(_safe_last(sma50)) and pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L490             df.loc[t,'MA200_SLOPE_5M'] = np.nan
L491             if len(sma200.dropna())>=105:
L492                 cur200, old200 = _safe_last(sma200), float(sma200.iloc[-105])
L493                 if old200 and old200!=0: df.loc[t,'MA200_SLOPE_5M'] = cur200/old200 - 1
L494             # NEW: 200æ—¥ç·šãŒé€£ç¶šã§ä¸Šå‘ãã®ã€Œæ—¥æ•°ã€
L495             df.loc[t,'MA200_UP_STREAK_D'] = np.nan
L496             try:
L497                 s200 = sma200.dropna()
L498                 if len(s200) >= 2:
L499                     diff200 = s200.diff()
L500                     up = 0
L501                     for v in diff200.iloc[::-1]:
L502                         if pd.isna(v) or v <= 0:
L503                             break
L504                         up += 1
L505                     df.loc[t,'MA200_UP_STREAK_D'] = float(up)
L506             except Exception:
L507                 pass
L508             df.loc[t,'LOW52PCT25_EXCESS'] = np.nan if (lo52 is None or lo52<=0 or pd.isna(p)) else (p/(lo52*1.25)-1)
L509             hi52 = s[-252:].max() if len(s)>=252 else s.max(); df.loc[t,'NEAR_52W_HIGH'] = np.nan
L510             if hi52 and hi52>0 and pd.notna(p):
L511                 d_hi = (p/hi52)-1.0; df.loc[t,'NEAR_52W_HIGH'] = -abs(min(0.0, d_hi))
L512             df.loc[t,'RS_SLOPE_6W'] = self.rs_line_slope(s, ib.spx, 30)
L513             df.loc[t,'RS_SLOPE_13W'] = self.rs_line_slope(s, ib.spx, 65)
L514
L515             df.loc[t,'DIV_STREAK'] = self.div_streak(t)
L516
L517             # --- æ¬ æãƒ¡ãƒ¢ ---
L518             fin_cols = ['REV','ROE','BETA','DIV','FCF']
L519             need_finnhub = [col for col in fin_cols if pd.isna(df.loc[t,col])]
L520             if need_finnhub:
L521                 fin_data = self.fetch_finnhub_metrics(t)
L522                 for col in need_finnhub:
L523                     val = fin_data.get(col)
L524                     if val is not None and not pd.isna(val): df.loc[t,col] = val
L525             for col in fin_cols + ['EPS','RS','TR_str','DIV_STREAK']:
L526                 if pd.isna(df.loc[t,col]):
L527                     if col=='DIV':
L528                         status = self.dividend_status(t)
L529                         if status!='none_confident': missing_logs.append({'Ticker':t,'Column':col,'Status':status})
L530                     else:
L531                         missing_logs.append({'Ticker':t,'Column':col})
L532
L533         def _trend_template_pass(row, rs_alpha_thresh=0.10):
L534             c1 = (row.get('P_OVER_150', np.nan) > 0) and (row.get('P_OVER_200', np.nan) > 0)
L535             c2 = (row.get('MA150_OVER_200', np.nan) > 0)
L536             c3 = (row.get('MA200_SLOPE_1M', np.nan) > 0)
L537             c4 = (row.get('MA50_OVER_150', np.nan) > 0) and (row.get('MA50_OVER_200', np.nan) > 0)
L538             c5 = (row.get('TR_str', np.nan) > 0)
L539             c6 = (row.get('P_OVER_LOW52', np.nan) >= 0.30)
L540             c7 = (row.get('NEAR_52W_HIGH', np.nan) >= -0.25)
L541             c8 = (row.get('RS', np.nan) >= 0.10)
L542             return bool(c1 and c2 and c3 and c4 and c5 and c6 and c7 and c8)
L543
L544         if 'trend_template' not in df.columns: df['trend_template'] = df.apply(_trend_template_pass, axis=1).fillna(False)
L545         assert 'trend_template' in df.columns
L546
L547         # === ZåŒ–ã¨åˆæˆ ===
L548         for col in ['ROE','FCF','REV','EPS']: df[f'{col}_W'] = winsorize_s(df[col], 0.02)
L549
L550         df_z = pd.DataFrame(index=df.index)
L551         for col in ['EPS','REV','ROE','FCF','RS','TR_str','BETA','DIV','DIV_STREAK']: df_z[col] = robust_z(df[col])
L552         df_z['REV'], df_z['EPS'], df_z['TR'] = robust_z(df['REV_W']), robust_z(df['EPS_W']), robust_z(df['TR'])
L553         for col in ['P_OVER_150','P_OVER_200','MA50_OVER_200','MA200_SLOPE_5M','LOW52PCT25_EXCESS','NEAR_52W_HIGH','RS_SLOPE_6W','RS_SLOPE_13W','MA200_UP_STREAK_D']: df_z[col] = robust_z(df[col])
L554         for col in ['REV_Q_YOY','EPS_Q_YOY','REV_YOY_ACC','REV_YOY_VAR','FCF_MGN','RULE40','REV_ANN_STREAK']: df_z[col] = robust_z(df[col])
L555         for col in ['DOWNSIDE_DEV','MDD_1Y','RESID_VOL','DOWN_OUTPERF','EXT_200','DIV_TTM_PS','DIV_VAR5','DIV_YOY','DIV_FCF_COVER','DEBT2EQ','CURR_RATIO','EPS_VAR_8Q','MARKET_CAP','ADV60_USD']: df_z[col] = robust_z(df[col])
L556
L557         df_z['SIZE'], df_z['LIQ'] = robust_z(np.log1p(df['MARKET_CAP'])), robust_z(np.log1p(df['ADV60_USD']))
L558         df_z['QUALITY_F'] = robust_z(0.6*df['FCF_W'] + 0.4*df['ROE_W']).clip(-3.0,3.0)
L559         df_z['YIELD_F']   = 0.3*df_z['DIV'] + 0.7*df_z['DIV_STREAK']
L560         df_z['GROWTH_F']  = robust_z(
L561               0.25*df_z['REV']          # â†“0.30â†’0.25
L562             + 0.20*df_z['EPS_Q_YOY']
L563             + 0.15*df_z['REV_Q_YOY']
L564             + 0.15*df_z['REV_YOY_ACC']
L565             + 0.10*df_z['RULE40']
L566             + 0.10*df_z['FCF_MGN']
L567             + 0.10*df_z['EPS']          # â˜…è¿½åŠ ï¼šé»’å­—å„ªé‡ï¼èµ¤å­—æ¸›ç‚¹
L568             + 0.05*df_z['REV_ANN_STREAK']
L569             - 0.05*df_z['REV_YOY_VAR']
L570         ).clip(-3.0,3.0)
L571         df_z['MOM_F'] = robust_z(
L572               0.40*df_z['RS']
L573             + 0.15*df_z['TR_str']
L574             + 0.15*df_z['RS_SLOPE_6W']
L575             + 0.15*df_z['RS_SLOPE_13W']
L576             + 0.10*df_z['MA200_SLOPE_5M']
L577             + 0.10*df_z['MA200_UP_STREAK_D']
L578         ).clip(-3.0,3.0)
L579         df_z['VOL'] = robust_z(df['BETA'])
L580         df_z.rename(columns={'GROWTH_F':'GRW','MOM_F':'MOM','QUALITY_F':'QAL','YIELD_F':'YLD'}, inplace=True)
L581
L582         # === begin: BIO LOSS PENALTY =====================================
L583         try:
L584             penalty_z = float(os.getenv("BIO_LOSS_PENALTY_Z", "0.8"))
L585         except Exception:
L586             penalty_z = 0.8
L587
L588         def _is_bio_like(t: str) -> bool:
L589             inf = info.get(t, {}) if isinstance(info, dict) else {}
L590             sec = str(inf.get("sector", "")).lower()
L591             ind = str(inf.get("industry", "")).lower()
L592             if "health" not in sec:
L593                 return False
L594             keys = ("biotech", "biopharma", "pharma")
L595             return any(k in ind for k in keys)
L596
L597         tickers_s = pd.Index(df_z.index)
L598         is_bio = pd.Series({t: _is_bio_like(t) for t in tickers_s})
L599         is_loss = pd.Series({t: (pd.notna(df.loc[t,"EPS"]) and df.loc[t,"EPS"] <= 0) for t in tickers_s})
L600         mask_bio_loss = (is_bio & is_loss).reindex(df_z.index).fillna(False)
L601
L602         if bool(mask_bio_loss.any()) and penalty_z > 0:
L603             df_z.loc[mask_bio_loss, "GRW"] = df_z.loc[mask_bio_loss, "GRW"] - penalty_z
L604             df_z["GRW"] = df_z["GRW"].clip(-3.0, 3.0)
L605         # === end: BIO LOSS PENALTY =======================================
L606
L607         df_z['TRD'] = 0.0  # TRDã¯ã‚¹ã‚³ã‚¢å¯„ä¸ã‹ã‚‰å¤–ã—ã€ãƒ†ãƒ³ãƒ—ãƒ¬åˆ¤å®šã¯ãƒ•ã‚£ãƒ«ã‚¿ã§è¡Œã†ï¼ˆåˆ—ã¯è¡¨ç¤ºäº’æ›ã®ãŸã‚æ®‹ã™ï¼‰
L608         if 'BETA' not in df_z.columns: df_z['BETA'] = robust_z(df['BETA'])
L609
L610         df_z['D_VOL_RAW'] = robust_z(0.40*df_z['DOWNSIDE_DEV'] + 0.22*df_z['RESID_VOL'] + 0.18*df_z['MDD_1Y'] - 0.10*df_z['DOWN_OUTPERF'] - 0.05*df_z['EXT_200'] - 0.08*df_z['SIZE'] - 0.10*df_z['LIQ'] + 0.10*df_z['BETA'])
L611         df_z['D_QAL']     = robust_z(0.35*df_z['QAL'] + 0.20*df_z['FCF'] + 0.15*df_z['CURR_RATIO'] - 0.15*df_z['DEBT2EQ'] - 0.15*df_z['EPS_VAR_8Q'])
L612         df_z['D_YLD']     = robust_z(0.45*df_z['DIV'] + 0.25*df_z['DIV_STREAK'] + 0.20*df_z['DIV_FCF_COVER'] - 0.10*df_z['DIV_VAR5'])
L613         df_z['D_TRD']     = robust_z(0.40*df_z.get('MA200_SLOPE_5M',0) - 0.30*df_z.get('EXT_200',0) + 0.15*df_z.get('NEAR_52W_HIGH',0) + 0.15*df_z['TR'])
L614
L615         # --- é‡ã¿ã¯ cfg ã‚’å„ªå…ˆï¼ˆå¤–éƒ¨ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ï¼‰ ---
L616         # â‘  å…¨éŠ˜æŸ„ã§ G/D ã‚¹ã‚³ã‚¢ã‚’ç®—å‡ºï¼ˆunmaskedï¼‰
L617         g_score_all = df_z.mul(pd.Series(cfg.weights.g)).sum(axis=1)
L618
L619         d_comp = pd.concat({
L620             'QAL': df_z['D_QAL'],
L621             'YLD': df_z['D_YLD'],
L622             'VOL': df_z['D_VOL_RAW'],
L623             'TRD': df_z['D_TRD']
L624         }, axis=1)
L625         dw = pd.Series(cfg.weights.d, dtype=float).reindex(['QAL','YLD','VOL','TRD']).fillna(0.0)
L626         globals()['D_WEIGHTS_EFF'] = dw.copy()
L627         d_score_all = d_comp.mul(dw, axis=1).sum(axis=1)
L628
L629         # â‘¡ ãƒ†ãƒ³ãƒ—ãƒ¬åˆ¤å®šï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ãã®ã¾ã¾ï¼‰
L630         mask = df['trend_template']
L631         if not bool(mask.any()):
L632             mask = (
L633                 (df.get('P_OVER_LOW52', np.nan) >= 0.25) &
L634                 (df.get('NEAR_52W_HIGH', np.nan) >= -0.30) &
L635                 (df.get('RS', np.nan) >= 0.08) &
L636                 (df.get('MA200_SLOPE_1M', np.nan) > 0) &
L637                 (df.get('P_OVER_150', np.nan) > 0) & (df.get('P_OVER_200', np.nan) > 0) &
L638                 (df.get('MA150_OVER_200', np.nan) > 0) &
L639                 (df.get('MA50_OVER_150', np.nan) > 0) & (df.get('MA50_OVER_200', np.nan) > 0) &
L640                 (df.get('TR_str', np.nan) > 0)
L641             ).fillna(False)
L642             df['trend_template'] = mask
L643
L644         # â‘¢ æ¡ç”¨ç”¨ã¯ maskã€è¡¨ç¤º/åˆ†æç”¨ã¯åˆ—ã§å…¨éŠ˜æŸ„ä¿å­˜
L645         g_score = g_score_all.loc[mask]
L646         Scorer.g_score = g_score
L647         df_z['GSC'] = g_score_all
L648         df_z['DSC'] = d_score_all
L649
L650         try:
L651             current = (
L652                 pd.read_csv("current_tickers.csv")
L653                   .iloc[:, 0]
L654                   .str.upper()
L655                   .tolist()
L656             )
L657         except FileNotFoundError:
L658             warnings.warn("current_tickers.csv not found â€” bonus skipped")
L659             current = []
L660
L661         mask_bonus = g_score.index.isin(current)
L662         if mask_bonus.any():
L663             # 1) factor.BONUS_COEFF ã‹ã‚‰ k ã‚’æ±ºã‚ã€ç„¡ã‘ã‚Œã° 0.4
L664             k = float(getattr(sys.modules.get("factor"), "BONUS_COEFF", 0.4))
L665             # 2) g å´ã® Ïƒ ã‚’å–ã‚Šã€NaN ãªã‚‰ 0 ã«ä¸¸ã‚ã‚‹
L666             sigma_g = g_score.std()
L667             if pd.isna(sigma_g):
L668                 sigma_g = 0.0
L669             bonus_g = round(k * sigma_g, 3)
L670             g_score.loc[mask_bonus] += bonus_g
L671             Scorer.g_score = g_score
L672             # 3) D å´ã‚‚åŒæ§˜ã« Ïƒ ã® NaN ã‚’ã‚±ã‚¢
L673             sigma_d = d_score_all.std()
L674             if pd.isna(sigma_d):
L675                 sigma_d = 0.0
L676             bonus_d = round(k * sigma_d, 3)
L677             d_score_all.loc[d_score_all.index.isin(current)] += bonus_d
L678
L679         try:
L680             df = _apply_growth_entry_flags(df, ib, self, win_breakout=5, win_pullback=5)
L681         except Exception:
L682             pass
L683
L684         from factor import FeatureBundle  # type: ignore  # å®Ÿè¡Œæ™‚importãªã—ï¼ˆå¾ªç’°å›é¿ï¼‰
L685         return FeatureBundle(
L686             df=df,
L687             df_z=df_z,
L688             g_score=g_score,
L689             d_score_all=d_score_all,
L690             missing_logs=pd.DataFrame(missing_logs)
L691         )
L692
L693
L694 def _apply_growth_entry_flags(feature_df, bundle, self_obj, win_breakout=5, win_pullback=5):
L695     """
L696     Gæ ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã«å¯¾ã—ã€ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®š/æŠ¼ã—ç›®åç™ºã®ã€Œç›´è¿‘Nå–¶æ¥­æ—¥å†…ã®ç™ºç«ã€ã‚’åˆ¤å®šã—ã€
L697     æ¬¡ã®åˆ—ã‚’ feature_df ã«è¿½åŠ ã™ã‚‹ï¼ˆindex=tickerï¼‰ã€‚
L698       - G_BREAKOUT_recent_5d : bool
L699       - G_BREAKOUT_last_date : str "YYYY-MM-DD"
L700       - G_PULLBACK_recent_5d : bool
L701       - G_PULLBACK_last_date : str "YYYY-MM-DD"
L702       - G_PIVOT_price        : float
L703     å¤±æ•—ã—ã¦ã‚‚ä¾‹å¤–ã¯æ¡ã‚Šæ½°ã—ã€æ—¢å­˜å‡¦ç†ã‚’é˜»å®³ã—ãªã„ã€‚
L704     """
L705     try:
L706         px   = bundle.px                      # çµ‚å€¤ DataFrame
L707         hi   = bundle.data['High']
L708         lo   = bundle.data['Low']
L709         vol  = bundle.data['Volume']
L710         bench= bundle.spx                     # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ Series
L711
L712         # Gãƒ¦ãƒ‹ãƒãƒ¼ã‚¹æ¨å®šï¼šself.g_universe å„ªå…ˆ â†’ feature_df['group']=='G' â†’ å…¨éŠ˜æŸ„
L713         g_universe = getattr(self_obj, "g_universe", None)
L714         if g_universe is None:
L715             try:
L716                 g_universe = feature_df.index[feature_df['group'].astype(str).str.upper().eq('G')].tolist()
L717             except Exception:
L718                 g_universe = list(feature_df.index)
L719         if not g_universe:
L720             return feature_df
L721
L722         # æŒ‡æ¨™
L723         ema21 = px[g_universe].ewm(span=21, adjust=False).mean()
L724         ma50  = px[g_universe].rolling(50).mean()
L725         ma150 = px[g_universe].rolling(150).mean()
L726         ma200 = px[g_universe].rolling(200).mean()
L727         atr20 = (hi[g_universe] - lo[g_universe]).rolling(20).mean()
L728         vol20 = vol[g_universe].rolling(20).mean()
L729         vol50 = vol[g_universe].rolling(50).mean()
L730
L731         # ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆåˆæ ¼
L732         trend_template_ok = (px[g_universe] > ma50) & (px[g_universe] > ma150) & (px[g_universe] > ma200) \
L733                             & (ma150 > ma200) & (ma200.diff() > 0)
L734
L735         # æ±ç”¨ãƒ”ãƒœãƒƒãƒˆï¼šç›´è¿‘65å–¶æ¥­æ—¥ã®é«˜å€¤ï¼ˆå½“æ—¥é™¤å¤–ï¼‰
L736         pivot_price = hi[g_universe].rolling(65).max().shift(1)
L737
L738         # ç›¸å¯¾åŠ›ï¼šå¹´å†…é«˜å€¤æ›´æ–°
L739         bench_aligned = bench.reindex(px.index).ffill()
L740         rs = px[g_universe].div(bench_aligned, axis=0)
L741         rs_high = rs.rolling(252).max().shift(1)
L742
L743         # ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆã€Œç™ºç”Ÿæ—¥ã€ï¼šæ¡ä»¶ç«‹ã¡ä¸ŠãŒã‚Š
L744         breakout_today = trend_template_ok & (px[g_universe] > pivot_price) \
L745                          & (vol[g_universe] >= 1.5 * vol50) & (rs > rs_high)
L746         breakout_event = breakout_today & ~breakout_today.shift(1).fillna(False)
L747
L748         # æŠ¼ã—ç›®åç™ºã€Œç™ºç”Ÿæ—¥ã€ï¼šEMA21å¸¯Ã—å‡ºæ¥é«˜ãƒ‰ãƒ©ã‚¤ã‚¢ãƒƒãƒ—Ã—å‰æ—¥é«˜å€¤è¶ŠãˆÃ—çµ‚å€¤EMA21ä¸Š
L749         near_ema21_band = px[g_universe].between(ema21 - atr20, ema21 + atr20)
L750         volume_dryup = (vol20 / vol50) <= 1.0
L751         pullback_bounce_confirmed = (px[g_universe] > hi[g_universe].shift(1)) & (px[g_universe] > ema21)
L752         pullback_today = trend_template_ok & near_ema21_band & volume_dryup & pullback_bounce_confirmed
L753         pullback_event = pullback_today & ~pullback_today.shift(1).fillna(False)
L754
L755         # ç›´è¿‘Nå–¶æ¥­æ—¥å†…ã®ç™ºç« / æœ€çµ‚ç™ºç”Ÿæ—¥
L756         rows = []
L757         for t in g_universe:
L758             def _recent_and_date(s, win):
L759                 sw = s[t].iloc[-win:]
L760                 if sw.any():
L761                     d = sw[sw].index[-1]
L762                     return True, d.strftime("%Y-%m-%d")
L763                 return False, ""
L764             br_recent, br_date = _recent_and_date(breakout_event, win_breakout)
L765             pb_recent, pb_date = _recent_and_date(pullback_event, win_pullback)
L766             rows.append((t, {
L767                 "G_BREAKOUT_recent_5d": br_recent,
L768                 "G_BREAKOUT_last_date": br_date,
L769                 "G_PULLBACK_recent_5d": pb_recent,
L770                 "G_PULLBACK_last_date": pb_date,
L771                 "G_PIVOT_price": float(pivot_price[t].iloc[-1]) if t in pivot_price.columns else float('nan'),
L772             }))
L773         flags = pd.DataFrame({k: v for k, v in rows}).T
L774
L775         # åˆ—ã‚’ä½œæˆãƒ»ä¸Šæ›¸ã
L776         cols = ["G_BREAKOUT_recent_5d","G_BREAKOUT_last_date","G_PULLBACK_recent_5d","G_PULLBACK_last_date","G_PIVOT_price"]
L777         for c in cols:
L778             if c not in feature_df.columns:
L779                 feature_df[c] = np.nan
L780         feature_df.loc[flags.index, flags.columns] = flags
L781
L782     except Exception:
L783         pass
L784     return feature_df
L785
L786
L787
```

## <.github/workflows/weekly-report.yml>
```text
L1 name: Weekly Stock Report
L2
L3 on:
L4   push:
L5     branches: [ main ]
L6     paths-ignore:
L7       - 'CodeForChat/**'
L8   schedule:
L9     - cron: '0 0 * * 6'  # UTC 00:00 â†’ JST 09:00ï¼ˆåœŸï¼‰
L10   workflow_dispatch:
L11
L12 jobs:
L13   build-and-report:
L14     runs-on: ubuntu-latest
L15     permissions:
L16       contents: write
L17
L18     steps:
L19       - name: Debug start
L20         run: echo 'ğŸš€ DEBUGstarted'
L21               
L22       - name: Checkout repository
L23         uses: actions/checkout@v3
L24
L25       - name: Setup Python
L26         uses: actions/setup-python@v5
L27         with:
L28           python-version: '3.x'
L29           cache: 'pip'
L30           cache-dependency-path: requirements.txt
L31
L32       - name: Install dependencies
L33         run: pip install -r requirements.txt
L34
L35       - name: Prepare results directory
L36         run: mkdir -p results
L37
L38       - name: Run factor & scoring
L39         env:
L40           SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
L41           FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
L42           FIN_THREADS: "8"
L43         run: python factor.py
L44       - name: Persist breadth mode (if changed)
L45         run: |
L46           git config user.name "github-actions[bot]"
L47           git config user.email "github-actions[bot]@users.noreply.github.com"
L48           git add results/breadth_state.json || true
L49           if ! git diff --cached --quiet; then
L50             git commit -m "chore: update breadth_state.json [skip ci]" || true
L51             git push || true
L52           else
L53             echo "No breadth_state.json changes."
L54           fi
```

## <documents/README.md>
```text
L1 # é‹ç”¨ãƒ«ãƒ¼ãƒ«
L2
L3 ## åŸºæœ¬æ§‹æˆ
L4 - 25éŠ˜æŸ„ã‚’å‡ç­‰é…åˆ†ï¼ˆç¾é‡‘ã‚’é™¤ã1éŠ˜æŸ„ã‚ãŸã‚Š4%ï¼‰
L5 - moomooè¨¼åˆ¸ã§é‹ç”¨
L6
L7 ## Barbell Growth-Defenseæ–¹é‡
L8 - Growthæ 12éŠ˜æŸ„ï¼šé«˜æˆé•·ã§ä¹–é›¢æºã¨ãªã‚‹æ”»ã‚ã®éŠ˜æŸ„
L9 - Defenseæ 13éŠ˜æŸ„ï¼šä½ãƒœãƒ©ã§å®‰å®šæˆé•·ã—é…å½“ã‚’å¢—ã‚„ã™å®ˆã‚Šã®éŠ˜æŸ„
L10 - ã€ŒçŒ›çƒˆã«ä¼¸ã³ã‚‹æ”»ã‚ Ã— ç€å®Ÿã«ç¨¼ãç›¾ã€ã®çµ„åˆã›ã§ä¹–é›¢â†’åŠæˆ»ã—ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ã‚’ç‹™ã†
L11
L12 ## ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¤å®šï¼ˆtrend_template åˆæ ¼â€œæœ¬æ•°â€ã§åˆ¤å®šï¼‰
L13 - åˆæ ¼æœ¬æ•° = current+candidate å…¨ä½“ã®ã†ã¡ã€trend_template æ¡ä»¶ã‚’æº€ãŸã—ãŸéŠ˜æŸ„ã®**æœ¬æ•°(C)**
L14 - ã—ãã„å€¤ã¯éå»~600å–¶æ¥­æ—¥ã®åˆ†å¸ƒã‹ã‚‰**æ¯å›è‡ªå‹•æ¡ç”¨**ï¼ˆåˆ†ä½ç‚¹ã¨é‹ç”¨â€œåºŠâ€ã®maxï¼‰
L15   - ç·Šæ€¥å…¥ã‚Š: `max(q05, 12æœ¬)`ï¼ˆ= N_Gï¼‰
L16   - ç·Šæ€¥è§£é™¤: `max(q20, 18æœ¬)`ï¼ˆ= 1.5Ã—N_Gï¼‰
L17   - é€šå¸¸å¾©å¸°: `max(q60, 36æœ¬)`ï¼ˆ= 3Ã—N_Gï¼‰
L18 - ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹: å‰å›ãƒ¢ãƒ¼ãƒ‰ã«ä¾å­˜ï¼ˆEMERGâ†’è§£é™¤ã¯18æœ¬ä»¥ä¸Šã€CAUTIONâ†’é€šå¸¸ã¯36æœ¬ä»¥ä¸Šï¼‰
L19
L20 ## ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¥ã®ç¾é‡‘ãƒ»ãƒ‰ãƒªãƒ•ãƒˆ
L21 - **é€šå¸¸(NORMAL)** : ç¾é‡‘ **10%** / ãƒ‰ãƒªãƒ•ãƒˆé–¾å€¤ **10%**
L22 - **è­¦æˆ’(CAUTION)** : ç¾é‡‘ **12.5%** / ãƒ‰ãƒªãƒ•ãƒˆé–¾å€¤ **12%**
L23 - **ç·Šæ€¥(EMERG)** : ç¾é‡‘ **20%** / **ãƒ‰ãƒªãƒ•ãƒˆå£²è²·åœæ­¢**ï¼ˆ25Ã—4%ã«å…¨æˆ»ã—ã®ã¿ï¼‰
L24
L25 ## ãƒˆãƒ¬ãƒ¼ãƒªãƒ³ã‚°ã‚¹ãƒˆãƒƒãƒ—ï¼ˆçµ±ä¸€ï¼‰
L26 - G/D å…±é€šã® **åŸºæœ¬TS=15%**
L27 - å«ã¿ç›ŠãŒ **+20% / +40% / +60%** åˆ°é”ã§ TS ã‚’ **12% / 9% / 7%** ã«æ®µéšå¼•ãä¸Šã’
L28 - TSç™ºå‹•ã§æ¸›å°‘ã—ãŸéŠ˜æŸ„ã¯ç¿Œæ—¥ä»¥é™ã«è£œå……ï¼ˆâ€»ç·Šæ€¥ãƒ¢ãƒ¼ãƒ‰ä¸­ã¯è£œå……ã—ãªã„ï¼‰
L29
L30 ## å…¥æ›¿éŠ˜æŸ„é¸å®š
L31 - Oxfordã‚­ãƒ£ãƒ”ã‚¿ãƒ«ï¼ã‚¤ãƒ³ã‚«ãƒ ã€Alpha Investorã€Motley Fool Stock Advisorã€moomooã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ç­‰ã‚’å‚è€ƒã«chatGPTã§æ¤œè¨
L32 - å¹´é–“NISAæ ã¯Growthç¾¤ã®ä¸­ã‹ã‚‰ä½ãƒœãƒ©éŠ˜æŸ„ã‚’é¸å®šã—åˆ©ç”¨ã€‚é•·æœŸä¿æŒã«ã¯ã“ã ã‚ã‚‰ãªã„ã€‚
L33
L34 ## å†ã‚¨ãƒ³ãƒˆãƒªãƒ¼ï¼ˆã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ï¼‰
L35 - TSãƒ’ãƒƒãƒˆå¾Œã®åŒéŠ˜æŸ„å†INã¯ **8å–¶æ¥­æ—¥** ã®ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ã‚’è¨­ã‘ã‚‹ï¼ˆæœŸé–“ä¸­ã¯å†INç¦æ­¢ï¼‰
L36
L37 ## å®Ÿè¡Œã‚¿ã‚¤ãƒŸãƒ³ã‚°
L38 - åˆ¤å®šï¼šç±³å›½å¸‚å ´çµ‚å€¤ç›´å¾Œ
L39 - åŸ·è¡Œï¼šç¿Œå–¶æ¥­æ—¥ã®ç±³å›½å¯„ä»˜ãæˆè¡Œ
```

## <documents/factor_design.md>
```text
L1 # factor.py è©³ç´°è¨­è¨ˆæ›¸
L2
L3 ## æ¦‚è¦
L4 - æ—¢å­˜ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®éŠ˜æŸ„ã¨æ¤œè¨ä¸­ã®éŠ˜æŸ„ç¾¤ã‚’åŒæ™‚ã«æ‰±ã†éŠ˜æŸ„é¸å®šãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€‚
L5 - ä¾¡æ ¼ãƒ»è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šè¾¼ã¿ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã¨DRRSé¸å®šã‚’è¡Œã†ã“ã¨ã§ã€ä»¥ä¸‹ã®ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆã‚’å¾—ã‚‹ã€‚
L6   - æ¡ç”¨éŠ˜æŸ„ã¨æƒœã—ãã‚‚æ¼ã‚ŒãŸéŠ˜æŸ„ã®ã‚¹ã‚³ã‚¢ä¸€è¦§
L7   - IN/OUTã®ãƒ†ã‚£ãƒƒã‚«ãƒ¼ãƒªã‚¹ãƒˆã¨OUTå´ã®ä½ã‚¹ã‚³ã‚¢éŠ˜æŸ„
L8   - æ–°æ—§ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®æ¯”è¼ƒè¡¨
L9   - æ¤œè¨ä¸­éŠ˜æŸ„ã®ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆæ•´ç†ç”¨ï¼‰
L10
L11 ## å…¨ä½“ãƒ•ãƒ­ãƒ¼
L12 1. **Input** â€“ `current_tickers.csv`ã¨`candidate_tickers.csv`ã‚’èª­ã¿è¾¼ã¿ã€yfinanceã‚„Finnhubã®APIã‹ã‚‰ä¾¡æ ¼ãƒ»è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã¦`InputBundle`ã‚’æ•´å‚™ã€‚
L13 2. **Score Calculation** â€“ ScorerãŒç‰¹å¾´é‡ã‚’è¨ˆç®—ã—å› å­ã‚¹ã‚³ã‚¢ã‚’åˆæˆã—ã¦`FeatureBundle`ã‚’ç”Ÿæˆã€‚
L14 3. **Correlation Reduction & Selection** â€“ SelectorãŒDRRSãƒ­ã‚¸ãƒƒã‚¯ã§ç›¸é–¢ã‚’æŠ‘ãˆã¤ã¤G/DéŠ˜æŸ„ã‚’é¸å®šã—`SelectionBundle`ã‚’å¾—ã‚‹ã€‚
L15 4. **Output** â€“ æ¡ç”¨çµæœã¨å‘¨è¾ºæƒ…å ±ã‚’è¡¨ãƒ»Slacké€šçŸ¥ã¨ã—ã¦å‡ºåŠ›ã€‚
L16
L17 ```mermaid
L18 flowchart LR
L19   A[Input\nAPI & å‰å‡¦ç†] --> B[Score Calculation\nç‰¹å¾´é‡ãƒ»å› å­åˆæˆ]
L20   B --> C[Correlation Reduction\nDRRSé¸å®š]
L21   C --> D[Output\nSlacké€šçŸ¥]
L22 ```
L23
L24 ## å®šæ•°ãƒ»è¨­å®š
L25 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L26 | --- | --- | --- |
L27 | `exist` / `cand` | ç¾è¡Œãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã¨æ¤œè¨ä¸­éŠ˜æŸ„ã®ãƒ†ã‚£ãƒƒã‚«ãƒ¼ãƒªã‚¹ãƒˆ | ã‚¹ã‚³ã‚¢å¯¾è±¡ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã®æ§‹æˆã€å€™è£œæ•´ç† |
L28 | `bench` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼ | ç›¸å¯¾å¼·ã•ãƒ»Î²ç®—å‡ºã€ãƒãƒ¼ãƒˆæ¯”è¼ƒ |
L29 | `CAND_PRICE_MAX` | å€™è£œéŠ˜æŸ„ã®è¨±å®¹ä¾¡æ ¼ä¸Šé™ | é«˜é¡éŠ˜æŸ„ã®äº‹å‰é™¤å¤– |
L30 | `N_G` / `N_D` | G/Dæ¡ç”¨æ ã®ä»¶æ•° | æœ€çµ‚çš„ã«é¸ã¶éŠ˜æŸ„æ•°ã®åˆ¶ç´„ |
L31 | `g_weights` / `D_weights` | å„å› å­ã®é‡ã¿dict | G/Dã‚¹ã‚³ã‚¢åˆæˆ |
L32 | `D_BETA_MAX` | Dãƒã‚±ãƒƒãƒˆã®è¨±å®¹Î²ä¸Šé™ | é«˜Î²éŠ˜æŸ„ã®é™¤å¤–ãƒ•ã‚£ãƒ«ã‚¿ |
L33 | `FILTER_SPEC` | G/Dã”ã¨ã®å‰å‡¦ç†ãƒ•ã‚£ãƒ«ã‚¿ | ãƒˆãƒ¬ãƒ³ãƒ‰ãƒã‚¹ã‚¯ã‚„Î²ä¸Šé™è¨­å®š |
L34 | `corrM` | DRRSåˆæœŸãƒ—ãƒ¼ãƒ«ã®æœ€å¤§ä»¶æ•° | ç›¸é–¢è¡Œåˆ—ã‚µã‚¤ã‚ºåˆ¶å¾¡ |
L35 | `DRRS_G` / `DRRS_D` | DRRSãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿dict | ãƒã‚±ãƒƒãƒˆåˆ¥ã®ç›¸é–¢ä½æ¸›è¨­å®š |
L36 | `DRRS_SHRINK` | æ®‹å·®ç›¸é–¢ã®å¯¾è§’ã‚·ãƒ¥ãƒªãƒ³ã‚¯ç‡ | `residual_corr`ã®å®‰å®šåŒ– |
L37 | `CROSS_MU_GD` | G-Dé–“ã‚¯ãƒ­ã‚¹ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£Î¼ | 2ãƒã‚±ãƒƒãƒˆåŒæ™‚æœ€é©åŒ–ã§ç›¸é–¢æŠ‘åˆ¶ |
L38 | `RESULTS_DIR` | é¸å®šçµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª | `_save_sel`/`_load_prev`ã®å…¥å‡ºåŠ› |
L39
L40 é¸å®šçµæœã¯`results/`é…ä¸‹ã«JSONã¨ã—ã¦ä¿å­˜ã—ã€æ¬¡å›å®Ÿè¡Œæ™‚ã«`_load_prev`ã§èª­ã¿è¾¼ã‚“ã§é¸å®šæ¡ä»¶ã«åæ˜ ã€‚
L41
L42 ## DTO/Config
L43 å„ã‚¹ãƒ†ãƒƒãƒ—é–“ã§å—ã‘æ¸¡ã™ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¨è¨­å®šå€¤ã€‚å¤‰æ•°ã®æ„å‘³åˆã„ã¨åˆ©ç”¨ç®‡æ‰€ã‚’ä»¥ä¸‹ã«ç¤ºã™ã€‚
L44
L45 ### InputBundleï¼ˆInput â†’ Scorerï¼‰
L46 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L47 | --- | --- | --- |
L48 | `cand` | å€™è£œéŠ˜æŸ„ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã®ãƒªã‚¹ãƒˆ | OUTãƒ†ãƒ¼ãƒ–ãƒ«ã‚„ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°å¯¾è±¡ã®æ¯é›†å›£ |
L49 | `tickers` | ç¾è¡Œ+å€™è£œã‚’åˆã‚ã›ãŸãƒ†ã‚£ãƒƒã‚«ãƒ¼ä¸€è¦§ | ä¾¡æ ¼ãƒ»å‡ºæ¥é«˜ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€ãƒªã‚¿ãƒ¼ãƒ³è¨ˆç®— |
L50 | `bench` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼ | ç›¸å¯¾å¼·ã•ãƒ»Î²ç®—å‡ºã€ãƒãƒ¼ãƒˆæ¯”è¼ƒ |
L51 | `data` | yfinanceã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœï¼ˆéšå±¤åˆ—ï¼‰ | `px`/`spx`/ãƒªã‚¿ãƒ¼ãƒ³ç­‰ã®åŸºç¤ãƒ‡ãƒ¼ã‚¿ |
L52 | `px` | `data['Close']`ã ã‘ã‚’æŠœãå‡ºã—ãŸä¾¡æ ¼ç³»åˆ— | æŒ‡æ¨™è¨ˆç®—ãƒ»ãƒªã‚¿ãƒ¼ãƒ³ç”Ÿæˆ |
L53 | `spx` | `data['Close'][bench]` ã®Series | `rs`ã‚„`calc_beta`ã®åŸºæº–æŒ‡æ•° |
L54 | `tickers_bulk` | `yf.Tickers`ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ | `info`ç­‰ã®ä¸€æ‹¬å–å¾— |
L55 | `info` | ãƒ†ã‚£ãƒƒã‚«ãƒ¼åˆ¥ã®yfinanceæƒ…å ±dict | ã‚»ã‚¯ã‚¿ãƒ¼åˆ¤å®šã‚„EPSè£œå®Œ |
L56 | `eps_df` | EPS TTM/ç›´è¿‘EPSç­‰ã‚’ã¾ã¨ã‚ãŸè¡¨ | æˆé•·æŒ‡æ¨™ã®ç®—å‡º |
L57 | `fcf_df` | CFOãƒ»CapExãƒ»FCF TTMã¨æƒ…å ±æºãƒ•ãƒ©ã‚° | FCF/EVã‚„é…å½“ã‚«ãƒãƒ¬ãƒƒã‚¸ |
L58 | `returns` | `px.pct_change()`ã®ãƒªã‚¿ãƒ¼ãƒ³è¡¨ | ç›¸é–¢è¡Œåˆ—ãƒ»DRRSè¨ˆç®— |
L59
L60 ### FeatureBundleï¼ˆScorer â†’ Selectorï¼‰
L61 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L62 | --- | --- | --- |
L63 | `df` | è¨ˆç®—æ¸ˆã¿æŒ‡æ¨™ã®ç”Ÿå€¤ãƒ†ãƒ¼ãƒ–ãƒ« | ãƒ‡ãƒãƒƒã‚°ãƒ»å‡ºåŠ›è¡¨ç¤º |
L64 | `df_z` | ã‚¦ã‚£ãƒ³ã‚¶ãƒ¼å¾ŒZã‚¹ã‚³ã‚¢åŒ–ã—ãŸæŒ‡æ¨™è¡¨ | å› å­ã‚¹ã‚³ã‚¢åˆæˆã€é¸å®šåŸºæº– |
L65 | `g_score` | Gãƒã‚±ãƒƒãƒˆç·åˆã‚¹ã‚³ã‚¢ | Gé¸å®šã€IN/OUTæ¯”è¼ƒ |
L66 | `d_score_all` | Dãƒã‚±ãƒƒãƒˆç·åˆã‚¹ã‚³ã‚¢ï¼ˆå…¨éŠ˜æŸ„ï¼‰ | Dé¸å®šã€ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚° |
L67 | `missing_logs` | æ¬ ææŒ‡æ¨™ã¨è£œå®ŒçŠ¶æ³ã®ãƒ­ã‚° | ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ |
L68
L69 ### SelectionBundleï¼ˆSelector â†’ Outputï¼‰
L70 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L71 | --- | --- | --- |
L72 | `resG` | Gé¸å®šçµæœã®è©³ç´°dictï¼ˆ`tickers`ã€ç›®çš„å€¤ç­‰ï¼‰ | çµæœä¿å­˜ãƒ»å¹³å‡ç›¸é–¢ãªã©ã®æŒ‡æ¨™è¡¨ç¤º |
L73 | `resD` | Dé¸å®šçµæœã®è©³ç´°dict | åŒä¸Š |
L74 | `top_G` | æœ€çµ‚æ¡ç”¨Gãƒ†ã‚£ãƒƒã‚«ãƒ¼ | æ–°ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæ§‹ç¯‰ |
L75 | `top_D` | æœ€çµ‚æ¡ç”¨Dãƒ†ã‚£ãƒƒã‚«ãƒ¼ | åŒä¸Š |
L76 | `init_G` | DRRSå‰ã®GåˆæœŸå€™è£œ | æƒœã—ãã‚‚å¤–ã‚ŒãŸéŠ˜æŸ„è¡¨ç¤º |
L77 | `init_D` | DRRSå‰ã®DåˆæœŸå€™è£œ | åŒä¸Š |
L78
L79 ### WeightsConfig
L80 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L81 | --- | --- | --- |
L82 | `g` | Gå› å­ï¼ˆGRW/MOM/VOLï¼‰ã®é‡ã¿dict | `g_score`åˆæˆ |
L83 | `d` | Då› å­ï¼ˆD_QAL/D_YLD/D_VOL_RAW/D_TRDï¼‰ã®é‡ã¿dict | `d_score_all`åˆæˆ |
L84
L85 ### DRRSParams
L86 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L87 | --- | --- | --- |
L88 | `corrM` | DRRSåˆæœŸãƒ—ãƒ¼ãƒ«ã®æœ€å¤§ä»¶æ•° | ç›¸é–¢è¡Œåˆ—ã‚µã‚¤ã‚ºåˆ¶å¾¡ |
L89 | `shrink` | æ®‹å·®ç›¸é–¢ã®ã‚·ãƒ¥ãƒªãƒ³ã‚¯ç‡ | `residual_corr`ã®å¯¾è§’å¼·èª¿ |
L90 | `G` | Gãƒã‚±ãƒƒãƒˆç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿dictï¼ˆ`lookback`ç­‰ï¼‰ | `select_bucket_drrs`è¨­å®š |
L91 | `D` | Dãƒã‚±ãƒƒãƒˆç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿dict | åŒä¸Š |
L92 | `cross_mu_gd` | G-Dã‚¯ãƒ­ã‚¹ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£ä¿‚æ•°Î¼ | `select_buckets`ã®ç›®çš„é–¢æ•° |
L93
L94 ### PipelineConfig
L95 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L96 | --- | --- | --- |
L97 | `weights` | `WeightsConfig`ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ | ã‚¹ã‚³ã‚¢åˆæˆã®é‡ã¿å‚ç…§ |
L98 | `drrs` | `DRRSParams`ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ | é¸å®šã‚¹ãƒ†ãƒƒãƒ—ã®è¨­å®šå€¤ |
L99 | `price_max` | å€™è£œéŠ˜æŸ„ã®è¨±å®¹ä¾¡æ ¼ä¸Šé™ | Inputæ®µéšã§ã®ãƒ•ã‚£ãƒ«ã‚¿ |
L100
L101 ## å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
L102 - `winsorize_s` / `robust_z` : å¤–ã‚Œå€¤å‡¦ç†ã¨Zã‚¹ã‚³ã‚¢åŒ–ã€‚
L103 - `_safe_div` / `_safe_last` : ä¾‹å¤–ã‚’æ½°ã—ãŸåˆ†å‰²ãƒ»æœ«å°¾å–å¾—ã€‚
L104 - `_load_prev` / `_save_sel` : é¸å®šçµæœã®èª­ã¿æ›¸ãã€‚
L105
L106 ## ã‚¯ãƒ©ã‚¹è¨­è¨ˆ
L107 ### Step1: Input
L108 `current_tickers.csv`ã®ç¾è¡ŒéŠ˜æŸ„ã¨`candidate_tickers.csv`ã®æ¤œè¨ä¸­éŠ˜æŸ„ã‚’èµ·ç‚¹ã«ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„ã™ã‚‹ã€‚å¤–éƒ¨I/Oã¨å‰å‡¦ç†ã‚’æ‹…å½“ã—ã€`prepare_data`ã§`InputBundle`ã‚’ç”Ÿæˆã€‚ä¾¡æ ¼ãƒ»è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã¯**yfinanceã‚’å„ªå…ˆã—ã€æ¬ æãŒã‚ã‚‹æŒ‡æ¨™ã®ã¿Finnhub APIã§è£œå®Œ**ã™ã‚‹ã€‚
L109 ä¸»ãªãƒ¡ã‚½ãƒƒãƒ‰:
L110 - `impute_eps_ttm` : å››åŠæœŸEPSÃ—4ã§TTMã‚’æ¨å®šã—æ¬ ææ™‚ã®ã¿å·®ã—æ›¿ãˆã€‚
L111 - `fetch_cfo_capex_ttm_yf` : yfinanceã®å››åŠæœŸ/å¹´æ¬¡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼ã‹ã‚‰CFOãƒ»CapExãƒ»FCF TTMã‚’ç®—å‡ºã€‚
L112 - `fetch_cfo_capex_ttm_finnhub` : yfinanceã§æ¬ ã‘ãŸéŠ˜æŸ„ã®ã¿Finnhub APIã§è£œå®Œã€‚
L113 - `compute_fcf_with_fallback` : yfinanceå€¤ã‚’åŸºæº–ã«Finnhubå€¤ã§ç©´åŸ‹ã‚ã—ã€CFO/CapEx/FCFã¨æƒ…å ±æºãƒ•ãƒ©ã‚°ã‚’è¿”ã™ã€‚
L114 - `_build_eps_df` : `info`ã‚„`quarterly_earnings`ã‹ã‚‰EPS TTMã¨ç›´è¿‘EPSã‚’è¨ˆç®—ã—ã€`impute_eps_ttm`ã§è£œå®Œã€‚
L115 - `prepare_data` :
L116     0. CSVã‹ã‚‰ç¾è¡ŒéŠ˜æŸ„ã¨å€™è£œéŠ˜æŸ„ã®ãƒ†ã‚£ãƒƒã‚«ãƒ¼ä¸€è¦§ã‚’èª­ã¿è¾¼ã‚€ã€‚
L117     1. å€™è£œéŠ˜æŸ„ã®ç¾åœ¨å€¤ã‚’å–å¾—ã—ä¾¡æ ¼ä¸Šé™ã§ãƒ•ã‚£ãƒ«ã‚¿ã€‚
L118     2. æ—¢å­˜+å€™è£œã‹ã‚‰å¯¾è±¡ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚’æ±ºå®šã—ã€ä¾¡æ ¼ãƒ»å‡ºæ¥é«˜ã‚’ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆyfinanceï¼‰ã€‚
L119     3. yfinanceå€¤ã‚’åŸºã«EPS/FCFãƒ†ãƒ¼ãƒ–ãƒ«ã‚„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç³»åˆ—ã€ãƒªã‚¿ãƒ¼ãƒ³ã‚’æ§‹ç¯‰ã—ã€æ¬ æã‚»ãƒ«ã¯Finnhubå‘¼ã³å‡ºã—ã§ç©´åŸ‹ã‚ã€‚
L120     4. ä¸Šè¨˜ã‚’`InputBundle`ã«æ ¼ç´ã—ã¦è¿”ã™ã€‚
L121
L122 ### Step2: Score Calculation (Scorer)
L123 ç‰¹å¾´é‡è¨ˆç®—ã¨ã‚¹ã‚³ã‚¢åˆæˆã‚’æ‹…å½“ã—ã€`FeatureBundle`ã‚’è¿”ã™ã€‚
L124
L125 #### è£œåŠ©é–¢æ•°
L126 - `trend(s)` : 50/150/200æ—¥ç§»å‹•å¹³å‡ã‚„52é€±ãƒ¬ãƒ³ã‚¸ã‹ã‚‰-0.5ã€œ0.5ã§æ§‹æˆã•ã‚ŒãŸãƒˆãƒ¬ãƒ³ãƒ‰æŒ‡æ¨™ã€‚
L127 - `rs(s,b)` / `tr_str(s)` / `rs_line_slope(s,b,win)` : ç›¸å¯¾å¼·ã•ã‚„çŸ­æœŸãƒˆãƒ¬ãƒ³ãƒ‰ã€RSå›å¸°å‚¾ãã‚’ç®—å‡ºã€‚
L128 - `ev_fallback` : `enterpriseValue`æ¬ ææ™‚ã«è² å‚µãƒ»ç¾é‡‘ã‹ã‚‰EVã‚’æ¨å®šã€‚
L129 - `dividend_status` / `div_streak` : é…å½“æœªè¨­å®šçŠ¶æ³ã®åˆ¤å®šã¨å¢—é…å¹´æ•°ã‚«ã‚¦ãƒ³ãƒˆã€‚
L130 - `fetch_finnhub_metrics` : Finnhub APIã‹ã‚‰EPSæˆé•·ãƒ»ROEãƒ»Î²ãªã©ä¸è¶³æŒ‡æ¨™ã‚’å–å¾—ã€‚
L131 - `calc_beta` : ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ã®å…±åˆ†æ•£ã‹ã‚‰Î²ã‚’ç®—å‡ºã€‚
L132 - `spx_to_alpha` : S&P500ã®ä½ç½®æƒ…å ±ã‹ã‚‰DRRSã§ç”¨ã„ã‚‹Î±ã‚’æ¨å®šã€‚
L133 - `soft_cap_effective_scores` / `pick_top_softcap` : ã‚»ã‚¯ã‚¿ãƒ¼ã‚½ãƒ•ãƒˆã‚­ãƒ£ãƒƒãƒ—ä»˜ãã‚¹ã‚³ã‚¢èª¿æ•´ã¨ä¸Šä½æŠ½å‡ºã€‚
L134
L135 **è£œåŠ©é–¢æ•°ã¨ç”ŸæˆæŒ‡æ¨™**
L136
L137 | è£œåŠ©é–¢æ•° | ç”ŸæˆæŒ‡æ¨™ | ç•¥ç§° |
L138 | --- | --- | --- |
L139 | `trend` | ãƒˆãƒ¬ãƒ³ãƒ‰ç·åˆå€¤ | `TR` |
L140 | `rs` | ç›¸å¯¾å¼·ã• | `RS` |
L141 | `tr_str` | ä¾¡æ ¼ã¨50æ—¥ç·šã®ä¹–é›¢ | `TR_str` |
L142 | `rs_line_slope` | RSç·šã®å›å¸°å‚¾ã | `RS_SLOPE_*` |
L143 | `calc_beta` | Î² | `BETA` |
L144 | `div_streak` | é€£ç¶šå¢—é…å¹´æ•° | `DIV_STREAK` |
L145
L146 #### `aggregate_scores` è©³ç´°
L147 1. å„éŠ˜æŸ„ã®ä¾¡æ ¼ç³»åˆ—ã‚„`info`ã‚’åŸºã«ä»¥ä¸‹ã‚’ç®—å‡ºã€‚
L148    - **ãƒˆãƒ¬ãƒ³ãƒ‰/ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ **: `TR`ã€`RS`ã€`TR_str`ã€å¤šæ§˜ãªç§»å‹•å¹³å‡æ¯”ã€`RS_SLOPE_*`ãªã©ã€‚
L149    - **ãƒªã‚¹ã‚¯**: `BETA`ã€`DOWNSIDE_DEV`ã€`MDD_1Y`ã€`RESID_VOL`ã€`DOWN_OUTPERF`ã€`EXT_200`ç­‰ã€‚
L150    - **é…å½“**: `DIV`ã€`DIV_TTM_PS`ã€`DIV_VAR5`ã€`DIV_YOY`ã€`DIV_FCF_COVER`ã€`DIV_STREAK`ã€‚
L151    - **è²¡å‹™ãƒ»æˆé•·**: `EPS`ã€`REV`ã€`ROE`ã€`FCF/EV`ã€`REV_Q_YOY`ã€`EPS_Q_YOY`ã€`REV_YOY_ACC`ã€`REV_YOY_VAR`ã€`REV_ANN_STREAK`ã€`RULE40`ã€`FCF_MGN` ç­‰ã€‚
L152    - **å®‰å®šæ€§/ã‚µã‚¤ã‚º**: `DEBT2EQ`ã€`CURR_RATIO`ã€`MARKET_CAP`ã€`ADV60_USD`ã€`EPS_VAR_8Q`ãªã©ã€‚
L153 2. æŒ‡æ¨™æ¬ æã¯Finnhub APIç­‰ã§è£œå®Œã—ã€æœªå–å¾—é …ç›®ã‚’`missing_logs`ã«è¨˜éŒ²ã€‚
L154 3. `winsorize_s`â†’`robust_z`ã§æ¨™æº–åŒ–ã—`df_z`ã¸ä¿å­˜ã€‚ã‚µã‚¤ã‚ºãƒ»æµå‹•æ€§ã¯å¯¾æ•°å¤‰æ›ã€‚
L155 4. æ­£è¦åŒ–æ¸ˆæŒ‡æ¨™ã‹ã‚‰å› å­ã‚¹ã‚³ã‚¢ã‚’åˆæˆã€‚
L156    - å„å› å­ã®æ§‹æˆã¨é‡ã¿ã¯ä»¥ä¸‹ã®é€šã‚Šã€‚
L157      - **GRW**: 0.30Ã—`REV` + 0.20Ã—`EPS_Q_YOY` + 0.15Ã—`REV_Q_YOY` + 0.15Ã—`REV_YOY_ACC` + 0.10Ã—`RULE40` + 0.10Ã—`FCF_MGN` + 0.10Ã—`REV_ANN_STREAK` âˆ’ 0.05Ã—`REV_YOY_VAR`ã€‚
L158      - **MOM**: 0.40Ã—`RS` + 0.15Ã—`TR_str` + 0.15Ã—`RS_SLOPE_6W` + 0.15Ã—`RS_SLOPE_13W` + 0.10Ã—`MA200_SLOPE_5M` + 0.10Ã—`MA200_UP_STREAK_D`ã€‚
L159      - **VOL**: `BETA`å˜ä½“ã‚’ä½¿ç”¨ã€‚
L160      - **QAL**: 0.60Ã—`FCF_W` + 0.40Ã—`ROE_W`ã§ä½œæˆã€‚
L161      - **YLD**: 0.30Ã—`DIV` + 0.70Ã—`DIV_STREAK`ã€‚
L162      - **D_QAL**: 0.35Ã—`QAL` + 0.20Ã—`FCF` + 0.15Ã—`CURR_RATIO` âˆ’ 0.15Ã—`DEBT2EQ` âˆ’ 0.15Ã—`EPS_VAR_8Q`ã€‚
L163      - **D_YLD**: 0.45Ã—`DIV` + 0.25Ã—`DIV_STREAK` + 0.20Ã—`DIV_FCF_COVER` âˆ’ 0.10Ã—`DIV_VAR5`ã€‚
L164      - **D_VOL_RAW**: 0.40Ã—`DOWNSIDE_DEV` + 0.22Ã—`RESID_VOL` + 0.18Ã—`MDD_1Y` âˆ’ 0.10Ã—`DOWN_OUTPERF` âˆ’ 0.05Ã—`EXT_200` âˆ’ 0.08Ã—`SIZE` âˆ’ 0.10Ã—`LIQ` + 0.10Ã—`BETA`ã€‚
L165      - **D_TRD**: 0.40Ã—`MA200_SLOPE_5M` âˆ’ 0.30Ã—`EXT_200` + 0.15Ã—`NEAR_52W_HIGH` + 0.15Ã—`TR`ã€‚
L166     - ä¸»ãªæŒ‡æ¨™ã®ç•¥ç§°ã¨æ„å‘³:
L167
L168       | ç•¥ç§° | è£œåŠ©é–¢æ•° | æ¦‚è¦ |
L169       | --- | --- | --- |
L170       | TR | `trend` | 50/150/200æ—¥ç§»å‹•å¹³å‡ã¨52é€±ãƒ¬ãƒ³ã‚¸ã‚’çµ„ã¿åˆã‚ã›ãŸãƒˆãƒ¬ãƒ³ãƒ‰ç·åˆå€¤ |
L171       | RS | `rs` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«å¯¾ã™ã‚‹ç›¸å¯¾å¼·ã•ï¼ˆ12M/1Mãƒªã‚¿ãƒ¼ãƒ³å·®ï¼‰ |
L172       | TR_str | `tr_str` | ä¾¡æ ¼ã¨50æ—¥ç§»å‹•å¹³å‡ã®ä¹–é›¢ |
L173       | RS_SLOPE_6W | `rs_line_slope` | ç›¸å¯¾å¼·ã•ç·šã®6é€±å›å¸°å‚¾ã |
L174       | RS_SLOPE_13W | `rs_line_slope` | ç›¸å¯¾å¼·ã•ç·šã®13é€±å›å¸°å‚¾ã |
L175       | MA200_SLOPE_5M | - | 200æ—¥ç§»å‹•å¹³å‡ã®5ã‹æœˆé¨°è½ç‡ |
L176       | MA200_UP_STREAK_D | - | 200æ—¥ç·šãŒé€£ç¶šã§ä¸Šå‘ã„ãŸæ—¥æ•° |
L177       | BETA | `calc_beta` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«å¯¾ã™ã‚‹Î² |
L178       | DOWNSIDE_DEV | - | ä¸‹æ–¹ãƒªã‚¿ãƒ¼ãƒ³ã®ã¿ã®å¹´ç‡åŒ–æ¨™æº–åå·® |
L179       | RESID_VOL | - | Î²ã§èª¿æ•´ã—ãŸæ®‹å·®ãƒªã‚¿ãƒ¼ãƒ³ã®å¹´ç‡åŒ–æ¨™æº–åå·® |
L180       | MDD_1Y | - | éå»1å¹´ã®æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³ |
L181       | DOWN_OUTPERF | - | å¸‚å ´ä¸‹è½æ—¥ã«å¯¾ã™ã‚‹å¹³å‡è¶…éãƒªã‚¿ãƒ¼ãƒ³ |
L182       | EXT_200 | - | 200æ—¥ç§»å‹•å¹³å‡ã‹ã‚‰ã®çµ¶å¯¾ä¹–é›¢ç‡ |
L183       | NEAR_52W_HIGH | - | 52é€±é«˜å€¤ã¾ã§ã®ä¸‹æ–¹è·é›¢ï¼ˆ0=é«˜å€¤ï¼‰ |
L184       | FCF_W | - | ã‚¦ã‚£ãƒ³ã‚¶ãƒ¼å‡¦ç†å¾Œã®FCF/EV |
L185       | ROE_W | - | ã‚¦ã‚£ãƒ³ã‚¶ãƒ¼å‡¦ç†å¾Œã®ROE |
L186       | FCF | - | FCF/EV |
L187       | QAL | - | FCF_Wã¨ROE_Wã‚’çµ„ã¿åˆã‚ã›ãŸå“è³ªã‚¹ã‚³ã‚¢ |
L188       | CURR_RATIO | - | æµå‹•æ¯”ç‡ |
L189       | DEBT2EQ | - | è² å‚µè³‡æœ¬å€ç‡ |
L190       | EPS_VAR_8Q | - | EPSã®8å››åŠæœŸæ¨™æº–åå·® |
L191       | DIV | - | å¹´ç‡æ›ç®—é…å½“åˆ©å›ã‚Š |
L192       | DIV_STREAK | `div_streak` | é€£ç¶šå¢—é…å¹´æ•° |
L193       | DIV_FCF_COVER | - | é…å½“ã®FCFã‚«ãƒãƒ¬ãƒƒã‚¸ |
L194       | DIV_VAR5 | - | 5å¹´é…å½“å¤‰å‹•ç‡ |
L195       | DIV_TTM_PS | - | 1æ ªå½“ãŸã‚ŠTTMé…å½“ |
L196       | DIV_YOY | - | å‰å¹´æ¯”é…å½“æˆé•·ç‡ |
L197       | REV | - | å£²ä¸Šæˆé•·ç‡TTM |
L198       | EPS_Q_YOY | - | å››åŠæœŸEPSã®å‰å¹´åŒæœŸæ¯” |
L199       | REV_Q_YOY | - | å››åŠæœŸå£²ä¸Šã®å‰å¹´åŒæœŸæ¯” |
L200       | REV_YOY_ACC | - | å£²ä¸Šæˆé•·ç‡ã®åŠ é€Ÿåˆ† |
L201       | RULE40 | - | å£²ä¸Šæˆé•·ç‡ã¨FCFãƒãƒ¼ã‚¸ãƒ³ã®åˆè¨ˆ |
L202       | FCF_MGN | - | FCFãƒãƒ¼ã‚¸ãƒ³ |
L203       | REV_ANN_STREAK | - | å¹´æ¬¡å£²ä¸Šæˆé•·ã®é€£ç¶šå¹´æ•° |
L204       | REV_YOY_VAR | - | å¹´æ¬¡å£²ä¸Šæˆé•·ç‡ã®å¤‰å‹•æ€§ |
L205       | SIZE | - | æ™‚ä¾¡ç·é¡ã®å¯¾æ•°å€¤ |
L206       | LIQ | - | 60æ—¥å¹³å‡å‡ºæ¥é«˜ãƒ‰ãƒ«ã®å¯¾æ•°å€¤ |
L207    - Gãƒã‚±ãƒƒãƒˆ: `GRW`ã€`MOM`ã€`VOL`ã‚’`cfg.weights.g`ï¼ˆ0.40/0.45/-0.15ï¼‰ã§åŠ é‡ã—`g_score`ã‚’å¾—ã‚‹ã€‚
L208    - Dãƒã‚±ãƒƒãƒˆ: `D_QAL`ã€`D_YLD`ã€`D_VOL_RAW`ã€`D_TRD`ã‚’`cfg.weights.d`ï¼ˆ0.15/0.15/-0.45/0.25ï¼‰ã§åŠ é‡ã—`d_score_all`ã‚’ç®—å‡ºã€‚
L209    - ã‚»ã‚¯ã‚¿ãƒ¼capã«ã‚ˆã‚‹`soft_cap_effective_scores`ã‚’é©ç”¨ã—ã€Gæ¡ç”¨éŠ˜æŸ„ã«ã¯ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨ã€‚
L210 5. `_apply_growth_entry_flags`ã§ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆ/æŠ¼ã—ç›®ç™ºç«çŠ¶æ³ã‚’ä»˜åŠ ã—ã€`FeatureBundle`ã‚’è¿”ã™ã€‚
L211
L212 ### Step3: Correlation Reduction & Selection (Selector)
L213 DRRSã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ç›¸é–¢ã‚’æŠ‘ãˆãŸéŠ˜æŸ„é¸å®šã‚’è¡Œã„ã€`SelectionBundle`ã‚’è¿”ã™ã€‚`results/`ã«ä¿å­˜ã•ã‚ŒãŸå‰å›é¸å®šï¼ˆ`G_selection.json` / `D_selection.json`ï¼‰ã‚’`_load_prev`ã§èª­ã¿è¾¼ã¿ã€ç›®çš„å€¤ãŒå¤§ããæ‚ªåŒ–ã—ãªã„é™ã‚Šç¶­æŒã™ã‚‹ã€‚æ–°ã—ã„æ¡ç”¨é›†åˆã¯`_save_sel`ã§JSONã«æ›¸ãå‡ºã—æ¬¡å›ä»¥é™ã®å…¥åŠ›ã«å‚™ãˆã‚‹ã€‚
L214 ä¸»ãªãƒ¡ã‚½ãƒƒãƒ‰:
L215 - `residual_corr` : åç›Šç‡è¡Œåˆ—ã‚’Zã‚¹ã‚³ã‚¢åŒ–ã—ã€ä¸Šä½ä¸»æˆåˆ†ã‚’é™¤å»ã—ãŸæ®‹å·®ã‹ã‚‰ç›¸é–¢è¡Œåˆ—ã‚’æ±‚ã‚ã€å¹³å‡ç›¸é–¢ã«å¿œã˜ã¦ã‚·ãƒ¥ãƒªãƒ³ã‚¯ã€‚
L216 - `rrqr_like_det` : ã‚¹ã‚³ã‚¢ã‚’é‡ã¿ä»˜ã‘ã—ãŸQRåˆ†è§£é¢¨ã®æ‰‹é †ã§åˆæœŸå€™è£œã‚’kä»¶æŠ½å‡ºã—ã€ã‚¹ã‚³ã‚¢ã®é«˜ã„éç›¸é–¢ãªé›†åˆã‚’å¾—ã‚‹ã€‚
L217 - `swap_local_det` / `swap_local_det_cross` : `sum(score) - Î»*within_corr - Î¼*cross_corr`ã‚’ç›®çš„é–¢æ•°ã¨ã—ã¦ã€å…¥ã‚Œæ›¿ãˆæ¢ç´¢ã§å±€æ‰€çš„ã«æœ€é©åŒ–ã€‚
L218 - `select_bucket_drrs` : ãƒ—ãƒ¼ãƒ«éŠ˜æŸ„ã¨ã‚¹ã‚³ã‚¢ã‹ã‚‰æ®‹å·®ç›¸é–¢ã‚’è¨ˆç®—ã—ã€ä¸Šè¨˜2æ®µéš(åˆæœŸé¸æŠâ†’å…¥ã‚Œæ›¿ãˆ)ã§kéŠ˜æŸ„ã‚’æ±ºå®šã€‚éå»æ¡ç”¨éŠ˜æŸ„ã¨ã®æ¯”è¼ƒã§ç›®çš„å€¤ãŒåŠ£åŒ–ã—ãªã‘ã‚Œã°ç¶­æŒã™ã‚‹ã€‚
L219 - `select_buckets` : Gãƒã‚±ãƒƒãƒˆã‚’é¸å®šå¾Œã€ãã®çµæœã‚’é™¤ã„ãŸå€™è£œã‹ã‚‰Dãƒã‚±ãƒƒãƒˆã‚’é¸ã¶ã€‚Dé¸å®šæ™‚ã¯Gã¨ã®ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£Î¼ã‚’ä»˜ä¸ã—ã€ä¸¡ãƒã‚±ãƒƒãƒˆã®åˆ†æ•£ã‚’åˆ¶å¾¡ã™ã‚‹ã€‚
L220
L221 #### ç›¸é–¢ä½æ¸›ãƒ­ã‚¸ãƒƒã‚¯è©³ç´°
L222 1. **æ®‹å·®ç›¸é–¢è¡Œåˆ—ã®æ§‹ç¯‰ (`residual_corr`)**
L223    - ãƒªã‚¿ãƒ¼ãƒ³è¡Œåˆ—`R`ã‚’Zã‚¹ã‚³ã‚¢åŒ–ã€‚
L224    - SVDã§ä¸Šä½`n_pc`ä¸»æˆåˆ†`F`ã‚’æ±‚ã‚ã€æœ€å°äºŒä¹—ã§ä¿‚æ•°`B`ã‚’ç®—å‡ºã—æ®‹å·®`E = Z - F@B`ã‚’å¾—ã‚‹ã€‚
L225    - `E`ã®ç›¸é–¢è¡Œåˆ—`C`ã‚’è¨ˆç®—ã—ã€å¹³å‡çµ¶å¯¾ç›¸é–¢ã«å¿œã˜ã¦ã‚·ãƒ¥ãƒªãƒ³ã‚¯é‡`shrink_eff`ã‚’è£œæ­£ã—ã¦å¯¾è§’ã‚’å¼·èª¿ã€‚
L226 2. **åˆæœŸå€™è£œã®æŠ½å‡º (`rrqr_like_det`)**
L227    - ã‚¹ã‚³ã‚¢ã‚’0-1æ­£è¦åŒ–ã—ãŸé‡ã¿`w`ã¨ã—ã€`Z*(1+Î³w)`ã§åˆ—ãƒãƒ«ãƒ ã‚’å¼·èª¿ã€‚
L228    - æ®‹å·®ãƒãƒ«ãƒ æœ€å¤§ã®åˆ—ã‚’é€æ¬¡é¸ã³ã€QRãƒ©ã‚¤ã‚¯ãªãƒ‡ãƒ•ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã£ã¦éç›¸é–¢ã‹ã¤é«˜ã‚¹ã‚³ã‚¢ãª`k`éŠ˜æŸ„é›†åˆ`S0`ã‚’å¾—ã‚‹ã€‚
L229 3. **å±€æ‰€æ¢ç´¢ (`swap_local_det` / `swap_local_det_cross`)**
L230    - ç›®çš„é–¢æ•°`Î£z_score âˆ’ Î»Â·within_corr âˆ’ Î¼Â·cross_corr`ã‚’æœ€å¤§åŒ–ã€‚
L231    - é¸æŠé›†åˆã®å„éŠ˜æŸ„ã‚’ä»–å€™è£œã¨å…¥ã‚Œæ›¿ãˆã€æ”¹å–„ãŒãªããªã‚‹ã¾ã§ã¾ãŸã¯`max_pass`å›ã¾ã§æ¢ç´¢ã€‚
L232    - `swap_local_det_cross`ã¯Gãƒã‚±ãƒƒãƒˆã¨ã®ã‚¯ãƒ­ã‚¹ç›¸é–¢è¡Œåˆ—`C_cross`ã‚’ä½¿ç”¨ã—ã€ãƒšãƒŠãƒ«ãƒ†ã‚£`Î¼`ã‚’ä»˜ä¸ã€‚
L233 4. **éå»æ¡ç”¨ã®ç¶­æŒã¨ã‚¯ãƒ­ã‚¹ãƒšãƒŠãƒ«ãƒ†ã‚£ (`select_bucket_drrs` / `select_buckets`)**
L234    - å±€æ‰€æ¢ç´¢çµæœ`S`ã¨éå»é›†åˆ`P`ã®ç›®çš„å€¤ã‚’æ¯”è¼ƒã—ã€`S`ãŒ`P`ã‚ˆã‚Š`Î·`æœªæº€ã®æ”¹å–„ãªã‚‰`P`ã‚’ç¶­æŒã€‚
L235    - `select_buckets`ã§ã¯Gã‚’å…ˆã«æ±ºå®šã—ã€Dé¸å®šæ™‚ã«Gã¨ã®ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£`Î¼`ã‚’åŠ ãˆã¦ã‚¯ãƒ­ã‚¹åˆ†æ•£ã‚’æŠ‘åˆ¶ã€‚
L236
L237 ### Step4: Output
L238 é¸å®šçµæœã‚’å¯è¦–åŒ–ã—å…±æœ‰ã™ã‚‹å·¥ç¨‹ã€‚ä»¥ä¸‹ã®å†…å®¹ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«åŒ–ã—ã¦æ¨™æº–å‡ºåŠ›ã¨Slackã¸é€ã‚‹ã€‚
L239 - æ¡ç”¨éŠ˜æŸ„ã¨æƒœã—ãã‚‚é¸å¤–ã¨ãªã£ãŸéŠ˜æŸ„ã®ã‚¹ã‚³ã‚¢ä¸€è¦§
L240 - IN/OUTãƒªã‚¹ãƒˆã¨OUTéŠ˜æŸ„ã®ã‚¹ã‚³ã‚¢ï¼ˆä½å¾—ç‚¹éŠ˜æŸ„ã‚’ç¢ºèªã—ã‚„ã™ãï¼‰
L241 - æ–°æ—§ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®æ¯”è¼ƒè¡¨ï¼ˆçµ„å…¥ã‚Œãƒ»é™¤å¤–ã€ã‚¹ã‚³ã‚¢å¤‰åŒ–ï¼‰
L242 - æ¤œè¨ä¸­éŠ˜æŸ„ã®ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°
L243
L244 ä¸»ãªãƒ¡ã‚½ãƒƒãƒ‰:
L245 - `display_results` : ä¸Šè¨˜ãƒ†ãƒ¼ãƒ–ãƒ«ã«åŠ ãˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã‚„åˆ†æ•£åŒ–æŒ‡æ¨™ã‚’è¡¨ç¤ºã€‚
L246 - `notify_slack` : Slack Webhookã¸åŒå†…å®¹ã‚’é€ä¿¡ã€‚
L247 - è£œåŠ©:`_avg_offdiag`ã€`_resid_avg_rho`ã€`_raw_avg_rho`ã€`_cross_block_raw_rho`ã€‚
L248
L249 ## ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
L250 1. `PipelineConfig`ã‚’æ§‹ç¯‰ã€‚
L251 2. **Step1** `Input.prepare_data`ã§`InputBundle`ã‚’ç”Ÿæˆã€‚
L252 3. **Step2** `Scorer.aggregate_scores`ã§`FeatureBundle`ã‚’å–å¾—ã€‚
L253 4. **Step3** `Selector.select_buckets`ã§`SelectionBundle`ã‚’ç®—å‡ºã€‚
L254 5. **Step4** `Output.display_results`ã¨`notify_slack`ã§çµæœã‚’å‡ºåŠ›ã€‚
```

# === Chat Paste Pack ===
# Repo: dakara32/GPT_Code @ main
# Files: config.py, factor.py, scorer.py, .github/workflows/weekly-report.yml, documents/README.md, documents/factor_design.md
# 作成日時: 2025-09-26 18:42:19 (JST)
# 使い方: 下のチャンクを順に貼ればこのチャットで全体把握できます。
# 注記: 各ファイルは個別に L1.. で行番号付与。
---

## <config.py>
```text
L1 # 共通設定（factor / drift から参照）
L2 TOTAL_TARGETS = 20
L3
L4 # 基準のバケット数（NORMAL）
L5 COUNTS_BASE = {"G": 12, "D": 8}
L6
L7 # モード別の推奨バケット数
L8 COUNTS_BY_MODE = {
L9     "NORMAL": {"G": 12, "D": 8},
L10     "CAUTION": {"G": 10, "D": 8},
L11     "EMERG": {"G": 8,  "D": 8},
L12 }
L13
L14 # モード別のドリフト閾値（%）
L15 DRIFT_THRESHOLD_BY_MODE = {"NORMAL": 12, "CAUTION": 14, "EMERG": float("inf")}
L16
L17 # モード別のTS（基本幅, 小数=割合）
L18 TS_BASE_BY_MODE = {"NORMAL": 0.15, "CAUTION": 0.13, "EMERG": 0.10}
L19 # 利益到達(+30/+60/+100%)時の段階タイト化（ポイント差）
L20 TS_STEP_DELTAS_PT = (3, 6, 8)
L21
L22 # Breadthの校正は N_G に連動（緊急解除=ceil(1.5*N_G), 通常復帰=3*N_G）
L23 N_G = COUNTS_BASE["G"]
L24 N_D = COUNTS_BASE["D"]
L25
```

## <factor.py>
```text
L1 '''ROLE: Orchestration ONLY（外部I/O・SSOT・Slack出力）, 計算は scorer.py'''
L2 # === NOTE: 機能・入出力・ログ文言・例外挙動は不変。安全な短縮（import統合/複数代入/内包表記/メソッドチェーン/一行化/空行圧縮など）のみ適用 ===
L3 import logging, os, time, requests
L4 from concurrent.futures import ThreadPoolExecutor
L5 from dataclasses import dataclass
L6 from time import perf_counter
L7 from typing import Any, Dict, List, Tuple
L8
L9 import numpy as np
L10 import pandas as pd
L11 import yfinance as yf
L12 from typing import Iterable, Optional
L13
L14
L15 # --- [MOD] bucket更新ヘルパー（対象拡張 & 優先順位つき） ---
L16 def _update_bucket_by_selection(
L17     csv_path: str,
L18     top_G: Iterable[str],
L19     top_D: Iterable[str],
L20     extra_G: Optional[Iterable[str]] = None,
L21     extra_D: Optional[Iterable[str]] = None,
L22 ) -> None:
L23     """
L24     current_tickers.csv の bucket 列を、選定結果に基づき部分上書きする。
L25     - 対象は (top_G/top_D) に加えて (resG['tickers']/resD['tickers']/init_G/init_D 等) も反映
L26     - 優先順位: top_G > top_D > extra_G > extra_D > 現状維持
L27     - 常にヘッダー無しで (ticker,qty,bucket) の3列で保存
L28     """
L29     df = pd.read_csv(csv_path, header=None, names=["ticker", "qty", "bucket"])
L30     df["ticker"] = df["ticker"].astype(str).str.strip().str.upper()
L31     df["qty"] = pd.to_numeric(df["qty"], errors="coerce").fillna(0).astype(int)
L32     df["bucket"] = df["bucket"].fillna("").astype(str).str.strip().str.upper()
L33
L34     g_top = set(t.upper() for t in (top_G or []))
L35     d_top = set(t.upper() for t in (top_D or []))
L36     g_ext = set(t.upper() for t in (extra_G or []))
L37     d_ext = set(t.upper() for t in (extra_D or []))
L38
L39     def _assign(row):
L40         t = row["ticker"]
L41         # 優先順位: top_G > top_D > extra_G > extra_D > 現状維持
L42         if t in g_top:
L43             return "G"
L44         if t in d_top:
L45             return "D"
L46         if t in g_ext:
L47             return "G"
L48         if t in d_ext:
L49             return "D"
L50         return row["bucket"]
L51
L52     df["bucket"] = df.apply(_assign, axis=1)
L53     df[["ticker", "qty", "bucket"]].to_csv(csv_path, index=False, header=False)
L54     logging.info("current_tickers.csv abspath: %s", os.path.abspath(csv_path))
L55     logging.info(
L56         "[I/O] current_tickers.csv bucket updated (topG=%d, topD=%d, extraG=%d, extraD=%d)",
L57         len(g_top), len(d_top), len(g_ext), len(d_ext)
L58     )
L59 from scipy.stats import zscore  # used via scorer
L60
L61 from scorer import Scorer, ttm_div_yield_portfolio, _log, _as_numeric_series
L62 import config
L63
L64 import warnings, atexit, threading
L65 from collections import Counter
L66
L67 # === 定数・設定・DTO（import直後に集約） ===
L68 BONUS_COEFF = 0.55  # 推奨: 攻め=0.45 / 中庸=0.55 / 守り=0.65
L69 SWAP_DELTA_Z = 0.15   # 僅差判定: σの15%。(緩め=0.10 / 標準=0.15 / 固め=0.20)
L70 SWAP_KEEP_BUFFER = 3  # n_target+この順位以内の現行は保持。(粘り弱=2 / 標準=3 / 粘り強=4〜5)
L71
L72 debug_mode, FINNHUB_API_KEY = True, os.environ.get("FINNHUB_API_KEY")
L73
L74 _CSV_LOAD_START = perf_counter()
L75 exist, cand = [pd.read_csv(f, header=None)[0].tolist() for f in ("current_tickers.csv","candidate_tickers.csv")]
L76 CAND_PRICE_MAX, bench = 450, '^GSPC'  # 価格上限・ベンチマーク
L77 N_G, N_D = config.N_G, config.N_D  # G/D枠サイズ（NORMAL基準: G12/D8）
L78 g_weights = {'GROWTH_F':0.30,'MOM':0.60,'VOL':-0.10}
L79 D_BETA_MODE = os.environ.get("D_BETA_MODE", "z").lower()   # "raw" or "z"
L80 D_BETA_CUTOFF = float(os.environ.get("D_BETA_CUTOFF", "-0.8"))
L81 FILTER_SPEC = {"G":{"pre_mask":["trend_template"]},"D":{"pre_filter":{"beta_max":D_BETA_CUTOFF}}}
L82 D_weights = {'QAL':0.15,'YLD':0.25,'VOL':-0.40,'TRD':0.20}
L83 _fmt_w = lambda w: " ".join(f"{k}{int(v*100)}" for k, v in w.items())
L84
L85 def _zscore_series(s: pd.Series) -> pd.Series:
L86     # NaNはそのまま、標準偏差0なら全NaNにする（暴走防止）
L87     v = s.astype(float)
L88     m, std = v.mean(skipna=True), v.std(skipna=True, ddof=0)
L89     if not np.isfinite(std) or std == 0:
L90         return pd.Series(index=v.index, dtype=float)
L91     return (v - m) / std
L92
L93 # DRRS 初期プール・各種パラメータ
L94 corrM = 45
L95 DRRS_G, DRRS_D = dict(lookback=252,n_pc=3,gamma=1.2,lam=0.68,eta=0.8), dict(lookback=504,n_pc=4,gamma=0.8,lam=0.85,eta=0.5)
L96 DRRS_SHRINK = 0.10  # 残差相関の対角シュリンク（基礎）
L97
L98 # クロス相関ペナルティ（未定義なら設定）
L99 try: CROSS_MU_GD
L100 except NameError: CROSS_MU_GD = 0.40  # 推奨 0.35–0.45（lam=0.85想定）
L101
L102 # 出力関連
L103 RESULTS_DIR = "results"
L104 os.makedirs(RESULTS_DIR, exist_ok=True)
L105
L106 # === 共有DTO（クラス間I/O契約）＋ Config ===
L107 @dataclass(frozen=True)
L108 class InputBundle:
L109     # Input → Scorer で受け渡す素材（I/O禁止の生データ）
L110     cand: List[str]
L111     tickers: List[str]
L112     bench: str
L113     data: pd.DataFrame              # yfinance download結果（'Close','Volume'等の階層列）
L114     px: pd.DataFrame                # data['Close']
L115     spx: pd.Series                  # data['Close'][bench]
L116     tickers_bulk: object            # yfinance.Tickers
L117     info: Dict[str, dict]           # yfinance info per ticker
L118     eps_df: pd.DataFrame            # ['eps_ttm','eps_q_recent',...]
L119     fcf_df: pd.DataFrame            # ['fcf_ttm', ...]
L120     returns: pd.DataFrame           # px[tickers].pct_change()
L121     missing_logs: pd.DataFrame
L122
L123 @dataclass(frozen=True)
L124 class FeatureBundle:
L125     df: pd.DataFrame
L126     df_z: pd.DataFrame
L127     g_score: pd.Series
L128     d_score_all: pd.Series
L129     missing_logs: pd.DataFrame
L130     df_full: pd.DataFrame | None = None
L131     df_full_z: pd.DataFrame | None = None
L132     scaler: Any | None = None
L133
L134 @dataclass(frozen=True)
L135 class SelectionBundle:
L136     resG: dict
L137     resD: dict
L138     top_G: List[str]
L139     top_D: List[str]
L140     init_G: List[str]
L141     init_D: List[str]
L142
L143 @dataclass(frozen=True)
L144 class WeightsConfig:
L145     g: Dict[str,float]
L146     d: Dict[str,float]
L147
L148 @dataclass(frozen=True)
L149 class DRRSParams:
L150     corrM: int
L151     shrink: float
L152     G: Dict[str,float]   # lookback, n_pc, gamma, lam, eta
L153     D: Dict[str,float]
L154     cross_mu_gd: float
L155
L156 @dataclass(frozen=True)
L157 class PipelineConfig:
L158     weights: WeightsConfig
L159     drrs: DRRSParams
L160     price_max: float
L161     debug_mode: bool = False
L162
L163 # ---------- 重複警告の集約ロジック ----------
L164 _warn_lock = threading.Lock()
L165 _warn_seen = set()                     # 初回表示済みキー
L166 _warn_count = Counter()                # (category, message, module) → 件数
L167 _warn_first_ctx = {}                   # 初回の (filename, lineno)
L168
L169 def _warn_key(message, category, filename, lineno, *_args, **_kwargs):
L170     # "同じ警告" を定義: カテゴリ + 正規化メッセージ + モジュールパス(先頭数階層)
L171     mod = filename.split("/site-packages/")[-1] if "/site-packages/" in filename else filename
L172     mod = mod.rsplit("/", 3)[-1]  # 長すぎ抑制（末尾3階層まで）
L173     msg = str(message).strip()
L174     return (category.__name__, msg, mod)
L175
L176 _orig_showwarning = warnings.showwarning
L177
L178 def _compact_showwarning(message, category, filename, lineno, file=None, line=None):
L179     key = _warn_key(message, category, filename, lineno)
L180     with _warn_lock:
L181         _warn_count[key] += 1
L182         if key not in _warn_seen:
L183             # 初回だけ1行で出す（カテゴリ | モジュール | メッセージ）
L184             _warn_seen.add(key)
L185             _warn_first_ctx[key] = (filename, lineno)
L186             # 1行フォーマット（行数節約）
L187             txt = f"[WARN][{category.__name__}] {message} | {filename}:{lineno}"
L188             print(txt)
L189         # 2回目以降は出さない（集約）
L190
L191 warnings.showwarning = _compact_showwarning
L192
L193 # ベースポリシー: 通常は警告を出す（default）→ ただし同一メッセージは集約
L194 warnings.resetwarnings()
L195 warnings.simplefilter("default")
L196
L197 # 2) ピンポイント間引き: yfinance 'Ticker.earnings' は "once"（初回のみ可視化）
L198 warnings.filterwarnings(
L199     "once",
L200     message="'Ticker.earnings' is deprecated",
L201     category=DeprecationWarning,
L202     module="yfinance"
L203 )
L204
L205 # 3) 最終サマリ: 同一警告が何回出たかを最後に1行で
L206 @atexit.register
L207 def _print_warning_summary():
L208     suppressed = []
L209     for key, cnt in _warn_count.items():
L210         if cnt > 1:
L211             (cat, msg, mod) = key
L212             filename, lineno = _warn_first_ctx.get(key, ("", 0))
L213             suppressed.append((cnt, cat, msg, mod, filename, lineno))
L214     if suppressed:
L215         suppressed.sort(reverse=True)  # 件数降順
L216         # 最多上位だけ出す（必要なら上限制御：ここでは上位10件）
L217         top = suppressed[:10]
L218         print(f"[WARN-SUMMARY] duplicated warning groups: {len(suppressed)}")
L219         for cnt, cat, msg, mod, filename, lineno in top:
L220             print(f"[WARN-SUMMARY] {cnt-1} more | [{cat}] {msg} | {mod} ({filename}:{lineno})")
L221         if len(suppressed) > len(top):
L222             print(f"[WARN-SUMMARY] ... and {len(suppressed)-len(top)} more groups suppressed")
L223
L224 # 4) 追加（任意）: 1ジョブあたりの総警告上限を設定したい場合
L225 #    例: 上限1000を超えたら以降は完全サイレント
L226 _WARN_HARD_LIMIT = int(os.getenv("WARN_HARD_LIMIT", "0") or "0")  # 0なら無効
L227 if _WARN_HARD_LIMIT > 0:
L228     _orig_warn_func = warnings.warn
L229     def _limited_warn(*a, **k):
L230         total = sum(_warn_count.values())
L231         if total < _WARN_HARD_LIMIT:
L232             return _orig_warn_func(*a, **k)
L233         # 超過後は捨てる（最後にsummaryだけ残る）
L234     warnings.warn = _limited_warn
L235
L236 # ---------- ここまでで警告の“可視性は維持”しつつ“重複で行数爆発”を抑止 ----------
L237
L238 # その他
L239 logger = logging.getLogger(__name__)
L240 logging.basicConfig(level=(logging.INFO if debug_mode else logging.WARNING), force=True)
L241
L242 _T0 = [perf_counter()]
L243
L244
L245 def _tlog(tag: str) -> None:
L246     now = perf_counter()
L247     print(f"[T] {tag}: {now - _T0[0]:.2f}s")
L248     _T0[0] = now
L249
L250
L251 _tlog("start")
L252 try:
L253     _T0[0] = _CSV_LOAD_START
L254 except NameError:
L255     pass
L256 _tlog(f"csv loaded: exist={len(exist)} cand={len(cand)}")
L257
L258 # === Utilities ===
L259 # （ここには “本体ロジック直下で使う軽量ヘルパ” のみを残す）
L260
L261
L262 _env_true = lambda name, default=False: (os.getenv(name) or str(default)).strip().lower() == "true"
L263
L264 def _disjoint_keepG(top_G, top_D, poolD):
L265     """G重複をDから除去し、poolDで順次補充（枯渇時は元銘柄維持）。"""
L266     used, D, i = set(top_G), list(top_D), 0
L267     for j, t in enumerate(D):
L268         if t not in used:
L269             continue
L270         while i < len(poolD) and (poolD[i] in used or poolD[i] in D):
L271             i += 1
L272         if i < len(poolD):
L273             D[j] = poolD[i]; used.add(D[j]); i += 1
L274     return top_G, D
L275
L276
L277 def _sticky_keep_current(agg: pd.Series, pick: list[str], incumbents: list[str],
L278                          n_target: int, delta_z: float, keep_buffer: int) -> list[str]:
L279     import pandas as pd, numpy as np
L280     sel = list(pick)
L281     if not sel: return sel
L282     ranked_sel = agg.reindex(sel).sort_values(ascending=False)
L283     kth = ranked_sel.iloc[min(len(sel), n_target)-1]
L284     std = agg.std()
L285     sigma = float(std) if pd.notna(std) else 0.0
L286     thresh = kth - delta_z * sigma
L287     ranked_all = agg.sort_values(ascending=False)
L288     cand = [t for t in incumbents if (t not in sel) and (t in agg.index)]
L289     for t in cand:
L290         within_score = pd.notna(agg[t]) and agg[t] >= thresh
L291         within_rank = t in ranked_all.index and ranked_all.index.get_loc(t) < n_target + keep_buffer
L292         if not (within_score or within_rank):
L293             continue
L294         non_inc = [x for x in sel if x not in incumbents]
L295         if not non_inc:
L296             break
L297         weakest = min(non_inc, key=lambda x: agg.get(x, -np.inf))
L298         if weakest in sel and agg.get(t, -np.inf) >= agg.get(weakest, -np.inf):
L299             sel.remove(weakest); sel.append(t)
L300     if len(sel) > n_target:
L301         sel = sorted(sel, key=lambda x: agg.get(x, -1e9), reverse=True)[:n_target]
L302     return sel
L303
L304
L305 # === Input：外部I/Oと前処理（CSV/API・欠損補完） ===
L306 class Input:
L307     def __init__(self, cand, exist, bench, price_max, finnhub_api_key=None):
L308         self.cand, self.exist, self.bench, self.price_max = cand, exist, bench, price_max
L309         self.api_key = finnhub_api_key or os.environ.get("FINNHUB_API_KEY")
L310
L311     # ---- （Input専用）EPS補完・FCF算出系 ----
L312     @staticmethod
L313     def _sec_headers():
L314         mail = (os.getenv("SEC_CONTACT_EMAIL") or os.getenv("SEC_EMAIL") or "").strip()
L315         ua = f"factor-selection/1 (+mailto:{mail})" if mail else "factor-selection/1"
L316         headers = {"User-Agent": ua[:200], "Accept": "application/json"}
L317         if mail:
L318             headers["From"] = mail[:200]
L319         return headers
L320
L321     @staticmethod
L322     def _sec_get(url: str, retries: int = 3, backoff: float = 0.5):
L323         for i in range(retries):
L324             r = requests.get(url, headers=Input._sec_headers(), timeout=20)
L325             if r.status_code in (429, 503, 403):
L326                 time.sleep(min(2 ** i * backoff, 8.0))
L327                 continue
L328             r.raise_for_status(); return r.json()
L329         r.raise_for_status()
L330
L331     @staticmethod
L332     def _sec_ticker_map():
L333         import requests
L334
L335         url_primary = "https://data.sec.gov/api/xbrl/company_tickers.json"
L336         url_fallback = "https://www.sec.gov/files/company_tickers.json"
L337         mp = {}
L338         try:
L339             j = Input._sec_get(url_primary)  # 既存の堅牢GET（リトライ・バックオフ）
L340         except Exception:
L341             r = requests.get(url_fallback, headers=Input._sec_headers(), timeout=20)
L342             r.raise_for_status()
L343             j = r.json()
L344         # 形状A: {"0": {"ticker":..., "cik_str":...}, ...}
L345         if isinstance(j, dict) and "0" in j:
L346             for _, v in (j or {}).items():
L347                 try:
L348                     mp[str(v["ticker"]).upper()] = f"{int(v['cik_str']):010d}"
L349                 except Exception:
L350                     pass
L351         # 形状B: [{"ticker":..., "cik_str":...}, ...]
L352         elif isinstance(j, list):
L353             for v in j:
L354                 try:
L355                     mp[str(v.get("ticker")).upper()] = f"{int(v.get('cik_str')):010d}"
L356                 except Exception:
L357                     pass
L358         # 形状C: {"data":[[idx,ticker,title,cik_str],...]}
L359         elif isinstance(j, dict) and "data" in j:
L360             for row in j.get("data") or []:
L361                 try:
L362                     t = str(row[1]).upper()
L363                     c = int(row[3])
L364                     mp[t] = f"{c:010d}"
L365                 except Exception:
L366                     pass
L367         return mp
L368
L369     # --- 追加: ADR/OTC向けの簡易正規化（末尾Y/F, ドット等） ---
L370     @staticmethod
L371     def _normalize_ticker(sym: str) -> list[str]:
L372         s = (sym or "").upper().strip()
L373         # 追加: 先頭の$や全角の記号を除去
L374         s = s.lstrip("$").replace("＄", "").replace("．", ".").replace("－", "-")
L375         cand: list[str] = []
L376
L377         def add(x: str) -> None:
L378             if x and x not in cand:
L379                 cand.append(x)
L380
L381         # 1) 原文を最優先（SECは BRK.B, BF.B など . を正式採用）
L382         add(s)
L383         # 2) Yahoo系バリアント（. と - の揺れを相互に）
L384         if "." in s:
L385             add(s.replace(".", "-"))
L386             add(s.replace(".", ""))
L387         if "-" in s:
L388             add(s.replace("-", "."))
L389             add(s.replace("-", ""))
L390         # 3) ドット・ハイフン・ピリオド無し版（最後の保険）
L391         add(s.replace("-", "").replace(".", ""))
L392         # 4) ADR簡易：末尾Y/Fの除去（SECマップは本体ティッカーを持つことがある）
L393         if len(s) >= 2 and s[-1] in {"Y", "F"}:
L394             add(s[:-1])
L395         return cand
L396
L397     @staticmethod
L398     def _sec_companyfacts(cik: str):
L399         return Input._sec_get(f"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json")
L400
L401     @staticmethod
L402     def _units_for_tags(facts: dict, namespaces: list[str], tags: list[str]) -> list[dict]:
L403         """facts から namespace/tag を横断して units 配列を収集（存在順に連結）。"""
L404         out: list[dict] = []
L405         facts = (facts or {}).get("facts", {})
L406         for ns in namespaces:
L407             node = facts.get(ns, {}) if isinstance(facts, dict) else {}
L408             for tg in tags:
L409                 try:
L410                     units = node[tg]["units"]
L411                 except Exception:
L412                     continue
L413                 picks: list[dict] = []
L414                 if "USD/shares" in units:
L415                     picks.extend(list(units["USD/shares"]))
L416                 if "USD" in units:
L417                     picks.extend(list(units["USD"]))
L418                 if not picks:
L419                     for arr in units.values():
L420                         picks.extend(list(arr))
L421                 out.extend(picks)
L422         return out
L423
L424     @staticmethod
L425     def _only_quarterly(arr: list[dict]) -> list[dict]:
L426         """companyfactsの混在配列から『四半期』だけを抽出。
L427
L428         - frame に "Q" を含む（例: CY2024Q2I）
L429         - fp が Q1/Q2/Q3/Q4
L430         - form が 10-Q/10-Q/A/6-K
L431         """
L432         if not arr:
L433             return []
L434         q_forms = {"10-Q", "10-Q/A", "6-K"}
L435         out = [
L436             x
L437             for x in arr
L438             if (
L439                 "Q" in (x.get("frame") or "").upper()
L440                 or (x.get("fp") or "").upper() in {"Q1", "Q2", "Q3", "Q4"}
L441                 or (x.get("form") or "").upper() in q_forms
L442             )
L443         ]
L444         out.sort(key=lambda x: (x.get("end") or ""), reverse=True)
L445         return out
L446
L447     @staticmethod
L448     def _series_from_facts_with_dates(arr, key_val="val", key_dt="end", normalize=float):
L449         """companyfactsアイテム配列から (date,value) を返す。dateはYYYY-MM-DDを想定。"""
L450         out: List[Tuple[str, float]] = []
L451         for x in (arr or []):
L452             try:
L453                 d = x.get(key_dt)
L454                 if d is None:
L455                     continue
L456                 v = x.get(key_val)
L457                 out.append((str(d), normalize(v) if v is not None else float("nan")))
L458             except Exception:
L459                 continue
L460         out.sort(key=lambda t: t[0], reverse=True)
L461         return out
L462
L463     def _series_q_and_a(self, facts: list[dict]) -> tuple[list[Tuple[str, float]], list[Tuple[str, float]]]:
L464         """四半期・年次の両seriesを抽出して返す（formで簡易判定）。"""
L465         if not facts:
L466             return [], []
L467         q_items = self._only_quarterly(list(facts))
L468         annual_forms = {"10-K", "10-K/A", "20-F", "20-F/A"}
L469         a_items = [x for x in facts if str((x or {}).get("form", "")).upper() in annual_forms]
L470         a_items.sort(key=lambda x: (x.get("end") or ""), reverse=True)
L471         return self._series_from_facts_with_dates(q_items), self._series_from_facts_with_dates(a_items)
L472
L473     @staticmethod
L474     def _ttm_from_q_or_a(q_vals: list[float], a_vals: list[float]) -> float:
L475         """四半期TTM（4本合算）を優先し、欠損時は年次値で補完。"""
L476         import math
L477
L478         def _clean(vals: list[float]) -> list[float]:
L479             out: list[float] = []
L480             for v in vals:
L481                 try:
L482                     f = float(v)
L483                 except Exception:
L484                     continue
L485                 if math.isfinite(f):
L486                     out.append(f)
L487                 else:
L488                     out.append(float("nan"))
L489             return out
L490
L491         def _sum4(vs: list[float]) -> float:
L492             filtered = [v for v in vs[:4] if v == v]
L493             if len(filtered) >= 2:
L494                 return float(sum(filtered))
L495             if len(filtered) == 1:
L496                 return float(filtered[0])
L497             return float("nan")
L498
L499         q_clean = _clean(q_vals or [])
L500         ttm_q = _sum4(q_clean)
L501         if ttm_q == ttm_q:
L502             return ttm_q
L503         for v in _clean(a_vals or []):
L504             if v == v:
L505                 return float(v)
L506         return float("nan")
L507
L508     def fetch_eps_rev_from_sec(self, tickers: list[str]) -> dict:
L509         out = {}
L510         t2cik = self._sec_ticker_map()
L511         n_map = n_rev = n_eps = 0
L512         miss_map: list[str] = []
L513         miss_facts: list[str] = []
L514         for t in tickers:
L515             base = (t or "").upper()
L516             candidates: list[str] = []
L517             for key in [base, *self._normalize_ticker(t)]:
L518                 if key and key not in candidates:
L519                     candidates.append(key)
L520             cik = next((t2cik.get(key) for key in candidates if t2cik.get(key)), None)
L521             if not cik:
L522                 out[t] = {}
L523                 miss_map.append(t)
L524                 continue
L525             try:
L526                 j = self._sec_companyfacts(cik)
L527                 facts = j or {}
L528                 rev_tags = [
L529                     "Revenues",
L530                     "RevenueFromContractWithCustomerExcludingAssessedTax",
L531                     "SalesRevenueNet",
L532                     "SalesRevenueGoodsNet",
L533                     "SalesRevenueServicesNet",
L534                     "Revenue",
L535                 ]
L536                 eps_tags = [
L537                     "EarningsPerShareDiluted",
L538                     "EarningsPerShareBasicAndDiluted",
L539                     "EarningsPerShare",
L540                     "EarningsPerShareBasic",
L541                 ]
L542                 rev_arr = self._units_for_tags(facts, ["us-gaap", "ifrs-full"], rev_tags)
L543                 eps_arr = self._units_for_tags(facts, ["us-gaap", "ifrs-full"], eps_tags)
L544                 rev_q_pairs, rev_a_pairs = self._series_q_and_a(rev_arr)
L545                 eps_q_pairs, eps_a_pairs = self._series_q_and_a(eps_arr)
L546
L547                 rev_q_pairs = rev_q_pairs[:12]
L548                 eps_q_pairs = eps_q_pairs[:12]
L549                 rev_a_pairs = rev_a_pairs[:6]
L550                 eps_a_pairs = eps_a_pairs[:6]
L551
L552                 def _vals(pairs: list[tuple[str, float]]) -> list[float]:
L553                     vals: list[float] = []
L554                     for _d, v in pairs:
L555                         try:
L556                             vals.append(float(v))
L557                         except Exception:
L558                             vals.append(float("nan"))
L559                     return vals
L560
L561                 rev_q_vals = _vals(rev_q_pairs)
L562                 eps_q_vals = _vals(eps_q_pairs)
L563                 rev_a_vals = _vals(rev_a_pairs)
L564                 eps_a_vals = _vals(eps_a_pairs)
L565
L566                 def _first_valid(vals: list[float]) -> float:
L567                     for v in vals:
L568                         if v == v:
L569                             return float(v)
L570                     return float("nan")
L571
L572                 def _nth_valid(vals: list[float], n: int) -> float:
L573                     idx = 0
L574                     for v in vals:
L575                         if v == v:
L576                             if idx == n:
L577                                 return float(v)
L578                             idx += 1
L579                     return float("nan")
L580
L581                 def _quarter_from_annual(vals: list[float]) -> float:
L582                     v = _first_valid(vals)
L583                     return float(v / 4.0) if v == v else float("nan")
L584
L585                 def _quarter_from_annual_prev(vals: list[float]) -> float:
L586                     v = _nth_valid(vals, 1)
L587                     return float(v / 4.0) if v == v else float("nan")
L588
L589                 rev_lastq = _first_valid(rev_q_vals)
L590                 if rev_lastq != rev_lastq:
L591                     rev_lastq = _quarter_from_annual(rev_a_vals)
L592                 eps_lastq = _first_valid(eps_q_vals)
L593                 if eps_lastq != eps_lastq:
L594                     eps_lastq = _quarter_from_annual(eps_a_vals)
L595
L596                 rev_lastq_prev = _nth_valid(rev_q_vals, 4)
L597                 if rev_lastq_prev != rev_lastq_prev:
L598                     rev_lastq_prev = _quarter_from_annual_prev(rev_a_vals)
L599                 eps_lastq_prev = _nth_valid(eps_q_vals, 4)
L600                 if eps_lastq_prev != eps_lastq_prev:
L601                     eps_lastq_prev = _quarter_from_annual_prev(eps_a_vals)
L602
L603                 rev_ttm = self._ttm_from_q_or_a(rev_q_vals, rev_a_vals)
L604                 eps_ttm = self._ttm_from_q_or_a(eps_q_vals, eps_a_vals)
L605                 rev_ttm_prev = self._ttm_from_q_or_a(rev_q_vals[4:], rev_a_vals[1:])
L606                 eps_ttm_prev = self._ttm_from_q_or_a(eps_q_vals[4:], eps_a_vals[1:])
L607
L608                 rev_annual_latest = _first_valid(rev_a_vals)
L609                 rev_annual_prev = _nth_valid(rev_a_vals, 1)
L610                 eps_annual_latest = _first_valid(eps_a_vals)
L611                 eps_annual_prev = _nth_valid(eps_a_vals, 1)
L612
L613                 def _cagr3(vals: list[float]) -> float:
L614                     vals_valid = [v for v in vals if v == v]
L615                     if len(vals_valid) >= 3:
L616                         latest, base = float(vals_valid[0]), float(vals_valid[2])
L617                         if latest > 0 and base > 0:
L618                             try:
L619                                 return float((latest / base) ** (1 / 2) - 1.0)
L620                             except Exception:
L621                                 return float("nan")
L622                     return float("nan")
L623
L624                 rev_cagr3 = _cagr3(rev_a_vals)
L625                 eps_cagr3 = _cagr3(eps_a_vals)
L626
L627                 out[t] = {
L628                     "eps_q_recent": eps_lastq,
L629                     "eps_ttm": eps_ttm,
L630                     "eps_ttm_prev": eps_ttm_prev,
L631                     "eps_lastq_prev": eps_lastq_prev,
L632                     "rev_q_recent": rev_lastq,
L633                     "rev_ttm": rev_ttm,
L634                     "rev_ttm_prev": rev_ttm_prev,
L635                     "rev_lastq_prev": rev_lastq_prev,
L636                     # 後段でDatetimeIndex化できるよう (date,value) を保持。値だけの互換キーも残す。
L637                     # 3年運用に合わせて四半期は直近12本のみ保持（約3年=12Q）
L638                     "eps_q_series_pairs": eps_q_pairs,
L639                     "rev_q_series_pairs": rev_q_pairs,
L640                     "eps_q_series": eps_q_vals,
L641                     "rev_q_series": rev_q_vals,
L642                     "eps_a_series_pairs": eps_a_pairs,
L643                     "rev_a_series_pairs": rev_a_pairs,
L644                     "eps_a_series": eps_a_vals,
L645                     "rev_a_series": rev_a_vals,
L646                     "eps_annual_latest": eps_annual_latest,
L647                     "eps_annual_prev": eps_annual_prev,
L648                     "rev_annual_latest": rev_annual_latest,
L649                     "rev_annual_prev": rev_annual_prev,
L650                     "eps_cagr3": eps_cagr3,
L651                     "rev_cagr3": rev_cagr3,
L652                 }
L653                 n_map += 1
L654                 if any(v == v for v in rev_q_vals) or any(v == v for v in rev_a_vals):
L655                     n_rev += 1
L656                 if any(v == v for v in eps_q_vals) or any(v == v for v in eps_a_vals):
L657                     n_eps += 1
L658             except Exception:
L659                 out[t] = {}
L660                 miss_facts.append(t)
L661             time.sleep(0.30)
L662         # 取得サマリをログ（Actionsで確認しやすいよう print）
L663         try:
L664             total = len(tickers)
L665             print(f"[SEC] map={n_map}/{total}  rev_q_hit={n_rev}  eps_q_hit={n_eps}")
L666             # デバッグ: 取得本数の分布（先頭のみ）
L667             try:
L668                 lens = [len((out.get(t, {}) or {}).get("rev_q_series", [])) for t in tickers]
L669                 print(f"[SEC] rev_q_series length: min={min(lens) if lens else 0} "
L670                       f"p25={np.percentile(lens,25) if lens else 0} median={np.median(lens) if lens else 0} "
L671                       f"p75={np.percentile(lens,75) if lens else 0} max={max(lens) if lens else 0}")
L672             except Exception:
L673                 pass
L674             if miss_map:
L675                 print(f"[SEC] no CIK map: {len(miss_map)} (サンプル例) {miss_map[:20]}")
L676             if miss_facts:
L677                 print(f"[SEC] CIKあり だが対象factなし: {len(miss_facts)} (サンプル例) {miss_facts[:20]}")
L678         except Exception:
L679             pass
L680         return out
L681
L682     def sec_dryrun_sample(self, tickers: list[str] | None = None) -> None:
L683         if not _env_true("SEC_DRYRUN_SAMPLE", False):
L684             return
L685         sample = tickers or ["BRK.B", "BF.B", "GOOGL", "META", "UBER", "PBR.A", "TSM", "NARI", "EVBN", "SWAV"]
L686         print(f"[SEC-DRYRUN] sample tickers: {sample}")
L687         try:
L688             t2cik = self._sec_ticker_map()
L689             hits = 0
L690             for sym in sample:
L691                 candidates: list[str] = []
L692
L693                 def add(key: str) -> None:
L694                     if key and key not in candidates:
L695                         candidates.append(key)
L696
L697                 add((sym or "").upper())
L698                 for alt in self._normalize_ticker(sym):
L699                     add(alt)
L700                 if any(t2cik.get(key) for key in candidates):
L701                     hits += 1
L702             sec_data = self.fetch_eps_rev_from_sec(sample)
L703             rev_hits = sum(1 for v in sec_data.values() if v.get("rev_q_series"))
L704             eps_hits = sum(1 for v in sec_data.values() if v.get("eps_q_series"))
L705             total = len(sample)
L706             print(f"[SEC-DRYRUN] CIK map hit: {hits}/{total}  rev_q_series hits: {rev_hits}  eps_q_series hits: {eps_hits}")
L707         except Exception as e:
L708             print(f"[SEC-DRYRUN] error: {e}")
L709     @staticmethod
L710     def impute_eps_ttm(df: pd.DataFrame, ttm_col: str="eps_ttm", q_col: str="eps_q_recent", out_col: str|None=None) -> pd.DataFrame:
L711         out_col = out_col or ttm_col; df = df.copy(); df["eps_imputed"] = False
L712         cand = df[q_col]*4; ok = df[ttm_col].isna() & cand.replace([np.inf,-np.inf], np.nan).notna()
L713         df.loc[ok, out_col], df.loc[ok,"eps_imputed"] = cand[ok], True; return df
L714
L715     _CF_ALIASES = {"cfo":["Operating Cash Flow","Total Cash From Operating Activities"], "capex":["Capital Expenditure","Capital Expenditures"]}
L716
L717     @staticmethod
L718     def _pick_row(df: pd.DataFrame, names: list[str]) -> pd.Series|None:
L719         if df is None or df.empty: return None
L720         idx_lower={str(i).lower():i for i in df.index}
L721         for n in names:
L722             k=n.lower()
L723             if k in idx_lower: return df.loc[idx_lower[k]]
L724         return None
L725
L726     @staticmethod
L727     def _sum_last_n(s: pd.Series|None, n: int) -> float|None:
L728         if s is None or s.empty: return None
L729         v=s.dropna().astype(float); return None if v.empty else v.iloc[:n].sum()
L730
L731     @staticmethod
L732     def _latest(s: pd.Series|None) -> float|None:
L733         if s is None or s.empty: return None
L734         v=s.dropna().astype(float); return v.iloc[0] if not v.empty else None
L735
L736     def fetch_cfo_capex_ttm_yf(self, tickers: list[str]) -> pd.DataFrame:
L737         from concurrent.futures import ThreadPoolExecutor, as_completed
L738         pick, sumn, latest, aliases = self._pick_row, self._sum_last_n, self._latest, self._CF_ALIASES
L739
L740         def one(t: str):
L741             try:
L742                 tk = yf.Ticker(t)  # ★ セッションは渡さない（YFがcurl_cffiで管理）
L743                 qcf = tk.quarterly_cashflow
L744                 cfo_q, capex_q = pick(qcf, aliases["cfo"]), pick(qcf, aliases["capex"])
L745                 fcf_q = pick(qcf, ["Free Cash Flow","FreeCashFlow","Free cash flow"])
L746                 cfo, capex, fcf = sumn(cfo_q,4), sumn(capex_q,4), sumn(fcf_q,4)
L747                 if any(v is None for v in (cfo, capex, fcf)):
L748                     acf = tk.cashflow
L749                     if cfo   is None: cfo   = latest(pick(acf, aliases["cfo"]))
L750                     if capex is None: capex = latest(pick(acf, aliases["capex"]))
L751                     if fcf   is None: fcf   = latest(pick(acf, ["Free Cash Flow","FreeCashFlow","Free cash flow"]))
L752             except Exception as e:
L753                 print(f"[warn] yf financials error: {t}: {e}"); cfo=capex=fcf=None
L754             n=np.nan
L755             return {"ticker":t,
L756                     "cfo_ttm_yf":   n if cfo   is None else cfo,
L757                     "capex_ttm_yf": n if capex is None else capex,
L758                     "fcf_ttm_yf_direct": n if fcf is None else fcf}
L759
L760         rows, mw = [], int(os.getenv("FIN_THREADS","8"))
L761         with ThreadPoolExecutor(max_workers=mw) as ex:
L762             rows=[f.result() for f in as_completed(ex.submit(one,t) for t in tickers)]
L763         return pd.DataFrame(rows).set_index("ticker")
L764
L765     _FINN_CFO_KEYS = ["netCashProvidedByOperatingActivities","netCashFromOperatingActivities","cashFlowFromOperatingActivities","operatingCashFlow"]
L766     _FINN_CAPEX_KEYS = ["capitalExpenditure","capitalExpenditures","purchaseOfPPE","investmentsInPropertyPlantAndEquipment"]
L767
L768     @staticmethod
L769     def _first_key(d: dict, keys: list[str]):
L770         for k in keys:
L771             if k in d and d[k] is not None: return d[k]
L772         return None
L773
L774     @staticmethod
L775     def _finn_get(session: requests.Session, url: str, params: dict, retries: int=3, sleep_s: float=0.5):
L776         for i in range(retries):
L777             r = session.get(url, params=params, timeout=15)
L778             if r.status_code==429: time.sleep(min(2**i*sleep_s,4.0)); continue
L779             r.raise_for_status(); return r.json()
L780         r.raise_for_status()
L781
L782     def fetch_cfo_capex_ttm_finnhub(self, tickers: list[str], api_key: str|None=None) -> pd.DataFrame:
L783         api_key = api_key or os.getenv("FINNHUB_API_KEY")
L784         if not api_key: raise ValueError("Finnhub API key not provided. Set FINNHUB_API_KEY or pass api_key=")
L785         base, s, rows = "https://finnhub.io/api/v1", requests.Session(), []
L786         for sym in tickers:
L787             cfo_ttm = capex_ttm = None
L788             try:
L789                 j = self._finn_get(s, f"{base}/stock/cash-flow", {"symbol":sym,"frequency":"quarterly","limit":8,"token":api_key})
L790                 arr = j.get("cashFlow") or []; cfo_vals, capex_vals = [], []
L791                 for item in arr[:4]:
L792                     cfo_vals.append(self._first_key(item,self._FINN_CFO_KEYS)); capex_vals.append(self._first_key(item,self._FINN_CAPEX_KEYS))
L793                 if any(v is not None for v in cfo_vals): cfo_ttm = float(np.nansum([np.nan if v is None else float(v) for v in cfo_vals]))
L794                 if any(v is not None for v in capex_vals): capex_ttm = float(np.nansum([np.nan if v is None else float(v) for v in capex_vals]))
L795             except Exception: pass
L796             if cfo_ttm is None or capex_ttm is None:
L797                 try:
L798                     j = self._finn_get(s, f"{base}/stock/cash-flow", {"symbol":sym,"frequency":"annual","limit":1,"token":api_key})
L799                     arr = j.get("cashFlow") or []
L800                     if arr:
L801                         item0 = arr[0]
L802                         if cfo_ttm is None:
L803                             v = self._first_key(item0,self._FINN_CFO_KEYS)
L804                             if v is not None: cfo_ttm = float(v)
L805                         if capex_ttm is None:
L806                             v = self._first_key(item0,self._FINN_CAPEX_KEYS)
L807                             if v is not None: capex_ttm = float(v)
L808                 except Exception: pass
L809             rows.append({"ticker":sym,"cfo_ttm_fh":np.nan if cfo_ttm is None else cfo_ttm,"capex_ttm_fh":np.nan if capex_ttm is None else capex_ttm})
L810         return pd.DataFrame(rows).set_index("ticker")
L811
L812     def compute_fcf_with_fallback(self, tickers: list[str], finnhub_api_key: str|None=None) -> pd.DataFrame:
L813         yf_df = self.fetch_cfo_capex_ttm_yf(tickers)
L814         _tlog("financials (yf) done")
L815         miss_mask = yf_df[["cfo_ttm_yf","capex_ttm_yf","fcf_ttm_yf_direct"]].isna().any(axis=1)
L816         need = yf_df.index[miss_mask].tolist(); print(f"[T] yf financials missing: {len(need)} {need[:10]}{'...' if len(need)>10 else ''}")
L817         if need:
L818             fh_df = self.fetch_cfo_capex_ttm_finnhub(need, api_key=finnhub_api_key)
L819             df = yf_df.join(fh_df, how="left")
L820             for col_yf, col_fh in [("cfo_ttm_yf","cfo_ttm_fh"),("capex_ttm_yf","capex_ttm_fh")]:
L821                 df[col_yf] = df[col_yf].fillna(df[col_fh])
L822             print("[T] financials (finnhub) done (fallback only)")
L823         else:
L824             df = yf_df.assign(cfo_ttm_fh=np.nan, capex_ttm_fh=np.nan)
L825             print("[T] financials (finnhub) skipped (no missing)")
L826         df["cfo_ttm"]  = df["cfo_ttm_yf"].where(df["cfo_ttm_yf"].notna(), df["cfo_ttm_fh"])
L827         df["capex_ttm"] = df["capex_ttm_yf"].where(df["capex_ttm_yf"].notna(), df["capex_ttm_fh"])
L828         cfo, capex = pd.to_numeric(df["cfo_ttm"], errors="coerce"), pd.to_numeric(df["capex_ttm"], errors="coerce").abs()
L829         fcf_calc = cfo - capex
L830         fcf_direct = pd.to_numeric(df.get("fcf_ttm_yf_direct"), errors="coerce")
L831         df["fcf_ttm"] = fcf_calc.where(fcf_calc.notna(), fcf_direct)
L832         df["cfo_source"]  = np.where(df["cfo_ttm_yf"].notna(),"yfinance",np.where(df["cfo_ttm_fh"].notna(),"finnhub",""))
L833         df["capex_source"] = np.where(df["capex_ttm_yf"].notna(),"yfinance",np.where(df["capex_ttm_fh"].notna(),"finnhub",""))
L834         df["fcf_imputed"] = df[["cfo_ttm","capex_ttm"]].isna().any(axis=1) & df["fcf_ttm"].notna()
L835         cols = ["cfo_ttm_yf","capex_ttm_yf","cfo_ttm_fh","capex_ttm_fh","cfo_ttm","capex_ttm","fcf_ttm","fcf_ttm_yf_direct","cfo_source","capex_source","fcf_imputed"]
L836         return df[cols].sort_index()
L837
L838     def _build_eps_df(self, tickers, tickers_bulk, info, sec_map: dict | None = None):
L839         eps_rows=[]
L840         for t in tickers:
L841             info_t = info[t]
L842             sec_t = (sec_map or {}).get(t, {})
L843             eps_ttm = sec_t.get("eps_ttm", info_t.get("trailingEps", np.nan))
L844             eps_q = sec_t.get("eps_q_recent", np.nan)
L845             try:
L846                 tk = tickers_bulk.tickers.get(t)
L847                 if tk is None:
L848                     sym = info_t.get("_yf_symbol") if isinstance(info_t, dict) else None
L849                     if sym:
L850                         tk = tickers_bulk.tickers.get(sym)
L851                 qearn = tk.quarterly_earnings if tk is not None else None
L852                 so = info_t.get("sharesOutstanding")
L853                 if so and qearn is not None and not qearn.empty and "Earnings" in qearn.columns:
L854                     eps_ttm_q = qearn["Earnings"].head(4).sum()/so
L855                     if pd.notna(eps_ttm_q) and (pd.isna(eps_ttm) or (abs(eps_ttm)>0 and abs(eps_ttm/eps_ttm_q)>3)): eps_ttm = eps_ttm_q
L856                     if pd.isna(eps_q):
L857                         eps_q = qearn["Earnings"].iloc[-1]/so
L858             except Exception: pass
L859             rev_ttm = sec_t.get("rev_ttm", np.nan)
L860             rev_q = sec_t.get("rev_q_recent", np.nan)
L861             if (not sec_t) or pd.isna(rev_ttm):
L862                 try:
L863                     tk = tickers_bulk.tickers.get(t)
L864                     if tk is None and isinstance(info_t, dict):
L865                         sym = info_t.get("_yf_symbol")
L866                         if sym:
L867                             tk = tickers_bulk.tickers.get(sym)
L868                     qfin = getattr(tk, "quarterly_financials", None)
L869                     if qfin is not None and not qfin.empty:
L870                         idx_lower = {str(i).lower(): i for i in qfin.index}
L871                         rev_idx = None
L872                         for name in ("Total Revenue", "TotalRevenue"):
L873                             key = name.lower()
L874                             if key in idx_lower:
L875                                 rev_idx = idx_lower[key]
L876                                 break
L877                         if rev_idx is not None:
L878                             rev_series = pd.to_numeric(qfin.loc[rev_idx], errors="coerce").dropna()
L879                             if not rev_series.empty:
L880                                 rev_ttm_yf = float(rev_series.head(4).sum())
L881                                 if pd.isna(rev_ttm):
L882                                     rev_ttm = rev_ttm_yf
L883                                 if pd.isna(rev_q):
L884                                     rev_q = float(rev_series.iloc[0])
L885                 except Exception:
L886                     pass
L887             eps_rows.append({
L888                 "ticker": t,
L889                 "eps_ttm": eps_ttm,
L890                 "eps_ttm_prev": sec_t.get("eps_ttm_prev", np.nan),
L891                 "eps_q_recent": eps_q,
L892                 "eps_q_prev": sec_t.get("eps_lastq_prev", np.nan),
L893                 "rev_ttm": rev_ttm,
L894                 "rev_ttm_prev": sec_t.get("rev_ttm_prev", np.nan),
L895                 "rev_q_recent": rev_q,
L896                 "rev_q_prev": sec_t.get("rev_lastq_prev", np.nan),
L897                 "eps_annual_latest": sec_t.get("eps_annual_latest", np.nan),
L898                 "eps_annual_prev": sec_t.get("eps_annual_prev", np.nan),
L899                 "rev_annual_latest": sec_t.get("rev_annual_latest", np.nan),
L900                 "rev_annual_prev": sec_t.get("rev_annual_prev", np.nan),
L901                 "eps_cagr3": sec_t.get("eps_cagr3", np.nan),
L902                 "rev_cagr3": sec_t.get("rev_cagr3", np.nan),
L903             })
L904         return self.impute_eps_ttm(pd.DataFrame(eps_rows).set_index("ticker"))
L905
L906     def prepare_data(self):
L907         """Fetch price and fundamental data for all tickers."""
L908         self.sec_dryrun_sample()
L909         # --- yfinance 用にティッカーを正規化（"$"剥がし、"."→"-"） ---
L910         def _to_yf(sym: str) -> str:
L911             s = (sym or "").strip().lstrip("$").replace("＄", "")
L912             # BRK.B / PBR.A などは Yahoo では '-' を使用
L913             yf_sym = s.replace("．", ".").replace(".", "-")
L914             return yf_sym or (sym or "")
L915
L916         cand_y = [_to_yf(t) for t in self.cand]
L917         cand_info = yf.Tickers(" ".join(cand_y))
L918
L919         def _price(orig: str, ysym: str) -> float:
L920             try:
L921                 return cand_info.tickers[ysym].fast_info.get("lastPrice", np.inf)
L922             except Exception as e:
L923                 print(f"{orig}: price fetch failed ({e})")
L924                 return np.inf
L925
L926         cand_prices = {orig: _price(orig, ysym) for orig, ysym in zip(self.cand, cand_y)}
L927         cand_f = [t for t, p in cand_prices.items() if p <= self.price_max]
L928         _tlog("price cap filter done (CAND_PRICE_MAX)")
L929         # 入力ティッカーの重複を除去し、現行→候補の順序を維持
L930         # ユニバース確定（元ティッカー保持）。yfinance には後で変換して渡す
L931         tickers = list(dict.fromkeys(self.exist + cand_f))
L932         yf_map = {t: _to_yf(t) for t in tickers}
L933         yf_list = list(dict.fromkeys([yf_map[t] for t in tickers]))
L934         _tlog(f"universe prepared: unique={len(tickers)} bench={self.bench}")
L935         data = yf.download(yf_list + [self.bench], period="600d",
L936                            auto_adjust=True, progress=False, threads=False)
L937         _tlog("yf.download done")
L938         inv = {v: k for k, v in yf_map.items()}
L939         px = data["Close"].dropna(how="all", axis=1).ffill(limit=2)
L940         px = px.rename(columns=inv)
L941         try:
L942             if isinstance(data.columns, pd.MultiIndex):
L943                 data = data.rename(columns=inv, level=1)
L944             else:
L945                 data = data.rename(columns=inv)
L946         except Exception:
L947             pass
L948         spx = data["Close"][self.bench].reindex(px.index).ffill()
L949         clip_days = int(os.getenv("PRICE_CLIP_DAYS", "0"))   # 0なら無効（既定）
L950         if clip_days > 0:
L951             px, spx = px.tail(clip_days + 1), spx.tail(clip_days + 1)
L952             logger.info("[T] price window clipped by env: %d rows (PRICE_CLIP_DAYS=%d)", len(px), clip_days)
L953         else:
L954             logger.debug("[T] price window clip skipped; rows=%d", len(px))
L955         tickers_bulk, info = yf.Tickers(" ".join(yf_list)), {}
L956         for orig, ysym in yf_map.items():
L957             if ysym in tickers_bulk.tickers:
L958                 tickers_bulk.tickers[orig] = tickers_bulk.tickers[ysym]
L959         for t in tickers:
L960             try:
L961                 tk = tickers_bulk.tickers.get(t) or tickers_bulk.tickers.get(yf_map[t])
L962                 info_entry = tk.info if tk is not None else {}
L963                 if not isinstance(info_entry, dict):
L964                     info_entry = {}
L965                 info_entry.setdefault("_yf_symbol", getattr(tk, "ticker", yf_map.get(t)))
L966                 info[t] = info_entry
L967             except Exception as e:
L968                 logger.info("[warn] %s: info fetch failed (%s)", t, e)
L969                 info[t] = {}
L970         try:
L971             sec_map = self.fetch_eps_rev_from_sec(tickers)
L972         except Exception as e:
L973             logger.warning("[SEC] fetch_eps_rev_from_sec failed: %s", e)
L974             sec_map = {}
L975
L976         def _brief_len(s):
L977             try:
L978                 if isinstance(s, pd.Series):
L979                     return int(s.dropna().size)
L980                 if isinstance(s, (list, tuple)):
L981                     return len([v for v in s if pd.notna(v)])
L982                 if isinstance(s, np.ndarray):
L983                     return int(np.count_nonzero(~pd.isna(s)))
L984                 return int(bool(s))
L985             except Exception:
L986                 return 0
L987
L988         def _has_entries(val) -> bool:
L989             try:
L990                 if isinstance(val, pd.Series):
L991                     return not val.dropna().empty
L992                 if isinstance(val, (list, tuple)):
L993                     return any(pd.notna(v) for v in val)
L994                 return bool(val)
L995             except Exception:
L996                 return False
L997
L998         have_rev = 0
L999         have_eps = 0
L1000         rev_lens: list[int] = []
L1001         eps_lens: list[int] = []
L1002         rev_y_lens: list[int] = []
L1003         samples: list[tuple[str, int, str, float | None, int, str, float | None]] = []
L1004
L1005         for t in tickers:
L1006             entry = info.get(t, {})
L1007             m = (sec_map or {}).get(t) or {}
L1008             if entry is None or not isinstance(entry, dict):
L1009                 entry = {}
L1010                 info[t] = entry
L1011
L1012             if m:
L1013                 pairs_r = m.get("rev_q_series_pairs") or []
L1014                 pairs_e = m.get("eps_q_series_pairs") or []
L1015                 if pairs_r:
L1016                     idx = pd.to_datetime([d for (d, _v) in pairs_r], errors="coerce")
L1017                     val = pd.to_numeric([v for (_d, v) in pairs_r], errors="coerce")
L1018                     s = pd.Series(val, index=idx).sort_index()
L1019                     entry["SEC_REV_Q_SERIES"] = s
L1020                 else:
L1021                     entry["SEC_REV_Q_SERIES"] = m.get("rev_q_series") or []
L1022                 if pairs_e:
L1023                     idx = pd.to_datetime([d for (d, _v) in pairs_e], errors="coerce")
L1024                     val = pd.to_numeric([v for (_d, v) in pairs_e], errors="coerce")
L1025                     s = pd.Series(val, index=idx).sort_index()
L1026                     entry["SEC_EPS_Q_SERIES"] = s
L1027                 else:
L1028                     entry["SEC_EPS_Q_SERIES"] = m.get("eps_q_series") or []
L1029
L1030             r = entry.get("SEC_REV_Q_SERIES")
L1031             e = entry.get("SEC_EPS_Q_SERIES")
L1032             # 年次は直近3件（約3年）だけ保持。重み分岐の nY 判定は従来通り。
L1033             try:
L1034                 if hasattr(r, "index") and isinstance(r.index, pd.DatetimeIndex):
L1035                     y = r.resample("Y").sum().dropna()
L1036                     entry["SEC_REV_Y_SERIES"] = y.tail(3)
L1037                 else:
L1038                     entry["SEC_REV_Y_SERIES"] = []
L1039             except Exception:
L1040                 entry["SEC_REV_Y_SERIES"] = []
L1041             ry = entry.get("SEC_REV_Y_SERIES")
L1042             if _has_entries(r):
L1043                 have_rev += 1
L1044             if _has_entries(e):
L1045                 have_eps += 1
L1046             lr = _brief_len(r)
L1047             le = _brief_len(e)
L1048             rev_lens.append(lr)
L1049             eps_lens.append(le)
L1050             rev_y_lens.append(_brief_len(ry))
L1051             if len(samples) < 8:
L1052                 try:
L1053                     rd = getattr(r, "index", [])[-1] if lr > 0 else None
L1054                     rv = float(r.iloc[-1]) if lr > 0 else None
L1055                     ed = getattr(e, "index", [])[-1] if le > 0 else None
L1056                     ev = float(e.iloc[-1]) if le > 0 else None
L1057                     samples.append((t, lr, str(rd) if rd is not None else "-", rv, le, str(ed) if ed is not None else "-", ev))
L1058                 except Exception:
L1059                     samples.append((t, lr, "-", None, le, "-", None))
L1060
L1061         logger.info("[SEC] series attach: rev_q=%d/%d, eps_q=%d/%d", have_rev, len(tickers), have_eps, len(tickers))
L1062         logger.info(
L1063             "[SEC_SERIES] rev_q=%d (<=12), eps_q=%d (<=12), rev_y=%d (<=3)",
L1064             max(rev_lens) if rev_lens else 0,
L1065             max(eps_lens) if eps_lens else 0,
L1066             max(rev_y_lens) if rev_y_lens else 0,
L1067         )
L1068
L1069         if rev_lens:
L1070             rev_lens_sorted = sorted(rev_lens)
L1071             eps_lens_sorted = sorted(eps_lens)
L1072             _log(
L1073                 "SEC_SERIES",
L1074                 f"rev_len min/med/max={rev_lens_sorted[0]}/{rev_lens_sorted[len(rev_lens)//2]}/{rev_lens_sorted[-1]} "
L1075                 f"eps_len min/med/max={eps_lens_sorted[0]}/{eps_lens_sorted[len(eps_lens)//2]}/{eps_lens_sorted[-1]}",
L1076             )
L1077         for (t, lr, rd, rv, le, ed, ev) in samples:
L1078             _log("SEC_SERIES_SMP", f"{t}  rev_len={lr} last=({rd},{rv})  eps_len={le} last=({ed},{ev})")
L1079         eps_df = self._build_eps_df(tickers, tickers_bulk, info, sec_map=sec_map)
L1080         # index 重複があると .loc[t, col] が Series になり代入時に ValueError を誘発する
L1081         if not eps_df.index.is_unique:
L1082             eps_df = eps_df[~eps_df.index.duplicated(keep="last")]
L1083         eps_df = eps_df.assign(
L1084             EPS_TTM=eps_df["eps_ttm"],
L1085             EPS_TTM_PREV=eps_df.get("eps_ttm_prev", np.nan),
L1086             EPS_Q_LastQ=eps_df["eps_q_recent"],
L1087             EPS_Q_Prev=eps_df.get("eps_q_prev", np.nan),
L1088             REV_TTM=eps_df["rev_ttm"],
L1089             REV_TTM_PREV=eps_df.get("rev_ttm_prev", np.nan),
L1090             REV_Q_LastQ=eps_df["rev_q_recent"],
L1091             REV_Q_Prev=eps_df.get("rev_q_prev", np.nan),
L1092             EPS_A_LATEST=eps_df.get("eps_annual_latest", np.nan),
L1093             EPS_A_PREV=eps_df.get("eps_annual_prev", np.nan),
L1094             REV_A_LATEST=eps_df.get("rev_annual_latest", np.nan),
L1095             REV_A_PREV=eps_df.get("rev_annual_prev", np.nan),
L1096             EPS_A_CAGR3=eps_df.get("eps_cagr3", np.nan),
L1097             REV_A_CAGR3=eps_df.get("rev_cagr3", np.nan),
L1098         )
L1099         missing_logs = pd.DataFrame()
L1100         # ここで非NaN件数をサマリ表示（欠損状況の即時把握用）
L1101         try:
L1102             n = len(eps_df)
L1103             c_eps = int(eps_df["EPS_TTM"].notna().sum())
L1104             c_rev = int(eps_df["REV_TTM"].notna().sum())
L1105             print(f"[SEC] eps_ttm non-NaN: {c_eps}/{n}  rev_ttm non-NaN: {c_rev}/{n}")
L1106         except Exception:
L1107             pass
L1108         fcf_df = self.compute_fcf_with_fallback(tickers, finnhub_api_key=self.api_key)
L1109         _tlog("eps/fcf prep done")
L1110         returns = px[tickers].pct_change()
L1111         _tlog("price prep/returns done")
L1112         return dict(cand=cand_f, tickers=tickers, data=data, px=px, spx=spx, tickers_bulk=tickers_bulk, info=info, eps_df=eps_df, fcf_df=fcf_df, returns=returns, missing_logs=missing_logs)
L1113
L1114 # === Selector：相関低減・選定（スコア＆リターンだけ読む） ===
L1115 class Selector:
L1116     # ---- DRRS helpers（Selector専用） ----
L1117     @staticmethod
L1118     def _z_np(X: np.ndarray) -> np.ndarray:
L1119         X = np.asarray(X, dtype=np.float32)
L1120         m = np.nanmean(X, axis=0, keepdims=True)
L1121         s = np.nanstd(X, axis=0, keepdims=True)
L1122         # 分母0/全NaN列の安全化：std==0 を 1 に置換（z=0に収束）
L1123         s = np.where(np.isfinite(s) & (s > 0), s, 1.0).astype(np.float32)
L1124         with np.errstate(invalid="ignore", divide="ignore"):
L1125             Z = (np.nan_to_num(X) - np.nan_to_num(m)) / s
L1126         return np.nan_to_num(Z)
L1127
L1128     @classmethod
L1129     def residual_corr(cls, R: np.ndarray, n_pc: int=3, shrink: float=0.1) -> np.ndarray:
L1130         Z = cls._z_np(R); U,S,_ = np.linalg.svd(Z, full_matrices=False); F = U[:,:n_pc]*S[:n_pc]; B = np.linalg.lstsq(F, Z, rcond=None)[0]
L1131         E = Z - F@B; C = np.corrcoef(E, rowvar=False)
L1132         off = C - np.diag(np.diag(C)); iu = np.triu_indices_from(off,1); avg_abs = np.nanmean(np.abs(off[iu])) if iu[0].size else 0.0
L1133         shrink_eff = float(np.clip(shrink + 0.5*avg_abs, 0.1, 0.6)); N = C.shape[0]
L1134         return (1.0 - shrink_eff)*C + shrink_eff*np.eye(N, dtype=C.dtype)
L1135
L1136     @classmethod
L1137     def rrqr_like_det(cls, R: np.ndarray, score: np.ndarray, k: int, gamma: float=1.0):
L1138         Z, w = cls._z_np(R), (score-score.min())/(np.ptp(score)+1e-12); X = Z*(1.0 + gamma*w)
L1139         N, k = X.shape[1], max(0, min(k, X.shape[1]))
L1140         if k==0: return []
L1141         S, selected, Rres = [], np.zeros(N, dtype=bool), X.copy()
L1142         for _ in range(k):
L1143             norms = (Rres*Rres).sum(axis=0); cand = np.where(~selected)[0]
L1144             j = sorted(cand, key=lambda c:(-norms[c], -w[c], c))[0]
L1145             S.append(j); selected[j]=True; u = X[:,j:j+1]; u/=(np.linalg.norm(u)+1e-12); Rres = Rres - u @ (u.T @ Rres)
L1146         return sorted(S)
L1147
L1148     @staticmethod
L1149     def _obj(corrM: np.ndarray, score: np.ndarray, idx, lam: float) -> float:
L1150         idx = list(idx); P = corrM[np.ix_(idx, idx)]; s = (score-score.mean())/(score.std()+1e-9)
L1151         return float(s[idx].sum() - lam*((P.sum()-np.trace(P))/2.0))
L1152
L1153     @classmethod
L1154     def swap_local_det(cls, corrM: np.ndarray, score: np.ndarray, idx, lam: float=0.6, max_pass: int=15):
L1155         S, best, improved, passes = sorted(idx), cls._obj(corrM, score, idx, lam), True, 0
L1156         while improved and passes<max_pass:
L1157             improved, passes = False, passes+1
L1158             for i,out in enumerate(list(S)):
L1159                 for inn in range(len(score)):
L1160                     if inn in S: continue
L1161                     cand = sorted(S[:i]+[inn]+S[i+1:]); v = cls._obj(corrM, score, cand, lam)
L1162                     if v>best+1e-10: S, best, improved = cand, v, True; break
L1163                 if improved: break
L1164         return S, best
L1165
L1166     @staticmethod
L1167     def _obj_with_cross(C_within: np.ndarray, C_cross: np.ndarray|None, score: np.ndarray, idx, lam: float, mu: float) -> float:
L1168         idx = list(idx); P = C_within[np.ix_(idx, idx)]; s = (score-score.mean())/(score.std()+1e-9)
L1169         within = (P.sum()-np.trace(P))/2.0; cross = 0.0
L1170         if C_cross is not None and C_cross.size>0: cross = C_cross[idx,:].sum()
L1171         return float(s[idx].sum() - lam*within - mu*cross)
L1172
L1173     @classmethod
L1174     def swap_local_det_cross(cls, C_within: np.ndarray, C_cross: np.ndarray|None, score: np.ndarray, idx, lam: float=0.6, mu: float=0.3, max_pass: int=15):
L1175         S, best, improved, passes, N = sorted(idx), cls._obj_with_cross(C_within,C_cross,score,idx,lam,mu), True, 0, len(score)
L1176         while improved and passes<max_pass:
L1177             improved, passes = False, passes+1
L1178             for i,out in enumerate(list(S)):
L1179                 for inn in range(N):
L1180                     if inn in S: continue
L1181                     cand = sorted(S[:i]+[inn]+S[i+1:]); v = cls._obj_with_cross(C_within,C_cross,score,cand,lam,mu)
L1182                     if v>best+1e-10: S, best, improved = cand, v, True; break
L1183                 if improved: break
L1184         return S, best
L1185
L1186     @staticmethod
L1187     def avg_corr(C: np.ndarray, idx) -> float:
L1188         k = len(idx); P = C[np.ix_(idx, idx)]
L1189         return float((P.sum()-np.trace(P))/(k*(k-1)+1e-12))
L1190
L1191     @classmethod
L1192     def select_bucket_drrs(cls, returns_df: pd.DataFrame, score_ser: pd.Series, pool_tickers: list[str], k: int, *, n_pc: int, gamma: float, lam: float, lookback: int, shrink: float=0.10, g_fixed_tickers: list[str]|None=None, mu: float=0.0):
L1193         g_fixed = [t for t in (g_fixed_tickers or []) if t in returns_df.columns]
L1194         union = [t for t in pool_tickers if t in returns_df.columns]
L1195         for t in g_fixed:
L1196             if t not in union: union.append(t)
L1197         Rdf_all = returns_df[union]
L1198         Rdf_all = Rdf_all.iloc[-lookback:] if len(Rdf_all) >= lookback else Rdf_all
L1199         _thresh = max(1, int(0.8 * len(Rdf_all)))
L1200         Rdf_all = Rdf_all.dropna(axis=1, thresh=_thresh)
L1201         Rdf_all = Rdf_all.dropna()
L1202         pool_eff, g_eff = (
L1203             [t for t in pool_tickers if t in Rdf_all.columns],
L1204             [t for t in g_fixed if t in Rdf_all.columns],
L1205         )
L1206         if len(pool_eff)==0: return dict(idx=[], tickers=[], avg_res_corr=np.nan, sum_score=0.0, objective=-np.inf)
L1207         score = score_ser.reindex(pool_eff).to_numpy(dtype=np.float32)
L1208         C_all = cls.residual_corr(Rdf_all.to_numpy(), n_pc=n_pc, shrink=shrink)
L1209         col_pos = {c:i for i,c in enumerate(Rdf_all.columns)}; pool_pos = [col_pos[t] for t in pool_eff]
L1210         C_within, C_cross = C_all[np.ix_(pool_pos,pool_pos)], None
L1211         if len(g_eff)>0 and mu>0.0:
L1212             g_pos = [col_pos[t] for t in g_eff]; C_cross = C_all[np.ix_(pool_pos,g_pos)]
L1213         R_pool = Rdf_all[pool_eff].to_numpy(); S0 = cls.rrqr_like_det(R_pool, score, k, gamma=gamma)
L1214         S, Jn = (cls.swap_local_det_cross(C_within, C_cross, score, S0, lam=lam, mu=mu, max_pass=15) if C_cross is not None else cls.swap_local_det(C_within, score, S0, lam=lam, max_pass=15))
L1215         selected_tickers = [pool_eff[i] for i in S]
L1216         return dict(idx=S, tickers=selected_tickers, avg_res_corr=cls.avg_corr(C_within,S), sum_score=float(score[S].sum()), objective=float(Jn))
L1217
L1218     # ---- 選定（スコア Series / returns だけを受ける）----
L1219 # === Output：出力整形と送信（表示・Slack） ===
L1220 class Output:
L1221
L1222     def __init__(self, debug=None):
L1223         # self.debug は使わない（互換のため引数は受けるが無視）
L1224         self.g_table = self.d_table = None
L1225         self.g_title = self.d_title = ""
L1226         self.g_formatters = self.d_formatters = {}
L1227         # 低スコア（GSC+DSC）Top10 表示/送信用
L1228         self.low10_table = None
L1229         self._changes_empty = True
L1230         self._changes_text = None
L1231         self._performance_text = ""
L1232
L1233     # --- 表示（元 display_results のロジックそのまま） ---
L1234     def display_results(self, *, exist, bench, df_raw=None, df_z, g_score, d_score_all,
L1235                         init_G, init_D, top_G, top_D, **kwargs):
L1236         logger.info("📌 reached display_results")
L1237         pd.set_option('display.float_format','{:.3f}'.format)
L1238         print("📈 ファクター分散最適化の結果")
L1239         # 欠損アラート関連処理は削除（Slack送信・ログに未使用のため）
L1240
L1241         # ---- 表示用：Changes/Near-Miss のスコア源を“最終集計”に統一するプロキシ ----
L1242         try:
L1243             sc = getattr(self, "_sc", None)
L1244             agg_G = getattr(sc, "_agg_G", None)
L1245             agg_D = getattr(sc, "_agg_D", None)
L1246         except Exception:
L1247             sc = agg_G = agg_D = None
L1248         class _SeriesProxy:
L1249             __slots__ = ("primary", "fallback")
L1250             def __init__(self, primary, fallback): self.primary, self.fallback = primary, fallback
L1251             def get(self, key, default=None):
L1252                 try:
L1253                     v = self.primary.get(key) if hasattr(self.primary, "get") else None
L1254                 except Exception:
L1255                     v = None
L1256                 if v is not None and not (isinstance(v, float) and v != v):
L1257                     return v
L1258                 try:
L1259                     return self.fallback.get(key) if hasattr(self.fallback, "get") else default
L1260                 except Exception:
L1261                     return default
L1262         g_score = _SeriesProxy(agg_G, g_score)
L1263         d_score_all = _SeriesProxy(agg_D, d_score_all)
L1264         near_G = getattr(sc, "_near_G", []) if sc else []
L1265         near_D = getattr(sc, "_near_D", []) if sc else []
L1266
L1267         extra_G = [t for t in init_G if t not in top_G][:5]; G_UNI = top_G + extra_G
L1268         gsc_series = pd.Series({t: g_score.get(t) for t in G_UNI}, name='GSC')
L1269         self.g_table = pd.concat([df_z.loc[G_UNI,['GROWTH_F','MOM','TRD','VOL']], gsc_series], axis=1)
L1270         self.g_table.index = [t + ("⭐️" if t in top_G else "") for t in G_UNI]
L1271         self.g_formatters = {col:"{:.2f}".format for col in ['GROWTH_F','MOM','TRD','VOL']}; self.g_formatters['GSC'] = "{:.3f}".format
L1272         self.g_title = (f"[G枠 / {N_G} / {_fmt_w(g_weights)} / corrM={corrM} / "
L1273                         f"LB={DRRS_G['lookback']} nPC={DRRS_G['n_pc']} γ={DRRS_G['gamma']} λ={DRRS_G['lam']} η={DRRS_G['eta']} shrink={DRRS_SHRINK}]")
L1274         if near_G:
L1275             add = [t for t in near_G if t not in set(G_UNI)][:10]
L1276             if len(add) < 10:
L1277                 try:
L1278                     aggG = getattr(sc, "_agg_G", pd.Series(dtype=float)).dropna().sort_values(ascending=False)
L1279                     out_now = sorted(set(exist) - set(top_G + top_D))  # 今回 OUT
L1280                     used = set(G_UNI + add)
L1281                     def _push(lst):
L1282                         nonlocal add, used
L1283                         for t in lst:
L1284                             if len(add) == 10: break
L1285                             if t in aggG.index and t not in used:
L1286                                 add.append(t); used.add(t)
L1287                     _push(out_now)           # ① 今回 OUT を優先
L1288                     _push(list(aggG.index))  # ② まだ足りなければ上位で充填
L1289                 except Exception:
L1290                     pass
L1291             if add:
L1292                 near_tbl = pd.concat([df_z.loc[add,['GROWTH_F','MOM','TRD','VOL']], pd.Series({t: g_score.get(t) for t in add}, name='GSC')], axis=1)
L1293                 self.g_table = pd.concat([self.g_table, near_tbl], axis=0)
L1294         print(self.g_title); print(self.g_table.to_string(formatters=self.g_formatters))
L1295
L1296         extra_D = [t for t in init_D if t not in top_D][:5]; D_UNI = top_D + extra_D
L1297         cols_D = ['QAL','YLD','VOL','TRD']; d_disp = pd.DataFrame(index=D_UNI)
L1298         d_disp['QAL'], d_disp['YLD'], d_disp['VOL'], d_disp['TRD'] = df_z.loc[D_UNI,'D_QAL'], df_z.loc[D_UNI,'D_YLD'], df_z.loc[D_UNI,'D_VOL_RAW'], df_z.loc[D_UNI,'D_TRD']
L1299         dsc_series = pd.Series({t: d_score_all.get(t) for t in D_UNI}, name='DSC')
L1300         self.d_table = pd.concat([d_disp, dsc_series], axis=1); self.d_table.index = [t + ("⭐️" if t in top_D else "") for t in D_UNI]
L1301         self.d_formatters = {col:"{:.2f}".format for col in cols_D}; self.d_formatters['DSC']="{:.3f}".format
L1302         import scorer
L1303         dw_eff = scorer.D_WEIGHTS_EFF
L1304         self.d_title = (f"[D枠 / {N_D} / {_fmt_w(dw_eff)} / corrM={corrM} / "
L1305                         f"LB={DRRS_D['lookback']} nPC={DRRS_D['n_pc']} γ={DRRS_D['gamma']} λ={DRRS_D['lam']} μ={CROSS_MU_GD} η={DRRS_D['eta']} shrink={DRRS_SHRINK}]")
L1306         if near_D:
L1307             add = [t for t in near_D if t not in set(D_UNI)][:10]
L1308             if add:
L1309                 d_disp2 = pd.DataFrame(index=add)
L1310                 d_disp2['QAL'], d_disp2['YLD'], d_disp2['VOL'], d_disp2['TRD'] = df_z.loc[add,'D_QAL'], df_z.loc[add,'D_YLD'], df_z.loc[add,'D_VOL_RAW'], df_z.loc[add,'D_TRD']
L1311                 near_tbl = pd.concat([d_disp2, pd.Series({t: d_score_all.get(t) for t in add}, name='DSC')], axis=1)
L1312                 self.d_table = pd.concat([self.d_table, near_tbl], axis=0)
L1313         print(self.d_title); print(self.d_table.to_string(formatters=self.d_formatters))
L1314
L1315         # === Changes（IN の GSC/DSC を表示。OUT は銘柄名のみ） ===
L1316         in_list = sorted(set(list(top_G)+list(top_D)) - set(exist))
L1317         out_list = sorted(set(exist) - set(list(top_G)+list(top_D)))
L1318
L1319         io_table = pd.DataFrame({
L1320             'IN': pd.Series(in_list),
L1321             '/ OUT': pd.Series(out_list)
L1322         })
L1323         g_list = [f"{g_score.get(t):.3f}" if pd.notna(g_score.get(t)) else '—' for t in out_list]
L1324         d_list = [f"{d_score_all.get(t):.3f}" if pd.notna(d_score_all.get(t)) else '—' for t in out_list]
L1325         io_table['GSC'] = pd.Series(g_list)
L1326         io_table['DSC'] = pd.Series(d_list)
L1327
L1328         print("Changes:")
L1329         changes_str = io_table.to_string(index=False)
L1330         print(changes_str)
L1331         self._changes_empty = io_table.empty
L1332         self._changes_text = None if self._changes_empty else changes_str
L1333
L1334         all_tickers = list(set(exist + list(top_G) + list(top_D) + [bench])); prices = yf.download(all_tickers, period='1y', auto_adjust=True, progress=False, threads=False)['Close'].ffill(limit=2)
L1335         ret = prices.pct_change(); portfolios = {'CUR':exist,'NEW':list(top_G)+list(top_D)}; metrics={}
L1336         for name,ticks in portfolios.items():
L1337             pr = ret[ticks].mean(axis=1, skipna=True).dropna(); cum = (1+pr).cumprod()-1; n = len(pr)
L1338             if n>=252: ann_ret, ann_vol = (1+cum.iloc[-1])**(252/n)-1, pr.std()*np.sqrt(252)
L1339             else: ann_ret, ann_vol = cum.iloc[-1], pr.std()*np.sqrt(n)
L1340             sharpe, drawdown = ann_ret/ann_vol, (cum - cum.cummax()).min()
L1341             if len(ticks)>=2:
L1342                 C_raw = ret[ticks].corr(); RAW_rho = C_raw.mask(np.eye(len(ticks), dtype=bool)).stack().mean()
L1343                 R = ret[ticks].dropna().to_numpy(); C_resid = Selector.residual_corr(R, n_pc=3, shrink=DRRS_SHRINK)
L1344                 RESID_rho = float((C_resid.sum()-np.trace(C_resid))/(C_resid.shape[0]*(C_resid.shape[0]-1)))
L1345             else: RAW_rho = RESID_rho = np.nan
L1346             divy = ttm_div_yield_portfolio(ticks); metrics[name] = {'RET':ann_ret,'VOL':ann_vol,'SHP':sharpe,'MDD':drawdown,'RAWρ':RAW_rho,'RESIDρ':RESID_rho,'DIVY':divy}
L1347         df_metrics = pd.DataFrame(metrics).T; df_metrics_pct = df_metrics.copy(); self.df_metrics = df_metrics
L1348         for col in ['RET','VOL','MDD','DIVY']: df_metrics_pct[col] = df_metrics_pct[col]*100
L1349         cols_order = ['RET','VOL','SHP','MDD','RAWρ','RESIDρ','DIVY']; df_metrics_pct = df_metrics_pct.reindex(columns=cols_order)
L1350         def _fmt_row(s):
L1351             return pd.Series({'RET':f"{s['RET']:.1f}%",'VOL':f"{s['VOL']:.1f}%",'SHP':f"{s['SHP']:.1f}",'MDD':f"{s['MDD']:.1f}%",'RAWρ':(f"{s['RAWρ']:.2f}" if pd.notna(s['RAWρ']) else "NaN"),'RESIDρ':(f"{s['RESIDρ']:.2f}" if pd.notna(s['RESIDρ']) else "NaN"),'DIVY':f"{s['DIVY']:.1f}%"})
L1352         df_metrics_fmt = df_metrics_pct.apply(_fmt_row, axis=1)
L1353         metrics_str = df_metrics_fmt.to_string()
L1354         print("Performance Comparison:")
L1355         print(metrics_str)
L1356         self._performance_text = metrics_str
L1357         # === 追加: GSC+DSC が低い順 TOP10 ===
L1358         try:
L1359             all_scores = pd.DataFrame({'GSC': df_z['GSC'], 'DSC': df_z['DSC']}).copy()
L1360             all_scores['G_plus_D'] = all_scores['GSC'] + all_scores['DSC']
L1361             all_scores = all_scores.dropna(subset=['G_plus_D'])
L1362             self.low10_table = all_scores.sort_values('G_plus_D', ascending=True).head(10).round(3)
L1363             print("Low Score Candidates (GSC+DSC bottom 10):")
L1364             print(self.low10_table.to_string())
L1365         except Exception as e:
L1366             print(f"[warn] low-score ranking failed: {e}")
L1367             self.low10_table = None
L1368         if debug_mode:
L1369             logger.info("debug_mode=True: df_z dump handled in scorer; skipping factor-side debug output")
L1370         else:
L1371             logger.debug(
L1372                 "skip debug log: debug_mode=%s debug_text_empty=%s",
L1373                 debug_mode, True
L1374             )
L1375
L1376     # --- Slack送信（元 notify_slack のロジックそのまま） ---
L1377     def notify_slack(self):
L1378         SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")
L1379
L1380         if not SLACK_WEBHOOK_URL:
L1381             print("⚠️ SLACK_WEBHOOK_URL not set (main report skipped)")
L1382             return
L1383
L1384         def _filter_suffix_from(spec: dict, group: str) -> str:
L1385             g = spec.get(group, {})
L1386             parts = [str(m) for m in g.get("pre_mask", [])]
L1387             for k, v in (g.get("pre_filter", {}) or {}).items():
L1388                 base, op = (k[:-4], "<") if k.endswith("_max") else ((k[:-4], ">") if k.endswith("_min") else (k, "="))
L1389                 name = {"beta": "β"}.get(base, base)
L1390                 try:
L1391                     val = f"{float(v):g}"
L1392                 except Exception:
L1393                     val = str(v)
L1394                 parts.append(f"{name}{op}{val}")
L1395             return "" if not parts else " / filter:" + " & ".join(parts)
L1396
L1397         def _inject_filter_suffix(title: str, group: str) -> str:
L1398             suf = _filter_suffix_from(FILTER_SPEC, group)
L1399             return f"{title[:-1]}{suf}]" if suf and title.endswith("]") else (title + suf)
L1400
L1401         def _blk(title, tbl, fmt=None, drop=()):
L1402             if tbl is None or getattr(tbl, 'empty', False):
L1403                 return f"{title}\n(選定なし)\n"
L1404             if drop and hasattr(tbl, 'columns'):
L1405                 keep = [c for c in tbl.columns if c not in drop]
L1406                 tbl, fmt = tbl[keep], {k: v for k, v in (fmt or {}).items() if k in keep}
L1407             return f"{title}\n```{tbl.to_string(formatters=fmt)}```\n"
L1408
L1409         message = "📈 ファクター分散最適化の結果\n"
L1410         message += _blk(_inject_filter_suffix(self.g_title, "G"), self.g_table, self.g_formatters, drop=("TRD",))
L1411         message += _blk(_inject_filter_suffix(self.d_title, "D"), self.d_table, self.d_formatters)
L1412         message += "Changes\n" + ("(変更なし)\n" if self._changes_empty else f"```{self._changes_text}```\n")
L1413         message += "Performance Comparison:\n```" + self._performance_text + "```"
L1414
L1415         try:
L1416             r = requests.post(SLACK_WEBHOOK_URL, json={"text": message})
L1417             print(f"[DBG] main_post status={getattr(r, 'status_code', None)} size={len(message)}")
L1418             if r is not None:
L1419                 r.raise_for_status()
L1420         except Exception as e:
L1421             print(f"[ERR] main_post_failed: {e}")
L1422
L1423 # === パイプライン可視化：G/D共通フロー（出力は不変） ===
L1424
L1425 def io_build_input_bundle() -> InputBundle:
L1426     """
L1427     既存の『データ取得→前処理』を実行し、InputBundle を返す。
L1428     処理内容・列名・丸め・例外・ログ文言は現行どおり（変更禁止）。
L1429     """
L1430     state = Input(cand=cand, exist=exist, bench=bench, price_max=CAND_PRICE_MAX, finnhub_api_key=FINNHUB_API_KEY).prepare_data()
L1431     return InputBundle(cand=state["cand"], tickers=state["tickers"], bench=bench, data=state["data"], px=state["px"], spx=state["spx"], tickers_bulk=state["tickers_bulk"], info=state["info"], eps_df=state["eps_df"], fcf_df=state["fcf_df"], returns=state["returns"], missing_logs=state["missing_logs"])
L1432
L1433 def run_group(sc: Scorer, group: str, inb: InputBundle, cfg: PipelineConfig,
L1434               n_target: int) -> tuple[list, float, float, float]:
L1435     """
L1436     G/Dを同一手順で処理：採点→フィルター→選定（相関低減込み）。
L1437     戻り値：(pick, avg_res_corr, sum_score, objective)
L1438     JSON保存は既存フォーマット（キー名・丸め桁・順序）を踏襲。
L1439     """
L1440     sc.cfg = cfg
L1441
L1442     if hasattr(sc, "score_build_features"):
L1443         feat = sc.score_build_features(inb)
L1444         if not hasattr(sc, "_feat_logged"):
L1445             _tlog("features built (scorer)")
L1446             sc._feat_logged = True
L1447         agg = sc.score_aggregate(feat, group, cfg) if hasattr(sc, "score_aggregate") else feat
L1448     else:
L1449         if not hasattr(sc, "_feat"):
L1450             fb = sc.aggregate_scores(inb, cfg)
L1451             sc._feat = fb
L1452         else:
L1453             fb = sc._feat
L1454         if not hasattr(sc, "_feat_logged"):
L1455             _tlog("features built (scorer)")
L1456             sc._feat_logged = True
L1457         agg = fb.g_score if group == "G" else fb.d_score_all
L1458         if group == "D" and hasattr(fb, "df"):
L1459             beta_raw = fb.df['BETA'].astype(float)
L1460             if D_BETA_MODE == "z":
L1461                 beta_for_filter = _zscore_series(beta_raw)
L1462             else:
L1463                 beta_for_filter = beta_raw
L1464
L1465             beta_mask = (beta_for_filter <= D_BETA_CUTOFF).reindex(agg.index, fill_value=False)
L1466             agg = agg[beta_mask]
L1467
L1468             if isinstance(agg, pd.Series):
L1469                 _min = agg.min(skipna=True)
L1470                 floor = (0.0 if not np.isfinite(_min) else float(_min)) - 1e6
L1471                 agg = agg.fillna(floor)
L1472
L1473             try:
L1474                 logger.info(
L1475                     "D-filter mode=%s cutoff=%s | pass=%d raw[mean=%.3f std=%.3f] z[mean≈0 std≈1]",
L1476                     D_BETA_MODE,
L1477                     D_BETA_CUTOFF,
L1478                     int(beta_mask.sum()),
L1479                     float(beta_raw.mean(skipna=True)),
L1480                     float(beta_raw.std(skipna=True, ddof=0)),
L1481                 )
L1482             except Exception:
L1483                 pass
L1484
L1485     if hasattr(sc, "filter_candidates"):
L1486         agg = agg[sc.filter_candidates(inb, agg, group, cfg)]
L1487
L1488     if isinstance(agg, pd.Series):
L1489         agg = _as_numeric_series(agg)
L1490
L1491     selector = Selector()
L1492     if hasattr(sc, "select_diversified"):
L1493         pick, avg_r, sum_sc, obj = sc.select_diversified(agg, group, cfg, n_target,
L1494             selector=selector, prev_tickers=None,
L1495             corrM=cfg.drrs.corrM, shrink=cfg.drrs.shrink,
L1496             cross_mu=cfg.drrs.cross_mu_gd)
L1497     else:
L1498         if group == "G":
L1499             init = agg.nlargest(min(cfg.drrs.corrM, len(agg))).index.tolist()
L1500             res = selector.select_bucket_drrs(returns_df=inb.returns, score_ser=agg, pool_tickers=init, k=n_target,
L1501                 n_pc=cfg.drrs.G.get("n_pc", 3), gamma=cfg.drrs.G.get("gamma", 1.2),
L1502                 lam=cfg.drrs.G.get("lam", 0.68),
L1503                 lookback=cfg.drrs.G.get("lookback", 252),
L1504                 shrink=cfg.drrs.shrink, g_fixed_tickers=None, mu=0.0)
L1505         else:
L1506             init = agg.nlargest(min(cfg.drrs.corrM, len(agg))).index.tolist()
L1507             g_fixed = getattr(sc, "_top_G", None)
L1508             res = selector.select_bucket_drrs(returns_df=inb.returns, score_ser=agg, pool_tickers=init, k=n_target,
L1509                 n_pc=cfg.drrs.D.get("n_pc", 4), gamma=cfg.drrs.D.get("gamma", 0.8),
L1510                 lam=cfg.drrs.D.get("lam", 0.85),
L1511                 lookback=cfg.drrs.D.get("lookback", 504),
L1512                 shrink=cfg.drrs.shrink, g_fixed_tickers=g_fixed,
L1513                 mu=cfg.drrs.cross_mu_gd)
L1514         pick = res["tickers"]; avg_r = res["avg_res_corr"]
L1515         sum_sc = res["sum_score"]; obj = res["objective"]
L1516         if group == "D":
L1517             _, pick = _disjoint_keepG(getattr(sc, "_top_G", []), pick, init)
L1518             _tlog("selection finalized (G/D)")
L1519     try:
L1520         inc = [t for t in exist if t in agg.index]
L1521         pick = _sticky_keep_current(
L1522             agg=agg, pick=pick, incumbents=inc, n_target=n_target,
L1523             delta_z=SWAP_DELTA_Z, keep_buffer=SWAP_KEEP_BUFFER
L1524         )
L1525     except Exception as _e:
L1526         print(f"[warn] sticky_keep_current skipped: {str(_e)}")
L1527     # --- Near-Miss: 惜しくも選ばれなかった上位10を保持（Slack表示用） ---
L1528     # 5) Near-Miss と最終集計Seriesを保持（表示専用。計算へ影響なし）
L1529     try:
L1530         pool = agg.drop(index=[t for t in pick if t in agg.index], errors="ignore")
L1531         near10 = list(pool.sort_values(ascending=False).head(10).index)
L1532         setattr(sc, f"_near_{group}", near10)
L1533         setattr(sc, f"_agg_{group}", agg)
L1534     except Exception:
L1535         pass
L1536
L1537     if group == "D":
L1538         _tlog("save done")
L1539     if group == "G":
L1540         sc._top_G = pick
L1541     return pick, avg_r, sum_sc, obj
L1542
L1543 def run_pipeline() -> SelectionBundle:
L1544     """
L1545     G/D共通フローの入口。I/Oはここだけで実施し、計算はScorerに委譲。
L1546     Slack文言・丸め・順序は既存の Output を用いて変更しない。
L1547     """
L1548     inb = io_build_input_bundle()
L1549     cfg = PipelineConfig(
L1550         weights=WeightsConfig(g=g_weights, d=D_weights),
L1551         drrs=DRRSParams(
L1552             corrM=corrM, shrink=DRRS_SHRINK,
L1553             G=DRRS_G, D=DRRS_D, cross_mu_gd=CROSS_MU_GD
L1554         ),
L1555         price_max=CAND_PRICE_MAX,
L1556         debug_mode=debug_mode
L1557     )
L1558     sc = Scorer()
L1559     top_G, avgG, sumG, objG = run_group(sc, "G", inb, cfg, N_G)
L1560     poolG = list(getattr(sc, "_agg_G", pd.Series(dtype=float)).dropna().sort_values(ascending=False).index)
L1561     alpha = Scorer.spx_to_alpha(inb.spx)
L1562     sectors = {t:(inb.info.get(t,{}).get("sector") or "U") for t in poolG}; scores = {t:Scorer.g_score.get(t,0.0) for t in poolG}
L1563     top_G = Scorer.pick_top_softcap(scores, sectors, N=N_G, cap=2, alpha=alpha, hard=5)
L1564     sc._top_G = top_G
L1565     try:
L1566         aggG = getattr(sc, "_agg_G", pd.Series(dtype=float)).dropna().sort_values(ascending=False)
L1567         sc._near_G = [t for t in aggG.index if t not in set(top_G)][:10]
L1568     except Exception:
L1569         pass
L1570     base = sum(Scorer.g_score.get(t,0.0) for t in poolG[:N_G])
L1571     effs = sum(Scorer.g_score.get(t,0.0) for t in top_G)
L1572     print(f"[soft_cap2] score_cost={(base-effs)/max(1e-9,abs(base)):.2%}, alpha={alpha:.3f}")
L1573     top_D, avgD, sumD, objD = run_group(sc, "D", inb, cfg, N_D)
L1574     fb = getattr(sc, "_feat", None)
L1575     out = Output()
L1576     # 表示側から選定時の集計へアクセスできるように保持（表示専用・副作用なし）
L1577     try:
L1578         out._sc = sc
L1579     except Exception:
L1580         pass
L1581     if hasattr(sc, "_feat"):
L1582         try:
L1583             fb = sc._feat
L1584             out.display_results(
L1585                 exist=exist,
L1586                 bench=bench,
L1587                 df_raw=fb.df,
L1588                 df_z=fb.df_z,
L1589                 g_score=fb.g_score,
L1590                 d_score_all=fb.d_score_all,
L1591                 init_G=top_G,
L1592                 init_D=top_D,
L1593                 top_G=top_G,
L1594                 top_D=top_D,
L1595                 df_full_z=getattr(fb, "df_full_z", None),
L1596                 prev_G=getattr(sc, "_prev_G", exist),
L1597                 prev_D=getattr(sc, "_prev_D", exist),
L1598             )
L1599         except Exception:
L1600             pass
L1601     out.notify_slack()
L1602     sb = SelectionBundle(resG={"tickers": top_G, "avg_res_corr": avgG,
L1603               "sum_score": sumG, "objective": objG},
L1604         resD={"tickers": top_D, "avg_res_corr": avgD,
L1605               "sum_score": sumD, "objective": objD},
L1606         top_G=top_G, top_D=top_D, init_G=top_G, init_D=top_D)
L1607
L1608     # [MOD] 選定確定後に current_tickers.csv の bucket を「全部紐づけ」で最新化
L1609     try:
L1610         # 追加対象（候補群も反映）: resG['tickers'], resD['tickers'], init_G, init_D
L1611         extra_G, extra_D = [], []
L1612         try:
L1613             extra_G += list((sb.resG or {}).get("tickers", []) or [])
L1614             extra_D += list((sb.resD or {}).get("tickers", []) or [])
L1615         except Exception:
L1616             pass
L1617         try:
L1618             extra_G += list(getattr(sb, "init_G", []) or [])
L1619             extra_D += list(getattr(sb, "init_D", []) or [])
L1620         except Exception:
L1621             pass
L1622         # 追加: 候補テーブル全体（スコア計算済みユニバース）も網羅
L1623         try:
L1624             import pandas as _pd  # 局所importで安全
L1625             if 'fb' in locals() and fb is not None:
L1626                 g_idx = getattr(fb, "g_score", _pd.Series(dtype=float))
L1627                 d_idx = getattr(fb, "d_score_all", _pd.Series(dtype=float))
L1628                 extra_G += list((g_idx.dropna().index if hasattr(g_idx, "dropna") else []))
L1629                 extra_D += list((d_idx.dropna().index if hasattr(d_idx, "dropna") else []))
L1630         except Exception:
L1631             pass
L1632
L1633         _update_bucket_by_selection(
L1634             "current_tickers.csv",
L1635             top_G,
L1636             top_D,
L1637             extra_G=extra_G,
L1638             extra_D=extra_D,
L1639         )
L1640     except Exception as e:
L1641         logging.warning("bucket update skipped: %s", e)
L1642         # 失敗しても本処理は継続（I/O都合で安全側）
L1643
L1644     # --- Low Score Candidates (GSC+DSC bottom 10) : send before debug dump ---
L1645     try:
L1646         _low_df = (pd.DataFrame({"GSC": fb.g_score, "DSC": fb.d_score_all})
L1647               .assign(G_plus_D=lambda x: x["GSC"] + x["DSC"])
L1648               .sort_values("G_plus_D")
L1649               .head(10)
L1650               .round(3))
L1651         low_msg = "Low Score Candidates (GSC+DSC bottom 10)\n" + _low_df.to_string(index=True, index_names=False)
L1652         _post_slack({"text": f"```{low_msg}```"})
L1653     except Exception as _e:
L1654         _post_slack({"text": f"```Low Score Candidates: 作成失敗: {_e}```"})
L1655
L1656     return sb
L1657
L1658
L1659 # --- Slack / warning helpers (relocated without logic changes) ---
L1660
L1661
L1662 def _post_slack(payload: dict):
L1663     url = os.getenv("SLACK_WEBHOOK_URL")
L1664     if not url:
L1665         print("⚠️ SLACK_WEBHOOK_URL 未設定")
L1666         return
L1667     try:
L1668         requests.post(url, json=payload).raise_for_status()
L1669     except Exception as e:
L1670         print(f"⚠️ Slack通知エラー: {e}")
L1671
L1672
L1673 if __name__ == "__main__":
L1674     run_pipeline()
```

## <scorer.py>
```text
L1 # scorer.py
L2 # kawatest
L3 # =============================================================================
L4 # Scorer: ファクター/指標の生成と合成スコア算出を担う純粋層
L5 #
L6 # 【このファイルだけ読めば分かるポイント】
L7 # - 入力(InputBundle)は「価格/出来高/ベンチ/基本情報/EPS/FCF/リターン」を含むDTO
L8 # - 出力(FeatureBundle)は「raw特徴量 df」「標準化 df_z」「G/D スコア」「欠損ログ」
L9 # - 重み等のコンフィグ(PipelineConfig)は factor から渡す（cfg 必須）
L10 # - 旧カラム名は Scorer 内で自動リネームして受け入れ（後方互換）
L11 #   例) eps_ttm -> EPS_TTM, eps_q_recent -> EPS_Q_LastQ, fcf_ttm -> FCF_TTM
L12 #
L13 # 【I/O契約（Scorerが参照するInputBundleフィールド）】
L14 #   - cand: List[str]    … 候補銘柄（単体実行では未使用）
L15 #   - tickers: List[str] … 対象銘柄リスト
L16 #   - bench: str         … ベンチマークティッカー（例 '^GSPC'）
L17 #   - data: pd.DataFrame … yfinance download結果 ('Close','Volume' 等の階層列)
L18 #   - px: pd.DataFrame   … data['Close'] 相当（終値）
L19 #   - spx: pd.Series     … ベンチマークの終値
L20 #   - tickers_bulk: object         … yfinance.Tickers
L21 #   - info: Dict[str, dict]        … yfinance info per ticker
L22 #   - eps_df: pd.DataFrame         … 必須列: EPS_TTM, EPS_Q_LastQ（旧名も可）
L23 #   - fcf_df: pd.DataFrame         … 必須列: FCF_TTM（旧名も可）
L24 #   - returns: pd.DataFrame        … px[tickers].pct_change() 相当
L25 #   - missing_logs: pd.DataFrame   … 補完後の欠損ログ
L26 #
L27 # ※入出力の形式・例外文言は既存実装を変えません（安全な短縮のみ）
L28 # =============================================================================
L29
L30 import json, logging, os, requests, sys, warnings
L31 import numpy as np
L32 import pandas as pd
L33 import yfinance as yf
L34 from typing import Any, TYPE_CHECKING
L35 from scipy.stats import zscore
L36 from datetime import datetime as _dt
L37
L38 if TYPE_CHECKING:
L39     from factor import PipelineConfig  # type: ignore  # 実行時importなし（循環回避）
L40
L41 logger = logging.getLogger(__name__)
L42
L43
L44 def _log(stage, msg):
L45     try:
L46         print(f"[DBG][{_dt.utcnow().isoformat(timespec='seconds')}Z][{stage}] {msg}")
L47     except Exception:
L48         print(f"[DBG][{stage}] {msg}")
L49
L50
L51 # ---- Dividend Helpers -------------------------------------------------------
L52 def _last_close(t, price_map=None):
L53     if price_map and (c := price_map.get(t)) is not None: return float(c)
L54     try:
L55         h = yf.Ticker(t).history(period="5d")["Close"].dropna()
L56         return float(h.iloc[-1]) if len(h) else np.nan
L57     except Exception:
L58         return np.nan
L59
L60 def _ttm_div_sum(t, lookback_days=400):
L61     try:
L62         div = yf.Ticker(t).dividends
L63         if div is None or len(div) == 0: return 0.0
L64         cutoff = pd.Timestamp.utcnow().tz_localize(None) - pd.Timedelta(days=lookback_days)
L65         ttm = float(div[div.index.tz_localize(None) >= cutoff].sum())
L66         return ttm if ttm > 0 else float(div.tail(4).sum())
L67     except Exception:
L68         return 0.0
L69
L70 def ttm_div_yield_portfolio(tickers, price_map=None):
L71     ys = [(lambda c, s: (s/c) if (np.isfinite(c) and c>0 and s>0) else 0.0)(_last_close(t, price_map), _ttm_div_sum(t)) for t in tickers]
L72     return float(np.mean(ys)) if ys else 0.0
L73
L74 # ---- 簡易ユーティリティ（安全な短縮のみ） -----------------------------------
L75 def _as_numeric_series(s: pd.Series) -> pd.Series:
L76     """Series を float dtype に強制変換し、index を保持する。"""
L77     if s is None:
L78         return pd.Series(dtype=float)
L79     v = pd.to_numeric(s, errors="coerce")
L80     return pd.Series(v.values, index=getattr(s, "index", None), dtype=float, name=getattr(s, "name", None))
L81
L82
L83 def _plain_zscore_series(s: pd.Series) -> pd.Series:
L84     v = pd.to_numeric(s, errors="coerce")
L85     mean = v.mean(skipna=True)
L86     std = v.std(skipna=True, ddof=0)
L87     if not np.isfinite(std) or std == 0:
L88         return pd.Series(index=v.index, dtype=float)
L89     out = (v - mean) / std
L90     return pd.Series(out.values, index=v.index, dtype=float)
L91
L92
L93 def _scalar(x):
L94     """
L95     入力を安全に float スカラへ変換する。
L96
L97     許容する入力パターン:
L98       - pandas.Series: 非NaNの最後の値を採用
L99       - numpy スカラ/配列: 最後の要素を採用
L100       - その他の数値っぽい値: float へ変換
L101
L102     変換できない場合は np.nan を返す。
L103     """
L104
L105     if x is None:
L106         return np.nan
L107
L108     # pandas.Series の場合は非NaNの最後の値を採用
L109     if isinstance(x, pd.Series):
L110         s = pd.to_numeric(x, errors="coerce").dropna()
L111         return float(s.iloc[-1]) if not s.empty else np.nan
L112
L113     # numpy スカラ (item() を持つ) ※文字列は除外
L114     if hasattr(x, "item") and not isinstance(x, (str, bytes)):
L115         try:
L116             return float(x.item())
L117         except Exception:
L118             pass
L119
L120     # 配列様のオブジェクト
L121     try:
L122         arr = np.asarray(x, dtype=float).ravel()
L123         return float(arr[-1]) if arr.size else np.nan
L124     except Exception:
L125         pass
L126
L127     # 最後に素直に float 変換を試す
L128     try:
L129         return float(x)
L130     except Exception:
L131         return np.nan
L132
L133
L134 def winsorize_s(s: pd.Series, p=0.02):
L135     if s is None or s.dropna().empty: return s
L136     lo, hi = np.nanpercentile(s.astype(float), [100*p, 100*(1-p)]); return s.clip(lo, hi)
L137
L138 def robust_z(s: pd.Series, p=0.02):
L139     s2 = winsorize_s(s,p); return np.nan_to_num(zscore(s2.fillna(s2.mean())))
L140
L141 def robust_z_keepnan(s: pd.Series) -> pd.Series:
L142     """robust_z variant that preserves NaNs and falls back to rank-z when needed."""
L143     if s is None:
L144         return pd.Series(dtype=float)
L145     v = pd.to_numeric(s, errors="coerce")
L146     m = np.nanmedian(v)
L147     mad = np.nanmedian(np.abs(v - m))
L148     z = (v - m) / (1.4826 * mad + 1e-9)
L149     if np.nanstd(z) < 1e-9:
L150         r = v.rank(method="average", na_option="keep")
L151         z = (r - np.nanmean(r)) / (np.nanstd(r) + 1e-9)
L152     return pd.Series(z, index=v.index, dtype=float)
L153
L154
L155 def _safe_div(a, b):
L156     try: return np.nan if (b is None or float(b)==0 or pd.isna(b)) else float(a)/float(b)
L157     except Exception: return np.nan
L158
L159 def _safe_last(series: pd.Series, default=np.nan):
L160     try: return float(series.iloc[-1])
L161     except Exception: return default
L162
L163
L164 def _ensure_series(x):
L165     if x is None:
L166         return pd.Series(dtype=float)
L167     if isinstance(x, pd.Series):
L168         return x.dropna()
L169     if isinstance(x, (list, tuple)):
L170         if len(x) and isinstance(x[0], (tuple, list)) and len(x[0]) == 2:
L171             dt = pd.to_datetime([d for d, _ in x], errors="coerce")
L172             v = pd.to_numeric([_v for _, _v in x], errors="coerce")
L173             return pd.Series(v, index=dt).dropna()
L174         return pd.Series(pd.to_numeric(list(x), errors="coerce")).dropna()
L175     try:
L176         return pd.Series(x).dropna()
L177     except Exception:
L178         return pd.Series(dtype=float)
L179
L180
L181 def _to_quarterly(s: pd.Series) -> pd.Series:
L182     if s.empty or not isinstance(s.index, pd.DatetimeIndex):
L183         return s
L184     return s.resample("Q").last().dropna()
L185
L186
L187 def _ttm_yoy_from_quarterly(qs: pd.Series) -> pd.Series:
L188     if qs is None or qs.empty:
L189         return pd.Series(dtype=float)
L190     ttm = qs.rolling(4, min_periods=2).sum()
L191     yoy = ttm.pct_change(4)
L192     return yoy
L193
L194
L195
L196
L197 class Scorer:
L198     """
L199     - factor.py からは `aggregate_scores(ib, cfg)` を呼ぶだけでOK。
L200     - cfg は必須（factor.PipelineConfig を渡す）。
L201     - 旧カラム名を自動リネームして新スキーマに吸収します。
L202     """
L203
L204     # === 先頭で旧→新カラム名マップ（移行用） ===
L205     EPS_RENAME = {"eps_ttm":"EPS_TTM", "eps_q_recent":"EPS_Q_LastQ"}
L206     FCF_RENAME = {"fcf_ttm":"FCF_TTM"}
L207
L208     # === スキーマ簡易チェック（最低限） ===
L209     @staticmethod
L210     def _validate_ib_for_scorer(ib: Any):
L211         miss = [a for a in ["tickers","bench","data","px","spx","tickers_bulk","info","eps_df","fcf_df","returns"] if not hasattr(ib,a) or getattr(ib,a) is None]
L212         if miss: raise ValueError(f"InputBundle is missing required attributes for Scorer: {miss}")
L213         if any(c in ib.eps_df.columns for c in Scorer.EPS_RENAME): ib.eps_df.rename(columns=Scorer.EPS_RENAME, inplace=True)
L214         if any(c in ib.fcf_df.columns for c in Scorer.FCF_RENAME): ib.fcf_df.rename(columns=Scorer.FCF_RENAME, inplace=True)
L215         need_eps, need_fcf = {"EPS_TTM","EPS_Q_LastQ"},{"FCF_TTM"}
L216         if not need_eps.issubset(ib.eps_df.columns): raise ValueError(f"eps_df must contain columns {need_eps} (accepts old names via auto-rename). Got: {list(ib.eps_df.columns)}")
L217         if not need_fcf.issubset(ib.fcf_df.columns): raise ValueError(f"fcf_df must contain columns {need_fcf} (accepts old names via auto-rename). Got: {list(ib.fcf_df.columns)}")
L218
L219     # ----（Scorer専用）テクニカル・指標系 ----
L220     @staticmethod
L221     def trend(s: pd.Series):
L222         if len(s)<200: return np.nan
L223         sma50, sma150, sma200 = s.rolling(50).mean().iloc[-1], s.rolling(150).mean().iloc[-1], s.rolling(200).mean().iloc[-1]
L224         prev200, p = s.rolling(200).mean().iloc[-21], s.iloc[-1]
L225         lo_52 = s[-252:].min() if len(s)>=252 else s.min(); hi_52 = s[-252:].max() if len(s)>=252 else s.max()
L226         rng = (hi_52 - lo_52) if hi_52>lo_52 else np.nan
L227         clip = lambda x,lo,hi: (np.nan if pd.isna(x) else max(lo,min(hi,x)))
L228         a = clip(p/(s.rolling(50).mean().iloc[-1]) - 1, -0.5, 0.5)
L229         b = clip(sma50/sma150 - 1, -0.5, 0.5)
L230         c = clip(sma150/sma200 - 1, -0.5, 0.5)
L231         d = clip(sma200/prev200 - 1, -0.2, 0.2)
L232         e = clip((p - lo_52) / (rng if rng and rng>0 else np.nan) - 0.5, -0.5, 0.5)
L233         parts = [0.0 if pd.isna(x) else x for x in (a,b,c,d,e)]
L234         return 0.30*parts[0] + 0.20*parts[1] + 0.15*parts[2] + 0.15*parts[3] + 0.20*parts[4]
L235
L236     @staticmethod
L237     def rs(s, b):
L238         n, nb = len(s), len(b)
L239         if n<60 or nb<60: return np.nan
L240         L12 = 252 if n>=252 and nb>=252 else min(n,nb)-1; L1 = 22 if n>=22 and nb>=22 else max(5, min(n,nb)//3)
L241         r12, r1, br12, br1 = s.iloc[-1]/s.iloc[-L12]-1, s.iloc[-1]/s.iloc[-L1]-1, b.iloc[-1]/b.iloc[-L12]-1, b.iloc[-1]/b.iloc[-L1]-1
L242         return (r12 - br12)*0.7 + (r1 - br1)*0.3
L243
L244     @staticmethod
L245     def tr_str(s):
L246         if s is None:
L247             return np.nan
L248         s = s.ffill(limit=2).dropna()
L249         if len(s) < 50:
L250             return np.nan
L251         ma50 = s.rolling(50, min_periods=50).mean()
L252         last_ma = ma50.iloc[-1]
L253         last_px = s.iloc[-1]
L254         return float(last_px/last_ma - 1.0) if pd.notna(last_ma) and pd.notna(last_px) else np.nan
L255
L256     @staticmethod
L257     def rs_line_slope(s: pd.Series, b: pd.Series, win: int) -> float:
L258         r = (s/b).dropna()
L259         if len(r) < win: return np.nan
L260         y, x = np.log(r.iloc[-win:]), np.arange(win, dtype=float)
L261         try: return float(np.polyfit(x, y, 1)[0])
L262         except Exception: return np.nan
L263
L264     @staticmethod
L265     def ev_fallback(info_t: dict, tk: yf.Ticker) -> float:
L266         ev = info_t.get('enterpriseValue', np.nan)
L267         if pd.notna(ev) and ev>0: return float(ev)
L268         mc, debt, cash = info_t.get('marketCap', np.nan), np.nan, np.nan
L269         try:
L270             bs = tk.quarterly_balance_sheet
L271             if bs is not None and not bs.empty:
L272                 c = bs.columns[0]
L273                 for k in ("Total Debt","Long Term Debt","Short Long Term Debt"):
L274                     if k in bs.index: debt = float(bs.loc[k,c]); break
L275                 for k in ("Cash And Cash Equivalents","Cash And Cash Equivalents And Short Term Investments","Cash"):
L276                     if k in bs.index: cash = float(bs.loc[k,c]); break
L277         except Exception: pass
L278         if pd.notna(mc): return float(mc + (0 if pd.isna(debt) else debt) - (0 if pd.isna(cash) else cash))
L279         return np.nan
L280
L281     @staticmethod
L282     def div_streak(t):
L283         try:
L284             divs = yf.Ticker(t).dividends.dropna(); ann = divs.groupby(divs.index.year).sum(); ann = ann[ann.index<pd.Timestamp.today().year]
L285             years, streak = sorted(ann.index), 0
L286             for i in range(len(years)-1,0,-1):
L287                 if ann[years[i]] > ann[years[i-1]]: streak += 1
L288                 else: break
L289             return streak
L290         except Exception: return 0
L291
L292     @staticmethod
L293     def fetch_finnhub_metrics(symbol):
L294         api_key = os.environ.get("FINNHUB_API_KEY")
L295         if not api_key: return {}
L296         url, params = "https://finnhub.io/api/v1/stock/metric", {"symbol":symbol,"metric":"all","token":api_key}
L297         try:
L298             r = requests.get(url, params=params, timeout=10); r.raise_for_status(); m = r.json().get("metric",{})
L299             return {'EPS':m.get('epsGrowthTTMYoy'),'REV':m.get('revenueGrowthTTMYoy'),'ROE':m.get('roeTTM'),'BETA':m.get('beta'),'DIV':m.get('dividendYieldIndicatedAnnual'),'FCF':(m.get('freeCashFlowTTM')/m.get('enterpriseValue')) if m.get('freeCashFlowTTM') and m.get('enterpriseValue') else None}
L300         except Exception: return {}
L301
L302     @staticmethod
L303     def calc_beta(series: pd.Series, market: pd.Series, lookback=252):
L304         r, m = series.pct_change().dropna(), market.pct_change().dropna()
L305         n = min(len(r), len(m), lookback)
L306         if n<60: return np.nan
L307         r, m = r.iloc[-n:], m.iloc[-n:]; cov, var = np.cov(r, m)[0,1], np.var(m)
L308         return np.nan if var==0 else cov/var
L309
L310     @staticmethod
L311     def spx_to_alpha(spx: pd.Series, bands=(0.03,0.10), w=(0.6,0.4),
L312                      span=5, q=(0.20,0.40), alphas=(0.05,0.08,0.10)) -> float:
L313         """
L314         S&P500指数のみから擬似breadthを作り、履歴分位でαを段階決定。
L315         bands=(±3%, ±10%), w=(50DMA,200DMA), 分位q=(20%,40%), alphas=(低,中,高)
L316         """
L317         ma50, ma200 = spx.rolling(50).mean(), spx.rolling(200).mean()
L318         b50, b200 = ((spx/ma50 - 1)+bands[0])/(2*bands[0]), ((spx/ma200 - 1)+bands[1])/(2*bands[1])
L319         hist = (w[0]*b50 + w[1]*b200).clip(0,1).ewm(span=span).mean()
L320         b, (lo, mid) = float(hist.iloc[-1]), (float(hist.quantile(q[0])), float(hist.quantile(q[1])))
L321         return alphas[0] if b < lo else alphas[1] if b < mid else alphas[2]
L322
L323     @staticmethod
L324     def soft_cap_effective_scores(scores: pd.Series|dict, sectors: dict, cap=2, alpha=0.08) -> pd.Series:
L325         """
L326         同一セクターcap超過（3本目以降）に α×段階減点を課した“有効スコア”Seriesを返す。
L327         戻り値は降順ソート済み。
L328         """
L329         s = pd.Series(scores, dtype=float); order = s.sort_values(ascending=False).index
L330         cnt, pen = {}, {}
L331         for t in order:
L332             sec = sectors.get(t, "U"); cnt[sec] = cnt.get(sec,0) + 1; pen[t] = alpha*max(0, cnt[sec]-cap)
L333         return (s - pd.Series(pen)).sort_values(ascending=False)
L334
L335     @staticmethod
L336     def pick_top_softcap(scores: pd.Series|dict, sectors: dict, N: int, cap=2, alpha=0.08, hard: int|None=5) -> list[str]:
L337         """
L338         soft-cap適用後の上位Nティッカーを返す。hard>0なら非常用ハード上限で同一セクター超過を間引く（既定=5）。
L339         """
L340         eff = Scorer.soft_cap_effective_scores(scores, sectors, cap, alpha)
L341         eff = eff.dropna()
L342         if not hard:
L343             return list(eff.head(N).index)
L344         pick, used = [], {}
L345         for t in eff.index:
L346             s = sectors.get(t, "U")
L347             if used.get(s,0) < hard:
L348                 pick.append(t); used[s] = used.get(s,0) + 1
L349             if len(pick) == N: break
L350         return pick
L351
L352     # ---- スコア集計（DTO/Configを受け取り、FeatureBundleを返す） ----
L353     def aggregate_scores(self, ib: Any, cfg):
L354         if cfg is None:
L355             raise ValueError("cfg is required; pass factor.PipelineConfig")
L356         self._validate_ib_for_scorer(ib)
L357
L358         px, spx, tickers = ib.px, ib.spx, ib.tickers
L359         try:
L360             vol = ib.data['Volume']
L361         except Exception:
L362             vol = getattr(ib, 'vol', None)
L363         tickers_bulk, info, eps_df, fcf_df = ib.tickers_bulk, ib.info, ib.eps_df, ib.fcf_df
L364
L365         df = pd.DataFrame(index=tickers)
L366         df['EPS_SERIES'] = pd.Series([None] * len(df), index=df.index, dtype=object)
L367         debug_mode = bool(getattr(cfg, "debug_mode", False))
L368         eps_cols = set(getattr(eps_df, "columns", []))
L369         for t in tickers:
L370             d, s = info[t], px[t]; ev = self.ev_fallback(d, tickers_bulk.tickers[t])
L371             try:
L372                 volume_series_full = ib.data['Volume'][t]
L373             except Exception:
L374                 volume_series_full = None
L375
L376             # --- 基本特徴 ---
L377             df.loc[t,'TR']   = self.trend(s)
L378
L379             def _eps_value(col: str) -> float:
L380                 if col not in eps_cols:
L381                     return np.nan
L382                 try:
L383                     return _scalar(eps_df[col].get(t, np.nan))
L384                 except Exception:
L385                     return np.nan
L386
L387             df.loc[t,'EPS']  = _eps_value('EPS_TTM')
L388             df.loc[t,'EPS_Q'] = _eps_value('EPS_Q_LastQ')
L389             df.loc[t,'REV_TTM'] = _eps_value('REV_TTM')
L390             df.loc[t,'REV_Q']   = _eps_value('REV_Q_LastQ')
L391             df.loc[t,'EPS_TTM_PREV'] = _eps_value('EPS_TTM_PREV')
L392             df.loc[t,'REV_TTM_PREV'] = _eps_value('REV_TTM_PREV')
L393             df.loc[t,'EPS_Q_PREV'] = _eps_value('EPS_Q_Prev')
L394             df.loc[t,'REV_Q_PREV'] = _eps_value('REV_Q_Prev')
L395             df.loc[t,'EPS_A_LATEST'] = _eps_value('EPS_A_LATEST')
L396             df.loc[t,'EPS_A_PREV'] = _eps_value('EPS_A_PREV')
L397             df.loc[t,'REV_A_LATEST'] = _eps_value('REV_A_LATEST')
L398             df.loc[t,'REV_A_PREV'] = _eps_value('REV_A_PREV')
L399             df.loc[t,'EPS_A_CAGR3'] = _eps_value('EPS_A_CAGR3')
L400             df.loc[t,'REV_A_CAGR3'] = _eps_value('REV_A_CAGR3')
L401             df.loc[t,'REV']  = d.get('revenueGrowth',np.nan)
L402             df.loc[t,'ROE']  = d.get('returnOnEquity',np.nan)
L403             df.loc[t,'BETA'] = self.calc_beta(s, spx, lookback=252)
L404
L405             # --- 配当（欠損補完含む） ---
L406             div = d.get('dividendYield') if d.get('dividendYield') is not None else d.get('trailingAnnualDividendYield')
L407             if div is None or pd.isna(div):
L408                 try:
L409                     divs = yf.Ticker(t).dividends
L410                     if divs is not None and not divs.empty:
L411                         last_close = s.iloc[-1]; div_1y = divs[divs.index >= (divs.index.max() - pd.Timedelta(days=365))].sum()
L412                         if last_close and last_close>0: div = float(div_1y/last_close)
L413                 except Exception: pass
L414             df.loc[t,'DIV'] = 0.0 if (div is None or pd.isna(div)) else float(div)
L415
L416             # --- FCF/EV ---
L417             fcf_val = fcf_df.loc[t,'FCF_TTM'] if t in fcf_df.index else np.nan
L418             df.loc[t,'FCF'] = (fcf_val/ev) if (pd.notna(fcf_val) and pd.notna(ev) and ev>0) else np.nan
L419
L420             # --- モメンタム・ボラ関連 ---
L421             df.loc[t,'RS'], df.loc[t,'TR_str'] = self.rs(s, spx), self.tr_str(s)
L422             r, rm = s.pct_change().dropna(), spx.pct_change().dropna()
L423             n = int(min(len(r), len(rm)))
L424
L425             DOWNSIDE_DEV = np.nan
L426             if n>=60:
L427                 r6 = r.iloc[-min(len(r),126):]; neg = r6[r6<0]
L428                 if len(neg)>=10: DOWNSIDE_DEV = float(neg.std(ddof=0)*np.sqrt(252))
L429             df.loc[t,'DOWNSIDE_DEV'] = DOWNSIDE_DEV
L430
L431             MDD_1Y = np.nan
L432             try:
L433                 w = s.iloc[-min(len(s),252):].dropna()
L434                 if len(w)>=30:
L435                     roll_max = w.cummax(); MDD_1Y = float((w/roll_max - 1.0).min())
L436             except Exception: pass
L437             df.loc[t,'MDD_1Y'] = MDD_1Y
L438
L439             RESID_VOL = np.nan
L440             if n>=120:
L441                 rr, rrm = r.iloc[-n:].align(rm.iloc[-n:], join='inner')
L442                 if len(rr)==len(rrm) and len(rr)>=120 and rrm.var()>0:
L443                     beta = float(np.cov(rr, rrm)[0,1]/np.var(rrm)); resid = rr - beta*rrm
L444                     RESID_VOL = float(resid.std(ddof=0)*np.sqrt(252))
L445             df.loc[t,'RESID_VOL'] = RESID_VOL
L446
L447             DOWN_OUTPERF = np.nan
L448             if n>=60:
L449                 m, x = rm.iloc[-n:], r.iloc[-n:]; mask = m<0
L450                 if mask.sum()>=10:
L451                     mr, sr = float(m[mask].mean()), float(x[mask].mean())
L452                     DOWN_OUTPERF = (sr - mr)/abs(mr) if mr!=0 else np.nan
L453             df.loc[t,'DOWN_OUTPERF'] = DOWN_OUTPERF
L454
L455             # --- 長期移動平均/位置 ---
L456             sma200 = s.rolling(200).mean(); df.loc[t,'EXT_200'] = np.nan
L457             if pd.notna(sma200.iloc[-1]) and sma200.iloc[-1]!=0: df.loc[t,'EXT_200'] = abs(float(s.iloc[-1]/sma200.iloc[-1]-1.0))
L458
L459             # --- 配当の詳細系 ---
L460             DIV_TTM_PS=DIV_VAR5=DIV_YOY=DIV_FCF_COVER=np.nan
L461             try:
L462                 divs = yf.Ticker(t).dividends.dropna()
L463                 if not divs.empty:
L464                     last_close = s.iloc[-1]; div_1y = float(divs[divs.index >= (divs.index.max()-pd.Timedelta(days=365))].sum())
L465                     DIV_TTM_PS = div_1y if div_1y>0 else np.nan
L466                     ann = divs.groupby(divs.index.year).sum()
L467                     if len(ann)>=2 and ann.iloc[-2]!=0: DIV_YOY = float(ann.iloc[-1]/ann.iloc[-2]-1.0)
L468                     tail = ann.iloc[-5:] if len(ann)>=5 else ann
L469                     if len(tail)>=3 and tail.mean()!=0: DIV_VAR5 = float(tail.std(ddof=1)/abs(tail.mean()))
L470                 so = d.get('sharesOutstanding',None)
L471                 if so and pd.notna(DIV_TTM_PS) and pd.notna(fcf_val) and fcf_val!=0:
L472                     DIV_FCF_COVER = float((fcf_val)/(DIV_TTM_PS*float(so)))
L473             except Exception: pass
L474             df.loc[t,'DIV_TTM_PS'], df.loc[t,'DIV_VAR5'], df.loc[t,'DIV_YOY'], df.loc[t,'DIV_FCF_COVER'] = DIV_TTM_PS, DIV_VAR5, DIV_YOY, DIV_FCF_COVER
L475
L476             # --- 財務安定性 ---
L477             df.loc[t,'DEBT2EQ'], df.loc[t,'CURR_RATIO'] = d.get('debtToEquity',np.nan), d.get('currentRatio',np.nan)
L478
L479             # --- EPS 変動 ---
L480             EPS_VAR_8Q = np.nan
L481             try:
L482                 qe, so = tickers_bulk.tickers[t].quarterly_earnings, d.get('sharesOutstanding',None)
L483                 if qe is not None and not qe.empty and so:
L484                     eps_q = (qe['Earnings'].dropna().astype(float)/float(so)).replace([np.inf,-np.inf],np.nan)
L485                     if len(eps_q)>=4: EPS_VAR_8Q = float(eps_q.iloc[-min(8,len(eps_q)):].std(ddof=1))
L486             except Exception: pass
L487             df.loc[t,'EPS_VAR_8Q'] = EPS_VAR_8Q
L488
L489             # --- サイズ/流動性 ---
L490             df.loc[t,'MARKET_CAP'] = d.get('marketCap',np.nan); adv60 = np.nan
L491             try:
L492                 if isinstance(volume_series_full, pd.Series):
L493                     vol_series = volume_series_full.reindex(s.index).dropna()
L494                     if len(vol_series) >= 5:
L495                         aligned_px = s.reindex(vol_series.index).dropna()
L496                         if len(aligned_px) == len(vol_series):
L497                             dv = (vol_series*aligned_px).rolling(60).mean()
L498                             if not dv.dropna().empty:
L499                                 adv60 = float(dv.dropna().iloc[-1])
L500             except Exception:
L501                 pass
L502             df.loc[t,'ADV60_USD'] = adv60
L503
L504             # --- Rule of 40 や周辺 ---
L505             total_rev_ttm = d.get('totalRevenue',np.nan)
L506             FCF_MGN = _safe_div(fcf_val, total_rev_ttm)
L507             df.loc[t,'FCF_MGN'] = FCF_MGN
L508             rule40 = np.nan
L509             try:
L510                 r = df.loc[t,'REV']; rule40 = (r if pd.notna(r) else np.nan) + (FCF_MGN if pd.notna(FCF_MGN) else np.nan)
L511             except Exception: pass
L512             df.loc[t,'RULE40'] = rule40
L513
L514             # --- トレンド補助 ---
L515             sma50  = s.rolling(50).mean()
L516             sma150 = s.rolling(150).mean()
L517             sma200 = s.rolling(200).mean()
L518             p = _safe_last(s)
L519
L520             df.loc[t,'MA50_OVER_150'] = (_safe_last(sma50)/_safe_last(sma150) - 1
L521                 if pd.notna(_safe_last(sma50)) and pd.notna(_safe_last(sma150)) and _safe_last(sma150)!=0 else np.nan)
L522             df.loc[t,'MA150_OVER_200'] = (_safe_last(sma150)/_safe_last(sma200) - 1
L523                 if pd.notna(_safe_last(sma150)) and pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan)
L524
L525             lo52 = s[-252:].min() if len(s)>=252 else s.min()
L526             df.loc[t,'P_OVER_LOW52'] = (p/lo52 - 1) if (lo52 and lo52>0 and pd.notna(p)) else np.nan
L527
L528             df.loc[t,'MA200_SLOPE_1M'] = np.nan
L529             if len(sma200.dropna()) >= 21:
L530                 cur200 = _safe_last(sma200)
L531                 old2001 = float(sma200.iloc[-21])
L532                 if old2001:
L533                     df.loc[t,'MA200_SLOPE_1M'] = cur200/old2001 - 1
L534
L535             df.loc[t,'P_OVER_150'] = p/_safe_last(sma150)-1 if pd.notna(_safe_last(sma150)) and _safe_last(sma150)!=0 else np.nan
L536             df.loc[t,'P_OVER_200'] = p/_safe_last(sma200)-1 if pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L537             df.loc[t,'MA50_OVER_200'] = _safe_last(sma50)/_safe_last(sma200)-1 if pd.notna(_safe_last(sma50)) and pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L538             df.loc[t,'MA200_SLOPE_5M'] = np.nan
L539             if len(sma200.dropna())>=105:
L540                 cur200, old200 = _safe_last(sma200), float(sma200.iloc[-105])
L541                 if old200 and old200!=0: df.loc[t,'MA200_SLOPE_5M'] = cur200/old200 - 1
L542             # NEW: 200日線が連続で上向きの「日数」
L543             df.loc[t,'MA200_UP_STREAK_D'] = np.nan
L544             try:
L545                 s200 = sma200.dropna()
L546                 if len(s200) >= 2:
L547                     diff200 = s200.diff()
L548                     up = 0
L549                     for v in diff200.iloc[::-1]:
L550                         if pd.isna(v) or v <= 0:
L551                             break
L552                         up += 1
L553                     df.loc[t,'MA200_UP_STREAK_D'] = float(up)
L554             except Exception:
L555                 pass
L556             df.loc[t,'LOW52PCT25_EXCESS'] = np.nan if (lo52 is None or lo52<=0 or pd.isna(p)) else (p/(lo52*1.25)-1)
L557             hi52 = s[-252:].max() if len(s)>=252 else s.max(); df.loc[t,'NEAR_52W_HIGH'] = np.nan
L558             if hi52 and hi52>0 and pd.notna(p):
L559                 d_hi = (p/hi52)-1.0; df.loc[t,'NEAR_52W_HIGH'] = -abs(min(0.0, d_hi))
L560             df.loc[t,'RS_SLOPE_6W'] = self.rs_line_slope(s, ib.spx, 30)
L561             df.loc[t,'RS_SLOPE_13W'] = self.rs_line_slope(s, ib.spx, 65)
L562
L563             df.loc[t,'DIV_STREAK'] = self.div_streak(t)
L564
L565             # --- 欠損メモ ---
L566             fin_cols = ['REV','ROE','BETA','DIV','FCF']
L567             need_finnhub = [col for col in fin_cols if pd.isna(df.loc[t,col])]
L568             if need_finnhub:
L569                 fin_data = self.fetch_finnhub_metrics(t)
L570                 for col in need_finnhub:
L571                     val = fin_data.get(col)
L572                     if val is not None and not pd.isna(val): df.loc[t,col] = val
L573             # 欠損ログは factor 側で補完後に集約する（ここでは検知のみ）
L574
L575         def _pick_series(entry: dict, keys: list[str]):
L576             for k in keys:
L577                 val = entry.get(k) if isinstance(entry, dict) else None
L578                 if val is None:
L579                     continue
L580                 try:
L581                     if hasattr(val, "empty") and getattr(val, "empty"):
L582                         continue
L583                 except Exception:
L584                     pass
L585                 if isinstance(val, (list, tuple)) and len(val) == 0:
L586                     continue
L587                 return val
L588             return None
L589
L590         for t in tickers:
L591             try:
L592                 d = info.get(t, {}) or {}
L593                 rev_series = d.get("SEC_REV_Q_SERIES")
L594                 eps_series = d.get("SEC_EPS_Q_SERIES")
L595                 fallback_qearn = False
L596                 try:
L597                     qe = tickers_bulk.tickers[t].quarterly_earnings
L598                     fallback_qearn = bool(qe is not None and not getattr(qe, "empty", True))
L599                 except Exception:
L600                     qe = None
L601
L602                 r_src = _pick_series(d, ["SEC_REV_Q_SERIES", "rev_q_series_pairs", "rev_q_series"])
L603                 e_src = _pick_series(d, ["SEC_EPS_Q_SERIES", "eps_q_series_pairs", "eps_q_series"])
L604                 r_raw = _ensure_series(r_src)
L605                 e_raw = _ensure_series(e_src)
L606
L607                 r_q = _to_quarterly(r_raw)
L608                 e_q = _to_quarterly(e_raw)
L609
L610                 df.at[t, "EPS_SERIES"] = e_q
L611
L612                 r_yoy_ttm = _ttm_yoy_from_quarterly(r_q)
L613                 e_yoy_ttm = _ttm_yoy_from_quarterly(e_q)
L614
L615                 def _q_yoy(qs):
L616                     return np.nan if qs is None or len(qs) < 5 else float(qs.iloc[-1] / qs.iloc[-5] - 1.0)
L617
L618                 rev_q_yoy = _q_yoy(r_q)
L619                 eps_q_yoy = _q_yoy(e_q)
L620
L621                 def _annual_from(qs: pd.Series, yoy_ttm: pd.Series):
L622                     if isinstance(qs.index, pd.DatetimeIndex) and len(qs) >= 8:
L623                         ann = qs.groupby(qs.index.year).last().pct_change()
L624                         ann_dn = ann.dropna()
L625                         if not ann_dn.empty:
L626                             y = float(ann_dn.iloc[-1])
L627                             acc = float(ann_dn.tail(3).mean()) if ann_dn.size >= 3 else np.nan
L628                             var = float(ann_dn.tail(4).var()) if ann_dn.size >= 4 else np.nan
L629                             return y, acc, var
L630                     yoy_dn = yoy_ttm.dropna()
L631                     if yoy_dn.empty:
L632                         return np.nan, np.nan, np.nan
L633                     return (
L634                         float(yoy_dn.iloc[-1]),
L635                         float(yoy_dn.tail(3).mean() if yoy_dn.size >= 3 else np.nan),
L636                         float(yoy_dn.tail(4).var() if yoy_dn.size >= 4 else np.nan),
L637                     )
L638
L639                 rev_yoy, rev_acc, rev_var = _annual_from(r_q, r_yoy_ttm)
L640                 eps_yoy, _, _ = _annual_from(e_q, e_yoy_ttm)
L641
L642                 def _pos_streak(s: pd.Series):
L643                     s = s.dropna()
L644                     if s.empty:
L645                         return np.nan
L646                     b = (s > 0).astype(int).to_numpy()[::-1]
L647                     k = 0
L648                     for v in b:
L649                         if v == 1:
L650                             k += 1
L651                         else:
L652                             break
L653                     return float(k)
L654
L655                 rev_ann_streak = _pos_streak(r_yoy_ttm)
L656
L657                 df.loc[t, "REV_Q_YOY"] = rev_q_yoy
L658                 df.loc[t, "EPS_Q_YOY"] = eps_q_yoy
L659                 df.loc[t, "REV_YOY"] = rev_yoy
L660                 df.loc[t, "EPS_YOY"] = eps_yoy
L661                 df.loc[t, "REV_YOY_ACC"] = rev_acc
L662                 df.loc[t, "REV_YOY_VAR"] = rev_var
L663                 df.loc[t, "REV_ANN_STREAK"] = rev_ann_streak
L664
L665             except Exception as e:
L666                 logger.warning("growth-derivatives failed: %s: %s", t, e)
L667
L668         def _pct_change(new, old):
L669             try:
L670                 if np.isfinite(new) and np.isfinite(old) and float(old) != 0:
L671                     return float((new - old) / abs(old))
L672             except Exception:
L673                 pass
L674             return np.nan
L675
L676         def _pct_series(a: pd.Series, b: pd.Series) -> list[float]:
L677             a_vals = pd.to_numeric(a, errors="coerce") if a is not None else pd.Series(np.nan, index=df.index)
L678             b_vals = pd.to_numeric(b, errors="coerce") if b is not None else pd.Series(np.nan, index=df.index)
L679             return [_pct_change(x, y) for x, y in zip(a_vals.reindex(df.index), b_vals.reindex(df.index))]
L680
L681         def _mean_valid(vals: list[float]) -> float:
L682             arr = [float(v) for v in vals if np.isfinite(v)]
L683             return float(np.mean(arr)) if arr else np.nan
L684
L685         grw_q_eps_last = _pct_series(df['EPS_Q'], df.get('EPS_Q_PREV', pd.Series(np.nan, index=df.index)))
L686         grw_q_rev_last = _pct_series(df['REV_Q'], df.get('REV_Q_PREV', pd.Series(np.nan, index=df.index)))
L687         grw_q_eps_ttm = _pct_series(df['EPS'], df.get('EPS_TTM_PREV', pd.Series(np.nan, index=df.index)))
L688         grw_q_rev_ttm = _pct_series(df['REV_TTM'], df.get('REV_TTM_PREV', pd.Series(np.nan, index=df.index)))
L689
L690         grw_a_eps_yoy = _pct_series(df.get('EPS_A_LATEST', pd.Series(np.nan, index=df.index)), df.get('EPS_A_PREV', pd.Series(np.nan, index=df.index)))
L691         grw_a_rev_yoy = _pct_series(df.get('REV_A_LATEST', pd.Series(np.nan, index=df.index)), df.get('REV_A_PREV', pd.Series(np.nan, index=df.index)))
L692         grw_a_eps_cagr = pd.to_numeric(df.get('EPS_A_CAGR3', pd.Series(np.nan, index=df.index)), errors="coerce").reindex(df.index).tolist()
L693         grw_a_rev_cagr = pd.to_numeric(df.get('REV_A_CAGR3', pd.Series(np.nan, index=df.index)), errors="coerce").reindex(df.index).tolist()
L694
L695         grw_q_combined = [
L696             _mean_valid([a, b, c, d])
L697             for a, b, c, d in zip(grw_q_eps_last, grw_q_rev_last, grw_q_eps_ttm, grw_q_rev_ttm)
L698         ]
L699         grw_a_combined = [
L700             _mean_valid([a, b, c, d])
L701             for a, b, c, d in zip(grw_a_eps_yoy, grw_a_rev_yoy, grw_a_eps_cagr, grw_a_rev_cagr)
L702         ]
L703
L704         df['GRW_Q_RAW'] = pd.Series(grw_q_combined, index=df.index, dtype=float)
L705         df['GRW_A_RAW'] = pd.Series(grw_a_combined, index=df.index, dtype=float)
L706
L707         def _trend_template_pass(row, rs_alpha_thresh=0.10):
L708             c1 = (row.get('P_OVER_150', np.nan) > 0) and (row.get('P_OVER_200', np.nan) > 0)
L709             c2 = (row.get('MA150_OVER_200', np.nan) > 0)
L710             c3 = (row.get('MA200_SLOPE_1M', np.nan) > 0)
L711             c4 = (row.get('MA50_OVER_150', np.nan) > 0) and (row.get('MA50_OVER_200', np.nan) > 0)
L712             c5 = (row.get('TR_str', np.nan) > 0)
L713             c6 = (row.get('P_OVER_LOW52', np.nan) >= 0.30)
L714             c7 = (row.get('NEAR_52W_HIGH', np.nan) >= -0.25)
L715             c8 = (row.get('RS', np.nan) >= 0.10)
L716             return bool(c1 and c2 and c3 and c4 and c5 and c6 and c7 and c8)
L717
L718         if 'trend_template' not in df.columns: df['trend_template'] = df.apply(_trend_template_pass, axis=1).fillna(False)
L719         assert 'trend_template' in df.columns
L720
L721         def _calc_eps_abs_slope(eps_series, n=12):
L722             try:
L723                 if isinstance(eps_series, pd.Series):
L724                     series = pd.to_numeric(eps_series, errors="coerce").dropna()
L725                 elif isinstance(eps_series, (list, tuple, np.ndarray)):
L726                     series = pd.Series(eps_series, dtype=float).dropna()
L727                 else:
L728                     return 0.0
L729             except Exception:
L730                 return 0.0
L731
L732             if series.empty:
L733                 return 0.0
L734
L735             tail = series.tail(n).to_numpy(dtype=float)
L736             if tail.size < 2:
L737                 return 0.0
L738
L739             x = np.arange(tail.size, dtype=float)
L740             x = x - x.mean()
L741             y = tail - tail.mean()
L742             denom = np.dot(x, x)
L743             if denom == 0:
L744                 return 0.0
L745             slope = float(np.dot(x, y) / denom)
L746             return slope
L747
L748         df['EPS_ABS_SLOPE'] = df['EPS_SERIES'].apply(_calc_eps_abs_slope).astype(float)
L749         df.drop(columns=['EPS_SERIES'], inplace=True)
L750
L751         # === Z化と合成 ===
L752         for col in ['ROE','FCF','REV','EPS']: df[f'{col}_W'] = winsorize_s(df[col], 0.02)
L753
L754         df_z = pd.DataFrame(index=df.index)
L755         for col in ['EPS','REV','ROE','FCF','RS','TR_str','BETA','DIV','DIV_STREAK']: df_z[col] = robust_z(df[col])
L756         df_z['REV'], df_z['EPS'], df_z['TR'] = robust_z(df['REV_W']), robust_z(df['EPS_W']), robust_z(df['TR'])
L757         for col in ['P_OVER_150','P_OVER_200','MA200_SLOPE_5M','NEAR_52W_HIGH','RS_SLOPE_6W','RS_SLOPE_13W','MA200_UP_STREAK_D']: df_z[col] = robust_z(df[col])
L758
L759         df_z['EPS_ABS_SLOPE'] = robust_z(df['EPS_ABS_SLOPE']).clip(-3.0, 3.0)
L760
L761         # === Growth深掘り系（欠損保持z + RAW併載） ===
L762         grw_cols = ['REV_Q_YOY','EPS_Q_YOY','REV_YOY','EPS_YOY','REV_YOY_ACC','REV_YOY_VAR','FCF_MGN','RULE40','REV_ANN_STREAK']
L763         for col in grw_cols:
L764             if col in df.columns:
L765                 raw = pd.to_numeric(df[col], errors="coerce")
L766                 df_z[col] = robust_z_keepnan(raw)
L767         for k in ("TREND_SLOPE_EPS", "TREND_SLOPE_REV"):
L768             if k in df.columns and k not in df_z.columns:
L769                 raw = pd.to_numeric(df[k], errors="coerce")
L770                 df_z[k] = robust_z_keepnan(raw)
L771         for col in ['DOWNSIDE_DEV','MDD_1Y','RESID_VOL','DOWN_OUTPERF','EXT_200','DIV_VAR5','DIV_FCF_COVER','DEBT2EQ','CURR_RATIO','EPS_VAR_8Q','MARKET_CAP','ADV60_USD']: df_z[col] = robust_z(df[col])
L772
L773         df_z['SIZE'], df_z['LIQ'] = robust_z(np.log1p(df['MARKET_CAP'])), robust_z(np.log1p(df['ADV60_USD']))
L774         df_z['QUALITY_F'] = robust_z(0.6*df['FCF_W'] + 0.4*df['ROE_W']).clip(-3.0,3.0)
L775         df_z['YIELD_F']   = 0.3*df_z['DIV'] + 0.7*df_z['DIV_STREAK']
L776
L777         # EPSが赤字でもFCFが黒字なら実質黒字とみなす
L778         eps_pos_mask = (df['EPS'] > 0) | (df['FCF_MGN'] > 0)
L779         df_z['EPS_POS'] = df_z['EPS'].where(eps_pos_mask, 0.0)
L780
L781         # ===== トレンドスロープ算出 =====
L782         def zpos(x):
L783             arr = robust_z(x)
L784             idx = getattr(x, 'index', df_z.index)
L785             return pd.Series(arr, index=idx).fillna(0.0)
L786
L787         def relu(x):
L788             ser = x if isinstance(x, pd.Series) else pd.Series(x, index=df_z.index)
L789             return ser.clip(lower=0).fillna(0.0)
L790
L791         # 売上トレンドスロープ（四半期）
L792         slope_rev = 0.70*zpos(df_z['REV_Q_YOY']) + 0.30*zpos(df_z['REV_YOY_ACC'])
L793         noise_rev = relu(robust_z(df_z['REV_YOY_VAR']) - 0.8)
L794         slope_rev_combo = slope_rev - 0.25*noise_rev
L795         df_z['TREND_SLOPE_REV'] = slope_rev_combo.clip(-3.0, 3.0)
L796
L797         # EPSトレンドスロープ（四半期）
L798         slope_eps = (
L799             0.40*zpos(df_z['EPS_Q_YOY']) +
L800             0.20*zpos(df_z['EPS_POS']) +
L801             0.40*zpos(df_z['EPS_ABS_SLOPE'])
L802         )
L803         df_z['TREND_SLOPE_EPS'] = slope_eps.clip(-3.0, 3.0)
L804
L805         # 年次トレンド（サブ）
L806         slope_rev_yr = zpos(df_z['REV_YOY'])
L807         slope_eps_yr = zpos(df_z.get('EPS_YOY', pd.Series(0.0, index=df.index)))
L808         streak_base = df['REV_ANN_STREAK'].clip(lower=0).fillna(0)
L809         streak_yr = streak_base / (streak_base.abs() + 1.0)
L810         slope_rev_yr_combo = 0.7*slope_rev_yr + 0.3*streak_yr
L811         df_z['TREND_SLOPE_REV_YR'] = slope_rev_yr_combo.clip(-3.0, 3.0)
L812         df_z['TREND_SLOPE_EPS_YR'] = slope_eps_yr.clip(-3.0, 3.0)
L813
L814         grw_q_z = robust_z_keepnan(df['GRW_Q_RAW']).clip(-3.0, 3.0)
L815         grw_a_z = robust_z_keepnan(df['GRW_A_RAW']).clip(-3.0, 3.0)
L816         df_z['GRW_Q'] = grw_q_z
L817         df_z['GRW_A'] = grw_a_z
L818
L819         try:
L820             mix = float(os.environ.get("GRW_Q_ANNUAL_MIX", "0.7"))
L821         except Exception:
L822             mix = 0.7
L823         if not np.isfinite(mix):
L824             mix = 0.7
L825         mix = float(np.clip(mix, 0.0, 1.0))
L826
L827         weights_q: list[float] = []
L828         weights_a: list[float] = []
L829         grw_mix: list[float] = []
L830         for idx in df.index:
L831             q_val = grw_q_z.get(idx, np.nan)
L832             a_val = grw_a_z.get(idx, np.nan)
L833             q_ok = np.isfinite(q_val)
L834             a_ok = np.isfinite(a_val)
L835             if q_ok and a_ok:
L836                 wq, wa = mix, 1.0 - mix
L837             elif q_ok:
L838                 wq, wa = 1.0, 0.0
L839             elif a_ok:
L840                 wq, wa = 0.0, 1.0
L841             else:
L842                 wq = wa = np.nan
L843                 grw_mix.append(np.nan)
L844                 weights_q.append(wq)
L845                 weights_a.append(wa)
L846                 continue
L847             weights_q.append(wq)
L848             weights_a.append(wa)
L849             grw_mix.append(q_val * wq + a_val * wa)
L850
L851         wq_series = pd.Series(weights_q, index=df.index, dtype=float)
L852         wa_series = pd.Series(weights_a, index=df.index, dtype=float)
L853         grw_series = pd.Series(grw_mix, index=df.index, dtype=float).clip(-3.0, 3.0)
L854
L855         df_z['GROWTH_F'] = grw_series
L856         df_z['GRW_FLEX_WEIGHT'] = 1.0  # 現状は固定（SECの可用性に依らず）
L857
L858         if 'GRW_Q_RAW' in df.columns:
L859             df_z['GRW_Q_DBG'] = pd.Series(df['GRW_Q_RAW'], index=df.index, dtype=float)
L860         if 'GRW_A_RAW' in df.columns:
L861             df_z['GRW_A_DBG'] = pd.Series(df['GRW_A_RAW'], index=df.index, dtype=float)
L862         df_z['GRW_WQ_DBG'] = wq_series
L863         df_z['GRW_WA_DBG'] = wa_series
L864
L865         # --- breakout features (常時寄与) ---
L866         # NEW_HIGH_20D: (終値 / 直近20日終値の最高値) - 1 → 0未満は0
L867         # VOL_RATIO_20D: 出来高(直近5日平均) / 出来高(直近20日平均)
L868         try:
L869             _px = px.copy()
L870             _vol = vol.copy() if vol is not None else None
L871             if _vol is None:
L872                 raise ValueError('volume data missing')
L873             _hi20 = _px.rolling(20, min_periods=10).max()
L874             _br = (_px / _hi20) - 1.0
L875             _new_high_20d = _br.iloc[-1].clip(lower=0)
L876             _vol5 = _vol.rolling(5, min_periods=3).mean()
L877             _vol20 = _vol.rolling(20, min_periods=10).mean()
L878             _vol_ratio_20d = (_vol5 / _vol20).iloc[-1]
L879             # Z化（NaNは保持系）→ df_z に整列
L880             df_z['NEW_HIGH_20D'] = robust_z_keepnan(pd.to_numeric(_new_high_20d, errors='coerce').reindex(df_z.index))
L881             df_z['VOL_RATIO_20D'] = robust_z_keepnan(pd.to_numeric(_vol_ratio_20d, errors='coerce').reindex(df_z.index))
L882         except Exception:
L883             # フォールバック（計算不能時は0寄与とする）
L884             df_z['NEW_HIGH_20D'] = 0.0
L885             df_z['VOL_RATIO_20D'] = 0.0
L886
L887         df_z['MOM_F'] = robust_z(
L888               0.30*df_z['RS']
L889             + 0.10*df_z['TR_str']
L890             + 0.15*df_z['RS_SLOPE_6W']
L891             + 0.15*df_z['RS_SLOPE_13W']
L892             + 0.10*df_z['MA200_SLOPE_5M']
L893             + 0.05*df_z['MA200_UP_STREAK_D']
L894             + 0.10*df_z['NEW_HIGH_20D']
L895             + 0.05*df_z['VOL_RATIO_20D']
L896         ).clip(-3.0, 3.0)
L897         df_z['VOL'] = robust_z(df['BETA'])
L898         df_z['QAL'], df_z['YLD'], df_z['MOM'] = df_z['QUALITY_F'], df_z['YIELD_F'], df_z['MOM_F']
L899         df_z.drop(columns=['QUALITY_F','YIELD_F','MOM_F'], inplace=True, errors='ignore')
L900
L901         # df_z 全明細をページングしてログ出力（最小版）
L902         if getattr(cfg, "debug_mode", False):
L903             beta_debug_cols = []
L904             if isinstance(df, pd.DataFrame):
L905                 try:
L906                     beta_raw = df.get('BETA') if 'BETA' in df.columns else None
L907                     if beta_raw is not None:
L908                         beta_raw = pd.to_numeric(beta_raw, errors="coerce")
L909                         df_z['BETA_RAW'] = beta_raw.reindex(df_z.index)
L910                         df_z['BETA_Z'] = _plain_zscore_series(df_z['BETA_RAW'])
L911                         beta_debug_cols.extend(['BETA_RAW', 'BETA_Z'])
L912                 except Exception:
L913                     beta_debug_cols.clear()
L914             pd.set_option("display.max_columns", None)
L915             pd.set_option("display.max_colwidth", None)
L916             pd.set_option("display.width", None)
L917             page = int(getattr(cfg, "debug_dfz_page", 50))  # デフォルト50行単位
L918             n = len(df_z)
L919             logger.info("=== df_z FULL DUMP start === rows=%d cols=%d page=%d", n, df_z.shape[1], page)
L920             try:
L921                 for i in range(0, n, page):
L922                     j = min(i + page, n)
L923                     try:
L924                         chunk_str = df_z.iloc[i:j].to_string()
L925                     except Exception:
L926                         chunk_str = df_z.iloc[i:j].astype(str).to_string()
L927                     logger.info("--- df_z rows %d..%d ---\n%s", i, j-1, chunk_str)
L928             finally:
L929                 if beta_debug_cols:
L930                     df_z.drop(columns=[c for c in beta_debug_cols if c in df_z.columns], inplace=True)
L931             logger.info("=== df_z FULL DUMP end ===")
L932
L933         # === begin: BIO LOSS PENALTY =====================================
L934         try:
L935             penalty_z = float(os.getenv("BIO_LOSS_PENALTY_Z", "0.8"))
L936         except Exception:
L937             penalty_z = 0.8
L938
L939         def _is_bio_like(t: str) -> bool:
L940             inf = info.get(t, {}) if isinstance(info, dict) else {}
L941             sec = str(inf.get("sector", "")).lower()
L942             ind = str(inf.get("industry", "")).lower()
L943             if "health" not in sec:
L944                 return False
L945             keys = ("biotech", "biopharma", "pharma")
L946             return any(k in ind for k in keys)
L947
L948         tickers_s = pd.Index(df_z.index)
L949         is_bio = pd.Series({t: _is_bio_like(t) for t in tickers_s})
L950         is_loss = pd.Series({t: (pd.notna(df.loc[t,"EPS"]) and df.loc[t,"EPS"] <= 0) for t in tickers_s})
L951         mask_bio_loss = (is_bio & is_loss).reindex(df_z.index).fillna(False)
L952
L953         if bool(mask_bio_loss.any()) and penalty_z > 0:
L954             df_z.loc[mask_bio_loss, "GROWTH_F"] = df_z.loc[mask_bio_loss, "GROWTH_F"] - penalty_z
L955             df_z["GROWTH_F"] = df_z["GROWTH_F"].clip(-3.0, 3.0)
L956         # === end: BIO LOSS PENALTY =======================================
L957
L958         _debug_only_cols = [c for c in df_z.columns if c.endswith("_RAW")]
L959         _no_score_cols = ["DIV_TTM_PS", "DIV_YOY", "LOW52PCT25_EXCESS", "MA50_OVER_200"]
L960         _drop_cols = [c for c in (_debug_only_cols + _no_score_cols) if c in df_z.columns]
L961         if _drop_cols:
L962             df_z = df_z.drop(columns=_drop_cols, errors="ignore")
L963
L964         assert not any(c.endswith("_RAW") for c in df_z.columns)
L965         for c in ["DIV_TTM_PS","DIV_YOY","LOW52PCT25_EXCESS","MA50_OVER_200"]:
L966             assert c not in df_z.columns
L967
L968         df_z['TRD'] = 0.0  # TRDはスコア寄与から外し、テンプレ判定はフィルタで行う（列は表示互換のため残す）
L969         if 'BETA' not in df_z.columns: df_z['BETA'] = robust_z(df['BETA'])
L970
L971         df_z['D_VOL_RAW'] = robust_z(0.40*df_z['DOWNSIDE_DEV'] + 0.22*df_z['RESID_VOL'] + 0.18*df_z['MDD_1Y'] - 0.10*df_z['DOWN_OUTPERF'] - 0.05*df_z['EXT_200'] - 0.08*df_z['SIZE'] - 0.10*df_z['LIQ'] + 0.10*df_z['BETA'])
L972         df_z['D_QAL']     = robust_z(0.35*df_z['QAL'] + 0.20*df_z['FCF'] + 0.15*df_z['CURR_RATIO'] - 0.15*df_z['DEBT2EQ'] - 0.15*df_z['EPS_VAR_8Q'])
L973         df_z['D_YLD']     = robust_z(0.45*df_z['DIV'] + 0.25*df_z['DIV_STREAK'] + 0.20*df_z['DIV_FCF_COVER'] - 0.10*df_z['DIV_VAR5'])
L974         df_z['D_TRD']     = robust_z(0.40*df_z.get('MA200_SLOPE_5M',0) - 0.30*df_z.get('EXT_200',0) + 0.15*df_z.get('NEAR_52W_HIGH',0) + 0.15*df_z['TR'])
L975
L976         # --- 重みは cfg を優先（外部があればそれを使用） ---
L977         # ① 全銘柄で G/D スコアを算出（unmasked）
L978         g_weights = pd.Series(cfg.weights.g, dtype=float)
L979         need_g = ["GROWTH_F", "MOM"]
L980         dbg_cols = ["GROWTH_F", "MOM", "VOL"]
L981         if all(c in df_z.columns for c in need_g):
L982             mask_g = df_z[need_g].notna().all(axis=1)
L983         else:
L984             mask_g = pd.Series(False, index=df_z.index, dtype=bool)
L985         for c in dbg_cols:
L986             if c in df_z.columns:
L987                 df_z[f"DBGRW.{c}"] = df_z[c]
L988         df_fill_g = df_z.reindex(columns=g_weights.index, fill_value=np.nan).copy()
L989         for c in df_fill_g.columns:
L990             if c not in need_g:
L991                 df_fill_g[c] = df_fill_g[c].fillna(0)
L992         g_score_all = _as_numeric_series(
L993             df_fill_g.mul(g_weights.reindex(df_fill_g.columns)).sum(axis=1, skipna=False)
L994         )
L995         g_score_all = g_score_all.where(mask_g)
L996
L997         d_comp = pd.concat({
L998             'QAL': df_z['D_QAL'],
L999             'YLD': df_z['D_YLD'],
L1000             'VOL': df_z['D_VOL_RAW'],
L1001             'TRD': df_z['D_TRD']
L1002         }, axis=1)
L1003         dw = pd.Series(cfg.weights.d, dtype=float).reindex(['QAL','YLD','VOL','TRD']).fillna(0.0)
L1004         globals()['D_WEIGHTS_EFF'] = dw.copy()
L1005         need_d_candidates = ["VOL", "QAL"]
L1006         mask_d = pd.Series(True, index=d_comp.index, dtype=bool)
L1007         for c in need_d_candidates:
L1008             if c in d_comp.columns:
L1009                 mask_d &= d_comp[c].notna()
L1010             else:
L1011                 mask_d &= False
L1012         df_fill_d = d_comp.copy()
L1013         for c in df_fill_d.columns:
L1014             if c not in need_d_candidates:
L1015                 df_fill_d[c] = df_fill_d[c].fillna(0)
L1016         d_score_all = _as_numeric_series(
L1017             df_fill_d.mul(dw, axis=1).sum(axis=1, skipna=False)
L1018         )
L1019         d_score_all = d_score_all.where(mask_d)
L1020
L1021         # ② テンプレ判定（既存ロジックそのまま）
L1022         mask = df['trend_template']
L1023         if not bool(mask.any()):
L1024             mask = ((df.get('P_OVER_LOW52', np.nan) >= 0.25) &
L1025                 (df.get('NEAR_52W_HIGH', np.nan) >= -0.30) &
L1026                 (df.get('RS', np.nan) >= 0.08) &
L1027                 (df.get('MA200_SLOPE_1M', np.nan) > 0) &
L1028                 (df.get('P_OVER_150', np.nan) > 0) & (df.get('P_OVER_200', np.nan) > 0) &
L1029                 (df.get('MA150_OVER_200', np.nan) > 0) &
L1030                 (df.get('MA50_OVER_150', np.nan) > 0) & (df.get('MA50_OVER_200', np.nan) > 0) &
L1031                 (df.get('TR_str', np.nan) > 0)).fillna(False)
L1032             df['trend_template'] = mask
L1033
L1034         # ③ 採用用は mask、表示/分析用は列で全銘柄保存
L1035         g_score = _as_numeric_series(g_score_all.loc[mask])
L1036         Scorer.g_score = g_score
L1037         df_z['GSC'] = g_score_all
L1038         df_z['DSC'] = d_score_all
L1039
L1040         try:
L1041             current = (pd.read_csv("current_tickers.csv")
L1042                   .iloc[:, 0]
L1043                   .str.upper()
L1044                   .tolist())
L1045         except FileNotFoundError:
L1046             warnings.warn("current_tickers.csv not found — bonus skipped")
L1047             current = []
L1048
L1049         mask_bonus = g_score.index.isin(current)
L1050         if mask_bonus.any():
L1051             # 1) factor.BONUS_COEFF から k を決め、無ければ 0.4
L1052             k = float(getattr(sys.modules.get("factor"), "BONUS_COEFF", 0.4))
L1053             # 2) g 側の σ を取り、NaN なら 0 に丸める
L1054             sigma_g = g_score.std()
L1055             if pd.isna(sigma_g):
L1056                 sigma_g = 0.0
L1057             bonus_g = round(k * sigma_g, 3)
L1058             g_score.loc[mask_bonus] += bonus_g
L1059             Scorer.g_score = g_score
L1060             # 3) D 側も同様に σ の NaN をケア
L1061             sigma_d = d_score_all.std()
L1062             if pd.isna(sigma_d):
L1063                 sigma_d = 0.0
L1064             bonus_d = round(k * sigma_d, 3)
L1065             d_score_all.loc[d_score_all.index.isin(current)] += bonus_d
L1066
L1067         try:
L1068             df = _apply_growth_entry_flags(df, ib, self, win_breakout=5, win_pullback=5)
L1069         except Exception:
L1070             pass
L1071
L1072         df_full = df.copy()
L1073         df_full_z = df_z.copy()
L1074
L1075         from factor import FeatureBundle  # type: ignore  # 実行時importなし（循環回避）
L1076         missing_logs_df = getattr(ib, "missing_logs", pd.DataFrame())
L1077         if not isinstance(missing_logs_df, pd.DataFrame):
L1078             try:
L1079                 missing_logs_df = pd.DataFrame(missing_logs_df)
L1080             except Exception:
L1081                 missing_logs_df = pd.DataFrame()
L1082
L1083         return FeatureBundle(df=df,
L1084             df_z=df_z,
L1085             g_score=g_score,
L1086             d_score_all=d_score_all,
L1087             missing_logs=missing_logs_df,
L1088             df_full=df_full,
L1089             df_full_z=df_full_z,
L1090             scaler=None)
L1091
L1092 def _apply_growth_entry_flags(feature_df, bundle, self_obj, win_breakout=5, win_pullback=5):
L1093     """以前はブレイクアウト/押し目反発フラグを付与していたが、現在は無効化。"""
L1094     return feature_df
L1095
L1096
```

## <.github/workflows/weekly-report.yml>
```text
L1 name: Weekly Stock Report
L2
L3 on:
L4   push:
L5     branches: [ main ]
L6     paths-ignore:
L7       - 'CodeForChat/**'
L8   schedule:
L9     - cron: '0 0 * * 6'  # UTC 00:00 → JST 09:00（土）
L10   workflow_dispatch:
L11
L12 jobs:
L13   build-and-report:
L14     runs-on: ubuntu-latest
L15     permissions:
L16       contents: write
L17
L18     steps:
L19       - name: Debug start
L20         run: echo '🚀 DEBUGstarted'
L21               
L22       - name: Checkout repository
L23         uses: actions/checkout@v4
L24         with:
L25           fetch-depth: 0
L26           persist-credentials: true
L27
L28       - name: Setup Python
L29         uses: actions/setup-python@v5
L30         with:
L31           python-version: '3.x'
L32           cache: 'pip'
L33           cache-dependency-path: requirements.txt
L34
L35       - name: Install dependencies
L36         run: pip install -r requirements.txt
L37
L38       - name: Prepare results directory
L39         run: mkdir -p results
L40
L41       - name: Run factor & scoring
L42         env:
L43           SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
L44           FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
L45           FIN_THREADS: "8"
L46           SEC_CONTACT_EMAIL: ${{ secrets.SEC_CONTACT_EMAIL }}
L47         run: python factor.py
L48
L49       - name: Commit & push current_tickers.csv (rebase-safe)
L50         shell: bash
L51         run: |
L52           set -euo pipefail
L53           BR="${GITHUB_REF_NAME:-main}"
L54
L55           git config user.name  "github-actions[bot]"
L56           git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
L57           git config pull.rebase true
L58           git config --global --add safe.directory "$PWD"
L59
L60           if [[ -n "$(git status --porcelain current_tickers.csv)" ]]; then
L61             git add current_tickers.csv
L62             git commit -m "chore: update current_tickers.csv bucket ($(date -u +'%Y-%m-%dT%H:%M:%SZ'))" || true
L63
L64             git fetch origin "$BR" --prune
L65             git pull --rebase origin "$BR" || true
L66
L67             if ! git push origin "HEAD:$BR"; then
L68               git push --force-with-lease origin "HEAD:$BR"
L69             fi
L70           else
L71             echo "No changes in current_tickers.csv"
L72           fi
```

## <documents/README.md>
```text
L1 # 運用ルール（改訂版）
L2
L3 ## 基本構成
L4 - 20銘柄を均等配分（現金を除き1銘柄あたり5%）  
L5 - moomoo証券で運用  
L6 - **Growth枠 12銘柄 / Defense枠 8銘柄**
L7
L8 ---
L9
L10 ## Barbell Growth-Defense方針
L11 - **Growth枠（12銘柄）**：トレンドを追う**スイングトレード**。高成長・高ボラ銘柄でリターン源泉を狙う。  
L12 - **Defense枠（8銘柄）**：安定重視の**ポジショントレード（やや長期）**。低ボラ・高品質でMDDを抑制。  
L13 - 「猛烈に伸びる攻め × 着実に稼ぐ盾」の組合せで乖離を生み、**半戻しリバランス**でプレミアムを獲得。
L14
L15 ---
L16
L17 ## モード判定（コンボ：先導株TS × ブレッドス）
L18
L19 **考え方：** *悪化はゆるく（OR）、回復は厳しく（AND）*
L20
L21 ### ① 先導株TSシグナル（Growthのみ）
L22 - 対象（Growthの定義）：当日保有銘柄のうち **β ≥ -0.6** を Growth とみなす（Defenseは無視）
L23 - 判定：直近60日高値からモード別基本TS幅（NORMAL:-15% / CAUTION:-13% / EMERG:-10%）以上の下落を「TS抵触」とみなす
L24 - 集計：直近5営業日のユニーク抵触銘柄数
L25   - 8銘柄以上 → ①=EMERG
L26   - 6銘柄以上 → ①=CAUTION
L27   - それ未満 → ①=NORMAL
L28 - 補足：同一日に複数回実行した場合は、**同日上書き**で管理
L29
L30 ### ② ブレッドス（trend_template 合格本数）
L31 - current+candidate 全体で trend_template 条件を満たした銘柄数（基準 N_G=12）
L32 - 閾値：過去600営業日の分布から自動採用（分位点と運用“床”のmax）
L33   - 緊急入り: max(q05, 12本)
L34   - 緊急解除: max(q20, 18本)
L35   - 通常復帰: max(q60, 36本)
L36 - ヒステリシス：前回モードに依存（EMERG→解除は23本以上、CAUTION→通常は45本以上）
L37
L38 ### コンボルール
L39 - **悪化（ダウングレード）**：
L40   final_mode = max(mode①, mode②)
L41   - 例：①=CAUTION, ②=NORMAL → final=CAUTION
L42   - 例：①=EMERG, ②=CAUTION → final=EMERG
L43
L44 - **回復（アップグレード）**：
L45   final_mode を1段階下げるには、mode① と mode② がともに下位モードに揃った場合のみ
L46   - 例：EMERG→CAUTION は ①=CAUTION **かつ** ②=CAUTION
L47   - 例：CAUTION→NORMAL は ①=NORMAL **かつ** ②=NORMAL
L48
L49 > 直感フレーズ：**「悪化はどちらか赤で赤、回復は両方青で青」**
L50
L51 ---
L52
L53 ## モード別設定（現金・ドリフト・保有数）
L54
L55 | モード       | 現金比率 | ドリフト閾値      | 基本TS幅 | Growth枠数 | Defense枠数 | 補足 |
L56 |--------------|----------|-------------------|----------|------------|-------------|------|
L57 | **NORMAL**   | 10%      | 12%               | -15%     | 12         | 8           | フル20銘柄（現金化枠なし） |
L58 | **CAUTION**  | 20%      | 14%               | -13%     | 10         | 8           | Gを2枠外し=現金化10% |
L59 | **EMERG**    | 30%      | ドリフト売買停止 | -10%     | 8          | 8           | Gを4枠外し=現金化20% |
L60
L61 - 含み益到達時のTSタイト化：+30% → -3pt、+60% → -6pt、+100% → -8pt
L62 - 含み益 +100% 達成時は50%を利確し、残りはフリーポジションとして -15%TS で保有継続
L63 - TS発動後のクールダウンは廃止（翌日以降すぐに再IN可）
L64
L65 ---
L66
L67 ## 新規買付
L68 - **新規INは等分比率（=5%）の半分まで**を上限。  
L69 - 追加補充や半戻し買付も同じ上限に従う。
L70
L71 ---
L72
L73 ## 半戻し（リバランス）
L74 1. **現金比率 ≤ 閾値**：過重量銘柄を売却し、不足銘柄を補充。  
L75 2. **現金比率 > 閾値**：**売却は行わず**、現金でドリフト不足銘柄を買付（現金比率を閾値以下へ戻すことを優先）。  
L76 3. **共通**：リバランス後は全銘柄のTSを再設定。EMERGでは「ドリフト売買停止」、20銘柄×5%全戻しのみ許容。
L77
L78 ---
L79
L80 ## モード移行の実務手順
L81 - モードが変わったら、**MMF≒現金**として扱い、Growth枠数だけ調整：  
L82   1. **Gを削る**（CAUTION/EMERG）：⭐️低スコアのGから順に外し、`current_tickers.csv` から行削除（=現金化）。  
L83   2. **現金として保持**。  
L84   3. **NORMAL復帰時の補充**：`current_tickers.csv` に銘柄を追加（スコア上位から）。以降は日次ドリフト/TSルールに従う。  
L85 > driftは `target_ratio = 1/銘柄数` を自動適用。行数に応じて均等比率を再計算。
L86
L87 ---
L88
L89 ## 入替銘柄選定
L90 - **ファクター分散最適化手法を用いて日次でスコア集計**し、**スコア上位からIN/OUT**を決定。  
L91 - 参考：Oxfordキャピタル、Alpha Investor、Motley Fool、moomooスクリーニング等。  
L92 - 年間NISA枠はGrowth群から低ボラ銘柄を選定し利用（長期保持に固執しない）。
L93
L94 ---
L95
L96 ## 実行タイミング
L97 - 判定：米国市場終値直後  
L98 - 執行：翌営業日の米国寄付き成行
```

## <documents/factor_design.md>
```text
L1 # factor.py 詳細設計書
L2
L3 ## 概要
L4 - 既存ポートフォリオの銘柄と検討中の銘柄群を同時に扱う銘柄選定パイプライン。
L5 - 価格・財務データを取り込みスコアリングとDRRS選定を行うことで、以下のアウトプットを得る。
L6   - 採用銘柄と惜しくも漏れた銘柄のスコア一覧
L7   - IN/OUTのティッカーリストとOUT側の低スコア銘柄
L8   - 新旧ポートフォリオの比較表
L9   - 検討中銘柄の低スコアランキング（整理用）
L10
L11 ## 全体フロー
L12 1. **Input** – `current_tickers.csv`と`candidate_tickers.csv`を読み込み、yfinanceやFinnhubのAPIから価格・財務データを収集して`InputBundle`を整備。
L13 2. **Score Calculation** – Scorerが特徴量を計算し因子スコアを合成して`FeatureBundle`を生成。
L14 3. **Correlation Reduction & Selection** – SelectorがDRRSロジックで相関を抑えつつG/D銘柄を選定し`SelectionBundle`を得る。
L15 4. **Output** – 採用結果と周辺情報を表・Slack通知として出力。
L16
L17 ```mermaid
L18 flowchart LR
L19   A[Input\nAPI & 前処理] --> B[Score Calculation\n特徴量・因子合成]
L20   B --> C[Correlation Reduction\nDRRS選定]
L21   C --> D[Output\nSlack通知]
L22 ```
L23
L24 ## 定数・設定
L25 | 変数 | 内容 | 主な用途 |
L26 | --- | --- | --- |
L27 | `exist` / `cand` | 現行ポートフォリオと検討中銘柄のティッカーリスト | スコア対象ユニバースの構成、候補整理 |
L28 | `bench` | ベンチマークティッカー | 相対強さ・β算出、ポート比較 |
L29 | `CAND_PRICE_MAX` | 候補銘柄の許容価格上限 | 高額銘柄の事前除外 |
L30 | `N_G` / `N_D` | G/D採用枠の件数（**既定: 12 / 8**） | 最終的に選ぶ銘柄数の制約 |
L31 | `g_weights` / `D_weights` | 各因子の重みdict | G/Dスコア合成 |
L32 | `D_BETA_MAX` | Dバケットの許容β上限 | 高β銘柄の除外フィルタ |
L33 | `FILTER_SPEC` | G/Dごとの前処理フィルタ | トレンドマスクやβ上限設定 |
L34 | `corrM` | DRRS初期プールの最大件数 | 相関行列サイズ制御 |
L35 | `DRRS_G` / `DRRS_D` | DRRSパラメータdict | バケット別の相関低減設定 |
L36 | `DRRS_SHRINK` | 残差相関の対角シュリンク率 | `residual_corr`の安定化 |
L37 | `CROSS_MU_GD` | G-D間クロス相関ペナルティμ | 2バケット同時最適化で相関抑制 |
L38 | `RESULTS_DIR` | 選定結果保存ディレクトリ | `_save_sel`/`_load_prev`の入出力 |
L39
L40 選定結果は`results/`配下にJSONとして保存し、次回実行時に`_load_prev`で読み込んで選定条件に反映。
L41
L42 ## DTO/Config
L43 各ステップ間で受け渡すデータ構造と設定値。変数の意味合いと利用箇所を以下に示す。
L44
L45 ### InputBundle（Input → Scorer）
L46 | 変数 | 内容 | 主な用途 |
L47 | --- | --- | --- |
L48 | `cand` | 候補銘柄ティッカーのリスト | OUTテーブルや低スコアランキング対象の母集団 |
L49 | `tickers` | 現行+候補を合わせたティッカー一覧 | 価格・出来高ダウンロード、リターン計算 |
L50 | `bench` | ベンチマークティッカー | 相対強さ・β算出、ポート比較 |
L51 | `data` | yfinanceのダウンロード結果（階層列） | `px`/`spx`/リターン等の基礎データ |
L52 | `px` | `data['Close']`だけを抜き出した価格系列 | 指標計算・リターン生成 |
L53 | `spx` | `data['Close'][bench]` のSeries | `rs`や`calc_beta`の基準指数 |
L54 | `tickers_bulk` | `yf.Tickers`オブジェクト | `info`等の一括取得 |
L55 | `info` | ティッカー別のyfinance情報dict | セクター判定やEPS補完 |
L56 | `eps_df` | EPS TTM/直近EPS等をまとめた表 | 成長指標の算出 |
L57 | `fcf_df` | CFO・CapEx・FCF TTMと情報源フラグ | FCF/EVや配当カバレッジ |
L58 | `returns` | `px.pct_change()`のリターン表 | 相関行列・DRRS計算 |
L59
L60 ### FeatureBundle（Scorer → Selector）
L61 | 変数 | 内容 | 主な用途 |
L62 | --- | --- | --- |
L63 | `df` | 計算済み指標の生値テーブル | デバッグ・出力表示 |
L64 | `df_z` | ウィンザー後Zスコア化した指標表 | 因子スコア合成、選定基準 |
L65 | `g_score` | Gバケット総合スコア | G選定、IN/OUT比較 |
L66 | `d_score_all` | Dバケット総合スコア（全銘柄） | D選定、低スコアランキング |
L67 | `missing_logs` | 欠損指標と補完状況のログ | データ品質チェック |
L68
L69 ### SelectionBundle（Selector → Output）
L70 | 変数 | 内容 | 主な用途 |
L71 | --- | --- | --- |
L72 | `resG` | G選定結果の詳細dict（`tickers`、目的値等） | 結果保存・平均相関などの指標表示 |
L73 | `resD` | D選定結果の詳細dict | 同上 |
L74 | `top_G` | 最終採用Gティッカー | 新ポートフォリオ構築 |
L75 | `top_D` | 最終採用Dティッカー | 同上 |
L76 | `init_G` | DRRS前のG初期候補 | 惜しくも外れた銘柄表示 |
L77 | `init_D` | DRRS前のD初期候補 | 同上 |
L78
L79 ### WeightsConfig
L80 | 変数 | 内容 | 主な用途 |
L81 | --- | --- | --- |
L82 | `g` | G因子（GRW/MOM/VOL）の重みdict | `g_score`合成 |
L83 | `d` | D因子（D_QAL/D_YLD/D_VOL_RAW/D_TRD）の重みdict | `d_score_all`合成 |
L84
L85 ### DRRSParams
L86 | 変数 | 内容 | 主な用途 |
L87 | --- | --- | --- |
L88 | `corrM` | DRRS初期プールの最大件数 | 相関行列サイズ制御 |
L89 | `shrink` | 残差相関のシュリンク率 | `residual_corr`の対角強調 |
L90 | `G` | Gバケット用パラメータdict（`lookback`等） | `select_bucket_drrs`設定 |
L91 | `D` | Dバケット用パラメータdict | 同上 |
L92 | `cross_mu_gd` | G-Dクロス相関ペナルティ係数μ | `select_buckets`の目的関数 |
L93
L94 ### PipelineConfig
L95 | 変数 | 内容 | 主な用途 |
L96 | --- | --- | --- |
L97 | `weights` | `WeightsConfig`のインスタンス | スコア合成の重み参照 |
L98 | `drrs` | `DRRSParams`のインスタンス | 選定ステップの設定値 |
L99 | `price_max` | 候補銘柄の許容価格上限 | Input段階でのフィルタ |
L100
L101 ## 共通ユーティリティ
L102 - `winsorize_s` / `robust_z` : 外れ値処理とZスコア化。
L103 - `_safe_div` / `_safe_last` : 例外を潰した分割・末尾取得。
L104 - `_load_prev` / `_save_sel` : 選定結果の読み書き。
L105
L106 ## クラス設計
L107 ### Step1: Input
L108 `current_tickers.csv`の現行銘柄と`candidate_tickers.csv`の検討中銘柄を起点にデータを集約する。外部I/Oと前処理を担当し、`prepare_data`で`InputBundle`を生成。価格・財務データの取得は**yfinanceを優先し、欠損がある指標のみFinnhub APIで補完**する。
L109 主なメソッド:
L110 - `impute_eps_ttm` : 四半期EPS×4でTTMを推定し欠損時のみ差し替え。
L111 - `fetch_cfo_capex_ttm_yf` : yfinanceの四半期/年次キャッシュフローからCFO・CapEx・FCF TTMを算出。
L112 - `fetch_cfo_capex_ttm_finnhub` : yfinanceで欠けた銘柄のみFinnhub APIで補完。
L113 - `compute_fcf_with_fallback` : yfinance値を基準にFinnhub値で穴埋めし、CFO/CapEx/FCFと情報源フラグを返す。
L114 - `_build_eps_df` : `info`や`quarterly_earnings`からEPS TTMと直近EPSを計算し、`impute_eps_ttm`で補完。
L115 - `prepare_data` :
L116     0. CSVから現行銘柄と候補銘柄のティッカー一覧を読み込む。
L117     1. 候補銘柄の現在値を取得し価格上限でフィルタ。
L118     2. 既存+候補から対象ティッカーを決定し、価格・出来高を一括ダウンロード（yfinance）。
L119     3. yfinance値を基にEPS/FCFテーブルやベンチマーク系列、リターンを構築し、欠損セルはFinnhub呼び出しで穴埋め。
L120     4. 上記を`InputBundle`に格納して返す。
L121
L122 ### Step2: Score Calculation (Scorer)
L123 特徴量計算とスコア合成を担当し、`FeatureBundle`を返す。
L124
L125 #### 補助関数
L126 - `trend(s)` : 50/150/200日移動平均や52週レンジから-0.5〜0.5で構成されたトレンド指標。
L127 - `rs(s,b)` / `tr_str(s)` / `rs_line_slope(s,b,win)` : 相対強さや短期トレンド、RS回帰傾きを算出。
L128 - `ev_fallback` : `enterpriseValue`欠損時に負債・現金からEVを推定。
L129 - `dividend_status` / `div_streak` : 配当未設定状況の判定と増配年数カウント。
L130 - `fetch_finnhub_metrics` : Finnhub APIからEPS成長・ROE・βなど不足指標を取得。
L131 - `calc_beta` : ベンチマークとの共分散からβを算出。
L132 - `spx_to_alpha` : S&P500の位置情報からDRRSで用いるαを推定。
L133 - `soft_cap_effective_scores` / `pick_top_softcap` : セクターソフトキャップ付きスコア調整と上位抽出。
L134
L135 **補助関数と生成指標**
L136
L137 | 補助関数 | 生成指標 | 略称 |
L138 | --- | --- | --- |
L139 | `trend` | トレンド総合値 | `TR` |
L140 | `rs` | 相対強さ | `RS` |
L141 | `tr_str` | 価格と50日線の乖離 | `TR_str` |
L142 | `rs_line_slope` | RS線の回帰傾き | `RS_SLOPE_*` |
L143 | `calc_beta` | β | `BETA` |
L144 | `div_streak` | 連続増配年数 | `DIV_STREAK` |
L145
L146 #### `aggregate_scores` 詳細
L147 1. 各銘柄の価格系列や`info`を基に以下を算出。
L148    - **トレンド/モメンタム**: `TR`、`RS`、`TR_str`、多様な移動平均比、`RS_SLOPE_*`など。
L149    - **リスク**: `BETA`、`DOWNSIDE_DEV`、`MDD_1Y`、`RESID_VOL`、`DOWN_OUTPERF`、`EXT_200`等。
L150    - **配当**: `DIV`、`DIV_TTM_PS`、`DIV_VAR5`、`DIV_YOY`、`DIV_FCF_COVER`、`DIV_STREAK`。
L151    - **財務・成長**: `EPS`、`REV`、`ROE`、`FCF/EV`、`REV_Q_YOY`、`EPS_Q_YOY`、`REV_YOY_ACC`、`REV_YOY_VAR`、`REV_ANN_STREAK`、`RULE40`、`FCF_MGN` 等。
L152    - **安定性/サイズ**: `DEBT2EQ`、`CURR_RATIO`、`MARKET_CAP`、`ADV60_USD`、`EPS_VAR_8Q`など。
L153 2. 指標欠損はFinnhub API等で補完し、未取得項目を`missing_logs`に記録。
L154 3. `winsorize_s`→`robust_z`で標準化し`df_z`へ保存。サイズ・流動性は対数変換。
L155 4. 正規化済指標から因子スコアを合成。
L156    - 各因子の構成と重みは以下の通り。
L157      - **GRW**: 0.30×`REV` + 0.20×`EPS_Q_YOY` + 0.15×`REV_Q_YOY` + 0.15×`REV_YOY_ACC` + 0.10×`RULE40` + 0.10×`FCF_MGN` + 0.10×`REV_ANN_STREAK` − 0.05×`REV_YOY_VAR`。
L158      - **MOM**: 0.40×`RS` + 0.15×`TR_str` + 0.15×`RS_SLOPE_6W` + 0.15×`RS_SLOPE_13W` + 0.10×`MA200_SLOPE_5M` + 0.10×`MA200_UP_STREAK_D`。
L159      - **VOL**: `BETA`単体を使用。
L160      - **QAL**: 0.60×`FCF_W` + 0.40×`ROE_W`で作成。
L161      - **YLD**: 0.30×`DIV` + 0.70×`DIV_STREAK`。
L162      - **D_QAL**: 0.35×`QAL` + 0.20×`FCF` + 0.15×`CURR_RATIO` − 0.15×`DEBT2EQ` − 0.15×`EPS_VAR_8Q`。
L163      - **D_YLD**: 0.45×`DIV` + 0.25×`DIV_STREAK` + 0.20×`DIV_FCF_COVER` − 0.10×`DIV_VAR5`。
L164      - **D_VOL_RAW**: 0.40×`DOWNSIDE_DEV` + 0.22×`RESID_VOL` + 0.18×`MDD_1Y` − 0.10×`DOWN_OUTPERF` − 0.05×`EXT_200` − 0.08×`SIZE` − 0.10×`LIQ` + 0.10×`BETA`。
L165      - **D_TRD**: 0.40×`MA200_SLOPE_5M` − 0.30×`EXT_200` + 0.15×`NEAR_52W_HIGH` + 0.15×`TR`。
L166     - 主な指標の略称と意味:
L167
L168       | 略称 | 補助関数 | 概要 |
L169       | --- | --- | --- |
L170       | TR | `trend` | 50/150/200日移動平均と52週レンジを組み合わせたトレンド総合値 |
L171       | RS | `rs` | ベンチマークに対する相対強さ（12M/1Mリターン差） |
L172       | TR_str | `tr_str` | 価格と50日移動平均の乖離 |
L173       | RS_SLOPE_6W | `rs_line_slope` | 相対強さ線の6週回帰傾き |
L174       | RS_SLOPE_13W | `rs_line_slope` | 相対強さ線の13週回帰傾き |
L175       | MA200_SLOPE_5M | - | 200日移動平均の5か月騰落率 |
L176       | MA200_UP_STREAK_D | - | 200日線が連続で上向いた日数 |
L177       | BETA | `calc_beta` | ベンチマークに対するβ |
L178       | DOWNSIDE_DEV | - | 下方リターンのみの年率化標準偏差 |
L179       | RESID_VOL | - | βで調整した残差リターンの年率化標準偏差 |
L180       | MDD_1Y | - | 過去1年の最大ドローダウン |
L181       | DOWN_OUTPERF | - | 市場下落日に対する平均超過リターン |
L182       | EXT_200 | - | 200日移動平均からの絶対乖離率 |
L183       | NEAR_52W_HIGH | - | 52週高値までの下方距離（0=高値） |
L184       | FCF_W | - | ウィンザー処理後のFCF/EV |
L185       | ROE_W | - | ウィンザー処理後のROE |
L186       | FCF | - | FCF/EV |
L187       | QAL | - | FCF_WとROE_Wを組み合わせた品質スコア |
L188       | CURR_RATIO | - | 流動比率 |
L189       | DEBT2EQ | - | 負債資本倍率 |
L190       | EPS_VAR_8Q | - | EPSの8四半期標準偏差 |
L191       | DIV | - | 年率換算配当利回り |
L192       | DIV_STREAK | `div_streak` | 連続増配年数 |
L193       | DIV_FCF_COVER | - | 配当のFCFカバレッジ |
L194       | DIV_VAR5 | - | 5年配当変動率 |
L195       | DIV_TTM_PS | - | 1株当たりTTM配当 |
L196       | DIV_YOY | - | 前年比配当成長率 |
L197       | REV | - | 売上成長率TTM |
L198       | EPS_Q_YOY | - | 四半期EPSの前年同期比 |
L199       | REV_Q_YOY | - | 四半期売上の前年同期比 |
L200       | REV_YOY_ACC | - | 売上成長率の加速分 |
L201       | RULE40 | - | 売上成長率とFCFマージンの合計 |
L202       | FCF_MGN | - | FCFマージン |
L203       | REV_ANN_STREAK | - | 年次売上成長の連続年数 |
L204       | REV_YOY_VAR | - | 年次売上成長率の変動性 |
L205       | SIZE | - | 時価総額の対数値 |
L206       | LIQ | - | 60日平均出来高ドルの対数値 |
L207    - Gバケット: `GRW`、`MOM`、`VOL`を`cfg.weights.g`（0.40/0.45/-0.15）で加重し`g_score`を得る。
L208    - Dバケット: `D_QAL`、`D_YLD`、`D_VOL_RAW`、`D_TRD`を`cfg.weights.d`（0.15/0.15/-0.45/0.25）で加重し`d_score_all`を算出。
L209    - セクターcapによる`soft_cap_effective_scores`を適用し、G採用銘柄にはトレンドテンプレートフィルタを適用。
L210 5. `_apply_growth_entry_flags`でブレイクアウト/押し目発火状況を付加し、`FeatureBundle`を返す。
L211
L212 ### Step3: Correlation Reduction & Selection (Selector)
L213 DRRSアルゴリズムで相関を抑えた銘柄選定を行い、`SelectionBundle`を返す。`results/`に保存された前回選定（`G_selection.json` / `D_selection.json`）を`_load_prev`で読み込み、目的値が大きく悪化しない限り維持する。新しい採用集合は`_save_sel`でJSONに書き出し次回以降の入力に備える。
L214 主なメソッド:
L215 - `residual_corr` : 収益率行列をZスコア化し、上位主成分を除去した残差から相関行列を求め、平均相関に応じてシュリンク。
L216 - `rrqr_like_det` : スコアを重み付けしたQR分解風の手順で初期候補をk件抽出し、スコアの高い非相関な集合を得る。
L217 - `swap_local_det` / `swap_local_det_cross` : `sum(score) - λ*within_corr - μ*cross_corr`を目的関数として、入れ替え探索で局所的に最適化。
L218 - `select_bucket_drrs` : プール銘柄とスコアから残差相関を計算し、上記2段階(初期選択→入れ替え)でk銘柄を決定。過去採用銘柄との比較で目的値が劣化しなければ維持する。
L219 - `select_buckets` : Gバケットを選定後、その結果を除いた候補からDバケットを選ぶ。D選定時はGとの相関ペナルティμを付与し、両バケットの分散を制御する。
L220
L221 #### 相関低減ロジック詳細
L222 1. **残差相関行列の構築 (`residual_corr`)**
L223    - リターン行列`R`をZスコア化。
L224    - SVDで上位`n_pc`主成分`F`を求め、最小二乗で係数`B`を算出し残差`E = Z - F@B`を得る。
L225    - `E`の相関行列`C`を計算し、平均絶対相関に応じてシュリンク量`shrink_eff`を補正して対角を強調。
L226 2. **初期候補の抽出 (`rrqr_like_det`)**
L227    - スコアを0-1正規化した重み`w`とし、`Z*(1+γw)`で列ノルムを強調。
L228    - 残差ノルム最大の列を逐次選び、QRライクなデフレーションを行って非相関かつ高スコアな`k`銘柄集合`S0`を得る。
L229 3. **局所探索 (`swap_local_det` / `swap_local_det_cross`)**
L230    - 目的関数`Σz_score − λ·within_corr − μ·cross_corr`を最大化。
L231    - 選択集合の各銘柄を他候補と入れ替え、改善がなくなるまでまたは`max_pass`回まで探索。
L232    - `swap_local_det_cross`はGバケットとのクロス相関行列`C_cross`を使用し、ペナルティ`μ`を付与。
L233 4. **過去採用の維持とクロスペナルティ (`select_bucket_drrs` / `select_buckets`)**
L234    - 局所探索結果`S`と過去集合`P`の目的値を比較し、`S`が`P`より`η`未満の改善なら`P`を維持。
L235    - `select_buckets`ではGを先に決定し、D選定時にGとの相関ペナルティ`μ`を加えてクロス分散を抑制。
L236
L237 ### Step4: Output
L238 選定結果を可視化し共有する工程。以下の内容をテーブル化して標準出力とSlackへ送る。
L239 - 採用銘柄と惜しくも選外となった銘柄のスコア一覧
L240 - IN/OUTリストとOUT銘柄のスコア（低得点銘柄を確認しやすく）
L241 - 新旧ポートフォリオの比較表（組入れ・除外、スコア変化）
L242 - 検討中銘柄の低スコアランキング
L243
L244 主なメソッド:
L245 - `display_results` : 上記テーブルに加えパフォーマンス指標や分散化指標を表示。
L246 - `notify_slack` : Slack Webhookへ同内容を送信。
L247 - 補助:`_avg_offdiag`、`_resid_avg_rho`、`_raw_avg_rho`、`_cross_block_raw_rho`。
L248
L249 ## エントリポイント
L250 1. `PipelineConfig`を構築。
L251 2. **Step1** `Input.prepare_data`で`InputBundle`を生成。
L252 3. **Step2** `Scorer.aggregate_scores`で`FeatureBundle`を取得。
L253 4. **Step3** `Selector.select_buckets`で`SelectionBundle`を算出。
L254 5. **Step4** `Output.display_results`と`notify_slack`で結果を出力。
```

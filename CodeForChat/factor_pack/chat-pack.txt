# === Chat Paste Pack ===
# Repo: dakara32/GPT_Code @ main
# Files: config.py, factor.py, scorer.py, .github/workflows/weekly-report.yml, documents/README.md, documents/factor_design.md
# ä½œæˆæ—¥æ™‚: 2025-09-19 20:58:24 (JST)
# ä½¿ã„æ–¹: ä¸‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’é †ã«è²¼ã‚Œã°ã“ã®ãƒãƒ£ãƒƒãƒˆã§å…¨ä½“æŠŠæ¡ã§ãã¾ã™ã€‚
# æ³¨è¨˜: å„ãƒ•ã‚¡ã‚¤ãƒ«ã¯å€‹åˆ¥ã« L1.. ã§è¡Œç•ªå·ä»˜ä¸ã€‚
---

## <config.py>
```text
L1 # å…±é€šè¨­å®šï¼ˆfactor / drift ã‹ã‚‰å‚ç…§ï¼‰
L2 TOTAL_TARGETS = 20
L3
L4 # åŸºæº–ã®ãƒã‚±ãƒƒãƒˆæ•°ï¼ˆNORMALï¼‰
L5 COUNTS_BASE = {"G": 12, "D": 8}
L6
L7 # ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®æ¨å¥¨ãƒã‚±ãƒƒãƒˆæ•°
L8 COUNTS_BY_MODE = {
L9     "NORMAL": {"G": 12, "D": 8},
L10     "CAUTION": {"G": 10, "D": 8},
L11     "EMERG": {"G": 8,  "D": 8},
L12 }
L13
L14 # ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®ãƒ‰ãƒªãƒ•ãƒˆé–¾å€¤ï¼ˆ%ï¼‰
L15 DRIFT_THRESHOLD_BY_MODE = {"NORMAL": 12, "CAUTION": 14, "EMERG": float("inf")}
L16
L17 # ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®TSï¼ˆåŸºæœ¬å¹…, å°æ•°=å‰²åˆï¼‰
L18 TS_BASE_BY_MODE = {"NORMAL": 0.15, "CAUTION": 0.13, "EMERG": 0.10}
L19 # åˆ©ç›Šåˆ°é”(+30/+60/+100%)æ™‚ã®æ®µéšã‚¿ã‚¤ãƒˆåŒ–ï¼ˆãƒã‚¤ãƒ³ãƒˆå·®ï¼‰
L20 TS_STEP_DELTAS_PT = (3, 6, 8)
L21
L22 # Breadthã®æ ¡æ­£ã¯ N_G ã«é€£å‹•ï¼ˆç·Šæ€¥è§£é™¤=ceil(1.5*N_G), é€šå¸¸å¾©å¸°=3*N_Gï¼‰
L23 N_G = COUNTS_BASE["G"]
L24 N_D = COUNTS_BASE["D"]
L25
```

## <factor.py>
```text
L1 '''ROLE: Orchestration ONLYï¼ˆå¤–éƒ¨I/Oãƒ»SSOTãƒ»Slackå‡ºåŠ›ï¼‰, è¨ˆç®—ã¯ scorer.py'''
L2 # === NOTE: æ©Ÿèƒ½ãƒ»å…¥å‡ºåŠ›ãƒ»ãƒ­ã‚°æ–‡è¨€ãƒ»ä¾‹å¤–æŒ™å‹•ã¯ä¸å¤‰ã€‚å®‰å…¨ãªçŸ­ç¸®ï¼ˆimportçµ±åˆ/è¤‡æ•°ä»£å…¥/å†…åŒ…è¡¨è¨˜/ãƒ¡ã‚½ãƒƒãƒ‰ãƒã‚§ãƒ¼ãƒ³/ä¸€è¡ŒåŒ–/ç©ºè¡Œåœ§ç¸®ãªã©ï¼‰ã®ã¿é©ç”¨ ===
L3 BONUS_COEFF = 0.55  # æ¨å¥¨: æ”»ã‚=0.45 / ä¸­åº¸=0.55 / å®ˆã‚Š=0.65
L4 SWAP_DELTA_Z = 0.15   # åƒ…å·®åˆ¤å®š: Ïƒã®15%ã€‚(ç·©ã‚=0.10 / æ¨™æº–=0.15 / å›ºã‚=0.20)
L5 SWAP_KEEP_BUFFER = 3  # n_target+ã“ã®é †ä½ä»¥å†…ã®ç¾è¡Œã¯ä¿æŒã€‚(ç²˜ã‚Šå¼±=2 / æ¨™æº–=3 / ç²˜ã‚Šå¼·=4ã€œ5)
L6 import logging, os, time, requests
L7 from concurrent.futures import ThreadPoolExecutor
L8 from dataclasses import dataclass
L9 from time import perf_counter
L10 from typing import Any, Dict, List, Tuple
L11
L12 import numpy as np
L13 import pandas as pd
L14 import yfinance as yf
L15 from scipy.stats import zscore  # used via scorer
L16
L17 from scorer import Scorer, ttm_div_yield_portfolio, _log
L18 import config
L19
L20 # ãã®ä»–
L21 debug_mode, FINNHUB_API_KEY = True, os.environ.get("FINNHUB_API_KEY")
L22
L23 logger = logging.getLogger(__name__)
L24 logging.basicConfig(level=(logging.INFO if debug_mode else logging.WARNING), force=True)
L25
L26 class T:
L27     t = perf_counter()
L28
L29     @staticmethod
L30     def log(tag):
L31         now = perf_counter()
L32         print(f"[T] {tag}: {now - T.t:.2f}s")
L33         T.t = now
L34
L35 T.log("start")
L36
L37 # === ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã¨å®šæ•°ï¼ˆå†’é ­ã«å›ºå®šï¼‰ ===
L38 exist, cand = [pd.read_csv(f, header=None)[0].tolist() for f in ("current_tickers.csv","candidate_tickers.csv")]
L39 T.log(f"csv loaded: exist={len(exist)} cand={len(cand)}")
L40 CAND_PRICE_MAX, bench = 450, '^GSPC'  # ä¾¡æ ¼ä¸Šé™ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
L41 N_G, N_D = config.N_G, config.N_D  # G/Dæ ã‚µã‚¤ã‚ºï¼ˆNORMALåŸºæº–: G12/D8ï¼‰
L42 g_weights = {'GROWTH_F':0.35,'MOM':0.55,'VOL':-0.10}
L43 D_BETA_MAX = float(os.environ.get("D_BETA_MAX", "0.8"))
L44 FILTER_SPEC = {"G":{"pre_mask":["trend_template"]},"D":{"pre_filter":{"beta_max":D_BETA_MAX}}}
L45 D_weights = {'QAL':0.1,'YLD':0.3,'VOL':-0.5,'TRD':0.1}
L46 _fmt_w = lambda w: " ".join(f"{k}{int(v*100)}" for k, v in w.items())
L47
L48 # DRRS åˆæœŸãƒ—ãƒ¼ãƒ«ãƒ»å„ç¨®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
L49 corrM = 45
L50 DRRS_G, DRRS_D = dict(lookback=252,n_pc=3,gamma=1.2,lam=0.68,eta=0.8), dict(lookback=504,n_pc=4,gamma=0.8,lam=0.85,eta=0.5)
L51 DRRS_SHRINK = 0.10  # æ®‹å·®ç›¸é–¢ã®å¯¾è§’ã‚·ãƒ¥ãƒªãƒ³ã‚¯ï¼ˆåŸºç¤ï¼‰
L52
L53 # ã‚¯ãƒ­ã‚¹ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆæœªå®šç¾©ãªã‚‰è¨­å®šï¼‰
L54 try: CROSS_MU_GD
L55 except NameError: CROSS_MU_GD = 0.40  # æ¨å¥¨ 0.35â€“0.45ï¼ˆlam=0.85æƒ³å®šï¼‰
L56
L57 # å‡ºåŠ›é–¢é€£
L58 RESULTS_DIR = "results"
L59 os.makedirs(RESULTS_DIR, exist_ok=True)
L60
L61 # === å…±æœ‰DTOï¼ˆã‚¯ãƒ©ã‚¹é–“I/Oå¥‘ç´„ï¼‰ï¼‹ Config ===
L62 @dataclass(frozen=True)
L63 class InputBundle:
L64     # Input â†’ Scorer ã§å—ã‘æ¸¡ã™ç´ æï¼ˆI/Oç¦æ­¢ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ï¼‰
L65     cand: List[str]
L66     tickers: List[str]
L67     bench: str
L68     data: pd.DataFrame              # yfinance downloadçµæœï¼ˆ'Close','Volume'ç­‰ã®éšå±¤åˆ—ï¼‰
L69     px: pd.DataFrame                # data['Close']
L70     spx: pd.Series                  # data['Close'][bench]
L71     tickers_bulk: object            # yfinance.Tickers
L72     info: Dict[str, dict]           # yfinance info per ticker
L73     eps_df: pd.DataFrame            # ['eps_ttm','eps_q_recent',...]
L74     fcf_df: pd.DataFrame            # ['fcf_ttm', ...]
L75     returns: pd.DataFrame           # px[tickers].pct_change()
L76
L77 @dataclass(frozen=True)
L78 class FeatureBundle:
L79     df: pd.DataFrame
L80     df_z: pd.DataFrame
L81     g_score: pd.Series
L82     d_score_all: pd.Series
L83     missing_logs: pd.DataFrame
L84     df_full: pd.DataFrame | None = None
L85     df_full_z: pd.DataFrame | None = None
L86     scaler: Any | None = None
L87
L88 @dataclass(frozen=True)
L89 class SelectionBundle:
L90     resG: dict
L91     resD: dict
L92     top_G: List[str]
L93     top_D: List[str]
L94     init_G: List[str]
L95     init_D: List[str]
L96
L97 @dataclass(frozen=True)
L98 class WeightsConfig:
L99     g: Dict[str,float]
L100     d: Dict[str,float]
L101
L102 @dataclass(frozen=True)
L103 class DRRSParams:
L104     corrM: int
L105     shrink: float
L106     G: Dict[str,float]   # lookback, n_pc, gamma, lam, eta
L107     D: Dict[str,float]
L108     cross_mu_gd: float
L109
L110 @dataclass(frozen=True)
L111 class PipelineConfig:
L112     weights: WeightsConfig
L113     drrs: DRRSParams
L114     price_max: float
L115     debug_mode: bool = False
L116
L117 # === å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆè¤‡æ•°ã‚¯ãƒ©ã‚¹ã§ä½¿ç”¨ï¼‰ ===
L118 # (unused local utils removed â€“ use scorer.py versions if needed)
L119
L120 _env_true = lambda name, default=False: (os.getenv(name) or str(default)).strip().lower() == "true"
L121
L122 def _post_slack(payload: dict):
L123     url = os.getenv("SLACK_WEBHOOK_URL")
L124     if not url: print("âš ï¸ SLACK_WEBHOOK_URL æœªè¨­å®š"); return
L125     try:
L126         requests.post(url, json=payload).raise_for_status()
L127     except Exception as e:
L128         print(f"âš ï¸ Slacké€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
L129
L130 def _slack_send_text_chunks(url: str, text: str, chunk: int = 2800) -> None:
L131     """Slackã¸ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²é€ä¿¡ï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å½¢å¼ï¼‰ã€‚"""
L132
L133     def _post_text(payload: str) -> None:
L134         try:
L135             resp = requests.post(url, json={"text": payload})
L136             print(f"[DBG] debug_post status={getattr(resp,'status_code',None)} size={len(payload)}")
L137             if resp is not None:
L138                 resp.raise_for_status()
L139         except Exception as e:
L140             print(f"[ERR] debug_post_failed: {e}")
L141
L142     body = (text or "").strip()
L143     if not body:
L144         print("[DBG] skip debug send: empty body")
L145         return
L146
L147     block, block_len = [], 0
L148
L149     def _flush():
L150         nonlocal block, block_len
L151         if block:
L152             _post_text("```" + "\n".join(block) + "```")
L153             block, block_len = [], 0
L154
L155     for raw in body.splitlines():
L156         line = raw or ""
L157         while len(line) > chunk:
L158             head, line = line[:chunk], line[chunk:]
L159             _flush()
L160             _post_text("```" + head + "```")
L161         add_len = len(line) if not block else len(line) + 1
L162         if block and block_len + add_len > chunk:
L163             _flush(); add_len = len(line)
L164         block.append(line)
L165         block_len += add_len
L166     _flush()
L167
L168 def _disjoint_keepG(top_G, top_D, poolD):
L169     """Gé‡è¤‡ã‚’Dã‹ã‚‰é™¤å»ã—ã€poolDã§é †æ¬¡è£œå……ï¼ˆæ¯æ¸‡æ™‚ã¯å…ƒéŠ˜æŸ„ç¶­æŒï¼‰ã€‚"""
L170     used, D, i = set(top_G), list(top_D), 0
L171     for j, t in enumerate(D):
L172         if t not in used:
L173             continue
L174         while i < len(poolD) and (poolD[i] in used or poolD[i] in D):
L175             i += 1
L176         if i < len(poolD):
L177             D[j] = poolD[i]; used.add(D[j]); i += 1
L178     return top_G, D
L179
L180
L181 def _sticky_keep_current(agg: pd.Series, pick: list[str], incumbents: list[str],
L182                          n_target: int, delta_z: float, keep_buffer: int) -> list[str]:
L183     import pandas as pd, numpy as np
L184     sel = list(pick)
L185     if not sel: return sel
L186     ranked_sel = agg.reindex(sel).sort_values(ascending=False)
L187     kth = ranked_sel.iloc[min(len(sel), n_target)-1]
L188     std = agg.std()
L189     sigma = float(std) if pd.notna(std) else 0.0
L190     thresh = kth - delta_z * sigma
L191     ranked_all = agg.sort_values(ascending=False)
L192     cand = [t for t in incumbents if (t not in sel) and (t in agg.index)]
L193     for t in cand:
L194         within_score = pd.notna(agg[t]) and agg[t] >= thresh
L195         within_rank = t in ranked_all.index and ranked_all.index.get_loc(t) < n_target + keep_buffer
L196         if not (within_score or within_rank):
L197             continue
L198         non_inc = [x for x in sel if x not in incumbents]
L199         if not non_inc:
L200             break
L201         weakest = min(non_inc, key=lambda x: agg.get(x, -np.inf))
L202         if weakest in sel and agg.get(t, -np.inf) >= agg.get(weakest, -np.inf):
L203             sel.remove(weakest); sel.append(t)
L204     if len(sel) > n_target:
L205         sel = sorted(sel, key=lambda x: agg.get(x, -1e9), reverse=True)[:n_target]
L206     return sel
L207
L208
L209 # === Inputï¼šå¤–éƒ¨I/Oã¨å‰å‡¦ç†ï¼ˆCSV/APIãƒ»æ¬ æè£œå®Œï¼‰ ===
L210 class Input:
L211     def __init__(self, cand, exist, bench, price_max, finnhub_api_key=None):
L212         self.cand, self.exist, self.bench, self.price_max = cand, exist, bench, price_max
L213         self.api_key = finnhub_api_key or os.environ.get("FINNHUB_API_KEY")
L214
L215     # ---- ï¼ˆInputå°‚ç”¨ï¼‰EPSè£œå®Œãƒ»FCFç®—å‡ºç³» ----
L216     @staticmethod
L217     def _sec_headers():
L218         mail = (os.getenv("SEC_CONTACT_EMAIL") or "yasonba55@gmail.com").strip()
L219         app = (os.getenv("SEC_APP_NAME") or "FactorBot/1.0").strip()
L220         return {"User-Agent": f"{app} ({mail})", "From": mail, "Accept": "application/json"}
L221
L222     @staticmethod
L223     def _sec_get(url: str, retries: int = 3, backoff: float = 0.5):
L224         for i in range(retries):
L225             r = requests.get(url, headers=Input._sec_headers(), timeout=20)
L226             if r.status_code in (429, 503, 403):
L227                 time.sleep(min(2 ** i * backoff, 8.0))
L228                 continue
L229             r.raise_for_status(); return r.json()
L230         r.raise_for_status()
L231
L232     @staticmethod
L233     def _sec_ticker_map():
L234         j = Input._sec_get("https://data.sec.gov/api/xbrl/company_tickers.json")
L235         mp = {}
L236         for _, v in (j or {}).items():
L237             try:
L238                 mp[str(v["ticker"]).upper()] = f"{int(v['cik_str']):010d}"
L239             except Exception:
L240                 continue
L241         return mp
L242
L243     # --- è¿½åŠ : ADR/OTCå‘ã‘ã®ç°¡æ˜“æ­£è¦åŒ–ï¼ˆæœ«å°¾Y/F, ãƒ‰ãƒƒãƒˆç­‰ï¼‰ ---
L244     @staticmethod
L245     def _normalize_ticker(sym: str) -> list[str]:
L246         s = (sym or "").upper().strip()
L247         # è¿½åŠ : å…ˆé ­ã®$ã‚„å…¨è§’ã®è¨˜å·ã‚’é™¤å»
L248         s = s.lstrip("$").replace("ï¼„", "").replace("ï¼", ".").replace("ï¼", "-")
L249         cand: list[str] = []
L250
L251         def add(x: str) -> None:
L252             if x and x not in cand:
L253                 cand.append(x)
L254
L255         # 1) åŸæ–‡ã‚’æœ€å„ªå…ˆï¼ˆSECã¯ BRK.B, BF.B ãªã© . ã‚’æ­£å¼æ¡ç”¨ï¼‰
L256         add(s)
L257         # 2) Yahooç³»ãƒãƒªã‚¢ãƒ³ãƒˆï¼ˆ. ã¨ - ã®æºã‚Œã‚’ç›¸äº’ã«ï¼‰
L258         if "." in s:
L259             add(s.replace(".", "-"))
L260             add(s.replace(".", ""))
L261         if "-" in s:
L262             add(s.replace("-", "."))
L263             add(s.replace("-", ""))
L264         # 3) ãƒ‰ãƒƒãƒˆãƒ»ãƒã‚¤ãƒ•ãƒ³ãƒ»ãƒ”ãƒªã‚ªãƒ‰ç„¡ã—ç‰ˆï¼ˆæœ€å¾Œã®ä¿é™ºï¼‰
L265         add(s.replace("-", "").replace(".", ""))
L266         # 4) ADRç°¡æ˜“ï¼šæœ«å°¾Y/Fã®é™¤å»ï¼ˆSECãƒãƒƒãƒ—ã¯æœ¬ä½“ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚’æŒã¤ã“ã¨ãŒã‚ã‚‹ï¼‰
L267         if len(s) >= 2 and s[-1] in {"Y", "F"}:
L268             add(s[:-1])
L269         return cand
L270
L271     @staticmethod
L272     def _sec_companyfacts(cik: str):
L273         return Input._sec_get(f"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json")
L274
L275     @staticmethod
L276     def _units_for_tags(facts: dict, namespaces: list[str], tags: list[str]) -> list[dict]:
L277         """facts ã‹ã‚‰ namespace/tag ã‚’æ¨ªæ–­ã—ã¦ units é…åˆ—ã‚’åé›†ï¼ˆå­˜åœ¨é †ã«é€£çµï¼‰ã€‚"""
L278         out: list[dict] = []
L279         facts = (facts or {}).get("facts", {})
L280         for ns in namespaces:
L281             node = facts.get(ns, {}) if isinstance(facts, dict) else {}
L282             for tg in tags:
L283                 try:
L284                     units = node[tg]["units"]
L285                 except Exception:
L286                     continue
L287                 picks: list[dict] = []
L288                 if "USD/shares" in units:
L289                     picks.extend(list(units["USD/shares"]))
L290                 if "USD" in units:
L291                     picks.extend(list(units["USD"]))
L292                 if not picks:
L293                     for arr in units.values():
L294                         picks.extend(list(arr))
L295                 out.extend(picks)
L296         return out
L297
L298     @staticmethod
L299     def _only_quarterly(arr: list[dict]) -> list[dict]:
L300         """companyfactsã®æ··åœ¨é…åˆ—ã‹ã‚‰ã€å››åŠæœŸã€ã ã‘ã‚’æŠ½å‡ºã€‚
L301
L302         - frame ã« "Q" ã‚’å«ã‚€ï¼ˆä¾‹: CY2024Q2Iï¼‰
L303         - fp ãŒ Q1/Q2/Q3/Q4
L304         - form ãŒ 10-Q/10-Q/A/6-K
L305         """
L306         if not arr:
L307             return []
L308         q_forms = {"10-Q", "10-Q/A", "6-K"}
L309         out = [
L310             x
L311             for x in arr
L312             if (
L313                 "Q" in (x.get("frame") or "").upper()
L314                 or (x.get("fp") or "").upper() in {"Q1", "Q2", "Q3", "Q4"}
L315                 or (x.get("form") or "").upper() in q_forms
L316             )
L317         ]
L318         out.sort(key=lambda x: (x.get("end") or ""), reverse=True)
L319         return out
L320
L321     @staticmethod
L322     def _series_from_facts_with_dates(arr, key_val="val", key_dt="end", normalize=float):
L323         """companyfactsã‚¢ã‚¤ãƒ†ãƒ é…åˆ—ã‹ã‚‰ (date,value) ã‚’è¿”ã™ã€‚dateã¯YYYY-MM-DDã‚’æƒ³å®šã€‚"""
L324         out: List[Tuple[str, float]] = []
L325         for x in (arr or []):
L326             try:
L327                 d = x.get(key_dt)
L328                 if d is None:
L329                     continue
L330                 v = x.get(key_val)
L331                 out.append((str(d), normalize(v) if v is not None else float("nan")))
L332             except Exception:
L333                 continue
L334         out.sort(key=lambda t: t[0], reverse=True)
L335         return out
L336
L337     def fetch_eps_rev_from_sec(self, tickers: list[str]) -> dict:
L338         out = {}
L339         t2cik = self._sec_ticker_map()
L340         n_map = n_rev = n_eps = 0
L341         miss_map: list[str] = []
L342         miss_facts: list[str] = []
L343         for t in tickers:
L344             base = (t or "").upper()
L345             candidates: list[str] = []
L346             for key in [base, *self._normalize_ticker(t)]:
L347                 if key and key not in candidates:
L348                     candidates.append(key)
L349             cik = next((t2cik.get(key) for key in candidates if t2cik.get(key)), None)
L350             if not cik:
L351                 out[t] = {}
L352                 miss_map.append(t)
L353                 continue
L354             try:
L355                 j = self._sec_companyfacts(cik)
L356                 facts = j or {}
L357                 rev_tags = [
L358                     "Revenues",
L359                     "RevenueFromContractWithCustomerExcludingAssessedTax",
L360                     "SalesRevenueNet",
L361                     "SalesRevenueGoodsNet",
L362                     "SalesRevenueServicesNet",
L363                     "Revenue",
L364                 ]
L365                 eps_tags = [
L366                     "EarningsPerShareDiluted",
L367                     "EarningsPerShareBasicAndDiluted",
L368                     "EarningsPerShare",
L369                     "EarningsPerShareBasic",
L370                 ]
L371                 rev_arr = self._units_for_tags(facts, ["us-gaap", "ifrs-full"], rev_tags)
L372                 eps_arr = self._units_for_tags(facts, ["us-gaap", "ifrs-full"], eps_tags)
L373                 rev_q_items = self._only_quarterly(rev_arr)
L374                 eps_q_items = self._only_quarterly(eps_arr)
L375                 # (date,value) ã§å–å¾—
L376                 rev_pairs = self._series_from_facts_with_dates(rev_q_items)
L377                 eps_pairs = self._series_from_facts_with_dates(eps_q_items)
L378                 rev_vals = [v for (_d, v) in rev_pairs]
L379                 eps_vals = [v for (_d, v) in eps_pairs]
L380                 rev_q = float(rev_vals[0]) if rev_vals else float("nan")
L381                 eps_q = float(eps_vals[0]) if eps_vals else float("nan")
L382                 rev_ttm = float(sum(v for v in rev_vals[:4] if v == v)) if rev_vals else float("nan")
L383                 eps_ttm = float(sum(v for v in eps_vals[:4] if v == v)) if eps_vals else float("nan")
L384                 out[t] = {
L385                     "eps_q_recent": eps_q,
L386                     "eps_ttm": eps_ttm,
L387                     "rev_q_recent": rev_q,
L388                     "rev_ttm": rev_ttm,
L389                     # å¾Œæ®µã§DatetimeIndexåŒ–ã§ãã‚‹ã‚ˆã† (date,value) ã‚’ä¿æŒã€‚å€¤ã ã‘ã®äº’æ›ã‚­ãƒ¼ã‚‚æ®‹ã™ã€‚
L390                     "eps_q_series_pairs": eps_pairs[:16],
L391                     "rev_q_series_pairs": rev_pairs[:16],
L392                     "eps_q_series": eps_vals[:16],
L393                     "rev_q_series": rev_vals[:16],
L394                 }
L395                 n_map += 1
L396                 if rev_vals:
L397                     n_rev += 1
L398                 if eps_vals:
L399                     n_eps += 1
L400             except Exception:
L401                 out[t] = {}
L402                 miss_facts.append(t)
L403             time.sleep(0.30)
L404         # å–å¾—ã‚µãƒãƒªã‚’ãƒ­ã‚°ï¼ˆActionsã§ç¢ºèªã—ã‚„ã™ã„ã‚ˆã† printï¼‰
L405         try:
L406             total = len(tickers)
L407             print(f"[SEC] map={n_map}/{total}  rev_q_hit={n_rev}  eps_q_hit={n_eps}")
L408             # ãƒ‡ãƒãƒƒã‚°: å–å¾—æœ¬æ•°ã®åˆ†å¸ƒï¼ˆå…ˆé ­ã®ã¿ï¼‰
L409             try:
L410                 lens = [len((out.get(t, {}) or {}).get("rev_q_series", [])) for t in tickers]
L411                 print(f"[SEC] rev_q_series length: min={min(lens) if lens else 0} "
L412                       f"p25={np.percentile(lens,25) if lens else 0} median={np.median(lens) if lens else 0} "
L413                       f"p75={np.percentile(lens,75) if lens else 0} max={max(lens) if lens else 0}")
L414             except Exception:
L415                 pass
L416             if miss_map:
L417                 print(f"[SEC] no CIK map: {len(miss_map)} (ã‚µãƒ³ãƒ—ãƒ«ä¾‹) {miss_map[:20]}")
L418             if miss_facts:
L419                 print(f"[SEC] CIKã‚ã‚Š ã ãŒå¯¾è±¡factãªã—: {len(miss_facts)} (ã‚µãƒ³ãƒ—ãƒ«ä¾‹) {miss_facts[:20]}")
L420         except Exception:
L421             pass
L422         return out
L423
L424     def sec_dryrun_sample(self, tickers: list[str] | None = None) -> None:
L425         if not _env_true("SEC_DRYRUN_SAMPLE", False):
L426             return
L427         sample = tickers or ["BRK.B", "BF.B", "GOOGL", "META", "UBER", "PBR.A", "TSM", "NARI", "EVBN", "SWAV"]
L428         print(f"[SEC-DRYRUN] sample tickers: {sample}")
L429         try:
L430             t2cik = self._sec_ticker_map()
L431             hits = 0
L432             for sym in sample:
L433                 candidates: list[str] = []
L434
L435                 def add(key: str) -> None:
L436                     if key and key not in candidates:
L437                         candidates.append(key)
L438
L439                 add((sym or "").upper())
L440                 for alt in self._normalize_ticker(sym):
L441                     add(alt)
L442                 if any(t2cik.get(key) for key in candidates):
L443                     hits += 1
L444             sec_data = self.fetch_eps_rev_from_sec(sample)
L445             rev_hits = sum(1 for v in sec_data.values() if v.get("rev_q_series"))
L446             eps_hits = sum(1 for v in sec_data.values() if v.get("eps_q_series"))
L447             total = len(sample)
L448             print(f"[SEC-DRYRUN] CIK map hit: {hits}/{total}  rev_q_series hits: {rev_hits}  eps_q_series hits: {eps_hits}")
L449         except Exception as e:
L450             print(f"[SEC-DRYRUN] error: {e}")
L451     @staticmethod
L452     def impute_eps_ttm(df: pd.DataFrame, ttm_col: str="eps_ttm", q_col: str="eps_q_recent", out_col: str|None=None) -> pd.DataFrame:
L453         out_col = out_col or ttm_col; df = df.copy(); df["eps_imputed"] = False
L454         cand = df[q_col]*4; ok = df[ttm_col].isna() & cand.replace([np.inf,-np.inf], np.nan).notna()
L455         df.loc[ok, out_col], df.loc[ok,"eps_imputed"] = cand[ok], True; return df
L456
L457     _CF_ALIASES = {"cfo":["Operating Cash Flow","Total Cash From Operating Activities"], "capex":["Capital Expenditure","Capital Expenditures"]}
L458
L459     @staticmethod
L460     def _pick_row(df: pd.DataFrame, names: list[str]) -> pd.Series|None:
L461         if df is None or df.empty: return None
L462         idx_lower={str(i).lower():i for i in df.index}
L463         for n in names:
L464             k=n.lower()
L465             if k in idx_lower: return df.loc[idx_lower[k]]
L466         return None
L467
L468     @staticmethod
L469     def _sum_last_n(s: pd.Series|None, n: int) -> float|None:
L470         if s is None or s.empty: return None
L471         v=s.dropna().astype(float); return None if v.empty else v.iloc[:n].sum()
L472
L473     @staticmethod
L474     def _latest(s: pd.Series|None) -> float|None:
L475         if s is None or s.empty: return None
L476         v=s.dropna().astype(float); return v.iloc[0] if not v.empty else None
L477
L478     def fetch_cfo_capex_ttm_yf(self, tickers: list[str]) -> pd.DataFrame:
L479         from concurrent.futures import ThreadPoolExecutor, as_completed
L480         pick, sumn, latest, aliases = self._pick_row, self._sum_last_n, self._latest, self._CF_ALIASES
L481
L482         def one(t: str):
L483             try:
L484                 tk = yf.Ticker(t)  # â˜… ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¯æ¸¡ã•ãªã„ï¼ˆYFãŒcurl_cffiã§ç®¡ç†ï¼‰
L485                 qcf = tk.quarterly_cashflow
L486                 cfo_q, capex_q = pick(qcf, aliases["cfo"]), pick(qcf, aliases["capex"])
L487                 fcf_q = pick(qcf, ["Free Cash Flow","FreeCashFlow","Free cash flow"])
L488                 cfo, capex, fcf = sumn(cfo_q,4), sumn(capex_q,4), sumn(fcf_q,4)
L489                 if any(v is None for v in (cfo, capex, fcf)):
L490                     acf = tk.cashflow
L491                     if cfo   is None: cfo   = latest(pick(acf, aliases["cfo"]))
L492                     if capex is None: capex = latest(pick(acf, aliases["capex"]))
L493                     if fcf   is None: fcf   = latest(pick(acf, ["Free Cash Flow","FreeCashFlow","Free cash flow"]))
L494             except Exception as e:
L495                 print(f"[warn] yf financials error: {t}: {e}"); cfo=capex=fcf=None
L496             n=np.nan
L497             return {"ticker":t,
L498                     "cfo_ttm_yf":   n if cfo   is None else cfo,
L499                     "capex_ttm_yf": n if capex is None else capex,
L500                     "fcf_ttm_yf_direct": n if fcf is None else fcf}
L501
L502         rows, mw = [], int(os.getenv("FIN_THREADS","8"))
L503         with ThreadPoolExecutor(max_workers=mw) as ex:
L504             rows=[f.result() for f in as_completed(ex.submit(one,t) for t in tickers)]
L505         return pd.DataFrame(rows).set_index("ticker")
L506
L507     _FINN_CFO_KEYS = ["netCashProvidedByOperatingActivities","netCashFromOperatingActivities","cashFlowFromOperatingActivities","operatingCashFlow"]
L508     _FINN_CAPEX_KEYS = ["capitalExpenditure","capitalExpenditures","purchaseOfPPE","investmentsInPropertyPlantAndEquipment"]
L509
L510     @staticmethod
L511     def _first_key(d: dict, keys: list[str]):
L512         for k in keys:
L513             if k in d and d[k] is not None: return d[k]
L514         return None
L515
L516     @staticmethod
L517     def _finn_get(session: requests.Session, url: str, params: dict, retries: int=3, sleep_s: float=0.5):
L518         for i in range(retries):
L519             r = session.get(url, params=params, timeout=15)
L520             if r.status_code==429: time.sleep(min(2**i*sleep_s,4.0)); continue
L521             r.raise_for_status(); return r.json()
L522         r.raise_for_status()
L523
L524     def fetch_cfo_capex_ttm_finnhub(self, tickers: list[str], api_key: str|None=None) -> pd.DataFrame:
L525         api_key = api_key or os.getenv("FINNHUB_API_KEY")
L526         if not api_key: raise ValueError("Finnhub API key not provided. Set FINNHUB_API_KEY or pass api_key=")
L527         base, s, rows = "https://finnhub.io/api/v1", requests.Session(), []
L528         for sym in tickers:
L529             cfo_ttm = capex_ttm = None
L530             try:
L531                 j = self._finn_get(s, f"{base}/stock/cash-flow", {"symbol":sym,"frequency":"quarterly","limit":8,"token":api_key})
L532                 arr = j.get("cashFlow") or []; cfo_vals, capex_vals = [], []
L533                 for item in arr[:4]:
L534                     cfo_vals.append(self._first_key(item,self._FINN_CFO_KEYS)); capex_vals.append(self._first_key(item,self._FINN_CAPEX_KEYS))
L535                 if any(v is not None for v in cfo_vals): cfo_ttm = float(np.nansum([np.nan if v is None else float(v) for v in cfo_vals]))
L536                 if any(v is not None for v in capex_vals): capex_ttm = float(np.nansum([np.nan if v is None else float(v) for v in capex_vals]))
L537             except Exception: pass
L538             if cfo_ttm is None or capex_ttm is None:
L539                 try:
L540                     j = self._finn_get(s, f"{base}/stock/cash-flow", {"symbol":sym,"frequency":"annual","limit":1,"token":api_key})
L541                     arr = j.get("cashFlow") or []
L542                     if arr:
L543                         item0 = arr[0]
L544                         if cfo_ttm is None:
L545                             v = self._first_key(item0,self._FINN_CFO_KEYS)
L546                             if v is not None: cfo_ttm = float(v)
L547                         if capex_ttm is None:
L548                             v = self._first_key(item0,self._FINN_CAPEX_KEYS)
L549                             if v is not None: capex_ttm = float(v)
L550                 except Exception: pass
L551             rows.append({"ticker":sym,"cfo_ttm_fh":np.nan if cfo_ttm is None else cfo_ttm,"capex_ttm_fh":np.nan if capex_ttm is None else capex_ttm})
L552         return pd.DataFrame(rows).set_index("ticker")
L553
L554     def compute_fcf_with_fallback(self, tickers: list[str], finnhub_api_key: str|None=None) -> pd.DataFrame:
L555         yf_df = self.fetch_cfo_capex_ttm_yf(tickers)
L556         T.log("financials (yf) done")
L557         miss_mask = yf_df[["cfo_ttm_yf","capex_ttm_yf","fcf_ttm_yf_direct"]].isna().any(axis=1)
L558         need = yf_df.index[miss_mask].tolist(); print(f"[T] yf financials missing: {len(need)} {need[:10]}{'...' if len(need)>10 else ''}")
L559         if need:
L560             fh_df = self.fetch_cfo_capex_ttm_finnhub(need, api_key=finnhub_api_key)
L561             df = yf_df.join(fh_df, how="left")
L562             for col_yf, col_fh in [("cfo_ttm_yf","cfo_ttm_fh"),("capex_ttm_yf","capex_ttm_fh")]:
L563                 df[col_yf] = df[col_yf].fillna(df[col_fh])
L564             print("[T] financials (finnhub) done (fallback only)")
L565         else:
L566             df = yf_df.assign(cfo_ttm_fh=np.nan, capex_ttm_fh=np.nan)
L567             print("[T] financials (finnhub) skipped (no missing)")
L568         df["cfo_ttm"]  = df["cfo_ttm_yf"].where(df["cfo_ttm_yf"].notna(), df["cfo_ttm_fh"])
L569         df["capex_ttm"] = df["capex_ttm_yf"].where(df["capex_ttm_yf"].notna(), df["capex_ttm_fh"])
L570         cfo, capex = pd.to_numeric(df["cfo_ttm"], errors="coerce"), pd.to_numeric(df["capex_ttm"], errors="coerce").abs()
L571         fcf_calc = cfo - capex
L572         fcf_direct = pd.to_numeric(df.get("fcf_ttm_yf_direct"), errors="coerce")
L573         df["fcf_ttm"] = fcf_calc.where(fcf_calc.notna(), fcf_direct)
L574         df["cfo_source"]  = np.where(df["cfo_ttm_yf"].notna(),"yfinance",np.where(df["cfo_ttm_fh"].notna(),"finnhub",""))
L575         df["capex_source"] = np.where(df["capex_ttm_yf"].notna(),"yfinance",np.where(df["capex_ttm_fh"].notna(),"finnhub",""))
L576         df["fcf_imputed"] = df[["cfo_ttm","capex_ttm"]].isna().any(axis=1) & df["fcf_ttm"].notna()
L577         cols = ["cfo_ttm_yf","capex_ttm_yf","cfo_ttm_fh","capex_ttm_fh","cfo_ttm","capex_ttm","fcf_ttm","fcf_ttm_yf_direct","cfo_source","capex_source","fcf_imputed"]
L578         return df[cols].sort_index()
L579
L580     def _build_eps_df(self, tickers, tickers_bulk, info, sec_map: dict | None = None):
L581         eps_rows=[]
L582         for t in tickers:
L583             info_t = info[t]
L584             sec_t = (sec_map or {}).get(t, {})
L585             eps_ttm = sec_t.get("eps_ttm", info_t.get("trailingEps", np.nan))
L586             eps_q = sec_t.get("eps_q_recent", np.nan)
L587             try:
L588                 qearn, so = tickers_bulk.tickers[t].quarterly_earnings, info_t.get("sharesOutstanding")
L589                 if so and qearn is not None and not qearn.empty and "Earnings" in qearn.columns:
L590                     eps_ttm_q = qearn["Earnings"].head(4).sum()/so
L591                     if pd.notna(eps_ttm_q) and (pd.isna(eps_ttm) or (abs(eps_ttm)>0 and abs(eps_ttm/eps_ttm_q)>3)): eps_ttm = eps_ttm_q
L592                     if pd.isna(eps_q):
L593                         eps_q = qearn["Earnings"].iloc[-1]/so
L594             except Exception: pass
L595             rev_ttm = sec_t.get("rev_ttm", np.nan)
L596             rev_q = sec_t.get("rev_q_recent", np.nan)
L597             if (not sec_t) or pd.isna(rev_ttm):
L598                 try:
L599                     tk = tickers_bulk.tickers[t]
L600                     qfin = getattr(tk, "quarterly_financials", None)
L601                     if qfin is not None and not qfin.empty:
L602                         idx_lower = {str(i).lower(): i for i in qfin.index}
L603                         rev_idx = None
L604                         for name in ("Total Revenue", "TotalRevenue"):
L605                             key = name.lower()
L606                             if key in idx_lower:
L607                                 rev_idx = idx_lower[key]
L608                                 break
L609                         if rev_idx is not None:
L610                             rev_series = pd.to_numeric(qfin.loc[rev_idx], errors="coerce").dropna()
L611                             if not rev_series.empty:
L612                                 rev_ttm_yf = float(rev_series.head(4).sum())
L613                                 if pd.isna(rev_ttm):
L614                                     rev_ttm = rev_ttm_yf
L615                                 if pd.isna(rev_q):
L616                                     rev_q = float(rev_series.iloc[0])
L617                 except Exception:
L618                     pass
L619             eps_rows.append({"ticker":t,"eps_ttm":eps_ttm,"eps_q_recent":eps_q,"rev_ttm":rev_ttm,"rev_q_recent":rev_q})
L620         return self.impute_eps_ttm(pd.DataFrame(eps_rows).set_index("ticker"))
L621
L622     def prepare_data(self):
L623         """Fetch price and fundamental data for all tickers."""
L624         self.sec_dryrun_sample()
L625         cand_info = yf.Tickers(" ".join(self.cand))
L626
L627         def _price(t: str) -> float:
L628             try:
L629                 return cand_info.tickers[t].fast_info.get("lastPrice", np.inf)
L630             except Exception as e:
L631                 print(f"{t}: price fetch failed ({e})")
L632                 return np.inf
L633
L634         cand_prices = {t: _price(t) for t in self.cand}
L635         cand_f = [t for t, p in cand_prices.items() if p <= self.price_max]
L636         T.log("price cap filter done (CAND_PRICE_MAX)")
L637         # å…¥åŠ›ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã®é‡è¤‡ã‚’é™¤å»ã—ã€ç¾è¡Œâ†’å€™è£œã®é †åºã‚’ç¶­æŒ
L638         tickers = list(dict.fromkeys(self.exist + cand_f))
L639         T.log(f"universe prepared: unique={len(tickers)} bench={self.bench}")
L640         data = yf.download(tickers + [self.bench], period="600d",
L641                            auto_adjust=True, progress=False, threads=False)
L642         T.log("yf.download done")
L643         px = data["Close"].dropna(how="all", axis=1).ffill(limit=2)
L644         spx = data["Close"][self.bench].reindex(px.index).ffill()
L645         clip_days = int(os.getenv("PRICE_CLIP_DAYS", "0"))   # 0ãªã‚‰ç„¡åŠ¹ï¼ˆæ—¢å®šï¼‰
L646         if clip_days > 0:
L647             px, spx = px.tail(clip_days + 1), spx.tail(clip_days + 1)
L648             logger.info("[T] price window clipped by env: %d rows (PRICE_CLIP_DAYS=%d)", len(px), clip_days)
L649         else:
L650             logger.info("[T] price window clip skipped; rows=%d", len(px))
L651         tickers_bulk, info = yf.Tickers(" ".join(tickers)), {}
L652         for t in tickers:
L653             try:
L654                 info[t] = tickers_bulk.tickers[t].info
L655             except Exception as e:
L656                 logger.info("[warn] %s: info fetch failed (%s)", t, e)
L657                 info[t] = {}
L658         try:
L659             sec_map = self.fetch_eps_rev_from_sec(tickers)
L660         except Exception as e:
L661             logger.warning("[SEC] fetch_eps_rev_from_sec failed: %s", e)
L662             sec_map = {}
L663
L664         def _brief_len(s):
L665             try:
L666                 if isinstance(s, pd.Series):
L667                     return int(s.dropna().size)
L668                 if isinstance(s, (list, tuple)):
L669                     return len([v for v in s if pd.notna(v)])
L670                 if isinstance(s, np.ndarray):
L671                     return int(np.count_nonzero(~pd.isna(s)))
L672                 return int(bool(s))
L673             except Exception:
L674                 return 0
L675
L676         def _has_entries(val) -> bool:
L677             try:
L678                 if isinstance(val, pd.Series):
L679                     return not val.dropna().empty
L680                 if isinstance(val, (list, tuple)):
L681                     return any(pd.notna(v) for v in val)
L682                 return bool(val)
L683             except Exception:
L684                 return False
L685
L686         have_rev = 0
L687         have_eps = 0
L688         rev_lens: list[int] = []
L689         eps_lens: list[int] = []
L690         samples: list[tuple[str, int, str, float | None, int, str, float | None]] = []
L691
L692         for t in tickers:
L693             entry = info.get(t, {})
L694             m = (sec_map or {}).get(t) or {}
L695             if entry is None or not isinstance(entry, dict):
L696                 entry = {}
L697                 info[t] = entry
L698
L699             if m:
L700                 pairs_r = m.get("rev_q_series_pairs") or []
L701                 pairs_e = m.get("eps_q_series_pairs") or []
L702                 if pairs_r:
L703                     idx = pd.to_datetime([d for (d, _v) in pairs_r], errors="coerce")
L704                     val = pd.to_numeric([v for (_d, v) in pairs_r], errors="coerce")
L705                     s = pd.Series(val, index=idx).sort_index()
L706                     entry["SEC_REV_Q_SERIES"] = s
L707                 else:
L708                     entry["SEC_REV_Q_SERIES"] = m.get("rev_q_series") or []
L709                 if pairs_e:
L710                     idx = pd.to_datetime([d for (d, _v) in pairs_e], errors="coerce")
L711                     val = pd.to_numeric([v for (_d, v) in pairs_e], errors="coerce")
L712                     s = pd.Series(val, index=idx).sort_index()
L713                     entry["SEC_EPS_Q_SERIES"] = s
L714                 else:
L715                     entry["SEC_EPS_Q_SERIES"] = m.get("eps_q_series") or []
L716
L717             r = entry.get("SEC_REV_Q_SERIES")
L718             e = entry.get("SEC_EPS_Q_SERIES")
L719             if _has_entries(r):
L720                 have_rev += 1
L721             if _has_entries(e):
L722                 have_eps += 1
L723             lr = _brief_len(r)
L724             le = _brief_len(e)
L725             rev_lens.append(lr)
L726             eps_lens.append(le)
L727             if len(samples) < 8:
L728                 try:
L729                     rd = getattr(r, "index", [])[-1] if lr > 0 else None
L730                     rv = float(r.iloc[-1]) if lr > 0 else None
L731                     ed = getattr(e, "index", [])[-1] if le > 0 else None
L732                     ev = float(e.iloc[-1]) if le > 0 else None
L733                     samples.append((t, lr, str(rd) if rd is not None else "-", rv, le, str(ed) if ed is not None else "-", ev))
L734                 except Exception:
L735                     samples.append((t, lr, "-", None, le, "-", None))
L736
L737         logger.info("[SEC] series attach: rev_q=%d/%d, eps_q=%d/%d", have_rev, len(tickers), have_eps, len(tickers))
L738
L739         if rev_lens:
L740             rev_lens_sorted = sorted(rev_lens)
L741             eps_lens_sorted = sorted(eps_lens)
L742             _log(
L743                 "SEC_SERIES",
L744                 f"rev_len min/med/max={rev_lens_sorted[0]}/{rev_lens_sorted[len(rev_lens)//2]}/{rev_lens_sorted[-1]} "
L745                 f"eps_len min/med/max={eps_lens_sorted[0]}/{eps_lens_sorted[len(eps_lens)//2]}/{eps_lens_sorted[-1]}",
L746             )
L747         for (t, lr, rd, rv, le, ed, ev) in samples:
L748             _log("SEC_SERIES_SMP", f"{t}  rev_len={lr} last=({rd},{rv})  eps_len={le} last=({ed},{ev})")
L749         eps_df = self._build_eps_df(tickers, tickers_bulk, info, sec_map=sec_map)
L750         # index é‡è¤‡ãŒã‚ã‚‹ã¨ .loc[t, col] ãŒ Series ã«ãªã‚Šä»£å…¥æ™‚ã« ValueError ã‚’èª˜ç™ºã™ã‚‹
L751         if not eps_df.index.is_unique:
L752             eps_df = eps_df[~eps_df.index.duplicated(keep="last")]
L753         eps_df = eps_df.assign(
L754             EPS_TTM=eps_df["eps_ttm"],
L755             EPS_Q_LastQ=eps_df["eps_q_recent"],
L756             REV_TTM=eps_df["rev_ttm"],
L757             REV_Q_LastQ=eps_df["rev_q_recent"],
L758         )
L759         # ã“ã“ã§éNaNä»¶æ•°ã‚’ã‚µãƒãƒªè¡¨ç¤ºï¼ˆæ¬ æçŠ¶æ³ã®å³æ™‚æŠŠæ¡ç”¨ï¼‰
L760         try:
L761             n = len(eps_df)
L762             c_eps = int(eps_df["EPS_TTM"].notna().sum())
L763             c_rev = int(eps_df["REV_TTM"].notna().sum())
L764             print(f"[SEC] eps_ttm non-NaN: {c_eps}/{n}  rev_ttm non-NaN: {c_rev}/{n}")
L765         except Exception:
L766             pass
L767         fcf_df = self.compute_fcf_with_fallback(tickers, finnhub_api_key=self.api_key)
L768         T.log("eps/fcf prep done")
L769         returns = px[tickers].pct_change()
L770         T.log("price prep/returns done")
L771         return dict(cand=cand_f, tickers=tickers, data=data, px=px, spx=spx, tickers_bulk=tickers_bulk, info=info, eps_df=eps_df, fcf_df=fcf_df, returns=returns)
L772
L773 # === Selectorï¼šç›¸é–¢ä½æ¸›ãƒ»é¸å®šï¼ˆã‚¹ã‚³ã‚¢ï¼†ãƒªã‚¿ãƒ¼ãƒ³ã ã‘èª­ã‚€ï¼‰ ===
L774 class Selector:
L775     # ---- DRRS helpersï¼ˆSelectorå°‚ç”¨ï¼‰ ----
L776     @staticmethod
L777     def _z_np(X: np.ndarray) -> np.ndarray:
L778         X = np.asarray(X, dtype=np.float32); m = np.nanmean(X, axis=0, keepdims=True); s = np.nanstd(X, axis=0, keepdims=True)+1e-9
L779         return (np.nan_to_num(X)-m)/s
L780
L781     @classmethod
L782     def residual_corr(cls, R: np.ndarray, n_pc: int=3, shrink: float=0.1) -> np.ndarray:
L783         Z = cls._z_np(R); U,S,_ = np.linalg.svd(Z, full_matrices=False); F = U[:,:n_pc]*S[:n_pc]; B = np.linalg.lstsq(F, Z, rcond=None)[0]
L784         E = Z - F@B; C = np.corrcoef(E, rowvar=False)
L785         off = C - np.diag(np.diag(C)); iu = np.triu_indices_from(off,1); avg_abs = np.nanmean(np.abs(off[iu])) if iu[0].size else 0.0
L786         shrink_eff = float(np.clip(shrink + 0.5*avg_abs, 0.1, 0.6)); N = C.shape[0]
L787         return (1.0 - shrink_eff)*C + shrink_eff*np.eye(N, dtype=C.dtype)
L788
L789     @classmethod
L790     def rrqr_like_det(cls, R: np.ndarray, score: np.ndarray, k: int, gamma: float=1.0):
L791         Z, w = cls._z_np(R), (score-score.min())/(np.ptp(score)+1e-12); X = Z*(1.0 + gamma*w)
L792         N, k = X.shape[1], max(0, min(k, X.shape[1]))
L793         if k==0: return []
L794         S, selected, Rres = [], np.zeros(N, dtype=bool), X.copy()
L795         for _ in range(k):
L796             norms = (Rres*Rres).sum(axis=0); cand = np.where(~selected)[0]
L797             j = sorted(cand, key=lambda c:(-norms[c], -w[c], c))[0]
L798             S.append(j); selected[j]=True; u = X[:,j:j+1]; u/=(np.linalg.norm(u)+1e-12); Rres = Rres - u @ (u.T @ Rres)
L799         return sorted(S)
L800
L801     @staticmethod
L802     def _obj(corrM: np.ndarray, score: np.ndarray, idx, lam: float) -> float:
L803         idx = list(idx); P = corrM[np.ix_(idx, idx)]; s = (score-score.mean())/(score.std()+1e-9)
L804         return float(s[idx].sum() - lam*((P.sum()-np.trace(P))/2.0))
L805
L806     @classmethod
L807     def swap_local_det(cls, corrM: np.ndarray, score: np.ndarray, idx, lam: float=0.6, max_pass: int=15):
L808         S, best, improved, passes = sorted(idx), cls._obj(corrM, score, idx, lam), True, 0
L809         while improved and passes<max_pass:
L810             improved, passes = False, passes+1
L811             for i,out in enumerate(list(S)):
L812                 for inn in range(len(score)):
L813                     if inn in S: continue
L814                     cand = sorted(S[:i]+[inn]+S[i+1:]); v = cls._obj(corrM, score, cand, lam)
L815                     if v>best+1e-10: S, best, improved = cand, v, True; break
L816                 if improved: break
L817         return S, best
L818
L819     @staticmethod
L820     def _obj_with_cross(C_within: np.ndarray, C_cross: np.ndarray|None, score: np.ndarray, idx, lam: float, mu: float) -> float:
L821         idx = list(idx); P = C_within[np.ix_(idx, idx)]; s = (score-score.mean())/(score.std()+1e-9)
L822         within = (P.sum()-np.trace(P))/2.0; cross = 0.0
L823         if C_cross is not None and C_cross.size>0: cross = C_cross[idx,:].sum()
L824         return float(s[idx].sum() - lam*within - mu*cross)
L825
L826     @classmethod
L827     def swap_local_det_cross(cls, C_within: np.ndarray, C_cross: np.ndarray|None, score: np.ndarray, idx, lam: float=0.6, mu: float=0.3, max_pass: int=15):
L828         S, best, improved, passes, N = sorted(idx), cls._obj_with_cross(C_within,C_cross,score,idx,lam,mu), True, 0, len(score)
L829         while improved and passes<max_pass:
L830             improved, passes = False, passes+1
L831             for i,out in enumerate(list(S)):
L832                 for inn in range(N):
L833                     if inn in S: continue
L834                     cand = sorted(S[:i]+[inn]+S[i+1:]); v = cls._obj_with_cross(C_within,C_cross,score,cand,lam,mu)
L835                     if v>best+1e-10: S, best, improved = cand, v, True; break
L836                 if improved: break
L837         return S, best
L838
L839     @staticmethod
L840     def avg_corr(C: np.ndarray, idx) -> float:
L841         k = len(idx); P = C[np.ix_(idx, idx)]
L842         return float((P.sum()-np.trace(P))/(k*(k-1)+1e-12))
L843
L844     @classmethod
L845     def select_bucket_drrs(cls, returns_df: pd.DataFrame, score_ser: pd.Series, pool_tickers: list[str], k: int, *, n_pc: int, gamma: float, lam: float, lookback: int, shrink: float=0.10, g_fixed_tickers: list[str]|None=None, mu: float=0.0):
L846         g_fixed = [t for t in (g_fixed_tickers or []) if t in returns_df.columns]
L847         union = [t for t in pool_tickers if t in returns_df.columns]
L848         for t in g_fixed:
L849             if t not in union: union.append(t)
L850         Rdf_all = returns_df[union]; Rdf_all = Rdf_all.iloc[-lookback:] if len(Rdf_all)>=lookback else Rdf_all; Rdf_all = Rdf_all.dropna()
L851         pool_eff, g_eff = [t for t in pool_tickers if t in Rdf_all.columns], [t for t in g_fixed if t in Rdf_all.columns]
L852         if len(pool_eff)==0: return dict(idx=[], tickers=[], avg_res_corr=np.nan, sum_score=0.0, objective=-np.inf)
L853         score = score_ser.reindex(pool_eff).to_numpy(dtype=np.float32)
L854         C_all = cls.residual_corr(Rdf_all.to_numpy(), n_pc=n_pc, shrink=shrink)
L855         col_pos = {c:i for i,c in enumerate(Rdf_all.columns)}; pool_pos = [col_pos[t] for t in pool_eff]
L856         C_within, C_cross = C_all[np.ix_(pool_pos,pool_pos)], None
L857         if len(g_eff)>0 and mu>0.0:
L858             g_pos = [col_pos[t] for t in g_eff]; C_cross = C_all[np.ix_(pool_pos,g_pos)]
L859         R_pool = Rdf_all[pool_eff].to_numpy(); S0 = cls.rrqr_like_det(R_pool, score, k, gamma=gamma)
L860         S, Jn = (cls.swap_local_det_cross(C_within, C_cross, score, S0, lam=lam, mu=mu, max_pass=15) if C_cross is not None else cls.swap_local_det(C_within, score, S0, lam=lam, max_pass=15))
L861         selected_tickers = [pool_eff[i] for i in S]
L862         return dict(idx=S, tickers=selected_tickers, avg_res_corr=cls.avg_corr(C_within,S), sum_score=float(score[S].sum()), objective=float(Jn))
L863
L864     # ---- é¸å®šï¼ˆã‚¹ã‚³ã‚¢ Series / returns ã ã‘ã‚’å—ã‘ã‚‹ï¼‰----
L865 # === Outputï¼šå‡ºåŠ›æ•´å½¢ã¨é€ä¿¡ï¼ˆè¡¨ç¤ºãƒ»Slackï¼‰ ===
L866 class Output:
L867
L868     def __init__(self, debug=None):
L869         # self.debug ã¯ä½¿ã‚ãªã„ï¼ˆäº’æ›ã®ãŸã‚å¼•æ•°ã¯å—ã‘ã‚‹ãŒç„¡è¦–ï¼‰
L870         self.miss_df = self.g_table = self.d_table = self.io_table = self.df_metrics_fmt = self.debug_table = None
L871         self.g_title = self.d_title = ""
L872         self.g_formatters = self.d_formatters = {}
L873         # ä½ã‚¹ã‚³ã‚¢ï¼ˆGSC+DSCï¼‰Top10 è¡¨ç¤º/é€ä¿¡ç”¨
L874         self.low10_table = None
L875         self.debug_text = ""   # ãƒ‡ãƒãƒƒã‚°ç”¨æœ¬æ–‡ã¯ã“ã“ã«ä¸€æœ¬åŒ–
L876         self._debug_logged = False
L877
L878     # --- è¡¨ç¤ºï¼ˆå…ƒ display_results ã®ãƒ­ã‚¸ãƒƒã‚¯ãã®ã¾ã¾ï¼‰ ---
L879     def display_results(self, *, exist, bench, df_z, g_score, d_score_all,
L880                         init_G, init_D, top_G, top_D, **kwargs):
L881         logger.info("ğŸ“Œ reached display_results")
L882         pd.set_option('display.float_format','{:.3f}'.format)
L883         print("ğŸ“ˆ ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼åˆ†æ•£æœ€é©åŒ–ã®çµæœ")
L884         if self.miss_df is not None and not self.miss_df.empty:
L885             print("Missing Data:")
L886             print(self.miss_df.to_string(index=False))
L887
L888         # ---- è¡¨ç¤ºç”¨ï¼šChanges/Near-Miss ã®ã‚¹ã‚³ã‚¢æºã‚’â€œæœ€çµ‚é›†è¨ˆâ€ã«çµ±ä¸€ã™ã‚‹ãƒ—ãƒ­ã‚­ã‚· ----
L889         try:
L890             sc = getattr(self, "_sc", None)
L891             agg_G = getattr(sc, "_agg_G", None)
L892             agg_D = getattr(sc, "_agg_D", None)
L893         except Exception:
L894             sc = agg_G = agg_D = None
L895         class _SeriesProxy:
L896             __slots__ = ("primary", "fallback")
L897             def __init__(self, primary, fallback): self.primary, self.fallback = primary, fallback
L898             def get(self, key, default=None):
L899                 try:
L900                     v = self.primary.get(key) if hasattr(self.primary, "get") else None
L901                 except Exception:
L902                     v = None
L903                 if v is not None and not (isinstance(v, float) and v != v):
L904                     return v
L905                 try:
L906                     return self.fallback.get(key) if hasattr(self.fallback, "get") else default
L907                 except Exception:
L908                     return default
L909         g_score = _SeriesProxy(agg_G, g_score)
L910         d_score_all = _SeriesProxy(agg_D, d_score_all)
L911         near_G = getattr(sc, "_near_G", []) if sc else []
L912         near_D = getattr(sc, "_near_D", []) if sc else []
L913
L914         extra_G = [t for t in init_G if t not in top_G][:5]; G_UNI = top_G + extra_G
L915         gsc_series = pd.Series({t: g_score.get(t) for t in G_UNI}, name='GSC')
L916         self.g_table = pd.concat([df_z.loc[G_UNI,['GROWTH_F','MOM','TRD','VOL']], gsc_series], axis=1)
L917         self.g_table.index = [t + ("â­ï¸" if t in top_G else "") for t in G_UNI]
L918         self.g_formatters = {col:"{:.2f}".format for col in ['GROWTH_F','MOM','TRD','VOL']}; self.g_formatters['GSC'] = "{:.3f}".format
L919         self.g_title = (f"[Gæ  / {N_G} / {_fmt_w(g_weights)} / corrM={corrM} / "
L920                         f"LB={DRRS_G['lookback']} nPC={DRRS_G['n_pc']} Î³={DRRS_G['gamma']} Î»={DRRS_G['lam']} Î·={DRRS_G['eta']} shrink={DRRS_SHRINK}]")
L921         if near_G:
L922             add = [t for t in near_G if t not in set(G_UNI)][:10]
L923             if len(add) < 10:
L924                 try:
L925                     aggG = getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False)
L926                     out_now = sorted(set(exist) - set(top_G + top_D))  # ä»Šå› OUT
L927                     used = set(G_UNI + add)
L928                     def _push(lst):
L929                         nonlocal add, used
L930                         for t in lst:
L931                             if len(add) == 10: break
L932                             if t in aggG.index and t not in used:
L933                                 add.append(t); used.add(t)
L934                     _push(out_now)           # â‘  ä»Šå› OUT ã‚’å„ªå…ˆ
L935                     _push(list(aggG.index))  # â‘¡ ã¾ã è¶³ã‚Šãªã‘ã‚Œã°ä¸Šä½ã§å……å¡«
L936                 except Exception:
L937                     pass
L938             if add:
L939                 near_tbl = pd.concat([df_z.loc[add,['GROWTH_F','MOM','TRD','VOL']], pd.Series({t: g_score.get(t) for t in add}, name='GSC')], axis=1)
L940                 self.g_table = pd.concat([self.g_table, near_tbl], axis=0)
L941         print(self.g_title); print(self.g_table.to_string(formatters=self.g_formatters))
L942
L943         extra_D = [t for t in init_D if t not in top_D][:5]; D_UNI = top_D + extra_D
L944         cols_D = ['QAL','YLD','VOL','TRD']; d_disp = pd.DataFrame(index=D_UNI)
L945         d_disp['QAL'], d_disp['YLD'], d_disp['VOL'], d_disp['TRD'] = df_z.loc[D_UNI,'D_QAL'], df_z.loc[D_UNI,'D_YLD'], df_z.loc[D_UNI,'D_VOL_RAW'], df_z.loc[D_UNI,'D_TRD']
L946         dsc_series = pd.Series({t: d_score_all.get(t) for t in D_UNI}, name='DSC')
L947         self.d_table = pd.concat([d_disp, dsc_series], axis=1); self.d_table.index = [t + ("â­ï¸" if t in top_D else "") for t in D_UNI]
L948         self.d_formatters = {col:"{:.2f}".format for col in cols_D}; self.d_formatters['DSC']="{:.3f}".format
L949         import scorer
L950         dw_eff = scorer.D_WEIGHTS_EFF
L951         self.d_title = (f"[Dæ  / {N_D} / {_fmt_w(dw_eff)} / corrM={corrM} / "
L952                         f"LB={DRRS_D['lookback']} nPC={DRRS_D['n_pc']} Î³={DRRS_D['gamma']} Î»={DRRS_D['lam']} Î¼={CROSS_MU_GD} Î·={DRRS_D['eta']} shrink={DRRS_SHRINK}]")
L953         if near_D:
L954             add = [t for t in near_D if t not in set(D_UNI)][:10]
L955             if add:
L956                 d_disp2 = pd.DataFrame(index=add)
L957                 d_disp2['QAL'], d_disp2['YLD'], d_disp2['VOL'], d_disp2['TRD'] = df_z.loc[add,'D_QAL'], df_z.loc[add,'D_YLD'], df_z.loc[add,'D_VOL_RAW'], df_z.loc[add,'D_TRD']
L958                 near_tbl = pd.concat([d_disp2, pd.Series({t: d_score_all.get(t) for t in add}, name='DSC')], axis=1)
L959                 self.d_table = pd.concat([self.d_table, near_tbl], axis=0)
L960         print(self.d_title); print(self.d_table.to_string(formatters=self.d_formatters))
L961
L962         # === Changesï¼ˆIN ã® GSC/DSC ã‚’è¡¨ç¤ºã€‚OUT ã¯éŠ˜æŸ„åã®ã¿ï¼‰ ===
L963         in_list = sorted(set(list(top_G)+list(top_D)) - set(exist))
L964         out_list = sorted(set(exist) - set(list(top_G)+list(top_D)))
L965
L966         self.io_table = pd.DataFrame({
L967             'IN': pd.Series(in_list),
L968             '/ OUT': pd.Series(out_list)
L969         })
L970         g_list = [f"{g_score.get(t):.3f}" if pd.notna(g_score.get(t)) else 'â€”' for t in out_list]
L971         d_list = [f"{d_score_all.get(t):.3f}" if pd.notna(d_score_all.get(t)) else 'â€”' for t in out_list]
L972         self.io_table['GSC'] = pd.Series(g_list)
L973         self.io_table['DSC'] = pd.Series(d_list)
L974
L975         print("Changes:")
L976         print(self.io_table.to_string(index=False))
L977
L978         all_tickers = list(set(exist + list(top_G) + list(top_D) + [bench])); prices = yf.download(all_tickers, period='1y', auto_adjust=True, progress=False, threads=False)['Close'].ffill(limit=2)
L979         ret = prices.pct_change(); portfolios = {'CUR':exist,'NEW':list(top_G)+list(top_D)}; metrics={}
L980         for name,ticks in portfolios.items():
L981             pr = ret[ticks].mean(axis=1, skipna=True).dropna(); cum = (1+pr).cumprod()-1; n = len(pr)
L982             if n>=252: ann_ret, ann_vol = (1+cum.iloc[-1])**(252/n)-1, pr.std()*np.sqrt(252)
L983             else: ann_ret, ann_vol = cum.iloc[-1], pr.std()*np.sqrt(n)
L984             sharpe, drawdown = ann_ret/ann_vol, (cum - cum.cummax()).min()
L985             if len(ticks)>=2:
L986                 C_raw = ret[ticks].corr(); RAW_rho = C_raw.mask(np.eye(len(ticks), dtype=bool)).stack().mean()
L987                 R = ret[ticks].dropna().to_numpy(); C_resid = Selector.residual_corr(R, n_pc=3, shrink=DRRS_SHRINK)
L988                 RESID_rho = float((C_resid.sum()-np.trace(C_resid))/(C_resid.shape[0]*(C_resid.shape[0]-1)))
L989             else: RAW_rho = RESID_rho = np.nan
L990             divy = ttm_div_yield_portfolio(ticks); metrics[name] = {'RET':ann_ret,'VOL':ann_vol,'SHP':sharpe,'MDD':drawdown,'RAWÏ':RAW_rho,'RESIDÏ':RESID_rho,'DIVY':divy}
L991         df_metrics = pd.DataFrame(metrics).T; df_metrics_pct = df_metrics.copy(); self.df_metrics = df_metrics
L992         for col in ['RET','VOL','MDD','DIVY']: df_metrics_pct[col] = df_metrics_pct[col]*100
L993         cols_order = ['RET','VOL','SHP','MDD','RAWÏ','RESIDÏ','DIVY']; df_metrics_pct = df_metrics_pct.reindex(columns=cols_order)
L994         def _fmt_row(s):
L995             return pd.Series({'RET':f"{s['RET']:.1f}%",'VOL':f"{s['VOL']:.1f}%",'SHP':f"{s['SHP']:.1f}",'MDD':f"{s['MDD']:.1f}%",'RAWÏ':(f"{s['RAWÏ']:.2f}" if pd.notna(s['RAWÏ']) else "NaN"),'RESIDÏ':(f"{s['RESIDÏ']:.2f}" if pd.notna(s['RESIDÏ']) else "NaN"),'DIVY':f"{s['DIVY']:.1f}%"})
L996         self.df_metrics_fmt = df_metrics_pct.apply(_fmt_row, axis=1); print("Performance Comparison:"); print(self.df_metrics_fmt.to_string())
L997         # === è¿½åŠ : GSC+DSC ãŒä½ã„é † TOP10 ===
L998         try:
L999             all_scores = pd.DataFrame({'GSC': df_z['GSC'], 'DSC': df_z['DSC']}).copy()
L1000             all_scores['G_plus_D'] = all_scores['GSC'] + all_scores['DSC']
L1001             all_scores = all_scores.dropna(subset=['G_plus_D'])
L1002             self.low10_table = all_scores.sort_values('G_plus_D', ascending=True).head(10).round(3)
L1003             print("Low Score Candidates (GSC+DSC bottom 10):")
L1004             print(self.low10_table.to_string())
L1005         except Exception as e:
L1006             print(f"[warn] low-score ranking failed: {e}")
L1007             self.low10_table = None
L1008         self.debug_text = ""
L1009         if debug_mode:
L1010             logger.info("debug_mode=True: df_z dump handled in scorer; skipping factor-side debug output")
L1011         else:
L1012             logger.debug(
L1013                 "skip debug log: debug_mode=%s debug_text_empty=%s",
L1014                 debug_mode, True
L1015             )
L1016         self._debug_logged = True
L1017
L1018     # --- Slacké€ä¿¡ï¼ˆå…ƒ notify_slack ã®ãƒ­ã‚¸ãƒƒã‚¯ãã®ã¾ã¾ï¼‰ ---
L1019     def notify_slack(self):
L1020         SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")
L1021
L1022         if not SLACK_WEBHOOK_URL:
L1023             print("âš ï¸ SLACK_WEBHOOK_URL not set (main report skipped)")
L1024             return
L1025
L1026         def _filter_suffix_from(spec: dict, group: str) -> str:
L1027             g = spec.get(group, {})
L1028             parts = [str(m) for m in g.get("pre_mask", [])]
L1029             for k, v in (g.get("pre_filter", {}) or {}).items():
L1030                 base, op = (k[:-4], "<") if k.endswith("_max") else ((k[:-4], ">") if k.endswith("_min") else (k, "="))
L1031                 name = {"beta": "Î²"}.get(base, base)
L1032                 try:
L1033                     val = f"{float(v):g}"
L1034                 except Exception:
L1035                     val = str(v)
L1036                 parts.append(f"{name}{op}{val}")
L1037             return "" if not parts else " / filter:" + " & ".join(parts)
L1038
L1039         def _inject_filter_suffix(title: str, group: str) -> str:
L1040             suf = _filter_suffix_from(FILTER_SPEC, group)
L1041             return f"{title[:-1]}{suf}]" if suf and title.endswith("]") else (title + suf)
L1042
L1043         def _blk(title, tbl, fmt=None, drop=()):
L1044             if tbl is None or getattr(tbl, 'empty', False):
L1045                 return f"{title}\n(é¸å®šãªã—)\n"
L1046             if drop and hasattr(tbl, 'columns'):
L1047                 keep = [c for c in tbl.columns if c not in drop]
L1048                 tbl, fmt = tbl[keep], {k: v for k, v in (fmt or {}).items() if k in keep}
L1049             return f"{title}\n```{tbl.to_string(formatters=fmt)}```\n"
L1050
L1051         message = "ğŸ“ˆ ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼åˆ†æ•£æœ€é©åŒ–ã®çµæœ\n"
L1052         if self.miss_df is not None and not self.miss_df.empty:
L1053             message += "Missing Data\n```" + self.miss_df.to_string(index=False) + "```\n"
L1054         message += _blk(_inject_filter_suffix(self.g_title, "G"), self.g_table, self.g_formatters, drop=("TRD",))
L1055         message += _blk(_inject_filter_suffix(self.d_title, "D"), self.d_table, self.d_formatters)
L1056         message += "Changes\n" + ("(å¤‰æ›´ãªã—)\n" if self.io_table is None or getattr(self.io_table, 'empty', False) else f"```{self.io_table.to_string(index=False)}```\n")
L1057         message += "Performance Comparison:\n```" + self.df_metrics_fmt.to_string() + "```"
L1058
L1059         try:
L1060             r = requests.post(SLACK_WEBHOOK_URL, json={"text": message})
L1061             print(f"[DBG] main_post status={getattr(r, 'status_code', None)} size={len(message)}")
L1062             if r is not None:
L1063                 r.raise_for_status()
L1064         except Exception as e:
L1065             print(f"[ERR] main_post_failed: {e}")
L1066
L1067 def _infer_g_universe(feature_df, selected12=None, near5=None):
L1068     try:
L1069         out = feature_df.index[feature_df['group'].astype(str).str.upper().eq('G')].tolist()
L1070         if out: return out
L1071     except Exception:
L1072         pass
L1073     base = set()
L1074     for lst in (selected12 or []), (near5 or []):
L1075         for x in (lst or []): base.add(x)
L1076     return list(base) if base else list(feature_df.index)
L1077
L1078 def _fmt_with_fire_mark(tickers, feature_df):
L1079     out = []
L1080     for t in tickers or []:
L1081         try:
L1082             br = bool(feature_df.at[t, "G_BREAKOUT_recent_5d"])
L1083             pb = bool(feature_df.at[t, "G_PULLBACK_recent_5d"])
L1084             out.append(f"{t}{' ğŸ”¥' if (br or pb) else ''}")
L1085         except Exception:
L1086             out.append(t)
L1087     return out
L1088
L1089 def _label_recent_event(t, feature_df):
L1090     try:
L1091         br = bool(feature_df.at[t, "G_BREAKOUT_recent_5d"]); dbr = str(feature_df.at[t, "G_BREAKOUT_last_date"]) if br else ""
L1092         pb = bool(feature_df.at[t, "G_PULLBACK_recent_5d"]); dpb = str(feature_df.at[t, "G_PULLBACK_last_date"]) if pb else ""
L1093         if   br and not pb: return f"{t}ï¼ˆãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®š {dbr}ï¼‰"
L1094         elif pb and not br: return f"{t}ï¼ˆæŠ¼ã—ç›®åç™º {dpb}ï¼‰"
L1095         elif br and pb:     return f"{t}ï¼ˆãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®š {dbr}ï¼æŠ¼ã—ç›®åç™º {dpb}ï¼‰"
L1096     except Exception:
L1097         pass
L1098     return t
L1099
L1100 # === ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å¯è¦–åŒ–ï¼šG/Då…±é€šãƒ•ãƒ­ãƒ¼ï¼ˆå‡ºåŠ›ã¯ä¸å¤‰ï¼‰ ===
L1101
L1102 def io_build_input_bundle() -> InputBundle:
L1103     """
L1104     æ—¢å­˜ã®ã€ãƒ‡ãƒ¼ã‚¿å–å¾—â†’å‰å‡¦ç†ã€ã‚’å®Ÿè¡Œã—ã€InputBundle ã‚’è¿”ã™ã€‚
L1105     å‡¦ç†å†…å®¹ãƒ»åˆ—åãƒ»ä¸¸ã‚ãƒ»ä¾‹å¤–ãƒ»ãƒ­ã‚°æ–‡è¨€ã¯ç¾è¡Œã©ãŠã‚Šï¼ˆå¤‰æ›´ç¦æ­¢ï¼‰ã€‚
L1106     """
L1107     state = Input(cand=cand, exist=exist, bench=bench, price_max=CAND_PRICE_MAX, finnhub_api_key=FINNHUB_API_KEY).prepare_data()
L1108     return InputBundle(cand=state["cand"], tickers=state["tickers"], bench=bench, data=state["data"], px=state["px"], spx=state["spx"], tickers_bulk=state["tickers_bulk"], info=state["info"], eps_df=state["eps_df"], fcf_df=state["fcf_df"], returns=state["returns"])
L1109
L1110 def run_group(sc: Scorer, group: str, inb: InputBundle, cfg: PipelineConfig,
L1111               n_target: int) -> tuple[list, float, float, float]:
L1112     """
L1113     G/Dã‚’åŒä¸€æ‰‹é †ã§å‡¦ç†ï¼šæ¡ç‚¹â†’ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼â†’é¸å®šï¼ˆç›¸é–¢ä½æ¸›è¾¼ã¿ï¼‰ã€‚
L1114     æˆ»ã‚Šå€¤ï¼š(pick, avg_res_corr, sum_score, objective)
L1115     JSONä¿å­˜ã¯æ—¢å­˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆã‚­ãƒ¼åãƒ»ä¸¸ã‚æ¡ãƒ»é †åºï¼‰ã‚’è¸è¥²ã€‚
L1116     """
L1117     sc.cfg = cfg
L1118
L1119     if hasattr(sc, "score_build_features"):
L1120         feat = sc.score_build_features(inb)
L1121         if not hasattr(sc, "_feat_logged"):
L1122             T.log("features built (scorer)")
L1123             sc._feat_logged = True
L1124         agg = sc.score_aggregate(feat, group, cfg) if hasattr(sc, "score_aggregate") else feat
L1125     else:
L1126         fb = sc.aggregate_scores(inb, cfg)
L1127         if not hasattr(sc, "_feat_logged"):
L1128             T.log("features built (scorer)")
L1129             sc._feat_logged = True
L1130         sc._feat = fb
L1131         agg = fb.g_score if group == "G" else fb.d_score_all
L1132         if group == "D" and hasattr(fb, "df"):
L1133             agg = agg[fb.df['BETA'] < D_BETA_MAX]
L1134
L1135     if hasattr(sc, "filter_candidates"):
L1136         agg = agg[sc.filter_candidates(inb, agg, group, cfg)]
L1137
L1138     selector = Selector()
L1139     if hasattr(sc, "select_diversified"):
L1140         pick, avg_r, sum_sc, obj = sc.select_diversified(agg, group, cfg, n_target,
L1141             selector=selector, prev_tickers=None,
L1142             corrM=cfg.drrs.corrM, shrink=cfg.drrs.shrink,
L1143             cross_mu=cfg.drrs.cross_mu_gd)
L1144     else:
L1145         if group == "G":
L1146             init = agg.nlargest(min(cfg.drrs.corrM, len(agg))).index.tolist()
L1147             res = selector.select_bucket_drrs(returns_df=inb.returns, score_ser=agg, pool_tickers=init, k=n_target,
L1148                 n_pc=cfg.drrs.G.get("n_pc", 3), gamma=cfg.drrs.G.get("gamma", 1.2),
L1149                 lam=cfg.drrs.G.get("lam", 0.68),
L1150                 lookback=cfg.drrs.G.get("lookback", 252),
L1151                 shrink=cfg.drrs.shrink, g_fixed_tickers=None, mu=0.0)
L1152         else:
L1153             init = agg.nlargest(min(cfg.drrs.corrM, len(agg))).index.tolist()
L1154             g_fixed = getattr(sc, "_top_G", None)
L1155             res = selector.select_bucket_drrs(returns_df=inb.returns, score_ser=agg, pool_tickers=init, k=n_target,
L1156                 n_pc=cfg.drrs.D.get("n_pc", 4), gamma=cfg.drrs.D.get("gamma", 0.8),
L1157                 lam=cfg.drrs.D.get("lam", 0.85),
L1158                 lookback=cfg.drrs.D.get("lookback", 504),
L1159                 shrink=cfg.drrs.shrink, g_fixed_tickers=g_fixed,
L1160                 mu=cfg.drrs.cross_mu_gd)
L1161         pick = res["tickers"]; avg_r = res["avg_res_corr"]
L1162         sum_sc = res["sum_score"]; obj = res["objective"]
L1163         if group == "D":
L1164             _, pick = _disjoint_keepG(getattr(sc, "_top_G", []), pick, init)
L1165             T.log("selection finalized (G/D)")
L1166     try:
L1167         inc = [t for t in exist if t in agg.index]
L1168         pick = _sticky_keep_current(
L1169             agg=agg, pick=pick, incumbents=inc, n_target=n_target,
L1170             delta_z=SWAP_DELTA_Z, keep_buffer=SWAP_KEEP_BUFFER
L1171         )
L1172     except Exception as _e:
L1173         print(f"[warn] sticky_keep_current skipped: {str(_e)}")
L1174     # --- Near-Miss: æƒœã—ãã‚‚é¸ã°ã‚Œãªã‹ã£ãŸä¸Šä½10ã‚’ä¿æŒï¼ˆSlackè¡¨ç¤ºç”¨ï¼‰ ---
L1175     # 5) Near-Miss ã¨æœ€çµ‚é›†è¨ˆSeriesã‚’ä¿æŒï¼ˆè¡¨ç¤ºå°‚ç”¨ã€‚è¨ˆç®—ã¸å½±éŸ¿ãªã—ï¼‰
L1176     try:
L1177         pool = agg.drop(index=[t for t in pick if t in agg.index], errors="ignore")
L1178         near10 = list(pool.sort_values(ascending=False).head(10).index)
L1179         setattr(sc, f"_near_{group}", near10)
L1180         setattr(sc, f"_agg_{group}", agg)
L1181     except Exception:
L1182         pass
L1183
L1184     if group == "D":
L1185         T.log("save done")
L1186     if group == "G":
L1187         sc._top_G = pick
L1188     return pick, avg_r, sum_sc, obj
L1189
L1190 def run_pipeline() -> SelectionBundle:
L1191     """
L1192     G/Då…±é€šãƒ•ãƒ­ãƒ¼ã®å…¥å£ã€‚I/Oã¯ã“ã“ã ã‘ã§å®Ÿæ–½ã—ã€è¨ˆç®—ã¯Scorerã«å§”è­²ã€‚
L1193     Slackæ–‡è¨€ãƒ»ä¸¸ã‚ãƒ»é †åºã¯æ—¢å­˜ã® Output ã‚’ç”¨ã„ã¦å¤‰æ›´ã—ãªã„ã€‚
L1194     """
L1195     inb = io_build_input_bundle()
L1196     cfg = PipelineConfig(
L1197         weights=WeightsConfig(g=g_weights, d=D_weights),
L1198         drrs=DRRSParams(
L1199             corrM=corrM, shrink=DRRS_SHRINK,
L1200             G=DRRS_G, D=DRRS_D, cross_mu_gd=CROSS_MU_GD
L1201         ),
L1202         price_max=CAND_PRICE_MAX,
L1203         debug_mode=debug_mode
L1204     )
L1205     sc = Scorer()
L1206     top_G, avgG, sumG, objG = run_group(sc, "G", inb, cfg, N_G)
L1207     poolG = list(getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False).index)
L1208     alpha = Scorer.spx_to_alpha(inb.spx)
L1209     sectors = {t:(inb.info.get(t,{}).get("sector") or "U") for t in poolG}; scores = {t:Scorer.g_score.get(t,0.0) for t in poolG}
L1210     top_G = Scorer.pick_top_softcap(scores, sectors, N=N_G, cap=2, alpha=alpha, hard=5)
L1211     sc._top_G = top_G
L1212     try:
L1213         aggG = getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False)
L1214         sc._near_G = [t for t in aggG.index if t not in set(top_G)][:10]
L1215     except Exception:
L1216         pass
L1217     base = sum(Scorer.g_score.get(t,0.0) for t in poolG[:N_G])
L1218     effs = sum(Scorer.g_score.get(t,0.0) for t in top_G)
L1219     print(f"[soft_cap2] score_cost={(base-effs)/max(1e-9,abs(base)):.2%}, alpha={alpha:.3f}")
L1220     top_D, avgD, sumD, objD = run_group(sc, "D", inb, cfg, N_D)
L1221     fb = getattr(sc, "_feat", None)
L1222     near_G = getattr(sc, "_near_G", [])
L1223     selected12 = list(top_G)
L1224     df = fb.df if fb is not None else pd.DataFrame()
L1225     guni = _infer_g_universe(df, selected12, near_G)
L1226     try:
L1227         fire_recent = [t for t in guni
L1228                        if (str(df.at[t, "G_BREAKOUT_recent_5d"]) == "True") or
L1229                           (str(df.at[t, "G_PULLBACK_recent_5d"]) == "True")]
L1230     except Exception: fire_recent = []
L1231
L1232     lines = [
L1233         "ã€Gæ ãƒ¬ãƒãƒ¼ãƒˆï½œé€±æ¬¡ãƒ¢ãƒ‹ã‚¿ï¼ˆç›´è¿‘5å–¶æ¥­æ—¥ï¼‰ã€‘",
L1234         "ã€å‡¡ä¾‹ã€‘ğŸ”¥=ç›´è¿‘5å–¶æ¥­æ—¥å†…ã«ã€Œãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®šã€ã¾ãŸã¯ã€ŒæŠ¼ã—ç›®åç™ºã€ã‚’æ¤œçŸ¥",
L1235         f"é¸å®š{N_G}: {', '.join(_fmt_with_fire_mark(selected12, df))}" if selected12 else f"é¸å®š{N_G}: ãªã—",
L1236         f"æ¬¡ç‚¹10: {', '.join(_fmt_with_fire_mark(near_G, df))}" if near_G else "æ¬¡ç‚¹10: ãªã—",]
L1237
L1238     if fire_recent:
L1239         fire_list = ", ".join([_label_recent_event(t, df) for t in fire_recent])
L1240         lines.append(f"éå»5å–¶æ¥­æ—¥ã®æ¤œçŸ¥: {fire_list}")
L1241     else:
L1242         lines.append("éå»5å–¶æ¥­æ—¥ã®æ¤œçŸ¥: ãªã—")
L1243
L1244     try:
L1245         webhook = os.environ.get("SLACK_WEBHOOK_URL", "")
L1246         if webhook:
L1247             requests.post(webhook, json={"text": "\n".join([s for s in lines if s != ""])}, timeout=10)
L1248     except Exception:
L1249         pass
L1250
L1251     out = Output()
L1252     # è¡¨ç¤ºå´ã‹ã‚‰é¸å®šæ™‚ã®é›†è¨ˆã¸ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†ã«ä¿æŒï¼ˆè¡¨ç¤ºå°‚ç”¨ãƒ»å‰¯ä½œç”¨ãªã—ï¼‰
L1253     try: out._sc = sc
L1254     except Exception: pass
L1255     if hasattr(sc, "_feat"):
L1256         try:
L1257             fb = sc._feat
L1258             out.miss_df = fb.missing_logs
L1259             out.display_results(
L1260                 exist=exist,
L1261                 bench=bench,
L1262                 df_z=fb.df_z,
L1263                 g_score=fb.g_score,
L1264                 d_score_all=fb.d_score_all,
L1265                 init_G=top_G,
L1266                 init_D=top_D,
L1267                 top_G=top_G,
L1268                 top_D=top_D,
L1269                 df_full_z=getattr(fb, "df_full_z", None),
L1270                 prev_G=getattr(sc, "_prev_G", exist),
L1271                 prev_D=getattr(sc, "_prev_D", exist),
L1272             )
L1273         except Exception:
L1274             pass
L1275     out.notify_slack()
L1276     sb = SelectionBundle(resG={"tickers": top_G, "avg_res_corr": avgG,
L1277               "sum_score": sumG, "objective": objG},
L1278         resD={"tickers": top_D, "avg_res_corr": avgD,
L1279               "sum_score": sumD, "objective": objD},
L1280         top_G=top_G, top_D=top_D, init_G=top_G, init_D=top_D)
L1281
L1282     # --- Low Score Candidates (GSC+DSC bottom 10) : send before debug dump ---
L1283     try:
L1284         _low_df = (pd.DataFrame({"GSC": fb.g_score, "DSC": fb.d_score_all})
L1285               .assign(G_plus_D=lambda x: x["GSC"] + x["DSC"])
L1286               .sort_values("G_plus_D")
L1287               .head(10)
L1288               .round(3))
L1289         low_msg = "Low Score Candidates (GSC+DSC bottom 10)\n" + _low_df.to_string(index=True, index_names=False)
L1290         _post_slack({"text": f"```{low_msg}```"})
L1291     except Exception as _e:
L1292         _post_slack({"text": f"```Low Score Candidates: ä½œæˆå¤±æ•—: {_e}```"})
L1293
L1294     return sb
L1295
L1296 if __name__ == "__main__":
L1297     run_pipeline()
```

## <scorer.py>
```text
L1 # scorer.py
L2 # kawatest
L3 # =============================================================================
L4 # Scorer: ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼/æŒ‡æ¨™ã®ç”Ÿæˆã¨åˆæˆã‚¹ã‚³ã‚¢ç®—å‡ºã‚’æ‹…ã†ç´”ç²‹å±¤
L5 #
L6 # ã€ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã ã‘èª­ã‚ã°åˆ†ã‹ã‚‹ãƒã‚¤ãƒ³ãƒˆã€‘
L7 # - å…¥åŠ›(InputBundle)ã¯ã€Œä¾¡æ ¼/å‡ºæ¥é«˜/ãƒ™ãƒ³ãƒ/åŸºæœ¬æƒ…å ±/EPS/FCF/ãƒªã‚¿ãƒ¼ãƒ³ã€ã‚’å«ã‚€DTO
L8 # - å‡ºåŠ›(FeatureBundle)ã¯ã€Œrawç‰¹å¾´é‡ dfã€ã€Œæ¨™æº–åŒ– df_zã€ã€ŒG/D ã‚¹ã‚³ã‚¢ã€ã€Œæ¬ æãƒ­ã‚°ã€
L9 # - é‡ã¿ç­‰ã®ã‚³ãƒ³ãƒ•ã‚£ã‚°(PipelineConfig)ã¯ factor ã‹ã‚‰æ¸¡ã™ï¼ˆcfg å¿…é ˆï¼‰
L10 # - æ—§ã‚«ãƒ©ãƒ åã¯ Scorer å†…ã§è‡ªå‹•ãƒªãƒãƒ¼ãƒ ã—ã¦å—ã‘å…¥ã‚Œï¼ˆå¾Œæ–¹äº’æ›ï¼‰
L11 #   ä¾‹) eps_ttm -> EPS_TTM, eps_q_recent -> EPS_Q_LastQ, fcf_ttm -> FCF_TTM
L12 #
L13 # ã€I/Oå¥‘ç´„ï¼ˆScorerãŒå‚ç…§ã™ã‚‹InputBundleãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰ã€‘
L14 #   - cand: List[str]    â€¦ å€™è£œéŠ˜æŸ„ï¼ˆå˜ä½“å®Ÿè¡Œã§ã¯æœªä½¿ç”¨ï¼‰
L15 #   - tickers: List[str] â€¦ å¯¾è±¡éŠ˜æŸ„ãƒªã‚¹ãƒˆ
L16 #   - bench: str         â€¦ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼ï¼ˆä¾‹ '^GSPC'ï¼‰
L17 #   - data: pd.DataFrame â€¦ yfinance downloadçµæœ ('Close','Volume' ç­‰ã®éšå±¤åˆ—)
L18 #   - px: pd.DataFrame   â€¦ data['Close'] ç›¸å½“ï¼ˆçµ‚å€¤ï¼‰
L19 #   - spx: pd.Series     â€¦ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®çµ‚å€¤
L20 #   - tickers_bulk: object         â€¦ yfinance.Tickers
L21 #   - info: Dict[str, dict]        â€¦ yfinance info per ticker
L22 #   - eps_df: pd.DataFrame         â€¦ å¿…é ˆåˆ—: EPS_TTM, EPS_Q_LastQï¼ˆæ—§åã‚‚å¯ï¼‰
L23 #   - fcf_df: pd.DataFrame         â€¦ å¿…é ˆåˆ—: FCF_TTMï¼ˆæ—§åã‚‚å¯ï¼‰
L24 #   - returns: pd.DataFrame        â€¦ px[tickers].pct_change() ç›¸å½“
L25 #
L26 # â€»å…¥å‡ºåŠ›ã®å½¢å¼ãƒ»ä¾‹å¤–æ–‡è¨€ã¯æ—¢å­˜å®Ÿè£…ã‚’å¤‰ãˆã¾ã›ã‚“ï¼ˆå®‰å…¨ãªçŸ­ç¸®ã®ã¿ï¼‰
L27 # =============================================================================
L28
L29 import json, logging, os, requests, sys, warnings
L30 import numpy as np
L31 import pandas as pd
L32 import yfinance as yf
L33 from typing import Any, TYPE_CHECKING
L34 from scipy.stats import zscore
L35 from datetime import datetime as _dt
L36
L37 DEBUG_SCOPE_STRICT = (
L38     os.getenv("DEBUG_SCOPE_STRICT", "false").strip().lower() == "true"
L39 )
L40
L41 FACTOR_COLUMNS = {
L42     "GRW": [
L43         "GROWTH_F",
L44         "GRW_FLEX_SCORE",
L45         "GRW_REV_YOY_Q",
L46         "GRW_REV_ACC_Q",
L47         "GRW_REV_QOQ",
L48         "GRW_REV_TTM2",
L49         "GRW_REV_YOY_Y",
L50         "GRW_PRICE_PROXY",
L51         "GRW_PATH",
L52     ],
L53     "MOM": ["MOM", "MOM_RAW", "MOM_P12M", "MOM_P6M", "MOM_P1M"],
L54     "VOL": ["VOL", "VOL_SD", "VOL_BETA"],
L55     "QUAL": ["QUAL", "ROE", "ROA", "FCF_MGN"],
L56     "VAL": ["VAL", "PE", "PB", "PS", "EVEBITDA"],
L57 }
L58
L59 if TYPE_CHECKING:
L60     from factor import PipelineConfig  # type: ignore  # å®Ÿè¡Œæ™‚importãªã—ï¼ˆå¾ªç’°å›é¿ï¼‰
L61
L62 logger = logging.getLogger(__name__)
L63
L64
L65 def _log(stage, msg):
L66     try:
L67         print(f"[DBG][{_dt.utcnow().isoformat(timespec='seconds')}Z][{stage}] {msg}")
L68     except Exception:
L69         print(f"[DBG][{stage}] {msg}")
L70
L71
L72 def _detect_used_cols(df, df_z):
L73     used = set()
L74
L75     try:
L76         import factor as _f
L77
L78         for k in getattr(_f, "g_weights", {}).keys():
L79             if k in df_z.columns:
L80                 used.add(k)
L81     except Exception:
L82         pass
L83
L84     try:
L85         import scorer as _sc
L86
L87         for k in getattr(_sc, "D_WEIGHTS_EFF", {}).keys():
L88             if k in df_z.columns:
L89                 used.add(k)
L90     except Exception:
L91         pass
L92
L93     for k in [
L94         "GROWTH_F",
L95         "MOM",
L96         "TRD",
L97         "VOL",
L98         "D_QAL",
L99         "D_YLD",
L100         "D_VOL_RAW",
L101         "D_TRD",
L102     ]:
L103         if k in df_z.columns:
L104             used.add(k)
L105
L106     grw_cols = [
L107         "GRW_PATH",
L108         "GRW_FLEX_SCORE",
L109         "GROWTH_F",
L110         "GRW_REV_YOY_Q",
L111         "GRW_REV_ACC_Q",
L112         "GRW_REV_QOQ",
L113         "GRW_REV_TTM2",
L114         "GRW_REV_YOY_Y",
L115         "GRW_PRICE_PROXY",
L116     ]
L117     for k in grw_cols:
L118         if (k in df.columns) or (k in df_z.columns):
L119             used.add(k)
L120
L121     for c in df_z.columns:
L122         if isinstance(c, str) and c.startswith("D_"):
L123             used.add(c)
L124
L125     num = df_z.select_dtypes(include=["number"])
L126     if not num.empty:
L127         var_top = num.var().sort_values(ascending=False).head(20).index.tolist()
L128         used.update(var_top)
L129
L130     return sorted(used)
L131
L132
L133 def _reorder_for_debug(df, df_z, factor_cols=FACTOR_COLUMNS):
L134     cols: list[str] = []
L135     for fac in ["GRW", "MOM", "VOL", "QUAL", "VAL"]:
L136         for c in factor_cols.get(fac, []):
L137             if c in getattr(df_z, "columns", []) or c in getattr(df, "columns", []):
L138                 cols.append(c)
L139     seen: set[str] = set()
L140     ordered: list[str] = []
L141     for c in cols:
L142         if c not in seen:
L143             ordered.append(c)
L144             seen.add(c)
L145     return ordered
L146
L147
L148 def dump_dfz_scoped(df, df_z, *, topk=20, logger=None):
L149     import numpy as np, pandas as pd, logging
L150
L151     lg = logger or logging.getLogger(__name__)
L152
L153     if not DEBUG_SCOPE_STRICT:
L154         dfz = df_z.copy()
L155         lg.info("DEBUG scope: disabled (showing ALL %d columns).", dfz.shape[1])
L156     else:
L157         rel = _detect_used_cols(df, df_z)
L158         dfz = df_z[[c for c in rel if c in df_z.columns]].copy()
L159         if dfz.shape[1] < 15:
L160             num = df_z.select_dtypes(include=["number"])
L161             add = []
L162             if not num.empty:
L163                 add = [
L164                     c
L165                     for c in num.var().sort_values(ascending=False).head(30).index
L166                     if c not in dfz.columns
L167                 ]
L168                 if dfz.shape[1] < 15:
L169                     add = add[: 15 - dfz.shape[1]]
L170                 else:
L171                     add = []
L172             dfz = pd.concat([dfz, df_z[add]], axis=1)
L173             lg.info("DEBUG scope too small â†’ fallback add %d cols", len(add))
L174         excluded = [c for c in df_z.columns if c not in dfz.columns]
L175         lg.info(
L176             "DEBUG scope: %d relevant cols kept, %d excluded.",
L177             dfz.shape[1],
L178             len(excluded),
L179         )
L180
L181     nan_top = dfz.isna().sum().sort_values(ascending=False).head(topk)
L182     lg.info("scorer:NaN columns (top%d):", topk)
L183     for c, n in nan_top.items():
L184         lg.info("%s\t%d", c, int(n))
L185
L186     num_dfz = dfz.select_dtypes(include=["number"])
L187     if not num_dfz.empty:
L188         ztop = (num_dfz == 0).mean().sort_values(ascending=False).head(topk)
L189         lg.info("scorer:Zero-dominated columns (top%d):", topk)
L190         for c, r in ztop.items():
L191             lg.info("%s\t%.2f%%", c, 100.0 * float(r))
L192
L193     return dfz
L194
L195
L196 def save_factor_debug_csv(df, df_z, path="out/factor_debug_latest.csv"):
L197     import os, pandas as pd, logging
L198
L199     lg = logging.getLogger(__name__)
L200     try:
L201         cols = _reorder_for_debug(df, df_z)
L202         dump = pd.DataFrame(index=df.index)
L203         for c in cols:
L204             if c in getattr(df, "columns", []):
L205                 dump[c] = df[c]
L206             if c in getattr(df_z, "columns", []):
L207                 dump[c] = df_z[c]
L208         dump.reset_index(names=["symbol"], inplace=True)
L209         if path:
L210             dirpath = os.path.dirname(path) or "."
L211             os.makedirs(dirpath, exist_ok=True)
L212             dump.to_csv(path, index=False)
L213         lg.info(
L214             "factor debug CSV saved: %s (cols=%d rows=%d)",
L215             path,
L216             dump.shape[1],
L217             dump.shape[0],
L218         )
L219     except Exception as e:
L220         lg.warning("factor debug CSV failed: %s", e)
L221
L222
L223 def log_grw_stats(df, df_z, logger):
L224     import numpy as np, pandas as pd
L225
L226     try:
L227         s = pd.to_numeric(df.get("GRW_FLEX_SCORE", pd.Series(dtype=float)), errors="coerce")
L228         z = pd.to_numeric(df_z.get("GROWTH_F", pd.Series(dtype=float)), errors="coerce")
L229         if s.size:
L230             logger.info(
L231                 "GRW raw stats: n=%d, median=%.3f, mad=%.3f, std=%.3f",
L232                 s.count(),
L233                 np.nanmedian(s),
L234                 np.nanmedian(np.abs(s - np.nanmedian(s))),
L235                 np.nanstd(s),
L236             )
L237         if z.size and not z.dropna().empty:
L238             clip_hi = float((z >= 2.95).mean() * 100.0)
L239             clip_lo = float((z <= -2.95).mean() * 100.0)
L240             logger.info(
L241                 "GRW z stats: min=%.2f, p25=%.2f, med=%.2f, p75=%.2f, max=%.2f, clipped_hi=%.1f%%, clipped_lo=%.1f%%",
L242                 np.nanmin(z),
L243                 np.nanpercentile(z.dropna(), 25),
L244                 np.nanmedian(z),
L245                 np.nanpercentile(z.dropna(), 75),
L246                 np.nanmax(z),
L247                 clip_hi,
L248                 clip_lo,
L249             )
L250         if "GRW_PATH" in getattr(df, "columns", []):
L251             logger.info(
L252                 "GRW path breakdown: %s",
L253                 df["GRW_PATH"].value_counts(dropna=False).to_dict(),
L254             )
L255     except Exception as e:
L256         logger.warning("GRW stats log failed: %s", e)
L257
L258
L259 def _grw_record_to_df(t: str, info_t: dict, df):
L260     if not isinstance(df, pd.DataFrame):
L261         return
L262     raw_parts = info_t.get("DEBUG_GRW_PARTS") if isinstance(info_t, dict) else None
L263     parts: dict[str, Any] = {}
L264     if isinstance(raw_parts, str):
L265         try:
L266             parts = json.loads(raw_parts)
L267         except Exception:
L268             parts = {}
L269     elif isinstance(raw_parts, dict):
L270         parts = raw_parts
L271     path = info_t.get("DEBUG_GRW_PATH") if isinstance(info_t, dict) else None
L272     score = info_t.get("GRW_SCORE") if isinstance(info_t, dict) else None
L273
L274     def _part_value(key: str):
L275         value = parts.get(key) if isinstance(parts, dict) else None
L276         if value is None:
L277             return np.nan
L278         try:
L279             return float(value)
L280         except Exception:
L281             return np.nan
L282
L283     df.loc[t, "GRW_PATH"] = path
L284     df.loc[t, "GRW_FLEX_SCORE"] = np.nan if score is None else float(score)
L285     df.loc[t, "GRW_REV_YOY_Q"] = _part_value("rev_yoy_q")
L286     df.loc[t, "GRW_REV_ACC_Q"] = _part_value("rev_acc_q")
L287     df.loc[t, "GRW_REV_QOQ"] = _part_value("rev_qoq")
L288     df.loc[t, "GRW_REV_TTM2"] = _part_value("rev_ttm2")
L289     df.loc[t, "GRW_REV_YOY_Y"] = _part_value("rev_yoy_y")
L290     df.loc[t, "GRW_PRICE_PROXY"] = _part_value("price_proxy")
L291
L292 # ---- Dividend Helpers -------------------------------------------------------
L293 def _last_close(t, price_map=None):
L294     if price_map and (c := price_map.get(t)) is not None: return float(c)
L295     try:
L296         h = yf.Ticker(t).history(period="5d")["Close"].dropna()
L297         return float(h.iloc[-1]) if len(h) else np.nan
L298     except Exception:
L299         return np.nan
L300
L301 def _ttm_div_sum(t, lookback_days=400):
L302     try:
L303         div = yf.Ticker(t).dividends
L304         if div is None or len(div) == 0: return 0.0
L305         cutoff = pd.Timestamp.utcnow().tz_localize(None) - pd.Timedelta(days=lookback_days)
L306         ttm = float(div[div.index.tz_localize(None) >= cutoff].sum())
L307         return ttm if ttm > 0 else float(div.tail(4).sum())
L308     except Exception:
L309         return 0.0
L310
L311 def ttm_div_yield_portfolio(tickers, price_map=None):
L312     ys = [(lambda c, s: (s/c) if (np.isfinite(c) and c>0 and s>0) else 0.0)(_last_close(t, price_map), _ttm_div_sum(t)) for t in tickers]
L313     return float(np.mean(ys)) if ys else 0.0
L314
L315 # ---- ç°¡æ˜“ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆå®‰å…¨ãªçŸ­ç¸®ã®ã¿ï¼‰ -----------------------------------
L316 def winsorize_s(s: pd.Series, p=0.02):
L317     if s is None or s.dropna().empty: return s
L318     lo, hi = np.nanpercentile(s.astype(float), [100*p, 100*(1-p)]); return s.clip(lo, hi)
L319
L320 def robust_z(s: pd.Series, p=0.02):
L321     s2 = winsorize_s(s,p); return np.nan_to_num(zscore(s2.fillna(s2.mean())))
L322
L323 def robust_z_keepnan(s: pd.Series) -> pd.Series:
L324     """robust_z variant that preserves NaNs and falls back to rank-z when needed."""
L325     if s is None:
L326         return pd.Series(dtype=float)
L327     v = pd.to_numeric(s, errors="coerce")
L328     m = np.nanmedian(v)
L329     mad = np.nanmedian(np.abs(v - m))
L330     z = (v - m) / (1.4826 * mad + 1e-9)
L331     if np.nanstd(z) < 1e-9:
L332         r = v.rank(method="average", na_option="keep")
L333         z = (r - np.nanmean(r)) / (np.nanstd(r) + 1e-9)
L334     return pd.Series(z, index=v.index, dtype=float)
L335
L336
L337 def _dump_dfz(
L338     df: pd.DataFrame,
L339     df_z: pd.DataFrame,
L340     debug_mode: bool,
L341     max_rows: int = 400,
L342     ndigits: int = 3,
L343 ) -> None:
L344     """df_z ã‚’ System log(INFO) ã¸ãƒ€ãƒ³ãƒ—ã™ã‚‹ç°¡æ½”ãªãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£."""
L345
L346     if not debug_mode:
L347         return
L348     try:
L349         dfz_scoped = dump_dfz_scoped(df, df_z, topk=20, logger=logger)
L350         ordered = _reorder_for_debug(df, df_z)
L351         rel_set = set(dfz_scoped.columns)
L352         view_cols = [c for c in ordered if c in rel_set]
L353         if not view_cols:
L354             view_cols = list(dfz_scoped.columns)
L355         view = dfz_scoped[view_cols].copy()
L356         view = view.apply(
L357             lambda s: s.round(ndigits)
L358             if getattr(getattr(s, "dtype", None), "kind", "") in ("f", "i")
L359             else s
L360         )
L361         if len(view) > max_rows:
L362             view = view.iloc[:max_rows]
L363
L364         logger.info("===== DF_Z DUMP START =====")
L365         logger.info("\n%s", view.to_string(max_rows=None, max_cols=None))
L366         logger.info("===== DF_Z DUMP END =====")
L367     except Exception as exc:
L368         logger.warning("df_z dump failed: %s", exc)
L369
L370 def _safe_div(a, b):
L371     try: return np.nan if (b is None or float(b)==0 or pd.isna(b)) else float(a)/float(b)
L372     except Exception: return np.nan
L373
L374 def _safe_last(series: pd.Series, default=np.nan):
L375     try: return float(series.iloc[-1])
L376     except Exception: return default
L377
L378
L379 def _ensure_series(x):
L380     if x is None:
L381         return pd.Series(dtype=float)
L382     if isinstance(x, pd.Series):
L383         return x.dropna()
L384     if isinstance(x, (list, tuple)):
L385         if len(x) and isinstance(x[0], (tuple, list)) and len(x[0]) == 2:
L386             dt = pd.to_datetime([d for d, _ in x], errors="coerce")
L387             v = pd.to_numeric([_v for _, _v in x], errors="coerce")
L388             return pd.Series(v, index=dt).dropna()
L389         return pd.Series(pd.to_numeric(list(x), errors="coerce")).dropna()
L390     try:
L391         return pd.Series(x).dropna()
L392     except Exception:
L393         return pd.Series(dtype=float)
L394
L395
L396 def _to_quarterly(s: pd.Series) -> pd.Series:
L397     if s.empty or not isinstance(s.index, pd.DatetimeIndex):
L398         return s
L399     return s.resample("Q").last().dropna()
L400
L401
L402 def _ttm_yoy_from_quarterly(qs: pd.Series) -> pd.Series:
L403     if qs is None or qs.empty:
L404         return pd.Series(dtype=float)
L405     ttm = qs.rolling(4, min_periods=2).sum()
L406     yoy = ttm.pct_change(4)
L407     return yoy
L408
L409
L410 def _nz(x) -> float:
L411     if x is None:
L412         return 0.0
L413     try:
L414         value = float(x)
L415     except Exception:
L416         return 0.0
L417     if not np.isfinite(value):
L418         return 0.0
L419     return value
L420
L421
L422 def _winsor(x, lo=-2.0, hi=2.0) -> float:
L423     v = _nz(x)
L424     if v < lo:
L425         return float(lo)
L426     if v > hi:
L427         return float(hi)
L428     return float(v)
L429
L430
L431 def _round_debug(x, ndigits: int = 4):
L432     try:
L433         value = float(x)
L434     except Exception:
L435         return None
L436     if not np.isfinite(value):
L437         return None
L438     return round(value, ndigits)
L439
L440
L441 def _calc_grw_flexible(
L442     ticker: str,
L443     info_entry: dict | None,
L444     close_series: pd.Series | None,
L445     volume_series: pd.Series | None,
L446 ):
L447     info_entry = info_entry if isinstance(info_entry, dict) else {}
L448
L449     s_rev_q = _ensure_series(info_entry.get("SEC_REV_Q_SERIES"))
L450     s_eps_q = _ensure_series(info_entry.get("SEC_EPS_Q_SERIES"))
L451     s_rev_y = _ensure_series(info_entry.get("SEC_REV_Y_SERIES"))
L452
L453     nQ = int(getattr(s_rev_q, "size", 0))
L454     nY = int(getattr(s_rev_y, "size", 0))
L455
L456     parts: dict[str, Any] = {"nQ": nQ, "nY": nY}
L457     path = "NONE"
L458     w = 0.0
L459
L460     def _valid_ratio(a, b):
L461         try:
L462             na, nb = float(a), float(b)
L463         except Exception:
L464             return None
L465         if not np.isfinite(na) or not np.isfinite(nb) or nb == 0:
L466             return None
L467         return na, nb
L468
L469     def yoy_q(series: pd.Series) -> float | None:
L470         s = _ensure_series(series)
L471         if s.empty:
L472             return None
L473         s = s.sort_index()
L474         if isinstance(s.index, pd.DatetimeIndex):
L475             last_idx = s.index[-1]
L476             window_start = last_idx - pd.DateOffset(months=15)
L477             window_end = last_idx - pd.DateOffset(months=9)
L478             candidates = s.loc[(s.index >= window_start) & (s.index <= window_end)]
L479             if candidates.empty:
L480                 candidates = s.loc[s.index <= window_end]
L481             if candidates.empty:
L482                 return None
L483             v1 = candidates.iloc[-1]
L484             v0 = s.iloc[-1]
L485         else:
L486             if s.size < 5:
L487                 return None
L488             v0 = s.iloc[-1]
L489             v1 = s.iloc[-5]
L490         pair = _valid_ratio(v0, v1)
L491         if pair is None:
L492             return None
L493         a, b = pair
L494         return float(a / b - 1.0)
L495
L496     def qoq(series: pd.Series) -> float | None:
L497         s = _ensure_series(series)
L498         if s.size < 2:
L499             return None
L500         s = s.sort_index()
L501         v0, v1 = s.iloc[-1], s.iloc[-2]
L502         pair = _valid_ratio(v0, v1)
L503         if pair is None:
L504             return None
L505         a, b = pair
L506         return float(a / b - 1.0)
L507
L508     def ttm_delta(series: pd.Series) -> float | None:
L509         s = _ensure_series(series)
L510         if s.size < 2:
L511             return None
L512         s = s.sort_index()
L513         k = int(min(4, s.size))
L514         cur_slice = s.iloc[-k:]
L515         prev_slice = s.iloc[:-k]
L516         if prev_slice.empty:
L517             return None
L518         prev_k = int(min(k, prev_slice.size))
L519         cur_sum = float(cur_slice.sum())
L520         prev_sum = float(prev_slice.iloc[-prev_k:].sum())
L521         pair = _valid_ratio(cur_sum, prev_sum)
L522         if pair is None:
L523             return None
L524         a, b = pair
L525         return float(a / b - 1.0)
L526
L527     def yoy_y(series: pd.Series) -> float | None:
L528         s = _ensure_series(series)
L529         if s.size < 2:
L530             return None
L531         s = s.sort_index()
L532         v0, v1 = s.iloc[-1], s.iloc[-2]
L533         pair = _valid_ratio(v0, v1)
L534         if pair is None:
L535             return None
L536         a, b = pair
L537         return float(a / b - 1.0)
L538
L539     def price_proxy_growth() -> float | None:
L540         if not isinstance(close_series, pd.Series):
L541             return None
L542         close = close_series.sort_index().dropna()
L543         if close.empty:
L544             return None
L545         hh_window = int(min(126, len(close)))
L546         if hh_window < 20:
L547             return None
L548         hh = close.rolling(hh_window).max().iloc[-1]
L549         prox = None
L550         if np.isfinite(hh) and hh > 0:
L551             prox = float(close.iloc[-1] / hh)
L552         rs6 = None
L553         if len(close) >= 63:
L554             rs6 = float(close.pct_change(63).iloc[-1])
L555         rs12 = None
L556         if len(close) >= 126:
L557             rs12 = float(close.pct_change(126).iloc[-1])
L558         vexp = None
L559         if isinstance(volume_series, pd.Series):
L560             vol = volume_series.reindex(close.index).dropna()
L561             if len(vol) >= 50:
L562                 v20 = vol.rolling(20).mean().iloc[-1]
L563                 v50 = vol.rolling(50).mean().iloc[-1]
L564                 if np.isfinite(v20) and np.isfinite(v50) and v50 > 0:
L565                     vexp = float(v20 / v50 - 1.0)
L566         prox = 0.0 if prox is None or not np.isfinite(prox) else prox
L567         rs6 = 0.0 if rs6 is None or not np.isfinite(rs6) else rs6
L568         rs12 = 0.0 if rs12 is None or not np.isfinite(rs12) else rs12
L569         vexp = 0.0 if vexp is None or not np.isfinite(vexp) else vexp
L570         return 0.5 * prox + 0.3 * rs6 + 0.2 * rs12 + 0.2 * vexp
L571
L572     price_alt = price_proxy_growth() or 0.0
L573     core = 0.0
L574     core_raw = 0.0
L575     price_raw = price_alt
L576
L577     if nQ >= 5:
L578         path = "P5"
L579         yq = yoy_q(s_rev_q)
L580         parts["rev_yoy_q"] = yq
L581         tmp_prev = s_rev_q.iloc[:-1] if s_rev_q.size > 1 else s_rev_q
L582         acc = None
L583         if tmp_prev.size >= 5 and yq is not None:
L584             yq_prev = yoy_q(tmp_prev)
L585             if yq_prev is not None:
L586                 acc = float(yq - yq_prev)
L587         parts["rev_acc_q"] = acc
L588         eps_yoy = yoy_q(s_eps_q) if s_eps_q.size >= 5 else None
L589         parts["eps_yoy_q"] = eps_yoy
L590         eps_acc = None
L591         if eps_yoy is not None and s_eps_q.size > 5:
L592             eps_prev = s_eps_q.iloc[:-1]
L593             if eps_prev.size >= 5:
L594                 eps_prev_yoy = yoy_q(eps_prev)
L595                 if eps_prev_yoy is not None:
L596                     eps_acc = float(eps_yoy - eps_prev_yoy)
L597         parts["eps_acc_q"] = eps_acc
L598         w = 1.0
L599         core_raw = (
L600             0.60 * _nz(yq)
L601             + 0.20 * _nz(acc)
L602             + 0.15 * _nz(eps_yoy)
L603             + 0.05 * _nz(eps_acc)
L604         )
L605         price_alt = 0.0
L606     elif 2 <= nQ <= 4:
L607         path = "P24"
L608         rev_qoq = qoq(s_rev_q)
L609         rev_ttm2 = ttm_delta(s_rev_q)
L610         parts["rev_qoq"] = rev_qoq
L611         parts["rev_ttm2"] = rev_ttm2
L612         eps_qoq = qoq(s_eps_q) if s_eps_q.size >= 2 else None
L613         parts["eps_qoq"] = eps_qoq
L614         w = min(1.0, nQ / 5.0)
L615         core_raw = 0.6 * _nz(rev_qoq) + 0.3 * _nz(rev_ttm2) + 0.1 * _nz(eps_qoq)
L616     else:
L617         path = "P1Y"
L618         rev_yoy_y = yoy_y(s_rev_y) if nY >= 2 else None
L619         parts["rev_yoy_y"] = rev_yoy_y
L620         w = 0.6 * min(1.0, nY / 3.0) if nY >= 2 else 0.4
L621         core_raw = _nz(rev_yoy_y)
L622         if nQ <= 1 and nY < 2 and price_alt == 0.0:
L623             price_alt = price_proxy_growth() or 0.0
L624
L625     core = _winsor(core_raw, lo=-1.5, hi=1.5)
L626     price_alt = _winsor(price_alt, lo=-1.5, hi=1.5)
L627     grw = _winsor(w * core + (1.0 - w) * (0.5 * _nz(price_alt)), lo=-2.0, hi=2.0)
L628
L629     parts.update(
L630         {
L631             "core_raw": core_raw,
L632             "core": core,
L633             "price_proxy_raw": price_raw,
L634             "price_proxy": price_alt,
L635             "weight": w,
L636             "score": grw,
L637         }
L638     )
L639
L640     parts_out: dict[str, Any] = {
L641         "nQ": nQ,
L642         "nY": nY,
L643     }
L644     for key, value in parts.items():
L645         if key in ("nQ", "nY"):
L646             continue
L647         rounded = _round_debug(value)
L648         parts_out[key] = rounded
L649
L650     info_entry["DEBUG_GRW_PATH"] = path
L651     info_entry["DEBUG_GRW_PARTS"] = json.dumps(parts_out, ensure_ascii=False, sort_keys=True)
L652     info_entry["GRW_SCORE"] = grw
L653     info_entry["GRW_WEIGHT"] = w
L654     info_entry["GRW_CORE"] = core
L655     info_entry["GRW_PRICE_PROXY"] = price_alt
L656
L657     return {
L658         "score": grw,
L659         "path": path,
L660         "parts": info_entry["DEBUG_GRW_PARTS"],
L661         "weight": w,
L662         "core": core,
L663         "price_proxy": price_alt,
L664     }
L665
L666
L667 D_WEIGHTS_EFF = None  # å‡ºåŠ›è¡¨ç¤ºäº’æ›ã®ãŸã‚
L668
L669
L670 def _scalar(v):
L671     """å˜ä¸€ã‚»ãƒ«ä»£å…¥ç”¨ã«å€¤ã‚’ã‚¹ã‚«ãƒ©ãƒ¼ã¸æ­£è¦åŒ–ã™ã‚‹ã€‚
L672
L673     - pandas Series -> .iloc[-1]ï¼ˆæœ€å¾Œã‚’æ¡ç”¨ï¼‰
L674     - list/tuple/ndarray -> æœ€å¾Œã®è¦ç´ 
L675     - ãã‚Œä»¥å¤–          -> ãã®ã¾ã¾
L676     å–å¾—å¤±æ•—æ™‚ã¯ np.nan ã‚’è¿”ã™ã€‚
L677     """
L678     import numpy as _np
L679     import pandas as _pd
L680     try:
L681         if isinstance(v, _pd.Series):
L682             return v.iloc[-1] if len(v) else _np.nan
L683         if isinstance(v, (list, tuple, _np.ndarray)):
L684             return v[-1] if len(v) else _np.nan
L685         return v
L686     except Exception:
L687         return _np.nan
L688
L689
L690 # ---- Scorer æœ¬ä½“ -------------------------------------------------------------
L691 class Scorer:
L692     """
L693     - factor.py ã‹ã‚‰ã¯ `aggregate_scores(ib, cfg)` ã‚’å‘¼ã¶ã ã‘ã§OKã€‚
L694     - cfg ã¯å¿…é ˆï¼ˆfactor.PipelineConfig ã‚’æ¸¡ã™ï¼‰ã€‚
L695     - æ—§ã‚«ãƒ©ãƒ åã‚’è‡ªå‹•ãƒªãƒãƒ¼ãƒ ã—ã¦æ–°ã‚¹ã‚­ãƒ¼ãƒã«å¸åã—ã¾ã™ã€‚
L696     """
L697
L698     # === å…ˆé ­ã§æ—§â†’æ–°ã‚«ãƒ©ãƒ åãƒãƒƒãƒ—ï¼ˆç§»è¡Œç”¨ï¼‰ ===
L699     EPS_RENAME = {"eps_ttm":"EPS_TTM", "eps_q_recent":"EPS_Q_LastQ"}
L700     FCF_RENAME = {"fcf_ttm":"FCF_TTM"}
L701
L702     # === ã‚¹ã‚­ãƒ¼ãƒç°¡æ˜“ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€ä½é™ï¼‰ ===
L703     @staticmethod
L704     def _validate_ib_for_scorer(ib: Any):
L705         miss = [a for a in ["tickers","bench","data","px","spx","tickers_bulk","info","eps_df","fcf_df","returns"] if not hasattr(ib,a) or getattr(ib,a) is None]
L706         if miss: raise ValueError(f"InputBundle is missing required attributes for Scorer: {miss}")
L707         if any(c in ib.eps_df.columns for c in Scorer.EPS_RENAME): ib.eps_df.rename(columns=Scorer.EPS_RENAME, inplace=True)
L708         if any(c in ib.fcf_df.columns for c in Scorer.FCF_RENAME): ib.fcf_df.rename(columns=Scorer.FCF_RENAME, inplace=True)
L709         need_eps, need_fcf = {"EPS_TTM","EPS_Q_LastQ"},{"FCF_TTM"}
L710         if not need_eps.issubset(ib.eps_df.columns): raise ValueError(f"eps_df must contain columns {need_eps} (accepts old names via auto-rename). Got: {list(ib.eps_df.columns)}")
L711         if not need_fcf.issubset(ib.fcf_df.columns): raise ValueError(f"fcf_df must contain columns {need_fcf} (accepts old names via auto-rename). Got: {list(ib.fcf_df.columns)}")
L712
L713     # ----ï¼ˆScorerå°‚ç”¨ï¼‰ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ»æŒ‡æ¨™ç³» ----
L714     @staticmethod
L715     def trend(s: pd.Series):
L716         if len(s)<200: return np.nan
L717         sma50, sma150, sma200 = s.rolling(50).mean().iloc[-1], s.rolling(150).mean().iloc[-1], s.rolling(200).mean().iloc[-1]
L718         prev200, p = s.rolling(200).mean().iloc[-21], s.iloc[-1]
L719         lo_52 = s[-252:].min() if len(s)>=252 else s.min(); hi_52 = s[-252:].max() if len(s)>=252 else s.max()
L720         rng = (hi_52 - lo_52) if hi_52>lo_52 else np.nan
L721         clip = lambda x,lo,hi: (np.nan if pd.isna(x) else max(lo,min(hi,x)))
L722         a = clip(p/(s.rolling(50).mean().iloc[-1]) - 1, -0.5, 0.5)
L723         b = clip(sma50/sma150 - 1, -0.5, 0.5)
L724         c = clip(sma150/sma200 - 1, -0.5, 0.5)
L725         d = clip(sma200/prev200 - 1, -0.2, 0.2)
L726         e = clip((p - lo_52) / (rng if rng and rng>0 else np.nan) - 0.5, -0.5, 0.5)
L727         parts = [0.0 if pd.isna(x) else x for x in (a,b,c,d,e)]
L728         return 0.30*parts[0] + 0.20*parts[1] + 0.15*parts[2] + 0.15*parts[3] + 0.20*parts[4]
L729
L730     @staticmethod
L731     def rs(s, b):
L732         n, nb = len(s), len(b)
L733         if n<60 or nb<60: return np.nan
L734         L12 = 252 if n>=252 and nb>=252 else min(n,nb)-1; L1 = 22 if n>=22 and nb>=22 else max(5, min(n,nb)//3)
L735         r12, r1, br12, br1 = s.iloc[-1]/s.iloc[-L12]-1, s.iloc[-1]/s.iloc[-L1]-1, b.iloc[-1]/b.iloc[-L12]-1, b.iloc[-1]/b.iloc[-L1]-1
L736         return (r12 - br12)*0.7 + (r1 - br1)*0.3
L737
L738     @staticmethod
L739     def tr_str(s):
L740         if s is None:
L741             return np.nan
L742         s = s.ffill(limit=2).dropna()
L743         if len(s) < 50:
L744             return np.nan
L745         ma50 = s.rolling(50, min_periods=50).mean()
L746         last_ma = ma50.iloc[-1]
L747         last_px = s.iloc[-1]
L748         return float(last_px/last_ma - 1.0) if pd.notna(last_ma) and pd.notna(last_px) else np.nan
L749
L750     @staticmethod
L751     def rs_line_slope(s: pd.Series, b: pd.Series, win: int) -> float:
L752         r = (s/b).dropna()
L753         if len(r) < win: return np.nan
L754         y, x = np.log(r.iloc[-win:]), np.arange(win, dtype=float)
L755         try: return float(np.polyfit(x, y, 1)[0])
L756         except Exception: return np.nan
L757
L758     @staticmethod
L759     def ev_fallback(info_t: dict, tk: yf.Ticker) -> float:
L760         ev = info_t.get('enterpriseValue', np.nan)
L761         if pd.notna(ev) and ev>0: return float(ev)
L762         mc, debt, cash = info_t.get('marketCap', np.nan), np.nan, np.nan
L763         try:
L764             bs = tk.quarterly_balance_sheet
L765             if bs is not None and not bs.empty:
L766                 c = bs.columns[0]
L767                 for k in ("Total Debt","Long Term Debt","Short Long Term Debt"):
L768                     if k in bs.index: debt = float(bs.loc[k,c]); break
L769                 for k in ("Cash And Cash Equivalents","Cash And Cash Equivalents And Short Term Investments","Cash"):
L770                     if k in bs.index: cash = float(bs.loc[k,c]); break
L771         except Exception: pass
L772         if pd.notna(mc): return float(mc + (0 if pd.isna(debt) else debt) - (0 if pd.isna(cash) else cash))
L773         return np.nan
L774
L775     @staticmethod
L776     def dividend_status(ticker: str) -> str:
L777         t = yf.Ticker(ticker)
L778         try:
L779             if not t.dividends.empty: return "has"
L780         except Exception: return "unknown"
L781         try:
L782             a = t.actions
L783             if (a is not None and not a.empty and "Stock Splits" in a.columns and a["Stock Splits"].abs().sum()>0): return "none_confident"
L784         except Exception: pass
L785         try:
L786             fi = t.fast_info
L787             if any(getattr(fi,k,None) for k in ("last_dividend_date","dividend_rate","dividend_yield")): return "maybe_missing"
L788         except Exception: pass
L789         return "unknown"
L790
L791     @staticmethod
L792     def div_streak(t):
L793         try:
L794             divs = yf.Ticker(t).dividends.dropna(); ann = divs.groupby(divs.index.year).sum(); ann = ann[ann.index<pd.Timestamp.today().year]
L795             years, streak = sorted(ann.index), 0
L796             for i in range(len(years)-1,0,-1):
L797                 if ann[years[i]] > ann[years[i-1]]: streak += 1
L798                 else: break
L799             return streak
L800         except Exception: return 0
L801
L802     @staticmethod
L803     def fetch_finnhub_metrics(symbol):
L804         api_key = os.environ.get("FINNHUB_API_KEY")
L805         if not api_key: return {}
L806         url, params = "https://finnhub.io/api/v1/stock/metric", {"symbol":symbol,"metric":"all","token":api_key}
L807         try:
L808             r = requests.get(url, params=params, timeout=10); r.raise_for_status(); m = r.json().get("metric",{})
L809             return {'EPS':m.get('epsGrowthTTMYoy'),'REV':m.get('revenueGrowthTTMYoy'),'ROE':m.get('roeTTM'),'BETA':m.get('beta'),'DIV':m.get('dividendYieldIndicatedAnnual'),'FCF':(m.get('freeCashFlowTTM')/m.get('enterpriseValue')) if m.get('freeCashFlowTTM') and m.get('enterpriseValue') else None}
L810         except Exception: return {}
L811
L812     @staticmethod
L813     def calc_beta(series: pd.Series, market: pd.Series, lookback=252):
L814         r, m = series.pct_change().dropna(), market.pct_change().dropna()
L815         n = min(len(r), len(m), lookback)
L816         if n<60: return np.nan
L817         r, m = r.iloc[-n:], m.iloc[-n:]; cov, var = np.cov(r, m)[0,1], np.var(m)
L818         return np.nan if var==0 else cov/var
L819
L820     @staticmethod
L821     def spx_to_alpha(spx: pd.Series, bands=(0.03,0.10), w=(0.6,0.4),
L822                      span=5, q=(0.20,0.40), alphas=(0.05,0.08,0.10)) -> float:
L823         """
L824         S&P500æŒ‡æ•°ã®ã¿ã‹ã‚‰æ“¬ä¼¼breadthã‚’ä½œã‚Šã€å±¥æ­´åˆ†ä½ã§Î±ã‚’æ®µéšæ±ºå®šã€‚
L825         bands=(Â±3%, Â±10%), w=(50DMA,200DMA), åˆ†ä½q=(20%,40%), alphas=(ä½,ä¸­,é«˜)
L826         """
L827         ma50, ma200 = spx.rolling(50).mean(), spx.rolling(200).mean()
L828         b50, b200 = ((spx/ma50 - 1)+bands[0])/(2*bands[0]), ((spx/ma200 - 1)+bands[1])/(2*bands[1])
L829         hist = (w[0]*b50 + w[1]*b200).clip(0,1).ewm(span=span).mean()
L830         b, (lo, mid) = float(hist.iloc[-1]), (float(hist.quantile(q[0])), float(hist.quantile(q[1])))
L831         return alphas[0] if b < lo else alphas[1] if b < mid else alphas[2]
L832
L833     @staticmethod
L834     def soft_cap_effective_scores(scores: pd.Series|dict, sectors: dict, cap=2, alpha=0.08) -> pd.Series:
L835         """
L836         åŒä¸€ã‚»ã‚¯ã‚¿ãƒ¼capè¶…éï¼ˆ3æœ¬ç›®ä»¥é™ï¼‰ã« Î±Ã—æ®µéšæ¸›ç‚¹ã‚’èª²ã—ãŸâ€œæœ‰åŠ¹ã‚¹ã‚³ã‚¢â€Seriesã‚’è¿”ã™ã€‚
L837         æˆ»ã‚Šå€¤ã¯é™é †ã‚½ãƒ¼ãƒˆæ¸ˆã¿ã€‚
L838         """
L839         s = pd.Series(scores, dtype=float); order = s.sort_values(ascending=False).index
L840         cnt, pen = {}, {}
L841         for t in order:
L842             sec = sectors.get(t, "U"); cnt[sec] = cnt.get(sec,0) + 1; pen[t] = alpha*max(0, cnt[sec]-cap)
L843         return (s - pd.Series(pen)).sort_values(ascending=False)
L844
L845     @staticmethod
L846     def pick_top_softcap(scores: pd.Series|dict, sectors: dict, N: int, cap=2, alpha=0.08, hard: int|None=5) -> list[str]:
L847         """
L848         soft-capé©ç”¨å¾Œã®ä¸Šä½Nãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚’è¿”ã™ã€‚hard>0ãªã‚‰éå¸¸ç”¨ãƒãƒ¼ãƒ‰ä¸Šé™ã§åŒä¸€ã‚»ã‚¯ã‚¿ãƒ¼è¶…éã‚’é–“å¼•ãï¼ˆæ—¢å®š=5ï¼‰ã€‚
L849         """
L850         eff = Scorer.soft_cap_effective_scores(scores, sectors, cap, alpha)
L851         if not hard:
L852             return list(eff.head(N).index)
L853         pick, used = [], {}
L854         for t in eff.index:
L855             s = sectors.get(t, "U")
L856             if used.get(s,0) < hard:
L857                 pick.append(t); used[s] = used.get(s,0) + 1
L858             if len(pick) == N: break
L859         return pick
L860
L861     @staticmethod
L862     def trend_template_breadth_series(px: pd.DataFrame, spx: pd.Series, win_days: int | None = None) -> pd.Series:
L863         """
L864         å„å–¶æ¥­æ—¥ã® trend_template åˆæ ¼æœ¬æ•°ï¼ˆåˆæ ¼â€œæœ¬æ•°â€=Cï¼‰ã‚’è¿”ã™ã€‚
L865         - px: åˆ—=tickerï¼ˆãƒ™ãƒ³ãƒã¯å«ã‚ãªã„ï¼‰
L866         - spx: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ Seriesï¼ˆpx.index ã«æ•´åˆ—ï¼‰
L867         - win_days: æœ«å°¾ã®è¨ˆç®—å¯¾è±¡å–¶æ¥­æ—¥æ•°ï¼ˆNoneâ†’å…¨ä½“ã€æ—¢å®š600ã¯å‘¼ã³å‡ºã—å´æŒ‡å®šï¼‰
L868         ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼†rollingã®ã¿ã§è»½é‡ã€‚æ¬ æã¯ False æ‰±ã„ã€‚
L869         """
L870         import numpy as np, pandas as pd
L871         if px is None or px.empty:
L872             return pd.Series(dtype=int)
L873         px = px.dropna(how="all", axis=1)
L874         if win_days and win_days > 0:
L875             px = px.tail(win_days)
L876         if px.empty:
L877             return pd.Series(dtype=int)
L878         spx = spx.reindex(px.index).ffill()
L879
L880         ma50  = px.rolling(50).mean()
L881         ma150 = px.rolling(150).mean()
L882         ma200 = px.rolling(200).mean()
L883
L884         tt = (px > ma150)
L885         tt &= (px > ma200)
L886         tt &= (ma150 > ma200)
L887         tt &= (ma200 - ma200.shift(21) > 0)
L888         tt &= (ma50  > ma150)
L889         tt &= (ma50  > ma200)
L890         tt &= (px    > ma50)
L891
L892         lo252 = px.rolling(252).min()
L893         hi252 = px.rolling(252).max()
L894         tt &= (px.divide(lo252).sub(1.0) >= 0.30)   # P_OVER_LOW52 >= 0.30
L895         tt &= (px >= (0.75 * hi252))                # NEAR_52W_HIGH >= -0.25
L896
L897         r12  = px.divide(px.shift(252)).sub(1.0)
L898         br12 = spx.divide(spx.shift(252)).sub(1.0)
L899         r1   = px.divide(px.shift(22)).sub(1.0)
L900         br1  = spx.divide(spx.shift(22)).sub(1.0)
L901         rs   = 0.7*(r12.sub(br12, axis=0)) + 0.3*(r1.sub(br1, axis=0))
L902         tt &= (rs >= 0.10)
L903
L904         return tt.fillna(False).sum(axis=1).astype(int)
L905
L906     # ---- ã‚¹ã‚³ã‚¢é›†è¨ˆï¼ˆDTO/Configã‚’å—ã‘å–ã‚Šã€FeatureBundleã‚’è¿”ã™ï¼‰ ----
L907     def aggregate_scores(self, ib: Any, cfg):
L908         if cfg is None:
L909             raise ValueError("cfg is required; pass factor.PipelineConfig")
L910         self._validate_ib_for_scorer(ib)
L911
L912         px, spx, tickers = ib.px, ib.spx, ib.tickers
L913         tickers_bulk, info, eps_df, fcf_df = ib.tickers_bulk, ib.info, ib.eps_df, ib.fcf_df
L914
L915         df, missing_logs = pd.DataFrame(index=tickers), []
L916         for t in tickers:
L917             d, s = info[t], px[t]; ev = self.ev_fallback(d, tickers_bulk.tickers[t])
L918             try:
L919                 volume_series_full = ib.data['Volume'][t]
L920             except Exception:
L921                 volume_series_full = None
L922
L923             grw_result = _calc_grw_flexible(t, d, s, volume_series_full)
L924             _grw_record_to_df(t, d, df)
L925             df.loc[t,'GRW_FLEX_SCORE'] = grw_result.get('score')
L926             df.loc[t,'GRW_FLEX_WEIGHT'] = grw_result.get('weight')
L927             df.loc[t,'GRW_FLEX_CORE'] = grw_result.get('core')
L928             df.loc[t,'GRW_FLEX_PRICE'] = grw_result.get('price_proxy')
L929             df.loc[t,'DEBUG_GRW_PATH'] = grw_result.get('path')
L930             df.loc[t,'DEBUG_GRW_PARTS'] = grw_result.get('parts')
L931
L932             # --- åŸºæœ¬ç‰¹å¾´ ---
L933             df.loc[t,'TR']   = self.trend(s)
L934             df.loc[t,'EPS']  = _scalar(eps_df.loc[t,'EPS_TTM']) if t in eps_df.index else np.nan
L935             df.loc[t,'EPS_Q'] = _scalar(eps_df.loc[t,'EPS_Q_LastQ']) if t in eps_df.index else np.nan
L936             df.loc[t,'REV_TTM'] = _scalar(eps_df.loc[t,'REV_TTM']) if t in eps_df.index else np.nan
L937             df.loc[t,'REV_Q']   = _scalar(eps_df.loc[t,'REV_Q_LastQ']) if t in eps_df.index else np.nan
L938             df.loc[t,'REV']  = d.get('revenueGrowth',np.nan)
L939             df.loc[t,'ROE']  = d.get('returnOnEquity',np.nan)
L940             df.loc[t,'BETA'] = self.calc_beta(s, spx, lookback=252)
L941
L942             # --- é…å½“ï¼ˆæ¬ æè£œå®Œå«ã‚€ï¼‰ ---
L943             div = d.get('dividendYield') if d.get('dividendYield') is not None else d.get('trailingAnnualDividendYield')
L944             if div is None or pd.isna(div):
L945                 try:
L946                     divs = yf.Ticker(t).dividends
L947                     if divs is not None and not divs.empty:
L948                         last_close = s.iloc[-1]; div_1y = divs[divs.index >= (divs.index.max() - pd.Timedelta(days=365))].sum()
L949                         if last_close and last_close>0: div = float(div_1y/last_close)
L950                 except Exception: pass
L951             df.loc[t,'DIV'] = 0.0 if (div is None or pd.isna(div)) else float(div)
L952
L953             # --- FCF/EV ---
L954             fcf_val = fcf_df.loc[t,'FCF_TTM'] if t in fcf_df.index else np.nan
L955             df.loc[t,'FCF'] = (fcf_val/ev) if (pd.notna(fcf_val) and pd.notna(ev) and ev>0) else np.nan
L956
L957             # --- ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ãƒ»ãƒœãƒ©é–¢é€£ ---
L958             df.loc[t,'RS'], df.loc[t,'TR_str'] = self.rs(s, spx), self.tr_str(s)
L959             r, rm = s.pct_change().dropna(), spx.pct_change().dropna()
L960             n = int(min(len(r), len(rm)))
L961
L962             DOWNSIDE_DEV = np.nan
L963             if n>=60:
L964                 r6 = r.iloc[-min(len(r),126):]; neg = r6[r6<0]
L965                 if len(neg)>=10: DOWNSIDE_DEV = float(neg.std(ddof=0)*np.sqrt(252))
L966             df.loc[t,'DOWNSIDE_DEV'] = DOWNSIDE_DEV
L967
L968             MDD_1Y = np.nan
L969             try:
L970                 w = s.iloc[-min(len(s),252):].dropna()
L971                 if len(w)>=30:
L972                     roll_max = w.cummax(); MDD_1Y = float((w/roll_max - 1.0).min())
L973             except Exception: pass
L974             df.loc[t,'MDD_1Y'] = MDD_1Y
L975
L976             RESID_VOL = np.nan
L977             if n>=120:
L978                 rr, rrm = r.iloc[-n:].align(rm.iloc[-n:], join='inner')
L979                 if len(rr)==len(rrm) and len(rr)>=120 and rrm.var()>0:
L980                     beta = float(np.cov(rr, rrm)[0,1]/np.var(rrm)); resid = rr - beta*rrm
L981                     RESID_VOL = float(resid.std(ddof=0)*np.sqrt(252))
L982             df.loc[t,'RESID_VOL'] = RESID_VOL
L983
L984             DOWN_OUTPERF = np.nan
L985             if n>=60:
L986                 m, x = rm.iloc[-n:], r.iloc[-n:]; mask = m<0
L987                 if mask.sum()>=10:
L988                     mr, sr = float(m[mask].mean()), float(x[mask].mean())
L989                     DOWN_OUTPERF = (sr - mr)/abs(mr) if mr!=0 else np.nan
L990             df.loc[t,'DOWN_OUTPERF'] = DOWN_OUTPERF
L991
L992             # --- é•·æœŸç§»å‹•å¹³å‡/ä½ç½® ---
L993             sma200 = s.rolling(200).mean(); df.loc[t,'EXT_200'] = np.nan
L994             if pd.notna(sma200.iloc[-1]) and sma200.iloc[-1]!=0: df.loc[t,'EXT_200'] = abs(float(s.iloc[-1]/sma200.iloc[-1]-1.0))
L995
L996             # --- é…å½“ã®è©³ç´°ç³» ---
L997             DIV_TTM_PS=DIV_VAR5=DIV_YOY=DIV_FCF_COVER=np.nan
L998             try:
L999                 divs = yf.Ticker(t).dividends.dropna()
L1000                 if not divs.empty:
L1001                     last_close = s.iloc[-1]; div_1y = float(divs[divs.index >= (divs.index.max()-pd.Timedelta(days=365))].sum())
L1002                     DIV_TTM_PS = div_1y if div_1y>0 else np.nan
L1003                     ann = divs.groupby(divs.index.year).sum()
L1004                     if len(ann)>=2 and ann.iloc[-2]!=0: DIV_YOY = float(ann.iloc[-1]/ann.iloc[-2]-1.0)
L1005                     tail = ann.iloc[-5:] if len(ann)>=5 else ann
L1006                     if len(tail)>=3 and tail.mean()!=0: DIV_VAR5 = float(tail.std(ddof=1)/abs(tail.mean()))
L1007                 so = d.get('sharesOutstanding',None)
L1008                 if so and pd.notna(DIV_TTM_PS) and pd.notna(fcf_val) and fcf_val!=0:
L1009                     DIV_FCF_COVER = float((fcf_val)/(DIV_TTM_PS*float(so)))
L1010             except Exception: pass
L1011             df.loc[t,'DIV_TTM_PS'], df.loc[t,'DIV_VAR5'], df.loc[t,'DIV_YOY'], df.loc[t,'DIV_FCF_COVER'] = DIV_TTM_PS, DIV_VAR5, DIV_YOY, DIV_FCF_COVER
L1012
L1013             # --- è²¡å‹™å®‰å®šæ€§ ---
L1014             df.loc[t,'DEBT2EQ'], df.loc[t,'CURR_RATIO'] = d.get('debtToEquity',np.nan), d.get('currentRatio',np.nan)
L1015
L1016             # --- EPS å¤‰å‹• ---
L1017             EPS_VAR_8Q = np.nan
L1018             try:
L1019                 qe, so = tickers_bulk.tickers[t].quarterly_earnings, d.get('sharesOutstanding',None)
L1020                 if qe is not None and not qe.empty and so:
L1021                     eps_q = (qe['Earnings'].dropna().astype(float)/float(so)).replace([np.inf,-np.inf],np.nan)
L1022                     if len(eps_q)>=4: EPS_VAR_8Q = float(eps_q.iloc[-min(8,len(eps_q)):].std(ddof=1))
L1023             except Exception: pass
L1024             df.loc[t,'EPS_VAR_8Q'] = EPS_VAR_8Q
L1025
L1026             # --- ã‚µã‚¤ã‚º/æµå‹•æ€§ ---
L1027             df.loc[t,'MARKET_CAP'] = d.get('marketCap',np.nan); adv60 = np.nan
L1028             try:
L1029                 if isinstance(volume_series_full, pd.Series):
L1030                     vol_series = volume_series_full.reindex(s.index).dropna()
L1031                     if len(vol_series) >= 5:
L1032                         aligned_px = s.reindex(vol_series.index).dropna()
L1033                         if len(aligned_px) == len(vol_series):
L1034                             dv = (vol_series*aligned_px).rolling(60).mean()
L1035                             if not dv.dropna().empty:
L1036                                 adv60 = float(dv.dropna().iloc[-1])
L1037             except Exception:
L1038                 pass
L1039             df.loc[t,'ADV60_USD'] = adv60
L1040
L1041             # --- Rule of 40 ã‚„å‘¨è¾º ---
L1042             total_rev_ttm = d.get('totalRevenue',np.nan)
L1043             FCF_MGN = _safe_div(fcf_val, total_rev_ttm)
L1044             df.loc[t,'FCF_MGN'] = FCF_MGN
L1045             rule40 = np.nan
L1046             try:
L1047                 r = df.loc[t,'REV']; rule40 = (r if pd.notna(r) else np.nan) + (FCF_MGN if pd.notna(FCF_MGN) else np.nan)
L1048             except Exception: pass
L1049             df.loc[t,'RULE40'] = rule40
L1050
L1051             # --- ãƒˆãƒ¬ãƒ³ãƒ‰è£œåŠ© ---
L1052             sma50  = s.rolling(50).mean()
L1053             sma150 = s.rolling(150).mean()
L1054             sma200 = s.rolling(200).mean()
L1055             p = _safe_last(s)
L1056
L1057             df.loc[t,'MA50_OVER_150'] = (_safe_last(sma50)/_safe_last(sma150) - 1
L1058                 if pd.notna(_safe_last(sma50)) and pd.notna(_safe_last(sma150)) and _safe_last(sma150)!=0 else np.nan)
L1059             df.loc[t,'MA150_OVER_200'] = (_safe_last(sma150)/_safe_last(sma200) - 1
L1060                 if pd.notna(_safe_last(sma150)) and pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan)
L1061
L1062             lo52 = s[-252:].min() if len(s)>=252 else s.min()
L1063             df.loc[t,'P_OVER_LOW52'] = (p/lo52 - 1) if (lo52 and lo52>0 and pd.notna(p)) else np.nan
L1064
L1065             df.loc[t,'MA200_SLOPE_1M'] = np.nan
L1066             if len(sma200.dropna()) >= 21:
L1067                 cur200 = _safe_last(sma200)
L1068                 old2001 = float(sma200.iloc[-21])
L1069                 if old2001:
L1070                     df.loc[t,'MA200_SLOPE_1M'] = cur200/old2001 - 1
L1071
L1072             df.loc[t,'P_OVER_150'] = p/_safe_last(sma150)-1 if pd.notna(_safe_last(sma150)) and _safe_last(sma150)!=0 else np.nan
L1073             df.loc[t,'P_OVER_200'] = p/_safe_last(sma200)-1 if pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L1074             df.loc[t,'MA50_OVER_200'] = _safe_last(sma50)/_safe_last(sma200)-1 if pd.notna(_safe_last(sma50)) and pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L1075             df.loc[t,'MA200_SLOPE_5M'] = np.nan
L1076             if len(sma200.dropna())>=105:
L1077                 cur200, old200 = _safe_last(sma200), float(sma200.iloc[-105])
L1078                 if old200 and old200!=0: df.loc[t,'MA200_SLOPE_5M'] = cur200/old200 - 1
L1079             # NEW: 200æ—¥ç·šãŒé€£ç¶šã§ä¸Šå‘ãã®ã€Œæ—¥æ•°ã€
L1080             df.loc[t,'MA200_UP_STREAK_D'] = np.nan
L1081             try:
L1082                 s200 = sma200.dropna()
L1083                 if len(s200) >= 2:
L1084                     diff200 = s200.diff()
L1085                     up = 0
L1086                     for v in diff200.iloc[::-1]:
L1087                         if pd.isna(v) or v <= 0:
L1088                             break
L1089                         up += 1
L1090                     df.loc[t,'MA200_UP_STREAK_D'] = float(up)
L1091             except Exception:
L1092                 pass
L1093             df.loc[t,'LOW52PCT25_EXCESS'] = np.nan if (lo52 is None or lo52<=0 or pd.isna(p)) else (p/(lo52*1.25)-1)
L1094             hi52 = s[-252:].max() if len(s)>=252 else s.max(); df.loc[t,'NEAR_52W_HIGH'] = np.nan
L1095             if hi52 and hi52>0 and pd.notna(p):
L1096                 d_hi = (p/hi52)-1.0; df.loc[t,'NEAR_52W_HIGH'] = -abs(min(0.0, d_hi))
L1097             df.loc[t,'RS_SLOPE_6W'] = self.rs_line_slope(s, ib.spx, 30)
L1098             df.loc[t,'RS_SLOPE_13W'] = self.rs_line_slope(s, ib.spx, 65)
L1099
L1100             df.loc[t,'DIV_STREAK'] = self.div_streak(t)
L1101
L1102             # --- æ¬ æãƒ¡ãƒ¢ ---
L1103             fin_cols = ['REV','ROE','BETA','DIV','FCF']
L1104             need_finnhub = [col for col in fin_cols if pd.isna(df.loc[t,col])]
L1105             if need_finnhub:
L1106                 fin_data = self.fetch_finnhub_metrics(t)
L1107                 for col in need_finnhub:
L1108                     val = fin_data.get(col)
L1109                     if val is not None and not pd.isna(val): df.loc[t,col] = val
L1110             for col in fin_cols + ['EPS','RS','TR_str','DIV_STREAK']:
L1111                 if pd.isna(df.loc[t,col]):
L1112                     if col=='DIV':
L1113                         status = self.dividend_status(t)
L1114                         if status!='none_confident': missing_logs.append({'Ticker':t,'Column':col,'Status':status})
L1115                     else:
L1116                         missing_logs.append({'Ticker':t,'Column':col})
L1117
L1118         def _pick_series(entry: dict, keys: list[str]):
L1119             for k in keys:
L1120                 val = entry.get(k) if isinstance(entry, dict) else None
L1121                 if val is None:
L1122                     continue
L1123                 try:
L1124                     if hasattr(val, "empty") and getattr(val, "empty"):
L1125                         continue
L1126                 except Exception:
L1127                     pass
L1128                 if isinstance(val, (list, tuple)) and len(val) == 0:
L1129                     continue
L1130                 return val
L1131             return None
L1132
L1133         def _has_sec_series(val) -> bool:
L1134             try:
L1135                 if isinstance(val, pd.Series):
L1136                     return not val.dropna().empty
L1137                 if isinstance(val, (list, tuple)):
L1138                     return any(pd.notna(v) for v in val)
L1139                 return bool(val)
L1140             except Exception:
L1141                 return False
L1142
L1143         def _series_len(val) -> int:
L1144             try:
L1145                 if isinstance(val, pd.Series):
L1146                     return int(val.dropna().size)
L1147                 if isinstance(val, (list, tuple)):
L1148                     return len(val)
L1149                 return int(bool(val))
L1150             except Exception:
L1151                 return 0
L1152
L1153         cnt_rev_series = sum(1 for _t, d in info.items() if _has_sec_series(d.get("SEC_REV_Q_SERIES")))
L1154         cnt_eps_series = sum(1 for _t, d in info.items() if _has_sec_series(d.get("SEC_EPS_Q_SERIES")))
L1155         logger.info(
L1156             "[DERIV] SEC series presence: REV_Q=%d, EPS_Q=%d (universe=%d)",
L1157             cnt_rev_series,
L1158             cnt_eps_series,
L1159             len(info),
L1160         )
L1161
L1162         rev_q_ge5 = 0
L1163         ttm_yoy_avail = 0
L1164         wrote_growth = 0
L1165
L1166         for t in tickers:
L1167             try:
L1168                 d = info.get(t, {}) or {}
L1169                 rev_series = d.get("SEC_REV_Q_SERIES")
L1170                 eps_series = d.get("SEC_EPS_Q_SERIES")
L1171                 fallback_qearn = False
L1172                 try:
L1173                     qe = tickers_bulk.tickers[t].quarterly_earnings
L1174                     fallback_qearn = bool(qe is not None and not getattr(qe, "empty", True))
L1175                 except Exception:
L1176                     qe = None
L1177                 logger.debug(
L1178                     "[DERIV] %s: rev_q_len=%s eps_q_len=%s fallback_qearn=%s",
L1179                     t,
L1180                     _series_len(rev_series),
L1181                     _series_len(eps_series),
L1182                     fallback_qearn,
L1183                 )
L1184
L1185                 r_src = _pick_series(d, ["SEC_REV_Q_SERIES", "rev_q_series_pairs", "rev_q_series"])
L1186                 e_src = _pick_series(d, ["SEC_EPS_Q_SERIES", "eps_q_series_pairs", "eps_q_series"])
L1187                 r_raw = _ensure_series(r_src)
L1188                 e_raw = _ensure_series(e_src)
L1189                 _log("DERIV_SRC", f"{t} rev_raw_len={r_raw.size} eps_raw_len={e_raw.size}")
L1190
L1191                 r_q = _to_quarterly(r_raw)
L1192                 e_q = _to_quarterly(e_raw)
L1193                 _log("DERIV_Q", f"{t} rev_q_len={r_q.size} eps_q_len={e_q.size}")
L1194                 if r_q.size >= 5:
L1195                     rev_q_ge5 += 1
L1196
L1197                 r_yoy_ttm = _ttm_yoy_from_quarterly(r_q)
L1198                 e_yoy_ttm = _ttm_yoy_from_quarterly(e_q)
L1199                 has_ttm = int(not r_yoy_ttm.dropna().empty)
L1200                 ttm_yoy_avail += has_ttm
L1201                 _log("DERIV_TTM", f"{t} rev_ttm_yoy_len={r_yoy_ttm.dropna().size} eps_ttm_yoy_len={e_yoy_ttm.dropna().size}")
L1202
L1203                 def _q_yoy(qs):
L1204                     return np.nan if qs is None or len(qs) < 5 else float(qs.iloc[-1] / qs.iloc[-5] - 1.0)
L1205
L1206                 rev_q_yoy = _q_yoy(r_q)
L1207                 eps_q_yoy = _q_yoy(e_q)
L1208
L1209                 def _annual_from(qs: pd.Series, yoy_ttm: pd.Series):
L1210                     if isinstance(qs.index, pd.DatetimeIndex) and len(qs) >= 8:
L1211                         ann = qs.groupby(qs.index.year).last().pct_change()
L1212                         ann_dn = ann.dropna()
L1213                         if not ann_dn.empty:
L1214                             y = float(ann_dn.iloc[-1])
L1215                             acc = float(ann_dn.tail(3).mean()) if ann_dn.size >= 3 else np.nan
L1216                             var = float(ann_dn.tail(4).var()) if ann_dn.size >= 4 else np.nan
L1217                             return y, acc, var
L1218                     yoy_dn = yoy_ttm.dropna()
L1219                     if yoy_dn.empty:
L1220                         return np.nan, np.nan, np.nan
L1221                     return (
L1222                         float(yoy_dn.iloc[-1]),
L1223                         float(yoy_dn.tail(3).mean() if yoy_dn.size >= 3 else np.nan),
L1224                         float(yoy_dn.tail(4).var() if yoy_dn.size >= 4 else np.nan),
L1225                     )
L1226
L1227                 rev_yoy, rev_acc, rev_var = _annual_from(r_q, r_yoy_ttm)
L1228                 eps_yoy, _, _ = _annual_from(e_q, e_yoy_ttm)
L1229
L1230                 def _pos_streak(s: pd.Series):
L1231                     s = s.dropna()
L1232                     if s.empty:
L1233                         return np.nan
L1234                     b = (s > 0).astype(int).to_numpy()[::-1]
L1235                     k = 0
L1236                     for v in b:
L1237                         if v == 1:
L1238                             k += 1
L1239                         else:
L1240                             break
L1241                     return float(k)
L1242
L1243                 rev_ann_streak = _pos_streak(r_yoy_ttm)
L1244
L1245                 df.loc[t, "REV_Q_YOY"] = rev_q_yoy
L1246                 df.loc[t, "EPS_Q_YOY"] = eps_q_yoy
L1247                 df.loc[t, "REV_YOY"] = rev_yoy
L1248                 df.loc[t, "EPS_YOY"] = eps_yoy
L1249                 df.loc[t, "REV_YOY_ACC"] = rev_acc
L1250                 df.loc[t, "REV_YOY_VAR"] = rev_var
L1251                 df.loc[t, "REV_ANN_STREAK"] = rev_ann_streak
L1252
L1253                 wrote_growth += 1
L1254                 _log(
L1255                     "DERIV_WRITE",
L1256                     f"{t} wrote: Q_YOY(rev={rev_q_yoy}, eps={eps_q_yoy}) ANN(rev_yoy={rev_yoy}, acc={rev_acc}, var={rev_var}) streak={rev_ann_streak}",
L1257                 )
L1258
L1259             except Exception as e:
L1260                 logger.warning("[DERIV_WARN] %s growth-derivatives failed: %s", t, e)
L1261                 _log("DERIV_WARN", f"{t} {type(e).__name__}: {e}")
L1262
L1263         _log("DERIV_SUMMARY", f"rev_q_len>=5: {rev_q_ge5}/{len(tickers)}  ttm_yoy_available: {ttm_yoy_avail}  wrote_growth_for: {wrote_growth}")
L1264
L1265         try:
L1266             cols = [
L1267                 "REV_Q_YOY",
L1268                 "EPS_Q_YOY",
L1269                 "REV_YOY",
L1270                 "EPS_YOY",
L1271                 "REV_YOY_ACC",
L1272                 "REV_YOY_VAR",
L1273                 "REV_ANN_STREAK",
L1274             ]
L1275             cnt = {c: int(df[c].count()) for c in cols if c in df.columns}
L1276             _log("DERIV_NONNAN_COUNTS", str(cnt))
L1277         except Exception as e:
L1278             _log("DERIV_NONNAN_COUNTS", f"error: {e}")
L1279
L1280         def _trend_template_pass(row, rs_alpha_thresh=0.10):
L1281             c1 = (row.get('P_OVER_150', np.nan) > 0) and (row.get('P_OVER_200', np.nan) > 0)
L1282             c2 = (row.get('MA150_OVER_200', np.nan) > 0)
L1283             c3 = (row.get('MA200_SLOPE_1M', np.nan) > 0)
L1284             c4 = (row.get('MA50_OVER_150', np.nan) > 0) and (row.get('MA50_OVER_200', np.nan) > 0)
L1285             c5 = (row.get('TR_str', np.nan) > 0)
L1286             c6 = (row.get('P_OVER_LOW52', np.nan) >= 0.30)
L1287             c7 = (row.get('NEAR_52W_HIGH', np.nan) >= -0.25)
L1288             c8 = (row.get('RS', np.nan) >= 0.10)
L1289             return bool(c1 and c2 and c3 and c4 and c5 and c6 and c7 and c8)
L1290
L1291         if 'trend_template' not in df.columns: df['trend_template'] = df.apply(_trend_template_pass, axis=1).fillna(False)
L1292         assert 'trend_template' in df.columns
L1293
L1294         # === ZåŒ–ã¨åˆæˆ ===
L1295         for col in ['ROE','FCF','REV','EPS']: df[f'{col}_W'] = winsorize_s(df[col], 0.02)
L1296
L1297         df_z = pd.DataFrame(index=df.index)
L1298         for col in ['EPS','REV','ROE','FCF','RS','TR_str','BETA','DIV','DIV_STREAK']: df_z[col] = robust_z(df[col])
L1299         df_z['REV'], df_z['EPS'], df_z['TR'] = robust_z(df['REV_W']), robust_z(df['EPS_W']), robust_z(df['TR'])
L1300         for col in ['P_OVER_150','P_OVER_200','MA50_OVER_200','MA200_SLOPE_5M','LOW52PCT25_EXCESS','NEAR_52W_HIGH','RS_SLOPE_6W','RS_SLOPE_13W','MA200_UP_STREAK_D']: df_z[col] = robust_z(df[col])
L1301
L1302         # === Growthæ·±æ˜ã‚Šç³»ï¼ˆæ¬ æä¿æŒz + RAWä½µè¼‰ï¼‰ ===
L1303         grw_cols = ['REV_Q_YOY','EPS_Q_YOY','REV_YOY','EPS_YOY','REV_YOY_ACC','REV_YOY_VAR','FCF_MGN','RULE40','REV_ANN_STREAK']
L1304         for col in grw_cols:
L1305             if col in df.columns:
L1306                 raw = pd.to_numeric(df[col], errors="coerce")
L1307                 df_z[col] = robust_z_keepnan(raw)
L1308                 df_z[f'{col}_RAW'] = raw
L1309         for k in ("TREND_SLOPE_EPS", "TREND_SLOPE_REV"):
L1310             if k in df.columns and k not in df_z.columns:
L1311                 raw = pd.to_numeric(df[k], errors="coerce")
L1312                 df_z[k] = robust_z_keepnan(raw)
L1313                 df_z[f'{k}_RAW'] = raw
L1314         for col in ['DOWNSIDE_DEV','MDD_1Y','RESID_VOL','DOWN_OUTPERF','EXT_200','DIV_TTM_PS','DIV_VAR5','DIV_YOY','DIV_FCF_COVER','DEBT2EQ','CURR_RATIO','EPS_VAR_8Q','MARKET_CAP','ADV60_USD']: df_z[col] = robust_z(df[col])
L1315
L1316         df_z['SIZE'], df_z['LIQ'] = robust_z(np.log1p(df['MARKET_CAP'])), robust_z(np.log1p(df['ADV60_USD']))
L1317         df_z['QUALITY_F'] = robust_z(0.6*df['FCF_W'] + 0.4*df['ROE_W']).clip(-3.0,3.0)
L1318         df_z['YIELD_F']   = 0.3*df_z['DIV'] + 0.7*df_z['DIV_STREAK']
L1319
L1320         # EPSãŒèµ¤å­—ã§ã‚‚FCFãŒé»’å­—ãªã‚‰å®Ÿè³ªé»’å­—ã¨ã¿ãªã™
L1321         eps_pos_mask = (df['EPS'] > 0) | (df['FCF_MGN'] > 0)
L1322         df_z['EPS_POS'] = df_z['EPS'].where(eps_pos_mask, 0.0)
L1323
L1324         # ===== ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ãƒ­ãƒ¼ãƒ—ç®—å‡º =====
L1325         def zpos(x):
L1326             arr = robust_z(x)
L1327             idx = getattr(x, 'index', df_z.index)
L1328             return pd.Series(arr, index=idx).fillna(0.0)
L1329
L1330         def relu(x):
L1331             ser = x if isinstance(x, pd.Series) else pd.Series(x, index=df_z.index)
L1332             return ser.clip(lower=0).fillna(0.0)
L1333
L1334         # å£²ä¸Šãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ãƒ­ãƒ¼ãƒ—ï¼ˆå››åŠæœŸï¼‰
L1335         slope_rev = 0.70*zpos(df_z['REV_Q_YOY']) + 0.30*zpos(df_z['REV_YOY_ACC'])
L1336         noise_rev = relu(robust_z(df_z['REV_YOY_VAR']) - 0.8)
L1337         slope_rev_combo = slope_rev - 0.25*noise_rev
L1338         df_z['TREND_SLOPE_REV_RAW'] = slope_rev_combo
L1339         df_z['TREND_SLOPE_REV'] = slope_rev_combo.clip(-3.0, 3.0)
L1340
L1341         # EPSãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ãƒ­ãƒ¼ãƒ—ï¼ˆå››åŠæœŸï¼‰
L1342         slope_eps = 0.60*zpos(df_z['EPS_Q_YOY']) + 0.40*zpos(df_z['EPS_POS'])
L1343         df_z['TREND_SLOPE_EPS_RAW'] = slope_eps
L1344         df_z['TREND_SLOPE_EPS'] = slope_eps.clip(-3.0, 3.0)
L1345
L1346         # å¹´æ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆã‚µãƒ–ï¼‰
L1347         slope_rev_yr = zpos(df_z['REV_YOY'])
L1348         slope_eps_yr = zpos(df_z.get('EPS_YOY', pd.Series(0.0, index=df.index)))
L1349         streak_base = df['REV_ANN_STREAK'].clip(lower=0).fillna(0)
L1350         streak_yr = streak_base / (streak_base.abs() + 1.0)
L1351         slope_rev_yr_combo = 0.7*slope_rev_yr + 0.3*streak_yr
L1352         df_z['TREND_SLOPE_REV_YR_RAW'] = slope_rev_yr_combo
L1353         df_z['TREND_SLOPE_REV_YR'] = slope_rev_yr_combo.clip(-3.0, 3.0)
L1354         df_z['TREND_SLOPE_EPS_YR_RAW'] = slope_eps_yr
L1355         df_z['TREND_SLOPE_EPS_YR'] = slope_eps_yr.clip(-3.0, 3.0)
L1356
L1357         # ===== GRW flexible score (variable data paths) =====
L1358         grw_raw = pd.to_numeric(df.get('GRW_FLEX_SCORE'), errors="coerce")
L1359         df_z['GRW_FLEX_SCORE_RAW'] = grw_raw
L1360         df_z['GROWTH_F_RAW'] = grw_raw
L1361         df_z['GROWTH_F'] = robust_z_keepnan(grw_raw).clip(-3.0, 3.0)
L1362         df_z['GRW_FLEX_WEIGHT'] = pd.to_numeric(df.get('GRW_FLEX_WEIGHT'), errors="coerce")
L1363         df_z['GRW_FLEX_CORE_RAW'] = pd.to_numeric(df.get('GRW_FLEX_CORE'), errors="coerce")
L1364         df_z['GRW_FLEX_PRICE_RAW'] = pd.to_numeric(df.get('GRW_FLEX_PRICE'), errors="coerce")
L1365
L1366         # Debug dump for GRW composition (console OFF by default; enable only with env)
L1367         if bool(os.getenv("GRW_CONSOLE_DEBUG")):
L1368             try:
L1369                 cols = ['GROWTH_F', 'GROWTH_F_RAW', 'GRW_FLEX_WEIGHT']
L1370                 use_cols = [c for c in cols if c in df_z.columns]
L1371                 i = df_z[use_cols].copy() if use_cols else pd.DataFrame(index=df_z.index)
L1372                 i.sort_values('GROWTH_F', ascending=False, inplace=True)
L1373                 limit = max(0, min(40, len(i)))
L1374                 print("[DEBUG: GRW]")
L1375                 for t in i.index[:limit]:
L1376                     row = i.loc[t]
L1377                     parts = []
L1378                     if pd.notna(row.get('GROWTH_F')):
L1379                         parts.append(f"GROWTH_F={row.get('GROWTH_F'):.3f}")
L1380                     raw_val = row.get('GROWTH_F_RAW')
L1381                     if pd.notna(raw_val):
L1382                         parts.append(f"GROWTH_F_RAW={raw_val:.3f}")
L1383                     weight_val = row.get('GRW_FLEX_WEIGHT')
L1384                     if pd.notna(weight_val):
L1385                         parts.append(f"w={weight_val:.2f}")
L1386                     path_val = None
L1387                     try:
L1388                         path_val = info.get(t, {}).get('DEBUG_GRW_PATH')
L1389                     except Exception:
L1390                         path_val = None
L1391                     if not path_val and 'DEBUG_GRW_PATH' in df.columns:
L1392                         path_val = df.at[t, 'DEBUG_GRW_PATH']
L1393                     if path_val:
L1394                         parts.append(f"PATH={path_val}")
L1395                     parts_json = None
L1396                     try:
L1397                         parts_json = info.get(t, {}).get('DEBUG_GRW_PARTS')
L1398                     except Exception:
L1399                         parts_json = None
L1400                     if not parts_json and 'DEBUG_GRW_PARTS' in df.columns:
L1401                         parts_json = df.at[t, 'DEBUG_GRW_PARTS']
L1402                     if parts_json:
L1403                         parts.append(f"PARTS={parts_json}")
L1404                     if not parts:
L1405                         parts.append('no-data')
L1406                     print(f"Ticker: {t} | " + " ".join(parts))
L1407                 print()
L1408             except Exception as exc:
L1409                 print(f"[ERR] GRW debug dump failed: {exc}")
L1410
L1411         df_z['MOM_F'] = robust_z(0.40*df_z['RS']
L1412             + 0.15*df_z['TR_str']
L1413             + 0.15*df_z['RS_SLOPE_6W']
L1414             + 0.15*df_z['RS_SLOPE_13W']
L1415             + 0.10*df_z['MA200_SLOPE_5M']
L1416             + 0.10*df_z['MA200_UP_STREAK_D']).clip(-3.0,3.0)
L1417         df_z['VOL'] = robust_z(df['BETA'])
L1418         df_z['QAL'], df_z['YLD'], df_z['MOM'] = df_z['QUALITY_F'], df_z['YIELD_F'], df_z['MOM_F']
L1419         df_z.drop(columns=['QUALITY_F','YIELD_F','MOM_F'], inplace=True, errors='ignore')
L1420
L1421         _dump_dfz(
L1422             df=df,
L1423             df_z=df_z,
L1424             debug_mode=getattr(cfg, "debug_mode", False),
L1425         )
L1426         if getattr(cfg, "debug_mode", False):
L1427             log_grw_stats(df, df_z, logger)
L1428         save_factor_debug_csv(df, df_z)
L1429
L1430         # === begin: BIO LOSS PENALTY =====================================
L1431         try:
L1432             penalty_z = float(os.getenv("BIO_LOSS_PENALTY_Z", "0.8"))
L1433         except Exception:
L1434             penalty_z = 0.8
L1435
L1436         def _is_bio_like(t: str) -> bool:
L1437             inf = info.get(t, {}) if isinstance(info, dict) else {}
L1438             sec = str(inf.get("sector", "")).lower()
L1439             ind = str(inf.get("industry", "")).lower()
L1440             if "health" not in sec:
L1441                 return False
L1442             keys = ("biotech", "biopharma", "pharma")
L1443             return any(k in ind for k in keys)
L1444
L1445         tickers_s = pd.Index(df_z.index)
L1446         is_bio = pd.Series({t: _is_bio_like(t) for t in tickers_s})
L1447         is_loss = pd.Series({t: (pd.notna(df.loc[t,"EPS"]) and df.loc[t,"EPS"] <= 0) for t in tickers_s})
L1448         mask_bio_loss = (is_bio & is_loss).reindex(df_z.index).fillna(False)
L1449
L1450         if bool(mask_bio_loss.any()) and penalty_z > 0:
L1451             df_z.loc[mask_bio_loss, "GROWTH_F"] = df_z.loc[mask_bio_loss, "GROWTH_F"] - penalty_z
L1452             df_z["GROWTH_F"] = df_z["GROWTH_F"].clip(-3.0, 3.0)
L1453         # === end: BIO LOSS PENALTY =======================================
L1454
L1455         df_z['TRD'] = 0.0  # TRDã¯ã‚¹ã‚³ã‚¢å¯„ä¸ã‹ã‚‰å¤–ã—ã€ãƒ†ãƒ³ãƒ—ãƒ¬åˆ¤å®šã¯ãƒ•ã‚£ãƒ«ã‚¿ã§è¡Œã†ï¼ˆåˆ—ã¯è¡¨ç¤ºäº’æ›ã®ãŸã‚æ®‹ã™ï¼‰
L1456         if 'BETA' not in df_z.columns: df_z['BETA'] = robust_z(df['BETA'])
L1457
L1458         df_z['D_VOL_RAW'] = robust_z(0.40*df_z['DOWNSIDE_DEV'] + 0.22*df_z['RESID_VOL'] + 0.18*df_z['MDD_1Y'] - 0.10*df_z['DOWN_OUTPERF'] - 0.05*df_z['EXT_200'] - 0.08*df_z['SIZE'] - 0.10*df_z['LIQ'] + 0.10*df_z['BETA'])
L1459         df_z['D_QAL']     = robust_z(0.35*df_z['QAL'] + 0.20*df_z['FCF'] + 0.15*df_z['CURR_RATIO'] - 0.15*df_z['DEBT2EQ'] - 0.15*df_z['EPS_VAR_8Q'])
L1460         df_z['D_YLD']     = robust_z(0.45*df_z['DIV'] + 0.25*df_z['DIV_STREAK'] + 0.20*df_z['DIV_FCF_COVER'] - 0.10*df_z['DIV_VAR5'])
L1461         df_z['D_TRD']     = robust_z(0.40*df_z.get('MA200_SLOPE_5M',0) - 0.30*df_z.get('EXT_200',0) + 0.15*df_z.get('NEAR_52W_HIGH',0) + 0.15*df_z['TR'])
L1462
L1463         # --- é‡ã¿ã¯ cfg ã‚’å„ªå…ˆï¼ˆå¤–éƒ¨ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ï¼‰ ---
L1464         # â‘  å…¨éŠ˜æŸ„ã§ G/D ã‚¹ã‚³ã‚¢ã‚’ç®—å‡ºï¼ˆunmaskedï¼‰
L1465         g_score_all = df_z.mul(pd.Series(cfg.weights.g)).sum(axis=1)
L1466
L1467         d_comp = pd.concat({
L1468             'QAL': df_z['D_QAL'],
L1469             'YLD': df_z['D_YLD'],
L1470             'VOL': df_z['D_VOL_RAW'],
L1471             'TRD': df_z['D_TRD']
L1472         }, axis=1)
L1473         dw = pd.Series(cfg.weights.d, dtype=float).reindex(['QAL','YLD','VOL','TRD']).fillna(0.0)
L1474         globals()['D_WEIGHTS_EFF'] = dw.copy()
L1475         d_score_all = d_comp.mul(dw, axis=1).sum(axis=1)
L1476
L1477         # â‘¡ ãƒ†ãƒ³ãƒ—ãƒ¬åˆ¤å®šï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ãã®ã¾ã¾ï¼‰
L1478         mask = df['trend_template']
L1479         if not bool(mask.any()):
L1480             mask = ((df.get('P_OVER_LOW52', np.nan) >= 0.25) &
L1481                 (df.get('NEAR_52W_HIGH', np.nan) >= -0.30) &
L1482                 (df.get('RS', np.nan) >= 0.08) &
L1483                 (df.get('MA200_SLOPE_1M', np.nan) > 0) &
L1484                 (df.get('P_OVER_150', np.nan) > 0) & (df.get('P_OVER_200', np.nan) > 0) &
L1485                 (df.get('MA150_OVER_200', np.nan) > 0) &
L1486                 (df.get('MA50_OVER_150', np.nan) > 0) & (df.get('MA50_OVER_200', np.nan) > 0) &
L1487                 (df.get('TR_str', np.nan) > 0)).fillna(False)
L1488             df['trend_template'] = mask
L1489
L1490         # â‘¢ æ¡ç”¨ç”¨ã¯ maskã€è¡¨ç¤º/åˆ†æç”¨ã¯åˆ—ã§å…¨éŠ˜æŸ„ä¿å­˜
L1491         g_score = g_score_all.loc[mask]
L1492         Scorer.g_score = g_score
L1493         df_z['GSC'] = g_score_all
L1494         df_z['DSC'] = d_score_all
L1495
L1496         try:
L1497             current = (pd.read_csv("current_tickers.csv")
L1498                   .iloc[:, 0]
L1499                   .str.upper()
L1500                   .tolist())
L1501         except FileNotFoundError:
L1502             warnings.warn("current_tickers.csv not found â€” bonus skipped")
L1503             current = []
L1504
L1505         mask_bonus = g_score.index.isin(current)
L1506         if mask_bonus.any():
L1507             # 1) factor.BONUS_COEFF ã‹ã‚‰ k ã‚’æ±ºã‚ã€ç„¡ã‘ã‚Œã° 0.4
L1508             k = float(getattr(sys.modules.get("factor"), "BONUS_COEFF", 0.4))
L1509             # 2) g å´ã® Ïƒ ã‚’å–ã‚Šã€NaN ãªã‚‰ 0 ã«ä¸¸ã‚ã‚‹
L1510             sigma_g = g_score.std()
L1511             if pd.isna(sigma_g):
L1512                 sigma_g = 0.0
L1513             bonus_g = round(k * sigma_g, 3)
L1514             g_score.loc[mask_bonus] += bonus_g
L1515             Scorer.g_score = g_score
L1516             # 3) D å´ã‚‚åŒæ§˜ã« Ïƒ ã® NaN ã‚’ã‚±ã‚¢
L1517             sigma_d = d_score_all.std()
L1518             if pd.isna(sigma_d):
L1519                 sigma_d = 0.0
L1520             bonus_d = round(k * sigma_d, 3)
L1521             d_score_all.loc[d_score_all.index.isin(current)] += bonus_d
L1522
L1523         try:
L1524             df = _apply_growth_entry_flags(df, ib, self, win_breakout=5, win_pullback=5)
L1525         except Exception:
L1526             pass
L1527
L1528         df_full = df.copy()
L1529         df_full_z = df_z.copy()
L1530
L1531         from factor import FeatureBundle  # type: ignore  # å®Ÿè¡Œæ™‚importãªã—ï¼ˆå¾ªç’°å›é¿ï¼‰
L1532         return FeatureBundle(df=df,
L1533             df_z=df_z,
L1534             g_score=g_score,
L1535             d_score_all=d_score_all,
L1536             missing_logs=pd.DataFrame(missing_logs),
L1537             df_full=df_full,
L1538             df_full_z=df_full_z,
L1539             scaler=None)
L1540
L1541 def _apply_growth_entry_flags(feature_df, bundle, self_obj, win_breakout=5, win_pullback=5):
L1542     """
L1543     Gæ ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã«å¯¾ã—ã€ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®š/æŠ¼ã—ç›®åç™ºã®ã€Œç›´è¿‘Nå–¶æ¥­æ—¥å†…ã®ç™ºç«ã€ã‚’åˆ¤å®šã—ã€
L1544     æ¬¡ã®åˆ—ã‚’ feature_df ã«è¿½åŠ ã™ã‚‹ï¼ˆindex=tickerï¼‰ã€‚
L1545       - G_BREAKOUT_recent_5d : bool
L1546       - G_BREAKOUT_last_date : str "YYYY-MM-DD"
L1547       - G_PULLBACK_recent_5d : bool
L1548       - G_PULLBACK_last_date : str "YYYY-MM-DD"
L1549       - G_PIVOT_price        : float
L1550     å¤±æ•—ã—ã¦ã‚‚ä¾‹å¤–ã¯æ¡ã‚Šæ½°ã—ã€æ—¢å­˜å‡¦ç†ã‚’é˜»å®³ã—ãªã„ã€‚
L1551     """
L1552     try:
L1553         px   = bundle.px                      # çµ‚å€¤ DataFrame
L1554         hi   = bundle.data['High']
L1555         lo   = bundle.data['Low']
L1556         vol  = bundle.data['Volume']
L1557         bench= bundle.spx                     # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ Series
L1558
L1559         # Gãƒ¦ãƒ‹ãƒãƒ¼ã‚¹æ¨å®šï¼šself.g_universe å„ªå…ˆ â†’ feature_df['group']=='G' â†’ å…¨éŠ˜æŸ„
L1560         g_universe = getattr(self_obj, "g_universe", None)
L1561         if g_universe is None:
L1562             try:
L1563                 g_universe = feature_df.index[feature_df['group'].astype(str).str.upper().eq('G')].tolist()
L1564             except Exception:
L1565                 g_universe = list(feature_df.index)
L1566         if not g_universe:
L1567             return feature_df
L1568
L1569         # æŒ‡æ¨™
L1570         px = px.ffill(limit=2)
L1571         ema21 = px[g_universe].ewm(span=21, adjust=False).mean()
L1572         ma50  = px[g_universe].rolling(50).mean()
L1573         ma150 = px[g_universe].rolling(150).mean()
L1574         ma200 = px[g_universe].rolling(200).mean()
L1575         atr20 = (hi[g_universe] - lo[g_universe]).rolling(20).mean()
L1576         vol20 = vol[g_universe].rolling(20).mean()
L1577         vol50 = vol[g_universe].rolling(50).mean()
L1578
L1579         # ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆåˆæ ¼
L1580         trend_template_ok = (px[g_universe] > ma50) & (px[g_universe] > ma150) & (px[g_universe] > ma200) \
L1581                             & (ma150 > ma200) & (ma200.diff() > 0)
L1582
L1583         # æ±ç”¨ãƒ”ãƒœãƒƒãƒˆï¼šç›´è¿‘65å–¶æ¥­æ—¥ã®é«˜å€¤ï¼ˆå½“æ—¥é™¤å¤–ï¼‰
L1584         pivot_price = hi[g_universe].rolling(65).max().shift(1)
L1585
L1586         # ç›¸å¯¾åŠ›ï¼šå¹´å†…é«˜å€¤æ›´æ–°
L1587         bench_aligned = bench.reindex(px.index).ffill()
L1588         rs = px[g_universe].div(bench_aligned, axis=0)
L1589         rs_high = rs.rolling(252).max().shift(1)
L1590
L1591         # ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆã€Œç™ºç”Ÿæ—¥ã€ï¼šæ¡ä»¶ç«‹ã¡ä¸ŠãŒã‚Š
L1592         breakout_today = trend_template_ok & (px[g_universe] > pivot_price) \
L1593                          & (vol[g_universe] >= 1.5 * vol50) & (rs > rs_high)
L1594         breakout_event = breakout_today & ~breakout_today.shift(1).fillna(False)
L1595
L1596         # æŠ¼ã—ç›®åç™ºã€Œç™ºç”Ÿæ—¥ã€ï¼šEMA21å¸¯Ã—å‡ºæ¥é«˜ãƒ‰ãƒ©ã‚¤ã‚¢ãƒƒãƒ—Ã—å‰æ—¥é«˜å€¤è¶ŠãˆÃ—çµ‚å€¤EMA21ä¸Š
L1597         near_ema21_band = px[g_universe].between(ema21 - atr20, ema21 + atr20)
L1598         volume_dryup = (vol20 / vol50) <= 1.0
L1599         pullback_bounce_confirmed = (px[g_universe] > hi[g_universe].shift(1)) & (px[g_universe] > ema21)
L1600         pullback_today = trend_template_ok & near_ema21_band & volume_dryup & pullback_bounce_confirmed
L1601         pullback_event = pullback_today & ~pullback_today.shift(1).fillna(False)
L1602
L1603         # ç›´è¿‘Nå–¶æ¥­æ—¥å†…ã®ç™ºç« / æœ€çµ‚ç™ºç”Ÿæ—¥
L1604         rows = []
L1605         for t in g_universe:
L1606             def _recent_and_date(s, win):
L1607                 sw = s[t].iloc[-win:]
L1608                 if sw.any():
L1609                     d = sw[sw].index[-1]
L1610                     return True, d.strftime("%Y-%m-%d")
L1611                 return False, ""
L1612             br_recent, br_date = _recent_and_date(breakout_event, win_breakout)
L1613             pb_recent, pb_date = _recent_and_date(pullback_event, win_pullback)
L1614             rows.append((t, {
L1615                 "G_BREAKOUT_recent_5d": br_recent,
L1616                 "G_BREAKOUT_last_date": br_date,
L1617                 "G_PULLBACK_recent_5d": pb_recent,
L1618                 "G_PULLBACK_last_date": pb_date,
L1619                 "G_PIVOT_price": float(pivot_price[t].iloc[-1]) if t in pivot_price.columns else float('nan'),
L1620             }))
L1621         flags = pd.DataFrame({k: v for k, v in rows}).T
L1622
L1623         # åˆ—ã‚’ä½œæˆãƒ»ä¸Šæ›¸ã
L1624         cols = ["G_BREAKOUT_recent_5d","G_BREAKOUT_last_date","G_PULLBACK_recent_5d","G_PULLBACK_last_date","G_PIVOT_price"]
L1625         for c in cols:
L1626             if c not in feature_df.columns:
L1627                 feature_df[c] = np.nan
L1628         feature_df.loc[flags.index, flags.columns] = flags
L1629
L1630     except Exception:
L1631         pass
L1632     return feature_df
L1633
```

## <.github/workflows/weekly-report.yml>
```text
L1 name: Weekly Stock Report
L2
L3 on:
L4   push:
L5     branches: [ main ]
L6     paths-ignore:
L7       - 'CodeForChat/**'
L8   schedule:
L9     - cron: '0 0 * * 6'  # UTC 00:00 â†’ JST 09:00ï¼ˆåœŸï¼‰
L10   workflow_dispatch:
L11
L12 jobs:
L13   build-and-report:
L14     runs-on: ubuntu-latest
L15     permissions:
L16       contents: write
L17
L18     steps:
L19       - name: Debug start
L20         run: echo 'ğŸš€ DEBUGstarted'
L21               
L22       - name: Checkout repository
L23         uses: actions/checkout@v3
L24
L25       - name: Setup Python
L26         uses: actions/setup-python@v5
L27         with:
L28           python-version: '3.x'
L29           cache: 'pip'
L30           cache-dependency-path: requirements.txt
L31
L32       - name: Install dependencies
L33         run: pip install -r requirements.txt
L34
L35       - name: Prepare results directory
L36         run: mkdir -p results
L37
L38       - name: Run factor & scoring
L39         env:
L40           SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
L41           FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
L42           FIN_THREADS: "8"
L43         run: python factor.py
```

## <documents/README.md>
```text
L1 # é‹ç”¨ãƒ«ãƒ¼ãƒ«
L2
L3 ## åŸºæœ¬æ§‹æˆ
L4 - 20éŠ˜æŸ„ã‚’å‡ç­‰é…åˆ†ï¼ˆç¾é‡‘ã‚’é™¤ã1éŠ˜æŸ„ã‚ãŸã‚Š5%ï¼‰
L5 - moomooè¨¼åˆ¸ã§é‹ç”¨
L6 - **Growthæ  12éŠ˜æŸ„ / Defenseæ  8éŠ˜æŸ„**ï¼ˆNORMAL åŸºæº–ï¼‰
L7
L8 ## Barbell Growth-Defenseæ–¹é‡
L9 - Growthæ  **12éŠ˜æŸ„**ï¼šé«˜æˆé•·ã§ä¹–é›¢æºã¨ãªã‚‹æ”»ã‚ã®éŠ˜æŸ„
L10 - Defenseæ  **8éŠ˜æŸ„**ï¼šä½ãƒœãƒ©ã§å®‰å®šæˆé•·ã—é…å½“ã‚’å¢—ã‚„ã™å®ˆã‚Šã®éŠ˜æŸ„
L11 - ã€ŒçŒ›çƒˆã«ä¼¸ã³ã‚‹æ”»ã‚ Ã— ç€å®Ÿã«ç¨¼ãç›¾ã€ã®çµ„åˆã›ã§ä¹–é›¢â†’åŠæˆ»ã—ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ã‚’ç‹™ã†
L12
L13 ## ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¤å®šï¼ˆtrend_template åˆæ ¼â€œæœ¬æ•°â€ã§åˆ¤å®šï¼‰
L14 - åˆæ ¼æœ¬æ•° = current+candidate å…¨ä½“ã®ã†ã¡ã€trend_template æ¡ä»¶ã‚’æº€ãŸã—ãŸéŠ˜æŸ„ã®**æœ¬æ•°(C)**ï¼ˆåŸºæº– N_G=12ï¼‰
L15 - ã—ãã„å€¤ã¯éå»~600å–¶æ¥­æ—¥ã®åˆ†å¸ƒã‹ã‚‰**æ¯å›è‡ªå‹•æ¡ç”¨**ï¼ˆåˆ†ä½ç‚¹ã¨é‹ç”¨â€œåºŠâ€ã®maxï¼‰
L16   - ç·Šæ€¥å…¥ã‚Š: `max(q05, 12æœ¬)`ï¼ˆ= N_Gï¼‰
L17   - ç·Šæ€¥è§£é™¤: `max(q20, 18æœ¬)`ï¼ˆ= ceil(1.5Ã—12)ï¼‰
L18   - é€šå¸¸å¾©å¸°: `max(q60, 36æœ¬)`ï¼ˆ= 3Ã—N_Gï¼‰
L19 - ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹: å‰å›ãƒ¢ãƒ¼ãƒ‰ã«ä¾å­˜ï¼ˆEMERGâ†’è§£é™¤ã¯23æœ¬ä»¥ä¸Šã€CAUTIONâ†’é€šå¸¸ã¯45æœ¬ä»¥ä¸Šï¼‰
L20
L21 ## ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¥ã®ç¾é‡‘ãƒ»ãƒ‰ãƒªãƒ•ãƒˆ
L22  - **é€šå¸¸(NORMAL)** : ç¾é‡‘ **10%** / ãƒ‰ãƒªãƒ•ãƒˆé–¾å€¤ **12%**
L23  - **è­¦æˆ’(CAUTION)** : ç¾é‡‘ **12.5%** / ãƒ‰ãƒªãƒ•ãƒˆé–¾å€¤ **14%**
L24  - **ç·Šæ€¥(EMERG)** : ç¾é‡‘ **20%** / **ãƒ‰ãƒªãƒ•ãƒˆå£²è²·åœæ­¢**ï¼ˆ20Ã—5%ã«å…¨æˆ»ã—ã®ã¿ï¼‰
L25
L26 ## ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®æ¨å¥¨â€œä¿æœ‰éŠ˜æŸ„æ•°â€ï¼ˆMMFâ‰’ç¾é‡‘ï¼‰
L27 *å„æ =5%ï¼ˆ20éŠ˜æŸ„å‡ç­‰ï¼‰ã€‚ãƒ¢ãƒ¼ãƒ‰ç§»è¡Œæ™‚ã¯**Gã®æ æ•°ã®ã¿**èª¿æ•´ã—ã€å¤–ã—ãŸæ ã¯ç¾é‡‘ã¨ã—ã¦ä¿æŒã€‚*
L28
L29 - **NORMAL:** G **12** / D **8** / ç¾é‡‘åŒ–æ  **0**  
L30 - **CAUTION:** G **10** / D **8** / ç¾é‡‘åŒ–æ  **2**ï¼ˆ= 10%ï¼‰  
L31 - **EMERG:** G **8**  / D **8** / ç¾é‡‘åŒ–æ  **4**ï¼ˆ= 20%ï¼‰  
L32
L33 > å®Ÿé‹ç”¨ï¼šâ­ï¸ä½ã‚¹ã‚³ã‚¢ã®Gã‹ã‚‰é †ã«å¤–ã™ã€‚è§£é™¤æ™‚ã¯factorä¸Šä½ã‹ã‚‰è£œå……ã€‚
L34
L35 ## ãƒˆãƒ¬ãƒ¼ãƒªãƒ³ã‚°ã‚¹ãƒˆãƒƒãƒ—
L36 - **åŸºæœ¬TS (ãƒ¢ãƒ¼ãƒ‰åˆ¥):** NORMAL **15%** / CAUTION **13%** / EMERG **10%**
L37 - å«ã¿ç›ŠãŒ **+30% / +60% / +100%** åˆ°é”ã§ã€åŸºæœ¬ã‹ã‚‰ **-3pt / -6pt / -8pt** å¼•ãä¸Šã’
L38 - TSç™ºå‹•ã§æ¸›å°‘ã—ãŸéŠ˜æŸ„ã¯ç¿Œæ—¥ä»¥é™ã«è£œå……ï¼ˆâ€»ç·Šæ€¥ãƒ¢ãƒ¼ãƒ‰ä¸­ã¯è£œå……ã—ãªã„ï¼‰
L39
L40 ## åŠæˆ»ã—ï¼ˆãƒªãƒãƒ©ãƒ³ã‚¹ï¼‰æ‰‹é †
L41 ãƒ‰ãƒªãƒ•ãƒˆãƒã‚§ãƒƒã‚¯ã§**ã‚¢ãƒ©ãƒ¼ãƒˆ**ãŒå‡ºãŸå ´åˆï¼ˆåˆè¨ˆ|drift| ãŒãƒ¢ãƒ¼ãƒ‰é–¾å€¤ã‚’è¶…éã€EMERGé™¤ãï¼‰ã€ç¿Œå–¶æ¥­æ—¥ã®ç±³å›½å¯„ä»˜ãã§ä¸‹è¨˜ã‚’å®Ÿæ–½ã™ã‚‹ã€‚
L42
L43 1. **å£²å´ï¼ˆå¿…é ˆï¼‰**  
L44    Slackãƒ†ãƒ¼ãƒ–ãƒ«ã® **Î”qty ãŒãƒã‚¤ãƒŠã‚¹ã®éŠ˜æŸ„ã‚’å£²å´** ã™ã‚‹ï¼ˆå¯„ä»˜ãæˆè¡Œæ¨å¥¨ï¼‰ã€‚  
L45    ã“ã‚Œã¯ã€ŒåŠæˆ»ã—ã€è¨ˆç®—ã«åŸºã¥ãéé‡é‡ã®å‰Šæ¸›ã‚’æ„å‘³ã™ã‚‹ã€‚
L46
L47 2. **è³¼å…¥ï¼ˆä»»æ„ãƒ»åŠæˆ»ã—ç›®å®‰ï¼‰**  
L48    åŠæˆ»ã—å¾Œã®åˆè¨ˆ|drift|ã‚’**ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤ï¼ˆSlackãƒ˜ãƒƒãƒ€ã«è¡¨ç¤ºï¼‰**ã«è¿‘ã¥ã‘ã‚‹ã“ã¨ã‚’ç›®å®‰ã«ã€  
L49    **ä»»æ„ã®éŠ˜æŸ„ã‚’è²·ã„å¢—ã—**ã—ã¦ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹ï¼ˆÎ”qtyãŒãƒ—ãƒ©ã‚¹ã®éŠ˜æŸ„ã‚’å„ªå…ˆã—ã¦ã‚‚ã‚ˆã„ï¼‰ã€‚
L50
L51 3. **ãƒˆãƒ¬ãƒ¼ãƒªãƒ³ã‚°ã‚¹ãƒˆãƒƒãƒ—ã®å†è¨­å®šï¼ˆå¿…é ˆï¼‰**  
L52    ã™ã¹ã¦ã®ä¿æœ‰éŠ˜æŸ„ã«ã¤ã„ã¦ã€æœ€æ–°ã®è©•ä¾¡é¡ã«åˆã‚ã›ã¦TSã‚’**å†ç™ºæ³¨ï¼æ›´æ–°**ã™ã‚‹ã€‚  
L53    ãƒ«ãƒ¼ãƒ«ã¯ä¸‹è¨˜ï¼ˆåˆ©ç›Šåˆ°é”ã§æ®µéšçš„ã«ã‚¿ã‚¤ãƒˆåŒ–ï¼‰ï¼š  
L54    - **åŸºæœ¬TS:** -15%  
L55    - **+30% åˆ°é” â†’ TS -12%**  
L56    - **+60% åˆ°é” â†’ TS -9%**  
L57    - **+100% åˆ°é” â†’ TS -7%**  
L58    â€»ã‚¹ãƒˆãƒƒãƒ—ä¾¡æ ¼ã®å¼•ãä¸Šã’ã¯è¨±å¯ã€**å¼•ãä¸‹ã’ã¯ä¸å¯**ï¼ˆåˆ©ç›Šä¿å…¨ã®åŸå‰‡ï¼‰ã€‚
L59
L60 4. **ä¾‹å¤–ï¼ˆEMERGãƒ¢ãƒ¼ãƒ‰ï¼‰**  
L61    ç·Šæ€¥(EMERG)ã§ã¯**ãƒ‰ãƒªãƒ•ãƒˆç”±æ¥ã®å£²è²·ã¯åœæ­¢ï¼ˆâˆï¼‰**ã€‚20éŠ˜æŸ„Ã—å„5%ã¸ã®**å…¨æˆ»ã—**ã®ã¿è¨±å®¹ã€‚
L62
L63 5. **å®Ÿè¡Œã‚¿ã‚¤ãƒŸãƒ³ã‚°**
L64    - åˆ¤å®šï¼šç±³å›½å¸‚å ´çµ‚å€¤ç›´å¾Œ
L65    - åŸ·è¡Œï¼šç¿Œå–¶æ¥­æ—¥ã®ç±³å›½å¯„ä»˜ãæˆè¡Œ
L66
L67 ## ãƒ¢ãƒ¼ãƒ‰ç§»è¡Œã®å®Ÿå‹™æ‰‹é †ï¼ˆè¶…ã‚·ãƒ³ãƒ—ãƒ«ï¼‰
L68 ãƒ¢ãƒ¼ãƒ‰ãŒå¤‰ã‚ã£ãŸã‚‰ã€**MMFâ‰’ç¾é‡‘**ã¨ã—ã¦æ‰±ã„ã€**Gã®æ æ•°ã ã‘**ã‚’èª¿æ•´ã™ã‚‹ï¼š
L69 1. **Gã‚’å‰Šã‚‹**ï¼ˆCAUTION/EMERGï¼‰  
L70    - â­ï¸ä½ã‚¹ã‚³ã‚¢ã®Gã‹ã‚‰é †ã«å¤–ã™ã€‚  
L71    - **`current_tickers.csv` ã‹ã‚‰å¤–ã™GéŠ˜æŸ„ã®è¡Œã‚’å‰Šé™¤**ï¼ˆï¼ãã®æ ã¯ç¾é‡‘åŒ–ï¼‰ã€‚
L72 2. **ç¾é‡‘ã¨ã—ã¦ä¿æŒ**  
L73    - å¤–ã—ãŸæ ã¯ç¾é‡‘ï¼ˆã¾ãŸã¯MMFç›¸å½“ï¼‰ã§ãƒ—ãƒ¼ãƒ«ã€‚  
L74 3. **å¾©å¸°æ™‚ã®è£œå……**ï¼ˆNORMALã¸ï¼‰  
L75    - **`current_tickers.csv` ã«éŠ˜æŸ„ã‚’è¿½åŠ **ï¼ˆfactorä¸Šä½ã‹ã‚‰ï¼‰ã€‚  
L76    - ä»¥é™ã¯æ—¥æ¬¡ãƒ‰ãƒªãƒ•ãƒˆ/TSãƒ«ãƒ¼ãƒ«ã«å¾“ã†ã€‚
L77
L78 > driftã¯ `target_ratio = 1/éŠ˜æŸ„æ•°` ã‚’è‡ªå‹•é©ç”¨ã€‚è¡Œæ•°ã«å¿œã˜ã¦è‡ªå‹•ã§å‡ç­‰æ¯”ç‡ãŒå†è¨ˆç®—ã•ã‚Œã‚‹ã€‚
L79
L80 ## å…¥æ›¿éŠ˜æŸ„é¸å®š
L81 - Oxfordã‚­ãƒ£ãƒ”ã‚¿ãƒ«ï¼ã‚¤ãƒ³ã‚«ãƒ ã€Alpha Investorã€Motley Fool Stock Advisorã€moomooã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ç­‰ã‚’å‚è€ƒã«chatGPTã§æ¤œè¨
L82 - å¹´é–“NISAæ ã¯Growthç¾¤ã®ä¸­ã‹ã‚‰ä½ãƒœãƒ©éŠ˜æŸ„ã‚’é¸å®šã—åˆ©ç”¨ã€‚é•·æœŸä¿æŒã«ã¯ã“ã ã‚ã‚‰ãªã„ã€‚
L83
L84 ## å†ã‚¨ãƒ³ãƒˆãƒªãƒ¼ï¼ˆã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ï¼‰
L85 - TSãƒ’ãƒƒãƒˆå¾Œã®åŒéŠ˜æŸ„å†INã¯ **8å–¶æ¥­æ—¥** ã®ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ã‚’è¨­ã‘ã‚‹ï¼ˆæœŸé–“ä¸­ã¯å†INç¦æ­¢ï¼‰
L86
L87 ## å®Ÿè¡Œã‚¿ã‚¤ãƒŸãƒ³ã‚°
L88 - åˆ¤å®šï¼šç±³å›½å¸‚å ´çµ‚å€¤ç›´å¾Œ
L89 - åŸ·è¡Œï¼šç¿Œå–¶æ¥­æ—¥ã®ç±³å›½å¯„ä»˜ãæˆè¡Œ
```

## <documents/factor_design.md>
```text
L1 # factor.py è©³ç´°è¨­è¨ˆæ›¸
L2
L3 ## æ¦‚è¦
L4 - æ—¢å­˜ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®éŠ˜æŸ„ã¨æ¤œè¨ä¸­ã®éŠ˜æŸ„ç¾¤ã‚’åŒæ™‚ã«æ‰±ã†éŠ˜æŸ„é¸å®šãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€‚
L5 - ä¾¡æ ¼ãƒ»è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šè¾¼ã¿ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã¨DRRSé¸å®šã‚’è¡Œã†ã“ã¨ã§ã€ä»¥ä¸‹ã®ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆã‚’å¾—ã‚‹ã€‚
L6   - æ¡ç”¨éŠ˜æŸ„ã¨æƒœã—ãã‚‚æ¼ã‚ŒãŸéŠ˜æŸ„ã®ã‚¹ã‚³ã‚¢ä¸€è¦§
L7   - IN/OUTã®ãƒ†ã‚£ãƒƒã‚«ãƒ¼ãƒªã‚¹ãƒˆã¨OUTå´ã®ä½ã‚¹ã‚³ã‚¢éŠ˜æŸ„
L8   - æ–°æ—§ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®æ¯”è¼ƒè¡¨
L9   - æ¤œè¨ä¸­éŠ˜æŸ„ã®ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆæ•´ç†ç”¨ï¼‰
L10
L11 ## å…¨ä½“ãƒ•ãƒ­ãƒ¼
L12 1. **Input** â€“ `current_tickers.csv`ã¨`candidate_tickers.csv`ã‚’èª­ã¿è¾¼ã¿ã€yfinanceã‚„Finnhubã®APIã‹ã‚‰ä¾¡æ ¼ãƒ»è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã¦`InputBundle`ã‚’æ•´å‚™ã€‚
L13 2. **Score Calculation** â€“ ScorerãŒç‰¹å¾´é‡ã‚’è¨ˆç®—ã—å› å­ã‚¹ã‚³ã‚¢ã‚’åˆæˆã—ã¦`FeatureBundle`ã‚’ç”Ÿæˆã€‚
L14 3. **Correlation Reduction & Selection** â€“ SelectorãŒDRRSãƒ­ã‚¸ãƒƒã‚¯ã§ç›¸é–¢ã‚’æŠ‘ãˆã¤ã¤G/DéŠ˜æŸ„ã‚’é¸å®šã—`SelectionBundle`ã‚’å¾—ã‚‹ã€‚
L15 4. **Output** â€“ æ¡ç”¨çµæœã¨å‘¨è¾ºæƒ…å ±ã‚’è¡¨ãƒ»Slacké€šçŸ¥ã¨ã—ã¦å‡ºåŠ›ã€‚
L16
L17 ```mermaid
L18 flowchart LR
L19   A[Input\nAPI & å‰å‡¦ç†] --> B[Score Calculation\nç‰¹å¾´é‡ãƒ»å› å­åˆæˆ]
L20   B --> C[Correlation Reduction\nDRRSé¸å®š]
L21   C --> D[Output\nSlacké€šçŸ¥]
L22 ```
L23
L24 ## å®šæ•°ãƒ»è¨­å®š
L25 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L26 | --- | --- | --- |
L27 | `exist` / `cand` | ç¾è¡Œãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã¨æ¤œè¨ä¸­éŠ˜æŸ„ã®ãƒ†ã‚£ãƒƒã‚«ãƒ¼ãƒªã‚¹ãƒˆ | ã‚¹ã‚³ã‚¢å¯¾è±¡ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã®æ§‹æˆã€å€™è£œæ•´ç† |
L28 | `bench` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼ | ç›¸å¯¾å¼·ã•ãƒ»Î²ç®—å‡ºã€ãƒãƒ¼ãƒˆæ¯”è¼ƒ |
L29 | `CAND_PRICE_MAX` | å€™è£œéŠ˜æŸ„ã®è¨±å®¹ä¾¡æ ¼ä¸Šé™ | é«˜é¡éŠ˜æŸ„ã®äº‹å‰é™¤å¤– |
L30 | `N_G` / `N_D` | G/Dæ¡ç”¨æ ã®ä»¶æ•°ï¼ˆ**æ—¢å®š: 12 / 8**ï¼‰ | æœ€çµ‚çš„ã«é¸ã¶éŠ˜æŸ„æ•°ã®åˆ¶ç´„ |
L31 | `g_weights` / `D_weights` | å„å› å­ã®é‡ã¿dict | G/Dã‚¹ã‚³ã‚¢åˆæˆ |
L32 | `D_BETA_MAX` | Dãƒã‚±ãƒƒãƒˆã®è¨±å®¹Î²ä¸Šé™ | é«˜Î²éŠ˜æŸ„ã®é™¤å¤–ãƒ•ã‚£ãƒ«ã‚¿ |
L33 | `FILTER_SPEC` | G/Dã”ã¨ã®å‰å‡¦ç†ãƒ•ã‚£ãƒ«ã‚¿ | ãƒˆãƒ¬ãƒ³ãƒ‰ãƒã‚¹ã‚¯ã‚„Î²ä¸Šé™è¨­å®š |
L34 | `corrM` | DRRSåˆæœŸãƒ—ãƒ¼ãƒ«ã®æœ€å¤§ä»¶æ•° | ç›¸é–¢è¡Œåˆ—ã‚µã‚¤ã‚ºåˆ¶å¾¡ |
L35 | `DRRS_G` / `DRRS_D` | DRRSãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿dict | ãƒã‚±ãƒƒãƒˆåˆ¥ã®ç›¸é–¢ä½æ¸›è¨­å®š |
L36 | `DRRS_SHRINK` | æ®‹å·®ç›¸é–¢ã®å¯¾è§’ã‚·ãƒ¥ãƒªãƒ³ã‚¯ç‡ | `residual_corr`ã®å®‰å®šåŒ– |
L37 | `CROSS_MU_GD` | G-Dé–“ã‚¯ãƒ­ã‚¹ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£Î¼ | 2ãƒã‚±ãƒƒãƒˆåŒæ™‚æœ€é©åŒ–ã§ç›¸é–¢æŠ‘åˆ¶ |
L38 | `RESULTS_DIR` | é¸å®šçµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª | `_save_sel`/`_load_prev`ã®å…¥å‡ºåŠ› |
L39
L40 é¸å®šçµæœã¯`results/`é…ä¸‹ã«JSONã¨ã—ã¦ä¿å­˜ã—ã€æ¬¡å›å®Ÿè¡Œæ™‚ã«`_load_prev`ã§èª­ã¿è¾¼ã‚“ã§é¸å®šæ¡ä»¶ã«åæ˜ ã€‚
L41
L42 ## DTO/Config
L43 å„ã‚¹ãƒ†ãƒƒãƒ—é–“ã§å—ã‘æ¸¡ã™ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¨è¨­å®šå€¤ã€‚å¤‰æ•°ã®æ„å‘³åˆã„ã¨åˆ©ç”¨ç®‡æ‰€ã‚’ä»¥ä¸‹ã«ç¤ºã™ã€‚
L44
L45 ### InputBundleï¼ˆInput â†’ Scorerï¼‰
L46 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L47 | --- | --- | --- |
L48 | `cand` | å€™è£œéŠ˜æŸ„ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã®ãƒªã‚¹ãƒˆ | OUTãƒ†ãƒ¼ãƒ–ãƒ«ã‚„ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°å¯¾è±¡ã®æ¯é›†å›£ |
L49 | `tickers` | ç¾è¡Œ+å€™è£œã‚’åˆã‚ã›ãŸãƒ†ã‚£ãƒƒã‚«ãƒ¼ä¸€è¦§ | ä¾¡æ ¼ãƒ»å‡ºæ¥é«˜ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€ãƒªã‚¿ãƒ¼ãƒ³è¨ˆç®— |
L50 | `bench` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼ | ç›¸å¯¾å¼·ã•ãƒ»Î²ç®—å‡ºã€ãƒãƒ¼ãƒˆæ¯”è¼ƒ |
L51 | `data` | yfinanceã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœï¼ˆéšå±¤åˆ—ï¼‰ | `px`/`spx`/ãƒªã‚¿ãƒ¼ãƒ³ç­‰ã®åŸºç¤ãƒ‡ãƒ¼ã‚¿ |
L52 | `px` | `data['Close']`ã ã‘ã‚’æŠœãå‡ºã—ãŸä¾¡æ ¼ç³»åˆ— | æŒ‡æ¨™è¨ˆç®—ãƒ»ãƒªã‚¿ãƒ¼ãƒ³ç”Ÿæˆ |
L53 | `spx` | `data['Close'][bench]` ã®Series | `rs`ã‚„`calc_beta`ã®åŸºæº–æŒ‡æ•° |
L54 | `tickers_bulk` | `yf.Tickers`ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ | `info`ç­‰ã®ä¸€æ‹¬å–å¾— |
L55 | `info` | ãƒ†ã‚£ãƒƒã‚«ãƒ¼åˆ¥ã®yfinanceæƒ…å ±dict | ã‚»ã‚¯ã‚¿ãƒ¼åˆ¤å®šã‚„EPSè£œå®Œ |
L56 | `eps_df` | EPS TTM/ç›´è¿‘EPSç­‰ã‚’ã¾ã¨ã‚ãŸè¡¨ | æˆé•·æŒ‡æ¨™ã®ç®—å‡º |
L57 | `fcf_df` | CFOãƒ»CapExãƒ»FCF TTMã¨æƒ…å ±æºãƒ•ãƒ©ã‚° | FCF/EVã‚„é…å½“ã‚«ãƒãƒ¬ãƒƒã‚¸ |
L58 | `returns` | `px.pct_change()`ã®ãƒªã‚¿ãƒ¼ãƒ³è¡¨ | ç›¸é–¢è¡Œåˆ—ãƒ»DRRSè¨ˆç®— |
L59
L60 ### FeatureBundleï¼ˆScorer â†’ Selectorï¼‰
L61 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L62 | --- | --- | --- |
L63 | `df` | è¨ˆç®—æ¸ˆã¿æŒ‡æ¨™ã®ç”Ÿå€¤ãƒ†ãƒ¼ãƒ–ãƒ« | ãƒ‡ãƒãƒƒã‚°ãƒ»å‡ºåŠ›è¡¨ç¤º |
L64 | `df_z` | ã‚¦ã‚£ãƒ³ã‚¶ãƒ¼å¾ŒZã‚¹ã‚³ã‚¢åŒ–ã—ãŸæŒ‡æ¨™è¡¨ | å› å­ã‚¹ã‚³ã‚¢åˆæˆã€é¸å®šåŸºæº– |
L65 | `g_score` | Gãƒã‚±ãƒƒãƒˆç·åˆã‚¹ã‚³ã‚¢ | Gé¸å®šã€IN/OUTæ¯”è¼ƒ |
L66 | `d_score_all` | Dãƒã‚±ãƒƒãƒˆç·åˆã‚¹ã‚³ã‚¢ï¼ˆå…¨éŠ˜æŸ„ï¼‰ | Dé¸å®šã€ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚° |
L67 | `missing_logs` | æ¬ ææŒ‡æ¨™ã¨è£œå®ŒçŠ¶æ³ã®ãƒ­ã‚° | ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ |
L68
L69 ### SelectionBundleï¼ˆSelector â†’ Outputï¼‰
L70 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L71 | --- | --- | --- |
L72 | `resG` | Gé¸å®šçµæœã®è©³ç´°dictï¼ˆ`tickers`ã€ç›®çš„å€¤ç­‰ï¼‰ | çµæœä¿å­˜ãƒ»å¹³å‡ç›¸é–¢ãªã©ã®æŒ‡æ¨™è¡¨ç¤º |
L73 | `resD` | Dé¸å®šçµæœã®è©³ç´°dict | åŒä¸Š |
L74 | `top_G` | æœ€çµ‚æ¡ç”¨Gãƒ†ã‚£ãƒƒã‚«ãƒ¼ | æ–°ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæ§‹ç¯‰ |
L75 | `top_D` | æœ€çµ‚æ¡ç”¨Dãƒ†ã‚£ãƒƒã‚«ãƒ¼ | åŒä¸Š |
L76 | `init_G` | DRRSå‰ã®GåˆæœŸå€™è£œ | æƒœã—ãã‚‚å¤–ã‚ŒãŸéŠ˜æŸ„è¡¨ç¤º |
L77 | `init_D` | DRRSå‰ã®DåˆæœŸå€™è£œ | åŒä¸Š |
L78
L79 ### WeightsConfig
L80 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L81 | --- | --- | --- |
L82 | `g` | Gå› å­ï¼ˆGRW/MOM/VOLï¼‰ã®é‡ã¿dict | `g_score`åˆæˆ |
L83 | `d` | Då› å­ï¼ˆD_QAL/D_YLD/D_VOL_RAW/D_TRDï¼‰ã®é‡ã¿dict | `d_score_all`åˆæˆ |
L84
L85 ### DRRSParams
L86 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L87 | --- | --- | --- |
L88 | `corrM` | DRRSåˆæœŸãƒ—ãƒ¼ãƒ«ã®æœ€å¤§ä»¶æ•° | ç›¸é–¢è¡Œåˆ—ã‚µã‚¤ã‚ºåˆ¶å¾¡ |
L89 | `shrink` | æ®‹å·®ç›¸é–¢ã®ã‚·ãƒ¥ãƒªãƒ³ã‚¯ç‡ | `residual_corr`ã®å¯¾è§’å¼·èª¿ |
L90 | `G` | Gãƒã‚±ãƒƒãƒˆç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿dictï¼ˆ`lookback`ç­‰ï¼‰ | `select_bucket_drrs`è¨­å®š |
L91 | `D` | Dãƒã‚±ãƒƒãƒˆç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿dict | åŒä¸Š |
L92 | `cross_mu_gd` | G-Dã‚¯ãƒ­ã‚¹ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£ä¿‚æ•°Î¼ | `select_buckets`ã®ç›®çš„é–¢æ•° |
L93
L94 ### PipelineConfig
L95 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L96 | --- | --- | --- |
L97 | `weights` | `WeightsConfig`ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ | ã‚¹ã‚³ã‚¢åˆæˆã®é‡ã¿å‚ç…§ |
L98 | `drrs` | `DRRSParams`ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ | é¸å®šã‚¹ãƒ†ãƒƒãƒ—ã®è¨­å®šå€¤ |
L99 | `price_max` | å€™è£œéŠ˜æŸ„ã®è¨±å®¹ä¾¡æ ¼ä¸Šé™ | Inputæ®µéšã§ã®ãƒ•ã‚£ãƒ«ã‚¿ |
L100
L101 ## å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
L102 - `winsorize_s` / `robust_z` : å¤–ã‚Œå€¤å‡¦ç†ã¨Zã‚¹ã‚³ã‚¢åŒ–ã€‚
L103 - `_safe_div` / `_safe_last` : ä¾‹å¤–ã‚’æ½°ã—ãŸåˆ†å‰²ãƒ»æœ«å°¾å–å¾—ã€‚
L104 - `_load_prev` / `_save_sel` : é¸å®šçµæœã®èª­ã¿æ›¸ãã€‚
L105
L106 ## ã‚¯ãƒ©ã‚¹è¨­è¨ˆ
L107 ### Step1: Input
L108 `current_tickers.csv`ã®ç¾è¡ŒéŠ˜æŸ„ã¨`candidate_tickers.csv`ã®æ¤œè¨ä¸­éŠ˜æŸ„ã‚’èµ·ç‚¹ã«ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„ã™ã‚‹ã€‚å¤–éƒ¨I/Oã¨å‰å‡¦ç†ã‚’æ‹…å½“ã—ã€`prepare_data`ã§`InputBundle`ã‚’ç”Ÿæˆã€‚ä¾¡æ ¼ãƒ»è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã¯**yfinanceã‚’å„ªå…ˆã—ã€æ¬ æãŒã‚ã‚‹æŒ‡æ¨™ã®ã¿Finnhub APIã§è£œå®Œ**ã™ã‚‹ã€‚
L109 ä¸»ãªãƒ¡ã‚½ãƒƒãƒ‰:
L110 - `impute_eps_ttm` : å››åŠæœŸEPSÃ—4ã§TTMã‚’æ¨å®šã—æ¬ ææ™‚ã®ã¿å·®ã—æ›¿ãˆã€‚
L111 - `fetch_cfo_capex_ttm_yf` : yfinanceã®å››åŠæœŸ/å¹´æ¬¡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼ã‹ã‚‰CFOãƒ»CapExãƒ»FCF TTMã‚’ç®—å‡ºã€‚
L112 - `fetch_cfo_capex_ttm_finnhub` : yfinanceã§æ¬ ã‘ãŸéŠ˜æŸ„ã®ã¿Finnhub APIã§è£œå®Œã€‚
L113 - `compute_fcf_with_fallback` : yfinanceå€¤ã‚’åŸºæº–ã«Finnhubå€¤ã§ç©´åŸ‹ã‚ã—ã€CFO/CapEx/FCFã¨æƒ…å ±æºãƒ•ãƒ©ã‚°ã‚’è¿”ã™ã€‚
L114 - `_build_eps_df` : `info`ã‚„`quarterly_earnings`ã‹ã‚‰EPS TTMã¨ç›´è¿‘EPSã‚’è¨ˆç®—ã—ã€`impute_eps_ttm`ã§è£œå®Œã€‚
L115 - `prepare_data` :
L116     0. CSVã‹ã‚‰ç¾è¡ŒéŠ˜æŸ„ã¨å€™è£œéŠ˜æŸ„ã®ãƒ†ã‚£ãƒƒã‚«ãƒ¼ä¸€è¦§ã‚’èª­ã¿è¾¼ã‚€ã€‚
L117     1. å€™è£œéŠ˜æŸ„ã®ç¾åœ¨å€¤ã‚’å–å¾—ã—ä¾¡æ ¼ä¸Šé™ã§ãƒ•ã‚£ãƒ«ã‚¿ã€‚
L118     2. æ—¢å­˜+å€™è£œã‹ã‚‰å¯¾è±¡ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚’æ±ºå®šã—ã€ä¾¡æ ¼ãƒ»å‡ºæ¥é«˜ã‚’ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆyfinanceï¼‰ã€‚
L119     3. yfinanceå€¤ã‚’åŸºã«EPS/FCFãƒ†ãƒ¼ãƒ–ãƒ«ã‚„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç³»åˆ—ã€ãƒªã‚¿ãƒ¼ãƒ³ã‚’æ§‹ç¯‰ã—ã€æ¬ æã‚»ãƒ«ã¯Finnhubå‘¼ã³å‡ºã—ã§ç©´åŸ‹ã‚ã€‚
L120     4. ä¸Šè¨˜ã‚’`InputBundle`ã«æ ¼ç´ã—ã¦è¿”ã™ã€‚
L121
L122 ### Step2: Score Calculation (Scorer)
L123 ç‰¹å¾´é‡è¨ˆç®—ã¨ã‚¹ã‚³ã‚¢åˆæˆã‚’æ‹…å½“ã—ã€`FeatureBundle`ã‚’è¿”ã™ã€‚
L124
L125 #### è£œåŠ©é–¢æ•°
L126 - `trend(s)` : 50/150/200æ—¥ç§»å‹•å¹³å‡ã‚„52é€±ãƒ¬ãƒ³ã‚¸ã‹ã‚‰-0.5ã€œ0.5ã§æ§‹æˆã•ã‚ŒãŸãƒˆãƒ¬ãƒ³ãƒ‰æŒ‡æ¨™ã€‚
L127 - `rs(s,b)` / `tr_str(s)` / `rs_line_slope(s,b,win)` : ç›¸å¯¾å¼·ã•ã‚„çŸ­æœŸãƒˆãƒ¬ãƒ³ãƒ‰ã€RSå›å¸°å‚¾ãã‚’ç®—å‡ºã€‚
L128 - `ev_fallback` : `enterpriseValue`æ¬ ææ™‚ã«è² å‚µãƒ»ç¾é‡‘ã‹ã‚‰EVã‚’æ¨å®šã€‚
L129 - `dividend_status` / `div_streak` : é…å½“æœªè¨­å®šçŠ¶æ³ã®åˆ¤å®šã¨å¢—é…å¹´æ•°ã‚«ã‚¦ãƒ³ãƒˆã€‚
L130 - `fetch_finnhub_metrics` : Finnhub APIã‹ã‚‰EPSæˆé•·ãƒ»ROEãƒ»Î²ãªã©ä¸è¶³æŒ‡æ¨™ã‚’å–å¾—ã€‚
L131 - `calc_beta` : ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ã®å…±åˆ†æ•£ã‹ã‚‰Î²ã‚’ç®—å‡ºã€‚
L132 - `spx_to_alpha` : S&P500ã®ä½ç½®æƒ…å ±ã‹ã‚‰DRRSã§ç”¨ã„ã‚‹Î±ã‚’æ¨å®šã€‚
L133 - `soft_cap_effective_scores` / `pick_top_softcap` : ã‚»ã‚¯ã‚¿ãƒ¼ã‚½ãƒ•ãƒˆã‚­ãƒ£ãƒƒãƒ—ä»˜ãã‚¹ã‚³ã‚¢èª¿æ•´ã¨ä¸Šä½æŠ½å‡ºã€‚
L134
L135 **è£œåŠ©é–¢æ•°ã¨ç”ŸæˆæŒ‡æ¨™**
L136
L137 | è£œåŠ©é–¢æ•° | ç”ŸæˆæŒ‡æ¨™ | ç•¥ç§° |
L138 | --- | --- | --- |
L139 | `trend` | ãƒˆãƒ¬ãƒ³ãƒ‰ç·åˆå€¤ | `TR` |
L140 | `rs` | ç›¸å¯¾å¼·ã• | `RS` |
L141 | `tr_str` | ä¾¡æ ¼ã¨50æ—¥ç·šã®ä¹–é›¢ | `TR_str` |
L142 | `rs_line_slope` | RSç·šã®å›å¸°å‚¾ã | `RS_SLOPE_*` |
L143 | `calc_beta` | Î² | `BETA` |
L144 | `div_streak` | é€£ç¶šå¢—é…å¹´æ•° | `DIV_STREAK` |
L145
L146 #### `aggregate_scores` è©³ç´°
L147 1. å„éŠ˜æŸ„ã®ä¾¡æ ¼ç³»åˆ—ã‚„`info`ã‚’åŸºã«ä»¥ä¸‹ã‚’ç®—å‡ºã€‚
L148    - **ãƒˆãƒ¬ãƒ³ãƒ‰/ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ **: `TR`ã€`RS`ã€`TR_str`ã€å¤šæ§˜ãªç§»å‹•å¹³å‡æ¯”ã€`RS_SLOPE_*`ãªã©ã€‚
L149    - **ãƒªã‚¹ã‚¯**: `BETA`ã€`DOWNSIDE_DEV`ã€`MDD_1Y`ã€`RESID_VOL`ã€`DOWN_OUTPERF`ã€`EXT_200`ç­‰ã€‚
L150    - **é…å½“**: `DIV`ã€`DIV_TTM_PS`ã€`DIV_VAR5`ã€`DIV_YOY`ã€`DIV_FCF_COVER`ã€`DIV_STREAK`ã€‚
L151    - **è²¡å‹™ãƒ»æˆé•·**: `EPS`ã€`REV`ã€`ROE`ã€`FCF/EV`ã€`REV_Q_YOY`ã€`EPS_Q_YOY`ã€`REV_YOY_ACC`ã€`REV_YOY_VAR`ã€`REV_ANN_STREAK`ã€`RULE40`ã€`FCF_MGN` ç­‰ã€‚
L152    - **å®‰å®šæ€§/ã‚µã‚¤ã‚º**: `DEBT2EQ`ã€`CURR_RATIO`ã€`MARKET_CAP`ã€`ADV60_USD`ã€`EPS_VAR_8Q`ãªã©ã€‚
L153 2. æŒ‡æ¨™æ¬ æã¯Finnhub APIç­‰ã§è£œå®Œã—ã€æœªå–å¾—é …ç›®ã‚’`missing_logs`ã«è¨˜éŒ²ã€‚
L154 3. `winsorize_s`â†’`robust_z`ã§æ¨™æº–åŒ–ã—`df_z`ã¸ä¿å­˜ã€‚ã‚µã‚¤ã‚ºãƒ»æµå‹•æ€§ã¯å¯¾æ•°å¤‰æ›ã€‚
L155 4. æ­£è¦åŒ–æ¸ˆæŒ‡æ¨™ã‹ã‚‰å› å­ã‚¹ã‚³ã‚¢ã‚’åˆæˆã€‚
L156    - å„å› å­ã®æ§‹æˆã¨é‡ã¿ã¯ä»¥ä¸‹ã®é€šã‚Šã€‚
L157      - **GRW**: 0.30Ã—`REV` + 0.20Ã—`EPS_Q_YOY` + 0.15Ã—`REV_Q_YOY` + 0.15Ã—`REV_YOY_ACC` + 0.10Ã—`RULE40` + 0.10Ã—`FCF_MGN` + 0.10Ã—`REV_ANN_STREAK` âˆ’ 0.05Ã—`REV_YOY_VAR`ã€‚
L158      - **MOM**: 0.40Ã—`RS` + 0.15Ã—`TR_str` + 0.15Ã—`RS_SLOPE_6W` + 0.15Ã—`RS_SLOPE_13W` + 0.10Ã—`MA200_SLOPE_5M` + 0.10Ã—`MA200_UP_STREAK_D`ã€‚
L159      - **VOL**: `BETA`å˜ä½“ã‚’ä½¿ç”¨ã€‚
L160      - **QAL**: 0.60Ã—`FCF_W` + 0.40Ã—`ROE_W`ã§ä½œæˆã€‚
L161      - **YLD**: 0.30Ã—`DIV` + 0.70Ã—`DIV_STREAK`ã€‚
L162      - **D_QAL**: 0.35Ã—`QAL` + 0.20Ã—`FCF` + 0.15Ã—`CURR_RATIO` âˆ’ 0.15Ã—`DEBT2EQ` âˆ’ 0.15Ã—`EPS_VAR_8Q`ã€‚
L163      - **D_YLD**: 0.45Ã—`DIV` + 0.25Ã—`DIV_STREAK` + 0.20Ã—`DIV_FCF_COVER` âˆ’ 0.10Ã—`DIV_VAR5`ã€‚
L164      - **D_VOL_RAW**: 0.40Ã—`DOWNSIDE_DEV` + 0.22Ã—`RESID_VOL` + 0.18Ã—`MDD_1Y` âˆ’ 0.10Ã—`DOWN_OUTPERF` âˆ’ 0.05Ã—`EXT_200` âˆ’ 0.08Ã—`SIZE` âˆ’ 0.10Ã—`LIQ` + 0.10Ã—`BETA`ã€‚
L165      - **D_TRD**: 0.40Ã—`MA200_SLOPE_5M` âˆ’ 0.30Ã—`EXT_200` + 0.15Ã—`NEAR_52W_HIGH` + 0.15Ã—`TR`ã€‚
L166     - ä¸»ãªæŒ‡æ¨™ã®ç•¥ç§°ã¨æ„å‘³:
L167
L168       | ç•¥ç§° | è£œåŠ©é–¢æ•° | æ¦‚è¦ |
L169       | --- | --- | --- |
L170       | TR | `trend` | 50/150/200æ—¥ç§»å‹•å¹³å‡ã¨52é€±ãƒ¬ãƒ³ã‚¸ã‚’çµ„ã¿åˆã‚ã›ãŸãƒˆãƒ¬ãƒ³ãƒ‰ç·åˆå€¤ |
L171       | RS | `rs` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«å¯¾ã™ã‚‹ç›¸å¯¾å¼·ã•ï¼ˆ12M/1Mãƒªã‚¿ãƒ¼ãƒ³å·®ï¼‰ |
L172       | TR_str | `tr_str` | ä¾¡æ ¼ã¨50æ—¥ç§»å‹•å¹³å‡ã®ä¹–é›¢ |
L173       | RS_SLOPE_6W | `rs_line_slope` | ç›¸å¯¾å¼·ã•ç·šã®6é€±å›å¸°å‚¾ã |
L174       | RS_SLOPE_13W | `rs_line_slope` | ç›¸å¯¾å¼·ã•ç·šã®13é€±å›å¸°å‚¾ã |
L175       | MA200_SLOPE_5M | - | 200æ—¥ç§»å‹•å¹³å‡ã®5ã‹æœˆé¨°è½ç‡ |
L176       | MA200_UP_STREAK_D | - | 200æ—¥ç·šãŒé€£ç¶šã§ä¸Šå‘ã„ãŸæ—¥æ•° |
L177       | BETA | `calc_beta` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«å¯¾ã™ã‚‹Î² |
L178       | DOWNSIDE_DEV | - | ä¸‹æ–¹ãƒªã‚¿ãƒ¼ãƒ³ã®ã¿ã®å¹´ç‡åŒ–æ¨™æº–åå·® |
L179       | RESID_VOL | - | Î²ã§èª¿æ•´ã—ãŸæ®‹å·®ãƒªã‚¿ãƒ¼ãƒ³ã®å¹´ç‡åŒ–æ¨™æº–åå·® |
L180       | MDD_1Y | - | éå»1å¹´ã®æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³ |
L181       | DOWN_OUTPERF | - | å¸‚å ´ä¸‹è½æ—¥ã«å¯¾ã™ã‚‹å¹³å‡è¶…éãƒªã‚¿ãƒ¼ãƒ³ |
L182       | EXT_200 | - | 200æ—¥ç§»å‹•å¹³å‡ã‹ã‚‰ã®çµ¶å¯¾ä¹–é›¢ç‡ |
L183       | NEAR_52W_HIGH | - | 52é€±é«˜å€¤ã¾ã§ã®ä¸‹æ–¹è·é›¢ï¼ˆ0=é«˜å€¤ï¼‰ |
L184       | FCF_W | - | ã‚¦ã‚£ãƒ³ã‚¶ãƒ¼å‡¦ç†å¾Œã®FCF/EV |
L185       | ROE_W | - | ã‚¦ã‚£ãƒ³ã‚¶ãƒ¼å‡¦ç†å¾Œã®ROE |
L186       | FCF | - | FCF/EV |
L187       | QAL | - | FCF_Wã¨ROE_Wã‚’çµ„ã¿åˆã‚ã›ãŸå“è³ªã‚¹ã‚³ã‚¢ |
L188       | CURR_RATIO | - | æµå‹•æ¯”ç‡ |
L189       | DEBT2EQ | - | è² å‚µè³‡æœ¬å€ç‡ |
L190       | EPS_VAR_8Q | - | EPSã®8å››åŠæœŸæ¨™æº–åå·® |
L191       | DIV | - | å¹´ç‡æ›ç®—é…å½“åˆ©å›ã‚Š |
L192       | DIV_STREAK | `div_streak` | é€£ç¶šå¢—é…å¹´æ•° |
L193       | DIV_FCF_COVER | - | é…å½“ã®FCFã‚«ãƒãƒ¬ãƒƒã‚¸ |
L194       | DIV_VAR5 | - | 5å¹´é…å½“å¤‰å‹•ç‡ |
L195       | DIV_TTM_PS | - | 1æ ªå½“ãŸã‚ŠTTMé…å½“ |
L196       | DIV_YOY | - | å‰å¹´æ¯”é…å½“æˆé•·ç‡ |
L197       | REV | - | å£²ä¸Šæˆé•·ç‡TTM |
L198       | EPS_Q_YOY | - | å››åŠæœŸEPSã®å‰å¹´åŒæœŸæ¯” |
L199       | REV_Q_YOY | - | å››åŠæœŸå£²ä¸Šã®å‰å¹´åŒæœŸæ¯” |
L200       | REV_YOY_ACC | - | å£²ä¸Šæˆé•·ç‡ã®åŠ é€Ÿåˆ† |
L201       | RULE40 | - | å£²ä¸Šæˆé•·ç‡ã¨FCFãƒãƒ¼ã‚¸ãƒ³ã®åˆè¨ˆ |
L202       | FCF_MGN | - | FCFãƒãƒ¼ã‚¸ãƒ³ |
L203       | REV_ANN_STREAK | - | å¹´æ¬¡å£²ä¸Šæˆé•·ã®é€£ç¶šå¹´æ•° |
L204       | REV_YOY_VAR | - | å¹´æ¬¡å£²ä¸Šæˆé•·ç‡ã®å¤‰å‹•æ€§ |
L205       | SIZE | - | æ™‚ä¾¡ç·é¡ã®å¯¾æ•°å€¤ |
L206       | LIQ | - | 60æ—¥å¹³å‡å‡ºæ¥é«˜ãƒ‰ãƒ«ã®å¯¾æ•°å€¤ |
L207    - Gãƒã‚±ãƒƒãƒˆ: `GRW`ã€`MOM`ã€`VOL`ã‚’`cfg.weights.g`ï¼ˆ0.40/0.45/-0.15ï¼‰ã§åŠ é‡ã—`g_score`ã‚’å¾—ã‚‹ã€‚
L208    - Dãƒã‚±ãƒƒãƒˆ: `D_QAL`ã€`D_YLD`ã€`D_VOL_RAW`ã€`D_TRD`ã‚’`cfg.weights.d`ï¼ˆ0.15/0.15/-0.45/0.25ï¼‰ã§åŠ é‡ã—`d_score_all`ã‚’ç®—å‡ºã€‚
L209    - ã‚»ã‚¯ã‚¿ãƒ¼capã«ã‚ˆã‚‹`soft_cap_effective_scores`ã‚’é©ç”¨ã—ã€Gæ¡ç”¨éŠ˜æŸ„ã«ã¯ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨ã€‚
L210 5. `_apply_growth_entry_flags`ã§ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆ/æŠ¼ã—ç›®ç™ºç«çŠ¶æ³ã‚’ä»˜åŠ ã—ã€`FeatureBundle`ã‚’è¿”ã™ã€‚
L211
L212 ### Step3: Correlation Reduction & Selection (Selector)
L213 DRRSã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ç›¸é–¢ã‚’æŠ‘ãˆãŸéŠ˜æŸ„é¸å®šã‚’è¡Œã„ã€`SelectionBundle`ã‚’è¿”ã™ã€‚`results/`ã«ä¿å­˜ã•ã‚ŒãŸå‰å›é¸å®šï¼ˆ`G_selection.json` / `D_selection.json`ï¼‰ã‚’`_load_prev`ã§èª­ã¿è¾¼ã¿ã€ç›®çš„å€¤ãŒå¤§ããæ‚ªåŒ–ã—ãªã„é™ã‚Šç¶­æŒã™ã‚‹ã€‚æ–°ã—ã„æ¡ç”¨é›†åˆã¯`_save_sel`ã§JSONã«æ›¸ãå‡ºã—æ¬¡å›ä»¥é™ã®å…¥åŠ›ã«å‚™ãˆã‚‹ã€‚
L214 ä¸»ãªãƒ¡ã‚½ãƒƒãƒ‰:
L215 - `residual_corr` : åç›Šç‡è¡Œåˆ—ã‚’Zã‚¹ã‚³ã‚¢åŒ–ã—ã€ä¸Šä½ä¸»æˆåˆ†ã‚’é™¤å»ã—ãŸæ®‹å·®ã‹ã‚‰ç›¸é–¢è¡Œåˆ—ã‚’æ±‚ã‚ã€å¹³å‡ç›¸é–¢ã«å¿œã˜ã¦ã‚·ãƒ¥ãƒªãƒ³ã‚¯ã€‚
L216 - `rrqr_like_det` : ã‚¹ã‚³ã‚¢ã‚’é‡ã¿ä»˜ã‘ã—ãŸQRåˆ†è§£é¢¨ã®æ‰‹é †ã§åˆæœŸå€™è£œã‚’kä»¶æŠ½å‡ºã—ã€ã‚¹ã‚³ã‚¢ã®é«˜ã„éç›¸é–¢ãªé›†åˆã‚’å¾—ã‚‹ã€‚
L217 - `swap_local_det` / `swap_local_det_cross` : `sum(score) - Î»*within_corr - Î¼*cross_corr`ã‚’ç›®çš„é–¢æ•°ã¨ã—ã¦ã€å…¥ã‚Œæ›¿ãˆæ¢ç´¢ã§å±€æ‰€çš„ã«æœ€é©åŒ–ã€‚
L218 - `select_bucket_drrs` : ãƒ—ãƒ¼ãƒ«éŠ˜æŸ„ã¨ã‚¹ã‚³ã‚¢ã‹ã‚‰æ®‹å·®ç›¸é–¢ã‚’è¨ˆç®—ã—ã€ä¸Šè¨˜2æ®µéš(åˆæœŸé¸æŠâ†’å…¥ã‚Œæ›¿ãˆ)ã§kéŠ˜æŸ„ã‚’æ±ºå®šã€‚éå»æ¡ç”¨éŠ˜æŸ„ã¨ã®æ¯”è¼ƒã§ç›®çš„å€¤ãŒåŠ£åŒ–ã—ãªã‘ã‚Œã°ç¶­æŒã™ã‚‹ã€‚
L219 - `select_buckets` : Gãƒã‚±ãƒƒãƒˆã‚’é¸å®šå¾Œã€ãã®çµæœã‚’é™¤ã„ãŸå€™è£œã‹ã‚‰Dãƒã‚±ãƒƒãƒˆã‚’é¸ã¶ã€‚Dé¸å®šæ™‚ã¯Gã¨ã®ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£Î¼ã‚’ä»˜ä¸ã—ã€ä¸¡ãƒã‚±ãƒƒãƒˆã®åˆ†æ•£ã‚’åˆ¶å¾¡ã™ã‚‹ã€‚
L220
L221 #### ç›¸é–¢ä½æ¸›ãƒ­ã‚¸ãƒƒã‚¯è©³ç´°
L222 1. **æ®‹å·®ç›¸é–¢è¡Œåˆ—ã®æ§‹ç¯‰ (`residual_corr`)**
L223    - ãƒªã‚¿ãƒ¼ãƒ³è¡Œåˆ—`R`ã‚’Zã‚¹ã‚³ã‚¢åŒ–ã€‚
L224    - SVDã§ä¸Šä½`n_pc`ä¸»æˆåˆ†`F`ã‚’æ±‚ã‚ã€æœ€å°äºŒä¹—ã§ä¿‚æ•°`B`ã‚’ç®—å‡ºã—æ®‹å·®`E = Z - F@B`ã‚’å¾—ã‚‹ã€‚
L225    - `E`ã®ç›¸é–¢è¡Œåˆ—`C`ã‚’è¨ˆç®—ã—ã€å¹³å‡çµ¶å¯¾ç›¸é–¢ã«å¿œã˜ã¦ã‚·ãƒ¥ãƒªãƒ³ã‚¯é‡`shrink_eff`ã‚’è£œæ­£ã—ã¦å¯¾è§’ã‚’å¼·èª¿ã€‚
L226 2. **åˆæœŸå€™è£œã®æŠ½å‡º (`rrqr_like_det`)**
L227    - ã‚¹ã‚³ã‚¢ã‚’0-1æ­£è¦åŒ–ã—ãŸé‡ã¿`w`ã¨ã—ã€`Z*(1+Î³w)`ã§åˆ—ãƒãƒ«ãƒ ã‚’å¼·èª¿ã€‚
L228    - æ®‹å·®ãƒãƒ«ãƒ æœ€å¤§ã®åˆ—ã‚’é€æ¬¡é¸ã³ã€QRãƒ©ã‚¤ã‚¯ãªãƒ‡ãƒ•ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã£ã¦éç›¸é–¢ã‹ã¤é«˜ã‚¹ã‚³ã‚¢ãª`k`éŠ˜æŸ„é›†åˆ`S0`ã‚’å¾—ã‚‹ã€‚
L229 3. **å±€æ‰€æ¢ç´¢ (`swap_local_det` / `swap_local_det_cross`)**
L230    - ç›®çš„é–¢æ•°`Î£z_score âˆ’ Î»Â·within_corr âˆ’ Î¼Â·cross_corr`ã‚’æœ€å¤§åŒ–ã€‚
L231    - é¸æŠé›†åˆã®å„éŠ˜æŸ„ã‚’ä»–å€™è£œã¨å…¥ã‚Œæ›¿ãˆã€æ”¹å–„ãŒãªããªã‚‹ã¾ã§ã¾ãŸã¯`max_pass`å›ã¾ã§æ¢ç´¢ã€‚
L232    - `swap_local_det_cross`ã¯Gãƒã‚±ãƒƒãƒˆã¨ã®ã‚¯ãƒ­ã‚¹ç›¸é–¢è¡Œåˆ—`C_cross`ã‚’ä½¿ç”¨ã—ã€ãƒšãƒŠãƒ«ãƒ†ã‚£`Î¼`ã‚’ä»˜ä¸ã€‚
L233 4. **éå»æ¡ç”¨ã®ç¶­æŒã¨ã‚¯ãƒ­ã‚¹ãƒšãƒŠãƒ«ãƒ†ã‚£ (`select_bucket_drrs` / `select_buckets`)**
L234    - å±€æ‰€æ¢ç´¢çµæœ`S`ã¨éå»é›†åˆ`P`ã®ç›®çš„å€¤ã‚’æ¯”è¼ƒã—ã€`S`ãŒ`P`ã‚ˆã‚Š`Î·`æœªæº€ã®æ”¹å–„ãªã‚‰`P`ã‚’ç¶­æŒã€‚
L235    - `select_buckets`ã§ã¯Gã‚’å…ˆã«æ±ºå®šã—ã€Dé¸å®šæ™‚ã«Gã¨ã®ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£`Î¼`ã‚’åŠ ãˆã¦ã‚¯ãƒ­ã‚¹åˆ†æ•£ã‚’æŠ‘åˆ¶ã€‚
L236
L237 ### Step4: Output
L238 é¸å®šçµæœã‚’å¯è¦–åŒ–ã—å…±æœ‰ã™ã‚‹å·¥ç¨‹ã€‚ä»¥ä¸‹ã®å†…å®¹ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«åŒ–ã—ã¦æ¨™æº–å‡ºåŠ›ã¨Slackã¸é€ã‚‹ã€‚
L239 - æ¡ç”¨éŠ˜æŸ„ã¨æƒœã—ãã‚‚é¸å¤–ã¨ãªã£ãŸéŠ˜æŸ„ã®ã‚¹ã‚³ã‚¢ä¸€è¦§
L240 - IN/OUTãƒªã‚¹ãƒˆã¨OUTéŠ˜æŸ„ã®ã‚¹ã‚³ã‚¢ï¼ˆä½å¾—ç‚¹éŠ˜æŸ„ã‚’ç¢ºèªã—ã‚„ã™ãï¼‰
L241 - æ–°æ—§ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®æ¯”è¼ƒè¡¨ï¼ˆçµ„å…¥ã‚Œãƒ»é™¤å¤–ã€ã‚¹ã‚³ã‚¢å¤‰åŒ–ï¼‰
L242 - æ¤œè¨ä¸­éŠ˜æŸ„ã®ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°
L243
L244 ä¸»ãªãƒ¡ã‚½ãƒƒãƒ‰:
L245 - `display_results` : ä¸Šè¨˜ãƒ†ãƒ¼ãƒ–ãƒ«ã«åŠ ãˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã‚„åˆ†æ•£åŒ–æŒ‡æ¨™ã‚’è¡¨ç¤ºã€‚
L246 - `notify_slack` : Slack Webhookã¸åŒå†…å®¹ã‚’é€ä¿¡ã€‚
L247 - è£œåŠ©:`_avg_offdiag`ã€`_resid_avg_rho`ã€`_raw_avg_rho`ã€`_cross_block_raw_rho`ã€‚
L248
L249 ## ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
L250 1. `PipelineConfig`ã‚’æ§‹ç¯‰ã€‚
L251 2. **Step1** `Input.prepare_data`ã§`InputBundle`ã‚’ç”Ÿæˆã€‚
L252 3. **Step2** `Scorer.aggregate_scores`ã§`FeatureBundle`ã‚’å–å¾—ã€‚
L253 4. **Step3** `Selector.select_buckets`ã§`SelectionBundle`ã‚’ç®—å‡ºã€‚
L254 5. **Step4** `Output.display_results`ã¨`notify_slack`ã§çµæœã‚’å‡ºåŠ›ã€‚
```

# === Chat Paste Pack ===
# Repo: dakara32/GPT_Code @ main
# Files: config.py, factor.py, scorer.py, .github/workflows/weekly-report.yml, documents/README.md, documents/factor_design.md
# 使い方: 下のチャンクを順に貼ればこのチャットで全体把握できます。
# 注記: 各ファイルは個別に L1.. で行番号付与。
---

## <config.py>
```text
L1 # 共通設定（factor / drift から参照）
L2 from dataclasses import dataclass
L3
L4 TOTAL_TARGETS = 20
L5
L6 # 基準のバケット数（NORMAL）
L7 COUNTS_BASE = {"G": 12, "D": 8}
L8
L9 # モード別の推奨バケット数
L10 COUNTS_BY_MODE = {
L11     "NORMAL": {"G": 12, "D": 8},
L12     "CAUTION": {"G": 10, "D": 8},
L13     "EMERG": {"G": 8,  "D": 8},
L14 }
L15
L16 # モード別のドリフト閾値（%）
L17 DRIFT_THRESHOLD_BY_MODE = {"NORMAL": 12, "CAUTION": 14, "EMERG": float("inf")}
L18
L19 # モード別のTS（基本幅, 小数=割合）
L20 TS_BASE_BY_MODE = {"NORMAL": 0.15, "CAUTION": 0.13, "EMERG": 0.10}
L21 # 利益到達(+30/+60/+100%)時の段階タイト化（ポイント差）
L22 TS_STEP_DELTAS_PT = (3, 6, 8)
L23
L24 # Breadthの校正は N_G に連動（緊急解除=ceil(1.5*N_G), 通常復帰=3*N_G）
L25 N_G = COUNTS_BASE["G"]
L26 N_D = COUNTS_BASE["D"]
L27
```

## <factor.py>
```text
L1 '''ROLE: Orchestration ONLY（外部I/O・SSOT・Slack出力）, 計算は scorer.py'''
L2 # === NOTE: 機能・入出力・ログ文言・例外挙動は不変。安全な短縮（import統合/複数代入/内包表記/メソッドチェーン/一行化/空行圧縮など）のみ適用 ===
L3 BONUS_COEFF = 0.55  # 推奨: 攻め=0.45 / 中庸=0.55 / 守り=0.65
L4 SWAP_DELTA_Z = 0.15   # 僅差判定: σの15%。(緩め=0.10 / 標準=0.15 / 固め=0.20)
L5 SWAP_KEEP_BUFFER = 3  # n_target+この順位以内の現行は保持。(粘り弱=2 / 標準=3 / 粘り強=4〜5)
L6 import os, time, requests
L7 from time import perf_counter
L8 from dataclasses import dataclass, replace
L9 from typing import Dict, List
L10 from concurrent.futures import ThreadPoolExecutor
L11 import numpy as np
L12 import pandas as pd
L13 import yfinance as yf
L14 from scipy.stats import zscore  # used via scorer
L15 from scorer import Scorer, ttm_div_yield_portfolio
L16 import config
L17
L18 class T:
L19     t = perf_counter()
L20     log = staticmethod(lambda tag: (lambda now=perf_counter(): (print(f"[T] {tag}: {now - T.t:.2f}s"), setattr(T, "t", now))[-1])())
L21
L22 T.log("start")
L23
L24 # === ユニバースと定数（冒頭に固定） ===
L25 exist, cand = [pd.read_csv(f, header=None)[0].tolist() for f in ("current_tickers.csv","candidate_tickers.csv")]
L26 T.log(f"csv loaded: exist={len(exist)} cand={len(cand)}")
L27 CAND_PRICE_MAX, bench = 450, '^GSPC'  # 価格上限・ベンチマーク
L28 N_G, N_D = config.N_G, config.N_D  # G/D枠サイズ（NORMAL基準: G12/D8）
L29 g_weights = {'GRW':0.30,'MOM':0.55,'VOL':-0.15}
L30 D_BETA_MAX = float(os.environ.get("D_BETA_MAX", "0.7"))
L31 FILTER_SPEC = {"G":{"pre_mask":["trend_template"]},"D":{"pre_filter":{"beta_max":D_BETA_MAX}}}
L32 D_weights = {'QAL':0.1,'YLD':0.3,'VOL':-0.5,'TRD':0.1}
L33 _fmt_w = lambda w: " ".join(f"{k}{int(v*100)}" for k, v in w.items())
L34
L35 # DRRS 初期プール・各種パラメータ
L36 corrM = 45
L37 DRRS_G, DRRS_D = dict(lookback=252,n_pc=3,gamma=1.2,lam=0.68,eta=0.8), dict(lookback=504,n_pc=4,gamma=0.8,lam=0.85,eta=0.5)
L38 DRRS_SHRINK = 0.10  # 残差相関の対角シュリンク（基礎）
L39
L40 # クロス相関ペナルティ（未定義なら設定）
L41 try: CROSS_MU_GD
L42 except NameError: CROSS_MU_GD = 0.40  # 推奨 0.35–0.45（lam=0.85想定）
L43
L44 # 出力関連
L45 RESULTS_DIR = "results"
L46 os.makedirs(RESULTS_DIR, exist_ok=True)
L47
L48 # その他
L49 debug_mode, FINNHUB_API_KEY = True, os.environ.get("FINNHUB_API_KEY")
L50 _API = (FINNHUB_API_KEY or "").strip()
L51
L52 def _fetch_eps_non_gaap_ttm_metric(sym: str, token: str) -> float:
L53     """
L54     Finnhub /stock/metric から Non-GAAPに相当するTTMを取得。
L55     第一候補: epsExclExtraItemsTTM（調整後EPSに相当）
L56     代替候補: epsNormalizedAnnual（年次だがNon-GAAPに近い）
L57     最終候補: epsTTM（GAAP、フォールバック）
L58     いずれも欠損なら NaN を返す（0.0は返さない）。
L59     """
L60     try:
L61         r = requests.get(
L62             "https://finnhub.io/api/v1/stock/metric",
L63             params={"symbol": sym, "metric": "all", "token": token},
L64             timeout=12,
L65         )
L66         j = r.json() if r.ok else {}
L67         m = (j or {}).get("metric", {}) or {}
L68         for k in ("epsExclExtraItemsTTM", "epsNormalizedAnnual", "epsTTM"):
L69             v = m.get(k, None)
L70             if v is not None:
L71                 try:
L72                     fv = float(v)
L73                     return fv
L74                 except Exception:
L75                     pass
L76         return np.nan
L77     except Exception:
L78         return np.nan
L79
L80 # === 共有DTO（クラス間I/O契約）＋ Config ===
L81 @dataclass(frozen=True)
L82 class InputBundle:
L83     # Input → Scorer で受け渡す素材（I/O禁止の生データ）
L84     cand: List[str]
L85     tickers: List[str]
L86     bench: str
L87     data: pd.DataFrame              # yfinance download結果（'Close','Volume'等の階層列）
L88     px: pd.DataFrame                # data['Close']
L89     spx: pd.Series                  # data['Close'][bench]
L90     tickers_bulk: object            # yfinance.Tickers
L91     info: Dict[str, dict]           # yfinance info per ticker
L92     eps_df: pd.DataFrame            # ['eps_ttm','eps_q_recent',...]
L93     fcf_df: pd.DataFrame            # ['fcf_ttm', ...]
L94     returns: pd.DataFrame           # px[tickers].pct_change()
L95
L96 @dataclass(frozen=True)
L97 class FeatureBundle:
L98     df: pd.DataFrame
L99     df_z: pd.DataFrame
L100     g_score: pd.Series
L101     d_score_all: pd.Series
L102     missing_logs: pd.DataFrame
L103
L104 @dataclass(frozen=True)
L105 class SelectionBundle:
L106     resG: dict
L107     resD: dict
L108     top_G: List[str]
L109     top_D: List[str]
L110     init_G: List[str]
L111     init_D: List[str]
L112
L113 @dataclass(frozen=True)
L114 class WeightsConfig:
L115     g: Dict[str,float]
L116     d: Dict[str,float]
L117
L118 @dataclass(frozen=True)
L119 class DRRSParams:
L120     corrM: int
L121     shrink: float
L122     G: Dict[str,float]   # lookback, n_pc, gamma, lam, eta
L123     D: Dict[str,float]
L124     cross_mu_gd: float
L125
L126 @dataclass(frozen=True)
L127 class PipelineConfig:
L128     weights: WeightsConfig
L129     drrs: DRRSParams
L130     price_max: float
L131
L132 # === 共通ユーティリティ（複数クラスで使用） ===
L133 # (unused local utils removed – use scorer.py versions if needed)
L134
L135 _env_true = lambda name, default=False: (os.getenv(name) or str(default)).strip().lower() == "true"
L136
L137 def _post_slack(payload: dict):
L138     url = os.getenv("SLACK_WEBHOOK_URL")
L139     if not url: print("⚠️ SLACK_WEBHOOK_URL 未設定"); return
L140     try:
L141         requests.post(url, json=payload).raise_for_status()
L142     except Exception as e:
L143         print(f"⚠️ Slack通知エラー: {e}")
L144
L145 _slack = lambda message, code=False: _post_slack({"text": f"```{message}```" if code else message})
L146
L147 def _slack_debug(text: str, chunk=2800):
L148     i = 0
L149     while i < len(text):
L150         j = min(len(text), i + chunk)
L151         k = text.rfind("\n", i, j)
L152         j = k if k > i + 100 else j
L153         _post_slack({"blocks":[{"type":"section","text":{"type":"mrkdwn","text":f"```{text[i:j]}```"}}]})
L154         i = j
L155
L156 def _compact_debug(fb, sb, prevG, prevD, max_rows=140):
L157     want=["TR","EPS","REV","ROE","BETA_RAW","FCF","RS","TR_str","DIV_STREAK","DSC","eps_ttm","nEPS_ttm","eps_imputed","fcf_ttm","fcf_imputed"]
L158     all_cols = _env_true("DEBUG_ALL_COLS", False)
L159     cols = list(fb.df_z.columns if all_cols else [c for c in want if c in fb.df_z.columns])
L160
L161     Gp, Dp = set(prevG or []), set(prevD or [])
L162     g_new=[t for t in (sb.top_G or []) if t not in Gp]; g_out=[t for t in Gp if t not in (sb.top_G or [])]
L163     d_new=[t for t in (sb.top_D or []) if t not in Dp]; d_out=[t for t in Dp if t not in (sb.top_D or [])]
L164
L165     show_near = _env_true("DEBUG_NEAR5", True)
L166     gs, ds = getattr(fb,"g_score",None), getattr(fb,"d_score_all",None)
L167     gs = (gs.sort_values(ascending=False) if show_near and hasattr(gs,"sort_values") else None)
L168     ds = (ds.sort_values(ascending=False) if show_near and hasattr(ds,"sort_values") else None)
L169     g_miss = ([t for t in gs.index if t not in (sb.top_G or [])][:10]) if gs is not None else []
L170     d_excl = set((sb.top_G or [])+(sb.top_D or []))
L171     d_miss = ([t for t in ds.index if t not in d_excl][:10]) if ds is not None else []
L172
L173     all_rows = _env_true("DEBUG_ALL_ROWS", False)
L174     focus = list(fb.df_z.index) if all_rows else sorted(set(g_new+g_out+d_new+d_out+(sb.top_G or [])+(sb.top_D or [])+g_miss+d_miss))[:max_rows]
L175
L176     def _fmt_near(lbl, ser, lst):
L177         if ser is None: return f"{lbl}: off"
L178         g = ser.get
L179         parts=[f"{t}:{g(t,float('nan')):.3f}" if pd.notna(g(t)) else f"{t}:nan" for t in lst]
L180         return f"{lbl}: " + (", ".join(parts) if parts else "-")
L181
L182     head=[f"G new/out: {len(g_new)}/{len(g_out)}  D new/out: {len(d_new)}/{len(d_out)}",
L183           _fmt_near("G near10", gs, g_miss),
L184           _fmt_near("D near10", ds, d_miss),
L185           f"Filters: G pre_mask=['trend_template'], D pre_filter={{'beta_max': {D_BETA_MAX}}}",
L186           f"Cols={'ALL' if all_cols else 'MIN'}  Rows={'ALL' if all_rows else 'SUBSET'}"]
L187
L188     tbl="(df_z or columns not available)"
L189     if not fb.df_z.empty and cols:
L190         idx=[t for t in focus if t in fb.df_z.index]
L191         tbl=fb.df_z.loc[idx, cols].round(3).to_string(max_rows=None, max_cols=None)
L192
L193     miss_txt=""
L194     if _env_true("DEBUG_MISSING_LOGS", False):
L195         miss=getattr(fb,"missing_logs",None)
L196         if miss is not None and not miss.empty:
L197             miss_txt="\nMissing data (head)\n"+miss.head(10).to_string(index=False)
L198
L199     return "\n".join(head+["\nChanged/Selected (+ Near Miss)", tbl])+miss_txt
L200
L201 def _disjoint_keepG(top_G, top_D, poolD):
L202     """G重複をDから除去し、poolDで順次補充（枯渇時は元銘柄維持）。"""
L203     used, D, i = set(top_G), list(top_D), 0
L204     for j, t in enumerate(D):
L205         if t in used:
L206             while i < len(poolD) and (poolD[i] in used or poolD[i] in D):
L207                 i += 1
L208             if i < len(poolD):
L209                 D[j] = poolD[i]; used.add(D[j]); i += 1
L210     return top_G, D
L211
L212
L213 def _sticky_keep_current(agg: pd.Series, pick: list[str], incumbents: list[str],
L214                          n_target: int, delta_z: float, keep_buffer: int) -> list[str]:
L215     import pandas as pd, numpy as np
L216     sel = list(pick)
L217     if not sel: return sel
L218     ranked_sel = agg.reindex(sel).sort_values(ascending=False)
L219     kth = ranked_sel.iloc[min(len(sel), n_target)-1]
L220     sigma = float(agg.std()) if pd.notna(agg.std()) else 0.0
L221     thresh = kth - delta_z * sigma
L222     ranked_all = agg.sort_values(ascending=False)
L223     cand = [t for t in incumbents if (t not in sel) and (t in agg.index)]
L224     for t in cand:
L225         within_score = (pd.notna(agg[t]) and agg[t] >= thresh)
L226         within_rank  = (t in ranked_all.index) and (ranked_all.index.get_loc(t) < n_target + keep_buffer)
L227         if within_score or within_rank:
L228             non_inc = [x for x in sel if x not in incumbents]
L229             if not non_inc: break
L230             weakest = min(non_inc, key=lambda x: agg.get(x, -np.inf))
L231             if weakest in sel and agg.get(t, -np.inf) >= agg.get(weakest, -np.inf):
L232                 sel.remove(weakest); sel.append(t)
L233     if len(sel) > n_target:
L234         sel = sorted(sel, key=lambda x: agg.get(x, -1e9), reverse=True)[:n_target]
L235     return sel
L236
L237
L238 # === Input：外部I/Oと前処理（CSV/API・欠損補完） ===
L239 class Input:
L240     def __init__(self, cand, exist, bench, price_max, finnhub_api_key=None):
L241         self.cand, self.exist, self.bench, self.price_max = cand, exist, bench, price_max
L242         self.api_key = finnhub_api_key or os.environ.get("FINNHUB_API_KEY")
L243
L244     # ---- （Input専用）EPS補完・FCF算出系 ----
L245     @staticmethod
L246     def impute_eps_ttm(df: pd.DataFrame, ttm_col: str="eps_ttm", q_col: str="eps_q_recent", out_col: str|None=None) -> pd.DataFrame:
L247         out_col = out_col or ttm_col; df = df.copy(); df["eps_imputed"] = False
L248         cand = df[q_col]*4; ok = df[ttm_col].isna() & cand.replace([np.inf,-np.inf], np.nan).notna()
L249         df.loc[ok, out_col], df.loc[ok,"eps_imputed"] = cand[ok], True; return df
L250
L251     _CF_ALIASES = {"cfo":["Operating Cash Flow","Total Cash From Operating Activities"], "capex":["Capital Expenditure","Capital Expenditures"]}
L252
L253     @staticmethod
L254     def _pick_row(df: pd.DataFrame, names: list[str]) -> pd.Series|None:
L255         if df is None or df.empty: return None
L256         idx_lower={str(i).lower():i for i in df.index}
L257         for n in names:
L258             k=n.lower()
L259             if k in idx_lower: return df.loc[idx_lower[k]]
L260         return None
L261
L262     @staticmethod
L263     def _sum_last_n(s: pd.Series|None, n: int) -> float|None:
L264         if s is None or s.empty: return None
L265         v=s.dropna().astype(float); return None if v.empty else v.iloc[:n].sum()
L266
L267     @staticmethod
L268     def _latest(s: pd.Series|None) -> float|None:
L269         if s is None or s.empty: return None
L270         v=s.dropna().astype(float); return v.iloc[0] if not v.empty else None
L271
L272     def fetch_cfo_capex_ttm_yf(self, tickers: list[str]) -> pd.DataFrame:
L273         from concurrent.futures import ThreadPoolExecutor, as_completed
L274         pick, sumn, latest, aliases = self._pick_row, self._sum_last_n, self._latest, self._CF_ALIASES
L275
L276         def one(t: str):
L277             try:
L278                 tk = yf.Ticker(t)  # ★ セッションは渡さない（YFがcurl_cffiで管理）
L279                 qcf = tk.quarterly_cashflow
L280                 cfo_q, capex_q = pick(qcf, aliases["cfo"]), pick(qcf, aliases["capex"])
L281                 fcf_q = pick(qcf, ["Free Cash Flow","FreeCashFlow","Free cash flow"])
L282                 cfo, capex, fcf = sumn(cfo_q,4), sumn(capex_q,4), sumn(fcf_q,4)
L283                 if any(v is None for v in (cfo, capex, fcf)):
L284                     acf = tk.cashflow
L285                     if cfo   is None: cfo   = latest(pick(acf, aliases["cfo"]))
L286                     if capex is None: capex = latest(pick(acf, aliases["capex"]))
L287                     if fcf   is None: fcf   = latest(pick(acf, ["Free Cash Flow","FreeCashFlow","Free cash flow"]))
L288             except Exception as e:
L289                 print(f"[warn] yf financials error: {t}: {e}"); cfo=capex=fcf=None
L290             n=np.nan
L291             return {"ticker":t,
L292                     "cfo_ttm_yf":   n if cfo   is None else cfo,
L293                     "capex_ttm_yf": n if capex is None else capex,
L294                     "fcf_ttm_yf_direct": n if fcf is None else fcf}
L295
L296         rows, mw = [], int(os.getenv("FIN_THREADS","8"))
L297         with ThreadPoolExecutor(max_workers=mw) as ex:
L298             rows=[f.result() for f in as_completed(ex.submit(one,t) for t in tickers)]
L299         return pd.DataFrame(rows).set_index("ticker")
L300
L301     _FINN_CFO_KEYS = ["netCashProvidedByOperatingActivities","netCashFromOperatingActivities","cashFlowFromOperatingActivities","operatingCashFlow"]
L302     _FINN_CAPEX_KEYS = ["capitalExpenditure","capitalExpenditures","purchaseOfPPE","investmentsInPropertyPlantAndEquipment"]
L303
L304     @staticmethod
L305     def _first_key(d: dict, keys: list[str]):
L306         for k in keys:
L307             if k in d and d[k] is not None: return d[k]
L308         return None
L309
L310     @staticmethod
L311     def _finn_get(session: requests.Session, url: str, params: dict, retries: int=3, sleep_s: float=0.5):
L312         for i in range(retries):
L313             r = session.get(url, params=params, timeout=15)
L314             if r.status_code==429: time.sleep(min(2**i*sleep_s,4.0)); continue
L315             r.raise_for_status(); return r.json()
L316         r.raise_for_status()
L317
L318     def fetch_cfo_capex_ttm_finnhub(self, tickers: list[str], api_key: str|None=None) -> pd.DataFrame:
L319         api_key = api_key or os.getenv("FINNHUB_API_KEY")
L320         if not api_key: raise ValueError("Finnhub API key not provided. Set FINNHUB_API_KEY or pass api_key=")
L321         base, s, rows = "https://finnhub.io/api/v1", requests.Session(), []
L322         for sym in tickers:
L323             cfo_ttm = capex_ttm = None
L324             try:
L325                 j = self._finn_get(s, f"{base}/stock/cash-flow", {"symbol":sym,"frequency":"quarterly","limit":8,"token":api_key})
L326                 arr = j.get("cashFlow") or []; cfo_vals, capex_vals = [], []
L327                 for item in arr[:4]:
L328                     cfo_vals.append(self._first_key(item,self._FINN_CFO_KEYS)); capex_vals.append(self._first_key(item,self._FINN_CAPEX_KEYS))
L329                 if any(v is not None for v in cfo_vals): cfo_ttm = float(np.nansum([np.nan if v is None else float(v) for v in cfo_vals]))
L330                 if any(v is not None for v in capex_vals): capex_ttm = float(np.nansum([np.nan if v is None else float(v) for v in capex_vals]))
L331             except Exception: pass
L332             if cfo_ttm is None or capex_ttm is None:
L333                 try:
L334                     j = self._finn_get(s, f"{base}/stock/cash-flow", {"symbol":sym,"frequency":"annual","limit":1,"token":api_key})
L335                     arr = j.get("cashFlow") or []
L336                     if arr:
L337                         item0 = arr[0]
L338                         if cfo_ttm is None:
L339                             v = self._first_key(item0,self._FINN_CFO_KEYS)
L340                             if v is not None: cfo_ttm = float(v)
L341                         if capex_ttm is None:
L342                             v = self._first_key(item0,self._FINN_CAPEX_KEYS)
L343                             if v is not None: capex_ttm = float(v)
L344                 except Exception: pass
L345             rows.append({"ticker":sym,"cfo_ttm_fh":np.nan if cfo_ttm is None else cfo_ttm,"capex_ttm_fh":np.nan if capex_ttm is None else capex_ttm})
L346         return pd.DataFrame(rows).set_index("ticker")
L347
L348     def compute_fcf_with_fallback(self, tickers: list[str], finnhub_api_key: str|None=None) -> pd.DataFrame:
L349         yf_df = self.fetch_cfo_capex_ttm_yf(tickers)
L350         T.log("financials (yf) done")
L351         miss_mask = yf_df[["cfo_ttm_yf","capex_ttm_yf","fcf_ttm_yf_direct"]].isna().any(axis=1)
L352         need = yf_df.index[miss_mask].tolist(); print(f"[T] yf financials missing: {len(need)} {need[:10]}{'...' if len(need)>10 else ''}")
L353         if need:
L354             fh_df = self.fetch_cfo_capex_ttm_finnhub(need, api_key=finnhub_api_key)
L355             df = yf_df.join(fh_df, how="left")
L356             for col_yf, col_fh in [("cfo_ttm_yf","cfo_ttm_fh"),("capex_ttm_yf","capex_ttm_fh")]:
L357                 df[col_yf] = df[col_yf].fillna(df[col_fh])
L358             print("[T] financials (finnhub) done (fallback only)")
L359         else:
L360             df = yf_df.assign(cfo_ttm_fh=np.nan, capex_ttm_fh=np.nan)
L361             print("[T] financials (finnhub) skipped (no missing)")
L362         df["cfo_ttm"]  = df["cfo_ttm_yf"].where(df["cfo_ttm_yf"].notna(), df["cfo_ttm_fh"])
L363         df["capex_ttm"] = df["capex_ttm_yf"].where(df["capex_ttm_yf"].notna(), df["capex_ttm_fh"])
L364         cfo, capex = pd.to_numeric(df["cfo_ttm"], errors="coerce"), pd.to_numeric(df["capex_ttm"], errors="coerce").abs()
L365         fcf_calc = cfo - capex
L366         fcf_direct = pd.to_numeric(df.get("fcf_ttm_yf_direct"), errors="coerce")
L367         df["fcf_ttm"] = fcf_calc.where(fcf_calc.notna(), fcf_direct)
L368         df["cfo_source"]  = np.where(df["cfo_ttm_yf"].notna(),"yfinance",np.where(df["cfo_ttm_fh"].notna(),"finnhub",""))
L369         df["capex_source"] = np.where(df["capex_ttm_yf"].notna(),"yfinance",np.where(df["capex_ttm_fh"].notna(),"finnhub",""))
L370         df["fcf_imputed"] = df[["cfo_ttm","capex_ttm"]].isna().any(axis=1) & df["fcf_ttm"].notna()
L371         cols = ["cfo_ttm_yf","capex_ttm_yf","cfo_ttm_fh","capex_ttm_fh","cfo_ttm","capex_ttm","fcf_ttm","fcf_ttm_yf_direct","cfo_source","capex_source","fcf_imputed"]
L372         return df[cols].sort_index()
L373
L374     def _build_eps_df(self, tickers, tickers_bulk, info):
L375         eps_rows=[]
L376         for t in tickers:
L377             info_t, eps_ttm, eps_q = info[t], info[t].get("trailingEps", np.nan), np.nan
L378             try:
L379                 qearn, so = tickers_bulk.tickers[t].quarterly_earnings, info_t.get("sharesOutstanding")
L380                 if so and qearn is not None and not qearn.empty and "Earnings" in qearn.columns:
L381                     eps_ttm_q = qearn["Earnings"].head(4).sum()/so
L382                     if pd.notna(eps_ttm_q) and (pd.isna(eps_ttm) or (abs(eps_ttm)>0 and abs(eps_ttm/eps_ttm_q)>3)): eps_ttm = eps_ttm_q
L383                     eps_q = qearn["Earnings"].iloc[-1]/so
L384             except Exception: pass
L385             eps_rows.append({"ticker":t,"eps_ttm":eps_ttm,"eps_q_recent":eps_q})
L386         return self.impute_eps_ttm(pd.DataFrame(eps_rows).set_index("ticker"))
L387
L388     def prepare_data(self):
L389         """Fetch price and fundamental data for all tickers."""
L390         cand_info = yf.Tickers(" ".join(self.cand)); cand_prices = {}
L391         for t in self.cand:
L392             try: cand_prices[t] = cand_info.tickers[t].fast_info.get("lastPrice", np.inf)
L393             except Exception as e: print(f"{t}: price fetch failed ({e})"); cand_prices[t] = np.inf
L394         cand_f = [t for t,p in cand_prices.items() if p<=self.price_max]
L395         T.log("price cap filter done (CAND_PRICE_MAX)")
L396         tickers = sorted(set(self.exist + cand_f))
L397         T.log(f"universe prepared: unique={len(tickers)} bench={self.bench}")
L398         data = yf.download(tickers + [self.bench], period="600d", auto_adjust=True, progress=False)
L399         T.log("yf.download done")
L400         px, spx = data["Close"], data["Close"][self.bench]
L401         clip_days = int(os.getenv("PRICE_CLIP_DAYS", "0"))   # 0なら無効（既定）
L402         if clip_days > 0:
L403             px  = px.tail(clip_days + 1)
L404             spx = spx.tail(clip_days + 1)
L405             print(f"[T] price window clipped by env: {len(px)} rows (PRICE_CLIP_DAYS={clip_days})")
L406         else:
L407             print(f"[T] price window clip skipped; rows={len(px)}")
L408         tickers_bulk, info = yf.Tickers(" ".join(tickers)), {}
L409         for t in tickers:
L410             try: info[t] = tickers_bulk.tickers[t].info
L411             except Exception as e: print(f"{t}: info fetch failed ({e})"); info[t] = {}
L412         eps_df = self._build_eps_df(tickers, tickers_bulk, info)
L413         fcf_df = self.compute_fcf_with_fallback(tickers, finnhub_api_key=self.api_key)
L414         T.log("eps/fcf prep done")
L415         returns = px[tickers].pct_change()
L416         T.log("price prep/returns done")
L417         return dict(cand=cand_f, tickers=tickers, data=data, px=px, spx=spx, tickers_bulk=tickers_bulk, info=info, eps_df=eps_df, fcf_df=fcf_df, returns=returns)
L418
L419 # === Selector：相関低減・選定（スコア＆リターンだけ読む） ===
L420 class Selector:
L421     # ---- DRRS helpers（Selector専用） ----
L422     @staticmethod
L423     def _z_np(X: np.ndarray) -> np.ndarray:
L424         X = np.asarray(X, dtype=np.float32); m = np.nanmean(X, axis=0, keepdims=True); s = np.nanstd(X, axis=0, keepdims=True)+1e-9
L425         return (np.nan_to_num(X)-m)/s
L426
L427     @classmethod
L428     def residual_corr(cls, R: np.ndarray, n_pc: int=3, shrink: float=0.1) -> np.ndarray:
L429         Z = cls._z_np(R); U,S,_ = np.linalg.svd(Z, full_matrices=False); F = U[:,:n_pc]*S[:n_pc]; B = np.linalg.lstsq(F, Z, rcond=None)[0]
L430         E = Z - F@B; C = np.corrcoef(E, rowvar=False)
L431         off = C - np.diag(np.diag(C)); iu = np.triu_indices_from(off,1); avg_abs = np.nanmean(np.abs(off[iu])) if iu[0].size else 0.0
L432         shrink_eff = float(np.clip(shrink + 0.5*avg_abs, 0.1, 0.6)); N = C.shape[0]
L433         return (1.0 - shrink_eff)*C + shrink_eff*np.eye(N, dtype=C.dtype)
L434
L435     @classmethod
L436     def rrqr_like_det(cls, R: np.ndarray, score: np.ndarray, k: int, gamma: float=1.0):
L437         Z, w = cls._z_np(R), (score-score.min())/(np.ptp(score)+1e-12); X = Z*(1.0 + gamma*w)
L438         N, k = X.shape[1], max(0, min(k, X.shape[1]))
L439         if k==0: return []
L440         S, selected, Rres = [], np.zeros(N, dtype=bool), X.copy()
L441         for _ in range(k):
L442             norms = (Rres*Rres).sum(axis=0); cand = np.where(~selected)[0]
L443             j = sorted(cand, key=lambda c:(-norms[c], -w[c], c))[0]
L444             S.append(j); selected[j]=True; u = X[:,j:j+1]; u/=(np.linalg.norm(u)+1e-12); Rres = Rres - u @ (u.T @ Rres)
L445         return sorted(S)
L446
L447     @staticmethod
L448     def _obj(corrM: np.ndarray, score: np.ndarray, idx, lam: float) -> float:
L449         idx = list(idx); P = corrM[np.ix_(idx, idx)]; s = (score-score.mean())/(score.std()+1e-9)
L450         return float(s[idx].sum() - lam*((P.sum()-np.trace(P))/2.0))
L451
L452     @classmethod
L453     def swap_local_det(cls, corrM: np.ndarray, score: np.ndarray, idx, lam: float=0.6, max_pass: int=15):
L454         S, best, improved, passes = sorted(idx), cls._obj(corrM, score, idx, lam), True, 0
L455         while improved and passes<max_pass:
L456             improved, passes = False, passes+1
L457             for i,out in enumerate(list(S)):
L458                 for inn in range(len(score)):
L459                     if inn in S: continue
L460                     cand = sorted(S[:i]+[inn]+S[i+1:]); v = cls._obj(corrM, score, cand, lam)
L461                     if v>best+1e-10: S, best, improved = cand, v, True; break
L462                 if improved: break
L463         return S, best
L464
L465     @staticmethod
L466     def _obj_with_cross(C_within: np.ndarray, C_cross: np.ndarray|None, score: np.ndarray, idx, lam: float, mu: float) -> float:
L467         idx = list(idx); P = C_within[np.ix_(idx, idx)]; s = (score-score.mean())/(score.std()+1e-9)
L468         within = (P.sum()-np.trace(P))/2.0; cross = 0.0
L469         if C_cross is not None and C_cross.size>0: cross = C_cross[idx,:].sum()
L470         return float(s[idx].sum() - lam*within - mu*cross)
L471
L472     @classmethod
L473     def swap_local_det_cross(cls, C_within: np.ndarray, C_cross: np.ndarray|None, score: np.ndarray, idx, lam: float=0.6, mu: float=0.3, max_pass: int=15):
L474         S, best, improved, passes, N = sorted(idx), cls._obj_with_cross(C_within,C_cross,score,idx,lam,mu), True, 0, len(score)
L475         while improved and passes<max_pass:
L476             improved, passes = False, passes+1
L477             for i,out in enumerate(list(S)):
L478                 for inn in range(N):
L479                     if inn in S: continue
L480                     cand = sorted(S[:i]+[inn]+S[i+1:]); v = cls._obj_with_cross(C_within,C_cross,score,cand,lam,mu)
L481                     if v>best+1e-10: S, best, improved = cand, v, True; break
L482                 if improved: break
L483         return S, best
L484
L485     @staticmethod
L486     def avg_corr(C: np.ndarray, idx) -> float:
L487         k = len(idx); P = C[np.ix_(idx, idx)]
L488         return float((P.sum()-np.trace(P))/(k*(k-1)+1e-12))
L489
L490     @classmethod
L491     def select_bucket_drrs(cls, returns_df: pd.DataFrame, score_ser: pd.Series, pool_tickers: list[str], k: int, *, n_pc: int, gamma: float, lam: float, lookback: int, shrink: float=0.10, g_fixed_tickers: list[str]|None=None, mu: float=0.0):
L492         g_fixed = [t for t in (g_fixed_tickers or []) if t in returns_df.columns]
L493         union = [t for t in pool_tickers if t in returns_df.columns]
L494         for t in g_fixed:
L495             if t not in union: union.append(t)
L496         Rdf_all = returns_df[union]; Rdf_all = Rdf_all.iloc[-lookback:] if len(Rdf_all)>=lookback else Rdf_all; Rdf_all = Rdf_all.dropna()
L497         pool_eff, g_eff = [t for t in pool_tickers if t in Rdf_all.columns], [t for t in g_fixed if t in Rdf_all.columns]
L498         if len(pool_eff)==0: return dict(idx=[], tickers=[], avg_res_corr=np.nan, sum_score=0.0, objective=-np.inf)
L499         score = score_ser.reindex(pool_eff).to_numpy(dtype=np.float32)
L500         C_all = cls.residual_corr(Rdf_all.to_numpy(), n_pc=n_pc, shrink=shrink)
L501         col_pos = {c:i for i,c in enumerate(Rdf_all.columns)}; pool_pos = [col_pos[t] for t in pool_eff]
L502         C_within, C_cross = C_all[np.ix_(pool_pos,pool_pos)], None
L503         if len(g_eff)>0 and mu>0.0:
L504             g_pos = [col_pos[t] for t in g_eff]; C_cross = C_all[np.ix_(pool_pos,g_pos)]
L505         R_pool = Rdf_all[pool_eff].to_numpy(); S0 = cls.rrqr_like_det(R_pool, score, k, gamma=gamma)
L506         S, Jn = (cls.swap_local_det_cross(C_within, C_cross, score, S0, lam=lam, mu=mu, max_pass=15) if C_cross is not None else cls.swap_local_det(C_within, score, S0, lam=lam, max_pass=15))
L507         selected_tickers = [pool_eff[i] for i in S]
L508         return dict(idx=S, tickers=selected_tickers, avg_res_corr=cls.avg_corr(C_within,S), sum_score=float(score[S].sum()), objective=float(Jn))
L509
L510     # ---- 選定（スコア Series / returns だけを受ける）----
L511 # === Output：出力整形と送信（表示・Slack） ===
L512 class Output:
L513
L514     def __init__(self, debug=False):
L515         self.debug = debug
L516         self.miss_df = self.g_table = self.d_table = self.io_table = self.df_metrics_fmt = self.debug_table = None
L517         self.g_title = self.d_title = ""
L518         self.g_formatters = self.d_formatters = {}
L519         # 低スコア（GSC+DSC）Top10 表示/送信用
L520         self.low10_table = None
L521
L522     # --- 表示（元 display_results のロジックそのまま） ---
L523     def display_results(self, *, exist, bench, df_z, g_score, d_score_all,
L524                         init_G, init_D, top_G, top_D, **kwargs):
L525         pd.set_option('display.float_format','{:.3f}'.format)
L526         print("📈 ファクター分散最適化の結果")
L527         if self.miss_df is not None and not self.miss_df.empty:
L528             print("Missing Data:")
L529             print(self.miss_df.to_string(index=False))
L530
L531         # ---- 表示用：Changes/Near-Miss のスコア源を“最終集計”に統一するプロキシ ----
L532         try:
L533             sc = getattr(self, "_sc", None)
L534             agg_G = getattr(sc, "_agg_G", None)
L535             agg_D = getattr(sc, "_agg_D", None)
L536         except Exception:
L537             sc = agg_G = agg_D = None
L538         class _SeriesProxy:
L539             __slots__ = ("primary", "fallback")
L540             def __init__(self, primary, fallback): self.primary, self.fallback = primary, fallback
L541             def get(self, key, default=None):
L542                 try:
L543                     v = self.primary.get(key) if hasattr(self.primary, "get") else None
L544                     if v is not None and not (isinstance(v, float) and v != v):
L545                         return v
L546                 except Exception:
L547                     pass
L548                 try:
L549                     return self.fallback.get(key) if hasattr(self.fallback, "get") else default
L550                 except Exception:
L551                     return default
L552         g_score = _SeriesProxy(agg_G, g_score)
L553         d_score_all = _SeriesProxy(agg_D, d_score_all)
L554         near_G = getattr(sc, "_near_G", []) if sc else []
L555         near_D = getattr(sc, "_near_D", []) if sc else []
L556
L557         extra_G = [t for t in init_G if t not in top_G][:5]; G_UNI = top_G + extra_G
L558         gsc_series = pd.Series({t: g_score.get(t) for t in G_UNI}, name='GSC')
L559         self.g_table = pd.concat([df_z.loc[G_UNI,['GRW','MOM','TRD','VOL']], gsc_series], axis=1)
L560         self.g_table.index = [t + ("⭐️" if t in top_G else "") for t in G_UNI]
L561         self.g_formatters = {col:"{:.2f}".format for col in ['GRW','MOM','TRD','VOL']}; self.g_formatters['GSC'] = "{:.3f}".format
L562         self.g_title = (f"[G枠 / {N_G} / {_fmt_w(g_weights)} / corrM={corrM} / "
L563                         f"LB={DRRS_G['lookback']} nPC={DRRS_G['n_pc']} γ={DRRS_G['gamma']} λ={DRRS_G['lam']} η={DRRS_G['eta']} shrink={DRRS_SHRINK}]")
L564         if near_G:
L565             add = [t for t in near_G if t not in set(G_UNI)][:10]
L566             if len(add) < 10:
L567                 try:
L568                     aggG = getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False)
L569                     out_now = sorted(set(exist) - set(top_G + top_D))  # 今回 OUT
L570                     used = set(G_UNI + add)
L571                     def _push(lst):
L572                         nonlocal add, used
L573                         for t in lst:
L574                             if len(add) == 10: break
L575                             if t in aggG.index and t not in used:
L576                                 add.append(t); used.add(t)
L577                     _push(out_now)           # ① 今回 OUT を優先
L578                     _push(list(aggG.index))  # ② まだ足りなければ上位で充填
L579                 except Exception:
L580                     pass
L581             if add:
L582                 near_tbl = pd.concat([df_z.loc[add,['GRW','MOM','TRD','VOL']], pd.Series({t: g_score.get(t) for t in add}, name='GSC')], axis=1)
L583                 self.g_table = pd.concat([self.g_table, near_tbl], axis=0)
L584         print(self.g_title); print(self.g_table.to_string(formatters=self.g_formatters))
L585
L586         extra_D = [t for t in init_D if t not in top_D][:5]; D_UNI = top_D + extra_D
L587         cols_D = ['QAL','YLD','VOL','TRD']; d_disp = pd.DataFrame(index=D_UNI)
L588         d_disp['QAL'], d_disp['YLD'], d_disp['VOL'], d_disp['TRD'] = df_z.loc[D_UNI,'D_QAL'], df_z.loc[D_UNI,'D_YLD'], df_z.loc[D_UNI,'D_VOL_RAW'], df_z.loc[D_UNI,'D_TRD']
L589         dsc_series = pd.Series({t: d_score_all.get(t) for t in D_UNI}, name='DSC')
L590         self.d_table = pd.concat([d_disp, dsc_series], axis=1); self.d_table.index = [t + ("⭐️" if t in top_D else "") for t in D_UNI]
L591         self.d_formatters = {col:"{:.2f}".format for col in cols_D}; self.d_formatters['DSC']="{:.3f}".format
L592         import scorer
L593         dw_eff = scorer.D_WEIGHTS_EFF
L594         self.d_title = (f"[D枠 / {N_D} / {_fmt_w(dw_eff)} / corrM={corrM} / "
L595                         f"LB={DRRS_D['lookback']} nPC={DRRS_D['n_pc']} γ={DRRS_D['gamma']} λ={DRRS_D['lam']} μ={CROSS_MU_GD} η={DRRS_D['eta']} shrink={DRRS_SHRINK}]")
L596         if near_D:
L597             add = [t for t in near_D if t not in set(D_UNI)][:10]
L598             if add:
L599                 d_disp2 = pd.DataFrame(index=add)
L600                 d_disp2['QAL'], d_disp2['YLD'], d_disp2['VOL'], d_disp2['TRD'] = df_z.loc[add,'D_QAL'], df_z.loc[add,'D_YLD'], df_z.loc[add,'D_VOL_RAW'], df_z.loc[add,'D_TRD']
L601                 near_tbl = pd.concat([d_disp2, pd.Series({t: d_score_all.get(t) for t in add}, name='DSC')], axis=1)
L602                 self.d_table = pd.concat([self.d_table, near_tbl], axis=0)
L603         print(self.d_title); print(self.d_table.to_string(formatters=self.d_formatters))
L604
L605         # === Changes（IN の GSC/DSC を表示。OUT は銘柄名のみ） ===
L606         in_list = sorted(set(list(top_G)+list(top_D)) - set(exist))
L607         out_list = sorted(set(exist) - set(list(top_G)+list(top_D)))
L608
L609         self.io_table = pd.DataFrame({
L610             'IN': pd.Series(in_list),
L611             '/ OUT': pd.Series(out_list)
L612         })
L613         g_list = [f"{g_score.get(t):.3f}" if pd.notna(g_score.get(t)) else '—' for t in out_list]
L614         d_list = [f"{d_score_all.get(t):.3f}" if pd.notna(d_score_all.get(t)) else '—' for t in out_list]
L615         self.io_table['GSC'] = pd.Series(g_list)
L616         self.io_table['DSC'] = pd.Series(d_list)
L617
L618         print("Changes:")
L619         print(self.io_table.to_string(index=False))
L620
L621         all_tickers = list(set(exist + list(top_G) + list(top_D) + [bench])); prices = yf.download(all_tickers, period='1y', auto_adjust=True, progress=False)['Close']
L622         ret = prices.pct_change(); portfolios = {'CUR':exist,'NEW':list(top_G)+list(top_D)}; metrics={}
L623         for name,ticks in portfolios.items():
L624             pr = ret[ticks].mean(axis=1, skipna=True).dropna(); cum = (1+pr).cumprod()-1; n = len(pr)
L625             if n>=252: ann_ret, ann_vol = (1+cum.iloc[-1])**(252/n)-1, pr.std()*np.sqrt(252)
L626             else: ann_ret, ann_vol = cum.iloc[-1], pr.std()*np.sqrt(n)
L627             sharpe, drawdown = ann_ret/ann_vol, (cum - cum.cummax()).min()
L628             if len(ticks)>=2:
L629                 C_raw = ret[ticks].corr(); RAW_rho = C_raw.mask(np.eye(len(ticks), dtype=bool)).stack().mean()
L630                 R = ret[ticks].dropna().to_numpy(); C_resid = Selector.residual_corr(R, n_pc=3, shrink=DRRS_SHRINK)
L631                 RESID_rho = float((C_resid.sum()-np.trace(C_resid))/(C_resid.shape[0]*(C_resid.shape[0]-1)))
L632             else: RAW_rho = RESID_rho = np.nan
L633             divy = ttm_div_yield_portfolio(ticks); metrics[name] = {'RET':ann_ret,'VOL':ann_vol,'SHP':sharpe,'MDD':drawdown,'RAWρ':RAW_rho,'RESIDρ':RESID_rho,'DIVY':divy}
L634         df_metrics = pd.DataFrame(metrics).T; df_metrics_pct = df_metrics.copy(); self.df_metrics = df_metrics
L635         for col in ['RET','VOL','MDD','DIVY']: df_metrics_pct[col] = df_metrics_pct[col]*100
L636         cols_order = ['RET','VOL','SHP','MDD','RAWρ','RESIDρ','DIVY']; df_metrics_pct = df_metrics_pct.reindex(columns=cols_order)
L637         def _fmt_row(s):
L638             return pd.Series({'RET':f"{s['RET']:.1f}%",'VOL':f"{s['VOL']:.1f}%",'SHP':f"{s['SHP']:.1f}",'MDD':f"{s['MDD']:.1f}%",'RAWρ':(f"{s['RAWρ']:.2f}" if pd.notna(s['RAWρ']) else "NaN"),'RESIDρ':(f"{s['RESIDρ']:.2f}" if pd.notna(s['RESIDρ']) else "NaN"),'DIVY':f"{s['DIVY']:.1f}%"})
L639         self.df_metrics_fmt = df_metrics_pct.apply(_fmt_row, axis=1); print("Performance Comparison:"); print(self.df_metrics_fmt.to_string())
L640         if self.debug:
L641             self.debug_table = pd.concat([df_z[['TR','EPS','REV','ROE','BETA','DIV','FCF','RS','TR_str','DIV_STREAK']], g_score.rename('GSC'), d_score_all.rename('DSC')], axis=1).round(3)
L642             print("Debug Data:"); print(self.debug_table.to_string())
L643
L644         # === 追加: GSC+DSC が低い順 TOP10 ===
L645         try:
L646             all_scores = pd.DataFrame({'GSC': df_z['GSC'], 'DSC': df_z['DSC']}).copy()
L647             all_scores['G_plus_D'] = all_scores['GSC'] + all_scores['DSC']
L648             all_scores = all_scores.dropna(subset=['G_plus_D'])
L649             self.low10_table = all_scores.sort_values('G_plus_D', ascending=True).head(10).round(3)
L650             print("Low Score Candidates (GSC+DSC bottom 10):")
L651             print(self.low10_table.to_string())
L652         except Exception as e:
L653             print(f"[warn] low-score ranking failed: {e}")
L654             self.low10_table = None
L655
L656     # --- Slack送信（元 notify_slack のロジックそのまま） ---
L657     def notify_slack(self):
L658         SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")
L659         if not SLACK_WEBHOOK_URL: raise ValueError("SLACK_WEBHOOK_URL not set (環境変数が未設定です)")
L660         def _filter_suffix_from(spec: dict, group: str) -> str:
L661             g = spec.get(group, {})
L662             parts = [str(m) for m in g.get("pre_mask", [])]
L663             for k, v in (g.get("pre_filter", {}) or {}).items():
L664                 base, op = (k[:-4], "<") if k.endswith("_max") else ((k[:-4], ">") if k.endswith("_min") else (k, "="))
L665                 name = {"beta": "β"}.get(base, base)
L666                 try: val = f"{float(v):g}"
L667                 except: val = str(v)
L668                 parts.append(f"{name}{op}{val}")
L669             return "" if not parts else " / filter:" + " & ".join(parts)
L670         def _inject_filter_suffix(title: str, group: str) -> str:
L671             suf = _filter_suffix_from(FILTER_SPEC, group)
L672             return f"{title[:-1]}{suf}]" if suf and title.endswith("]") else (title + suf)
L673         def _blk(title, tbl, fmt=None, drop=()):
L674             if tbl is None or getattr(tbl,'empty',False): return f"{title}\n(選定なし)\n"
L675             if drop and hasattr(tbl,'columns'):
L676                 keep = [c for c in tbl.columns if c not in drop]
L677                 tbl, fmt = tbl[keep], {k:v for k,v in (fmt or {}).items() if k in keep}
L678             return f"{title}\n```{tbl.to_string(formatters=fmt)}```\n"
L679
L680         g_title = _inject_filter_suffix(self.g_title, "G")
L681         d_title = _inject_filter_suffix(self.d_title, "D")
L682         message  = "📈 ファクター分散最適化の結果\n"
L683         if self.miss_df is not None and not self.miss_df.empty:
L684             message += "Missing Data\n```" + self.miss_df.to_string(index=False) + "```\n"
L685         message += _blk(g_title, self.g_table, self.g_formatters, drop=("TRD",))
L686         message += _blk(d_title, self.d_table, self.d_formatters)
L687         message += "Changes\n" + ("(変更なし)\n" if self.io_table is None or getattr(self.io_table,'empty',False) else f"```{self.io_table.to_string(index=False)}```\n")
L688         message += "Performance Comparison:\n```" + self.df_metrics_fmt.to_string() + "```"
L689         if self.debug and self.debug_table is not None:
L690             message += "\nDebug Data\n```" + self.debug_table.to_string() + "```"
L691         payload = {"text": message}
L692         try:
L693             resp = requests.post(SLACK_WEBHOOK_URL, json=payload); resp.raise_for_status(); print("✅ Slack（Webhook）へ送信しました")
L694         except Exception as e: print(f"⚠️ Slack通知エラー: {e}")
L695
L696 def _infer_g_universe(feature_df, selected12=None, near5=None):
L697     try:
L698         out = feature_df.index[feature_df['group'].astype(str).str.upper().eq('G')].tolist()
L699         if out: return out
L700     except Exception:
L701         pass
L702     base = set()
L703     for lst in (selected12 or []), (near5 or []):
L704         for x in (lst or []): base.add(x)
L705     return list(base) if base else list(feature_df.index)
L706
L707 def _fmt_with_fire_mark(tickers, feature_df):
L708     out = []
L709     for t in tickers or []:
L710         try:
L711             br = bool(feature_df.at[t, "G_BREAKOUT_recent_5d"])
L712             pb = bool(feature_df.at[t, "G_PULLBACK_recent_5d"])
L713             out.append(f"{t}{' 🔥' if (br or pb) else ''}")
L714         except Exception:
L715             out.append(t)
L716     return out
L717
L718 def _label_recent_event(t, feature_df):
L719     try:
L720         br = bool(feature_df.at[t, "G_BREAKOUT_recent_5d"]); dbr = str(feature_df.at[t, "G_BREAKOUT_last_date"]) if br else ""
L721         pb = bool(feature_df.at[t, "G_PULLBACK_recent_5d"]); dpb = str(feature_df.at[t, "G_PULLBACK_last_date"]) if pb else ""
L722         if   br and not pb: return f"{t}（ブレイクアウト確定 {dbr}）"
L723         elif pb and not br: return f"{t}（押し目反発 {dpb}）"
L724         elif br and pb:     return f"{t}（ブレイクアウト確定 {dbr}／押し目反発 {dpb}）"
L725     except Exception:
L726         pass
L727     return t
L728
L729 # === パイプライン可視化：G/D共通フロー（出力は不変） ===
L730
L731 def io_build_input_bundle() -> InputBundle:
L732     """
L733     既存の『データ取得→前処理』を実行し、InputBundle を返す。
L734     処理内容・列名・丸め・例外・ログ文言は現行どおり（変更禁止）。
L735     """
L736     state = Input(cand=cand, exist=exist, bench=bench, price_max=CAND_PRICE_MAX, finnhub_api_key=FINNHUB_API_KEY).prepare_data()
L737     return InputBundle(cand=state["cand"], tickers=state["tickers"], bench=bench, data=state["data"], px=state["px"], spx=state["spx"], tickers_bulk=state["tickers_bulk"], info=state["info"], eps_df=state["eps_df"], fcf_df=state["fcf_df"], returns=state["returns"])
L738
L739 def run_group(sc: Scorer, group: str, inb: InputBundle, cfg: PipelineConfig,
L740               n_target: int) -> tuple[list, float, float, float]:
L741     """
L742     G/Dを同一手順で処理：採点→フィルター→選定（相関低減込み）。
L743     戻り値：(pick, avg_res_corr, sum_score, objective)
L744     JSON保存は既存フォーマット（キー名・丸め桁・順序）を踏襲。
L745     """
L746     sc.cfg = cfg
L747
L748     if hasattr(sc, "score_build_features"):
L749         feat = sc.score_build_features(inb)
L750         if not hasattr(sc, "_feat_logged"):
L751             T.log("features built (scorer)")
L752             sc._feat_logged = True
L753         agg = sc.score_aggregate(feat, group, cfg) if hasattr(sc, "score_aggregate") else feat
L754     else:
L755         fb = sc.aggregate_scores(inb, cfg)
L756         if not hasattr(sc, "_feat_logged"):
L757             T.log("features built (scorer)")
L758             sc._feat_logged = True
L759         sc._feat = fb
L760         agg = fb.g_score if group == "G" else fb.d_score_all
L761         if group == "D" and hasattr(fb, "df"):
L762             agg = agg[fb.df['BETA'] < D_BETA_MAX]
L763
L764     if hasattr(sc, "filter_candidates"):
L765         agg = agg[sc.filter_candidates(inb, agg, group, cfg)]
L766
L767     selector = Selector()
L768     if hasattr(sc, "select_diversified"):
L769         pick, avg_r, sum_sc, obj = sc.select_diversified(agg, group, cfg, n_target,
L770             selector=selector, prev_tickers=None,
L771             corrM=cfg.drrs.corrM, shrink=cfg.drrs.shrink,
L772             cross_mu=cfg.drrs.cross_mu_gd)
L773     else:
L774         if group == "G":
L775             init = agg.nlargest(min(cfg.drrs.corrM, len(agg))).index.tolist()
L776             res = selector.select_bucket_drrs(returns_df=inb.returns, score_ser=agg, pool_tickers=init, k=n_target,
L777                 n_pc=cfg.drrs.G.get("n_pc", 3), gamma=cfg.drrs.G.get("gamma", 1.2),
L778                 lam=cfg.drrs.G.get("lam", 0.68),
L779                 lookback=cfg.drrs.G.get("lookback", 252),
L780                 shrink=cfg.drrs.shrink, g_fixed_tickers=None, mu=0.0)
L781         else:
L782             init = agg.nlargest(min(cfg.drrs.corrM, len(agg))).index.tolist()
L783             g_fixed = getattr(sc, "_top_G", None)
L784             res = selector.select_bucket_drrs(returns_df=inb.returns, score_ser=agg, pool_tickers=init, k=n_target,
L785                 n_pc=cfg.drrs.D.get("n_pc", 4), gamma=cfg.drrs.D.get("gamma", 0.8),
L786                 lam=cfg.drrs.D.get("lam", 0.85),
L787                 lookback=cfg.drrs.D.get("lookback", 504),
L788                 shrink=cfg.drrs.shrink, g_fixed_tickers=g_fixed,
L789                 mu=cfg.drrs.cross_mu_gd)
L790         pick = res["tickers"]; avg_r = res["avg_res_corr"]
L791         sum_sc = res["sum_score"]; obj = res["objective"]
L792         if group == "D":
L793             _, pick = _disjoint_keepG(getattr(sc, "_top_G", []), pick, init)
L794             T.log("selection finalized (G/D)")
L795     try:
L796         inc = [t for t in exist if t in agg.index]
L797         pick = _sticky_keep_current(
L798             agg=agg, pick=pick, incumbents=inc, n_target=n_target,
L799             delta_z=SWAP_DELTA_Z, keep_buffer=SWAP_KEEP_BUFFER
L800         )
L801     except Exception as _e:
L802         print(f"[warn] sticky_keep_current skipped: {str(_e)}")
L803     # --- Near-Miss: 惜しくも選ばれなかった上位10を保持（Slack表示用） ---
L804     # 5) Near-Miss と最終集計Seriesを保持（表示専用。計算へ影響なし）
L805     try:
L806         pool = agg.drop(index=[t for t in pick if t in agg.index], errors="ignore")
L807         near10 = list(pool.sort_values(ascending=False).head(10).index)
L808         setattr(sc, f"_near_{group}", near10)
L809         setattr(sc, f"_agg_{group}", agg)
L810     except Exception:
L811         pass
L812
L813     if group == "D":
L814         T.log("save done")
L815     if group == "G":
L816         sc._top_G = pick
L817     return pick, avg_r, sum_sc, obj
L818
L819 def run_pipeline() -> SelectionBundle:
L820     """
L821     G/D共通フローの入口。I/Oはここだけで実施し、計算はScorerに委譲。
L822     Slack文言・丸め・順序は既存の Output を用いて変更しない。
L823     """
L824     inb = io_build_input_bundle()
L825     cfg = PipelineConfig(weights=WeightsConfig(g=g_weights, d=D_weights),
L826         drrs=DRRSParams(corrM=corrM, shrink=DRRS_SHRINK,
L827                          G=DRRS_G, D=DRRS_D, cross_mu_gd=CROSS_MU_GD),
L828         price_max=CAND_PRICE_MAX)
L829
L830     # --- Pass-1: 暫定選定（GAAPのみ） ---
L831     sc_p1 = Scorer()
L832     top_G1, _, _, _ = run_group(sc_p1, "G", inb, cfg, N_G)
L833     poolG1 = list(getattr(sc_p1, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False).index)
L834     alpha1 = Scorer.spx_to_alpha(inb.spx)
L835     sectors1 = {t:(inb.info.get(t,{}).get("sector") or "U") for t in poolG1}; scores1 = {t:Scorer.g_score.get(t,0.0) for t in poolG1}
L836     top_G1 = Scorer.pick_top_softcap(scores1, sectors1, N=N_G, cap=2, alpha=alpha1, hard=5)
L837     sc_p1._top_G = top_G1
L838     try:
L839         aggG1 = getattr(sc_p1, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False)
L840         sc_p1._near_G = [t for t in aggG1.index if t not in set(top_G1)][:10]
L841     except Exception:
L842         pass
L843     top_D1, _, _, _ = run_group(sc_p1, "D", inb, cfg, N_D)
L844     hopeful = sorted(set(top_G1 + top_D1 + getattr(sc_p1, "_near_G", []) + getattr(sc_p1, "_near_D", [])))
L845
L846     # --- hopeful only: Non-GAAP EPS 取得 ---
L847     eps_df2 = inb.eps_df.copy()
L848     if "nEPS_ttm" not in eps_df2.columns:
L849         eps_df2["nEPS_ttm"] = np.nan
L850     if _API and hopeful:
L851         n_map = {t: _fetch_eps_non_gaap_ttm_metric(t, _API) for t in hopeful}
L852         for t, v in n_map.items():
L853             if t in eps_df2.index:
L854                 eps_df2.at[t, "nEPS_ttm"] = v
L855         print(f"[nEPS] fetched={sum(np.isfinite(list(n_map.values())))}  sample: "
L856               f"{[(k, round(v,3)) for k,v in list(n_map.items())[:5]]}")
L857     else:
L858         pass  # APIキー無しの場合でも列は必ず保持（後段のKeyError防止）
L859
L860     # Frozen 回避: InputBundle を replace で再生成
L861     inb2 = replace(inb, eps_df=eps_df2)
L862
L863     # --- Pass-2: 最終選定（nEPS 反映後） ---
L864     sc = Scorer()
L865     top_G, avgG, sumG, objG = run_group(sc, "G", inb2, cfg, N_G)
L866     poolG = list(getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False).index)
L867     alpha = Scorer.spx_to_alpha(inb2.spx)
L868     sectors = {t:(inb2.info.get(t,{}).get("sector") or "U") for t in poolG}; scores = {t:Scorer.g_score.get(t,0.0) for t in poolG}
L869     top_G = Scorer.pick_top_softcap(scores, sectors, N=N_G, cap=2, alpha=alpha, hard=5)
L870     sc._top_G = top_G
L871     try:
L872         aggG = getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False)
L873         sc._near_G = [t for t in aggG.index if t not in set(top_G)][:10]
L874     except Exception:
L875         pass
L876     base = sum(Scorer.g_score.get(t,0.0) for t in poolG[:N_G])
L877     effs = sum(Scorer.g_score.get(t,0.0) for t in top_G)
L878     print(f"[soft_cap2] score_cost={(base-effs)/max(1e-9,abs(base)):.2%}, alpha={alpha:.3f}")
L879     top_D, avgD, sumD, objD = run_group(sc, "D", inb2, cfg, N_D)
L880     fb = getattr(sc, "_feat", None)
L881     if fb is not None:
L882         try:
L883             fb.df_z["nEPS_ttm"] = inb2.eps_df["nEPS_ttm"]
L884         except Exception:
L885             pass
L886     near_G = getattr(sc, "_near_G", [])
L887     selected12 = list(top_G)
L888     df = fb.df if fb is not None else pd.DataFrame()
L889     guni = _infer_g_universe(df, selected12, near_G)
L890     try:
L891         fire_recent = [t for t in guni
L892                        if (str(df.at[t, "G_BREAKOUT_recent_5d"]) == "True") or
L893                           (str(df.at[t, "G_PULLBACK_recent_5d"]) == "True")]
L894     except Exception: fire_recent = []
L895
L896     lines = [
L897         "【G枠レポート｜週次モニタ（直近5営業日）】",
L898         "【凡例】🔥=直近5営業日内に「ブレイクアウト確定」または「押し目反発」を検知",
L899         f"選定{N_G}: {', '.join(_fmt_with_fire_mark(selected12, df))}" if selected12 else f"選定{N_G}: なし",
L900         f"次点10: {', '.join(_fmt_with_fire_mark(near_G, df))}" if near_G else "次点10: なし",]
L901
L902     if fire_recent:
L903         fire_list = ", ".join([_label_recent_event(t, df) for t in fire_recent])
L904         lines.append(f"過去5営業日の検知: {fire_list}")
L905     else:
L906         lines.append("過去5営業日の検知: なし")
L907
L908     try:
L909         webhook = os.environ.get("SLACK_WEBHOOK_URL", "")
L910         if webhook:
L911             requests.post(webhook, json={"text": "\n".join([s for s in lines if s != ""])}, timeout=10)
L912     except Exception:
L913         pass
L914
L915     out = Output(debug=debug_mode)
L916     # 表示側から選定時の集計へアクセスできるように保持（表示専用・副作用なし）
L917     try: out._sc = sc
L918     except Exception: pass
L919     if hasattr(sc, "_feat"):
L920         try:
L921             out.miss_df = sc._feat.missing_logs
L922             out.display_results(exist=exist, bench=bench, df_z=sc._feat.df_z,
L923                 g_score=sc._feat.g_score, d_score_all=sc._feat.d_score_all,
L924                 init_G=top_G, init_D=top_D, top_G=top_G, top_D=top_D)
L925         except Exception:
L926             pass
L927     out.notify_slack()
L928     sb = SelectionBundle(resG={"tickers": top_G, "avg_res_corr": avgG,
L929               "sum_score": sumG, "objective": objG},
L930         resD={"tickers": top_D, "avg_res_corr": avgD,
L931               "sum_score": sumD, "objective": objD},
L932         top_G=top_G, top_D=top_D, init_G=top_G, init_D=top_D)
L933
L934     # --- Low Score Candidates (GSC+DSC bottom 10) : send before debug dump ---
L935     try:
L936         _low_df = (pd.DataFrame({"GSC": fb.g_score, "DSC": fb.d_score_all})
L937               .assign(G_plus_D=lambda x: x["GSC"] + x["DSC"])
L938               .sort_values("G_plus_D")
L939               .head(10)
L940               .round(3))
L941         _slack("Low Score Candidates (GSC+DSC bottom 10)\n"
L942                "```"
L943                + _low_df.to_string(index=True, index_names=False)
L944                + "\n```")
L945     except Exception as _e:
L946         _slack(f"Low Score Candidates: 作成失敗: {_e}")
L947
L948     if debug_mode:
L949         try:
L950             _slack_debug(_compact_debug(fb, sb, [], []))
L951         except Exception as e:
L952             print(f"[debug skipped] {e}")
L953
L954     return sb
L955
L956 if __name__ == "__main__":
L957     run_pipeline()
```

## <scorer.py>
```text
L1 # scorer.py
L2 # kawatest
L3 # =============================================================================
L4 # Scorer: ファクター/指標の生成と合成スコア算出を担う純粋層
L5 #
L6 # 【このファイルだけ読めば分かるポイント】
L7 # - 入力(InputBundle)は「価格/出来高/ベンチ/基本情報/EPS/FCF/リターン」を含むDTO
L8 # - 出力(FeatureBundle)は「raw特徴量 df」「標準化 df_z」「G/D スコア」「欠損ログ」
L9 # - 重み等のコンフィグ(PipelineConfig)は factor から渡す（cfg 必須）
L10 # - 旧カラム名は Scorer 内で自動リネームして受け入れ（後方互換）
L11 #   例) eps_ttm -> EPS_TTM, eps_q_recent -> EPS_Q_LastQ, fcf_ttm -> FCF_TTM
L12 #
L13 # 【I/O契約（Scorerが参照するInputBundleフィールド）】
L14 #   - cand: List[str]    … 候補銘柄（単体実行では未使用）
L15 #   - tickers: List[str] … 対象銘柄リスト
L16 #   - bench: str         … ベンチマークティッカー（例 '^GSPC'）
L17 #   - data: pd.DataFrame … yfinance download結果 ('Close','Volume' 等の階層列)
L18 #   - px: pd.DataFrame   … data['Close'] 相当（終値）
L19 #   - spx: pd.Series     … ベンチマークの終値
L20 #   - tickers_bulk: object         … yfinance.Tickers
L21 #   - info: Dict[str, dict]        … yfinance info per ticker
L22 #   - eps_df: pd.DataFrame         … 必須列: EPS_TTM, EPS_Q_LastQ（旧名も可）
L23 #   - fcf_df: pd.DataFrame         … 必須列: FCF_TTM（旧名も可）
L24 #   - returns: pd.DataFrame        … px[tickers].pct_change() 相当
L25 #
L26 # ※入出力の形式・例外文言は既存実装を変えません（安全な短縮のみ）
L27 # =============================================================================
L28
L29 import os, sys, warnings
L30 import requests
L31 import numpy as np
L32 import pandas as pd
L33 import yfinance as yf
L34 from typing import Any, TYPE_CHECKING
L35 from scipy.stats import zscore
L36
L37 if TYPE_CHECKING:
L38     from factor import PipelineConfig  # type: ignore  # 実行時importなし（循環回避）
L39
L40 # ---- Dividend Helpers -------------------------------------------------------
L41 def _last_close(t, price_map=None):
L42     if price_map and (c := price_map.get(t)) is not None: return float(c)
L43     try:
L44         h = yf.Ticker(t).history(period="5d")["Close"].dropna()
L45         return float(h.iloc[-1]) if len(h) else np.nan
L46     except Exception:
L47         return np.nan
L48
L49 def _ttm_div_sum(t, lookback_days=400):
L50     try:
L51         div = yf.Ticker(t).dividends
L52         if div is None or len(div) == 0: return 0.0
L53         cutoff = pd.Timestamp.utcnow().tz_localize(None) - pd.Timedelta(days=lookback_days)
L54         ttm = float(div[div.index.tz_localize(None) >= cutoff].sum())
L55         return ttm if ttm > 0 else float(div.tail(4).sum())
L56     except Exception:
L57         return 0.0
L58
L59 def ttm_div_yield_portfolio(tickers, price_map=None):
L60     ys = [(lambda c, s: (s/c) if (np.isfinite(c) and c>0 and s>0) else 0.0)(_last_close(t, price_map), _ttm_div_sum(t)) for t in tickers]
L61     return float(np.mean(ys)) if ys else 0.0
L62
L63 # ---- 簡易ユーティリティ（安全な短縮のみ） -----------------------------------
L64 def winsorize_s(s: pd.Series, p=0.02):
L65     if s is None or s.dropna().empty: return s
L66     lo, hi = np.nanpercentile(s.astype(float), [100*p, 100*(1-p)]); return s.clip(lo, hi)
L67
L68 def robust_z(s: pd.Series, p=0.02):
L69     s2 = winsorize_s(s,p); return np.nan_to_num(zscore(s2.fillna(s2.mean())))
L70
L71 def _safe_div(a, b):
L72     try: return np.nan if (b is None or float(b)==0 or pd.isna(b)) else float(a)/float(b)
L73     except Exception: return np.nan
L74
L75 def _safe_last(series: pd.Series, default=np.nan):
L76     try: return float(series.iloc[-1])
L77     except Exception: return default
L78
L79 D_WEIGHTS_EFF = None  # 出力表示互換のため
L80
L81 # ---- Scorer 本体 -------------------------------------------------------------
L82 class Scorer:
L83     """
L84     - factor.py からは `aggregate_scores(ib, cfg)` を呼ぶだけでOK。
L85     - cfg は必須（factor.PipelineConfig を渡す）。
L86     - 旧カラム名を自動リネームして新スキーマに吸収します。
L87     """
L88
L89     # === 先頭で旧→新カラム名マップ（移行用） ===
L90     EPS_RENAME = {"eps_ttm":"EPS_TTM", "eps_q_recent":"EPS_Q_LastQ"}
L91     FCF_RENAME = {"fcf_ttm":"FCF_TTM"}
L92
L93     # === スキーマ簡易チェック（最低限） ===
L94     @staticmethod
L95     def _validate_ib_for_scorer(ib: Any):
L96         miss = [a for a in ["tickers","bench","data","px","spx","tickers_bulk","info","eps_df","fcf_df","returns"] if not hasattr(ib,a) or getattr(ib,a) is None]
L97         if miss: raise ValueError(f"InputBundle is missing required attributes for Scorer: {miss}")
L98         if any(c in ib.eps_df.columns for c in Scorer.EPS_RENAME): ib.eps_df.rename(columns=Scorer.EPS_RENAME, inplace=True)
L99         if any(c in ib.fcf_df.columns for c in Scorer.FCF_RENAME): ib.fcf_df.rename(columns=Scorer.FCF_RENAME, inplace=True)
L100         need_eps, need_fcf = {"EPS_TTM","EPS_Q_LastQ"},{"FCF_TTM"}
L101         if not need_eps.issubset(ib.eps_df.columns): raise ValueError(f"eps_df must contain columns {need_eps} (accepts old names via auto-rename). Got: {list(ib.eps_df.columns)}")
L102         if not need_fcf.issubset(ib.fcf_df.columns): raise ValueError(f"fcf_df must contain columns {need_fcf} (accepts old names via auto-rename). Got: {list(ib.fcf_df.columns)}")
L103
L104     # ----（Scorer専用）テクニカル・指標系 ----
L105     @staticmethod
L106     def trend(s: pd.Series):
L107         if len(s)<200: return np.nan
L108         sma50, sma150, sma200 = s.rolling(50).mean().iloc[-1], s.rolling(150).mean().iloc[-1], s.rolling(200).mean().iloc[-1]
L109         prev200, p = s.rolling(200).mean().iloc[-21], s.iloc[-1]
L110         lo_52 = s[-252:].min() if len(s)>=252 else s.min(); hi_52 = s[-252:].max() if len(s)>=252 else s.max()
L111         rng = (hi_52 - lo_52) if hi_52>lo_52 else np.nan
L112         clip = lambda x,lo,hi: (np.nan if pd.isna(x) else max(lo,min(hi,x)))
L113         a = clip(p/(s.rolling(50).mean().iloc[-1]) - 1, -0.5, 0.5)
L114         b = clip(sma50/sma150 - 1, -0.5, 0.5)
L115         c = clip(sma150/sma200 - 1, -0.5, 0.5)
L116         d = clip(sma200/prev200 - 1, -0.2, 0.2)
L117         e = clip((p - lo_52) / (rng if rng and rng>0 else np.nan) - 0.5, -0.5, 0.5)
L118         parts = [0.0 if pd.isna(x) else x for x in (a,b,c,d,e)]
L119         return 0.30*parts[0] + 0.20*parts[1] + 0.15*parts[2] + 0.15*parts[3] + 0.20*parts[4]
L120
L121     @staticmethod
L122     def rs(s, b):
L123         n, nb = len(s), len(b)
L124         if n<60 or nb<60: return np.nan
L125         L12 = 252 if n>=252 and nb>=252 else min(n,nb)-1; L1 = 22 if n>=22 and nb>=22 else max(5, min(n,nb)//3)
L126         r12, r1, br12, br1 = s.iloc[-1]/s.iloc[-L12]-1, s.iloc[-1]/s.iloc[-L1]-1, b.iloc[-1]/b.iloc[-L12]-1, b.iloc[-1]/b.iloc[-L1]-1
L127         return (r12 - br12)*0.7 + (r1 - br1)*0.3
L128
L129     @staticmethod
L130     def tr_str(s):
L131         if len(s)<50: return np.nan
L132         return s.iloc[-1]/s.rolling(50).mean().iloc[-1] - 1
L133
L134     @staticmethod
L135     def rs_line_slope(s: pd.Series, b: pd.Series, win: int) -> float:
L136         r = (s/b).dropna()
L137         if len(r) < win: return np.nan
L138         y, x = np.log(r.iloc[-win:]), np.arange(win, dtype=float)
L139         try: return float(np.polyfit(x, y, 1)[0])
L140         except Exception: return np.nan
L141
L142     @staticmethod
L143     def ev_fallback(info_t: dict, tk: yf.Ticker) -> float:
L144         ev = info_t.get('enterpriseValue', np.nan)
L145         if pd.notna(ev) and ev>0: return float(ev)
L146         mc, debt, cash = info_t.get('marketCap', np.nan), np.nan, np.nan
L147         try:
L148             bs = tk.quarterly_balance_sheet
L149             if bs is not None and not bs.empty:
L150                 c = bs.columns[0]
L151                 for k in ("Total Debt","Long Term Debt","Short Long Term Debt"):
L152                     if k in bs.index: debt = float(bs.loc[k,c]); break
L153                 for k in ("Cash And Cash Equivalents","Cash And Cash Equivalents And Short Term Investments","Cash"):
L154                     if k in bs.index: cash = float(bs.loc[k,c]); break
L155         except Exception: pass
L156         if pd.notna(mc): return float(mc + (0 if pd.isna(debt) else debt) - (0 if pd.isna(cash) else cash))
L157         return np.nan
L158
L159     @staticmethod
L160     def dividend_status(ticker: str) -> str:
L161         t = yf.Ticker(ticker)
L162         try:
L163             if not t.dividends.empty: return "has"
L164         except Exception: return "unknown"
L165         try:
L166             a = t.actions
L167             if (a is not None and not a.empty and "Stock Splits" in a.columns and a["Stock Splits"].abs().sum()>0): return "none_confident"
L168         except Exception: pass
L169         try:
L170             fi = t.fast_info
L171             if any(getattr(fi,k,None) for k in ("last_dividend_date","dividend_rate","dividend_yield")): return "maybe_missing"
L172         except Exception: pass
L173         return "unknown"
L174
L175     @staticmethod
L176     def div_streak(t):
L177         try:
L178             divs = yf.Ticker(t).dividends.dropna(); ann = divs.groupby(divs.index.year).sum(); ann = ann[ann.index<pd.Timestamp.today().year]
L179             years, streak = sorted(ann.index), 0
L180             for i in range(len(years)-1,0,-1):
L181                 if ann[years[i]] > ann[years[i-1]]: streak += 1
L182                 else: break
L183             return streak
L184         except Exception: return 0
L185
L186     @staticmethod
L187     def fetch_finnhub_metrics(symbol):
L188         api_key = os.environ.get("FINNHUB_API_KEY")
L189         if not api_key: return {}
L190         url, params = "https://finnhub.io/api/v1/stock/metric", {"symbol":symbol,"metric":"all","token":api_key}
L191         try:
L192             r = requests.get(url, params=params, timeout=10); r.raise_for_status(); m = r.json().get("metric",{})
L193             return {'EPS':m.get('epsGrowthTTMYoy'),'REV':m.get('revenueGrowthTTMYoy'),'ROE':m.get('roeTTM'),'BETA':m.get('beta'),'DIV':m.get('dividendYieldIndicatedAnnual'),'FCF':(m.get('freeCashFlowTTM')/m.get('enterpriseValue')) if m.get('freeCashFlowTTM') and m.get('enterpriseValue') else None}
L194         except Exception: return {}
L195
L196     @staticmethod
L197     def calc_beta(series: pd.Series, market: pd.Series, lookback=252):
L198         r, m = series.pct_change().dropna(), market.pct_change().dropna()
L199         n = min(len(r), len(m), lookback)
L200         if n<60: return np.nan
L201         r, m = r.iloc[-n:], m.iloc[-n:]; cov, var = np.cov(r, m)[0,1], np.var(m)
L202         return np.nan if var==0 else cov/var
L203
L204     @staticmethod
L205     def spx_to_alpha(spx: pd.Series, bands=(0.03,0.10), w=(0.6,0.4),
L206                      span=5, q=(0.20,0.40), alphas=(0.05,0.08,0.10)) -> float:
L207         """
L208         S&P500指数のみから擬似breadthを作り、履歴分位でαを段階決定。
L209         bands=(±3%, ±10%), w=(50DMA,200DMA), 分位q=(20%,40%), alphas=(低,中,高)
L210         """
L211         ma50, ma200 = spx.rolling(50).mean(), spx.rolling(200).mean()
L212         b50, b200 = ((spx/ma50 - 1)+bands[0])/(2*bands[0]), ((spx/ma200 - 1)+bands[1])/(2*bands[1])
L213         hist = (w[0]*b50 + w[1]*b200).clip(0,1).ewm(span=span).mean()
L214         b, (lo, mid) = float(hist.iloc[-1]), (float(hist.quantile(q[0])), float(hist.quantile(q[1])))
L215         return alphas[0] if b < lo else alphas[1] if b < mid else alphas[2]
L216
L217     @staticmethod
L218     def soft_cap_effective_scores(scores: pd.Series|dict, sectors: dict, cap=2, alpha=0.08) -> pd.Series:
L219         """
L220         同一セクターcap超過（3本目以降）に α×段階減点を課した“有効スコア”Seriesを返す。
L221         戻り値は降順ソート済み。
L222         """
L223         s = pd.Series(scores, dtype=float); order = s.sort_values(ascending=False).index
L224         cnt, pen = {}, {}
L225         for t in order:
L226             sec = sectors.get(t, "U"); cnt[sec] = cnt.get(sec,0) + 1; pen[t] = alpha*max(0, cnt[sec]-cap)
L227         return (s - pd.Series(pen)).sort_values(ascending=False)
L228
L229     @staticmethod
L230     def pick_top_softcap(scores: pd.Series|dict, sectors: dict, N: int, cap=2, alpha=0.08, hard: int|None=5) -> list[str]:
L231         """
L232         soft-cap適用後の上位Nティッカーを返す。hard>0なら非常用ハード上限で同一セクター超過を間引く（既定=5）。
L233         """
L234         eff = Scorer.soft_cap_effective_scores(scores, sectors, cap, alpha)
L235         if not hard:
L236             return list(eff.head(N).index)
L237         pick, used = [], {}
L238         for t in eff.index:
L239             s = sectors.get(t, "U")
L240             if used.get(s,0) < hard:
L241                 pick.append(t); used[s] = used.get(s,0) + 1
L242             if len(pick) == N: break
L243         return pick
L244
L245     @staticmethod
L246     def trend_template_breadth_series(px: pd.DataFrame, spx: pd.Series, win_days: int | None = None) -> pd.Series:
L247         """
L248         各営業日の trend_template 合格本数（合格“本数”=C）を返す。
L249         - px: 列=ticker（ベンチは含めない）
L250         - spx: ベンチマーク Series（px.index に整列）
L251         - win_days: 末尾の計算対象営業日数（None→全体、既定600は呼び出し側指定）
L252         ベクトル化＆rollingのみで軽量。欠損は False 扱い。
L253         """
L254         import numpy as np, pandas as pd
L255         if px is None or px.empty:
L256             return pd.Series(dtype=int)
L257         px = px.dropna(how="all", axis=1)
L258         if win_days and win_days > 0:
L259             px = px.tail(win_days)
L260         if px.empty:
L261             return pd.Series(dtype=int)
L262         spx = spx.reindex(px.index).ffill()
L263
L264         ma50  = px.rolling(50).mean()
L265         ma150 = px.rolling(150).mean()
L266         ma200 = px.rolling(200).mean()
L267
L268         tt = (px > ma150)
L269         tt &= (px > ma200)
L270         tt &= (ma150 > ma200)
L271         tt &= (ma200 - ma200.shift(21) > 0)
L272         tt &= (ma50  > ma150)
L273         tt &= (ma50  > ma200)
L274         tt &= (px    > ma50)
L275
L276         lo252 = px.rolling(252).min()
L277         hi252 = px.rolling(252).max()
L278         tt &= (px.divide(lo252).sub(1.0) >= 0.30)   # P_OVER_LOW52 >= 0.30
L279         tt &= (px >= (0.75 * hi252))                # NEAR_52W_HIGH >= -0.25
L280
L281         r12  = px.divide(px.shift(252)).sub(1.0)
L282         br12 = spx.divide(spx.shift(252)).sub(1.0)
L283         r1   = px.divide(px.shift(22)).sub(1.0)
L284         br1  = spx.divide(spx.shift(22)).sub(1.0)
L285         rs   = 0.7*(r12.sub(br12, axis=0)) + 0.3*(r1.sub(br1, axis=0))
L286         tt &= (rs >= 0.10)
L287
L288         return tt.fillna(False).sum(axis=1).astype(int)
L289
L290     def filter_candidates(self, ib, agg, group, cfg):
L291         df = ib.eps_df.join(ib.fcf_df, how="outer")
L292         eps_any = (df.get("EPS_TTM", 0) > 0) | (df.get("nEPS_ttm", 0) > 0)
L293         profitable = eps_any & (df.get("FCF_TTM", 0) > 0)
L294         return profitable.reindex(agg.index).fillna(False)
L295
L296     # ---- スコア集計（DTO/Configを受け取り、FeatureBundleを返す） ----
L297     def aggregate_scores(self, ib: Any, cfg):
L298         if cfg is None:
L299             raise ValueError("cfg is required; pass factor.PipelineConfig")
L300         self._validate_ib_for_scorer(ib)
L301
L302         px, spx, tickers = ib.px, ib.spx, ib.tickers
L303         tickers_bulk, info, eps_df, fcf_df = ib.tickers_bulk, ib.info, ib.eps_df, ib.fcf_df
L304
L305         df, missing_logs = pd.DataFrame(index=tickers), []
L306         for t in tickers:
L307             d, s = info[t], px[t]; ev = self.ev_fallback(d, tickers_bulk.tickers[t])
L308             # --- 基本特徴 ---
L309             df.loc[t,'TR']   = self.trend(s)
L310             df.loc[t,'EPS']  = eps_df.loc[t,'EPS_TTM'] if t in eps_df.index else np.nan
L311             if 'nEPS_ttm' in eps_df.columns:
L312                 df.loc[t,'nEPS_ttm'] = eps_df.loc[t,'nEPS_ttm'] if t in eps_df.index else np.nan
L313             else:
L314                 df.loc[t,'nEPS_ttm'] = np.nan
L315             df.loc[t,'REV']  = d.get('revenueGrowth',np.nan)
L316             df.loc[t,'ROE']  = d.get('returnOnEquity',np.nan)
L317             df.loc[t,'BETA'] = self.calc_beta(s, spx, lookback=252)
L318
L319             # --- 配当（直近1年無配は DIV=0 扱い） ---
L320             div = 0.0
L321             try:
L322                 divs = yf.Ticker(t).dividends
L323                 if divs is not None and not divs.empty:
L324                     last_close = s.iloc[-1]
L325                     cutoff = divs.index.max() - pd.Timedelta(days=365)
L326                     ttm_sum = float(divs[divs.index >= cutoff].sum())
L327                     # 直近1年の現金配当合計が正ならのみ採用（特別配の過去分は無視）
L328                     if last_close and last_close > 0 and ttm_sum > 0:
L329                         div = ttm_sum / float(last_close)
L330             except Exception:
L331                 pass
L332
L333             # dividends 時系列が取れなかったときだけ、info側にフォールバック
L334             if div == 0.0:
L335                 yi = d.get('dividendYield', None)
L336                 if yi is None:
L337                     yi = d.get('trailingAnnualDividendYield', None)
L338                 try:
L339                     if yi is not None and not pd.isna(yi):
L340                         div = float(yi)
L341                 except Exception:
L342                     pass
L343
L344             df.loc[t, 'DIV'] = float(div)  # 最終確定。直近1年ゼロなら必ず 0.0
L345
L346             # --- FCF/EV ---
L347             fcf_val = fcf_df.loc[t,'FCF_TTM'] if t in fcf_df.index else np.nan
L348             df.loc[t,'FCF_TTM'] = fcf_val
L349             df.loc[t,'FCF'] = (fcf_val/ev) if (pd.notna(fcf_val) and pd.notna(ev) and ev>0) else np.nan
L350
L351             # --- モメンタム・ボラ関連 ---
L352             df.loc[t,'RS'], df.loc[t,'TR_str'] = self.rs(s, spx), self.tr_str(s)
L353             r, rm = s.pct_change().dropna(), spx.pct_change().dropna()
L354             n = int(min(len(r), len(rm)))
L355
L356             DOWNSIDE_DEV = np.nan
L357             if n>=60:
L358                 r6 = r.iloc[-min(len(r),126):]; neg = r6[r6<0]
L359                 if len(neg)>=10: DOWNSIDE_DEV = float(neg.std(ddof=0)*np.sqrt(252))
L360             df.loc[t,'DOWNSIDE_DEV'] = DOWNSIDE_DEV
L361
L362             MDD_1Y = np.nan
L363             try:
L364                 w = s.iloc[-min(len(s),252):].dropna()
L365                 if len(w)>=30:
L366                     roll_max = w.cummax(); MDD_1Y = float((w/roll_max - 1.0).min())
L367             except Exception: pass
L368             df.loc[t,'MDD_1Y'] = MDD_1Y
L369
L370             RESID_VOL = np.nan
L371             if n>=120:
L372                 rr, rrm = r.iloc[-n:].align(rm.iloc[-n:], join='inner')
L373                 if len(rr)==len(rrm) and len(rr)>=120 and rrm.var()>0:
L374                     beta = float(np.cov(rr, rrm)[0,1]/np.var(rrm)); resid = rr - beta*rrm
L375                     RESID_VOL = float(resid.std(ddof=0)*np.sqrt(252))
L376             df.loc[t,'RESID_VOL'] = RESID_VOL
L377
L378             DOWN_OUTPERF = np.nan
L379             if n>=60:
L380                 m, x = rm.iloc[-n:], r.iloc[-n:]; mask = m<0
L381                 if mask.sum()>=10:
L382                     mr, sr = float(m[mask].mean()), float(x[mask].mean())
L383                     DOWN_OUTPERF = (sr - mr)/abs(mr) if mr!=0 else np.nan
L384             df.loc[t,'DOWN_OUTPERF'] = DOWN_OUTPERF
L385
L386             # --- 長期移動平均/位置 ---
L387             sma200 = s.rolling(200).mean(); df.loc[t,'EXT_200'] = np.nan
L388             if pd.notna(sma200.iloc[-1]) and sma200.iloc[-1]!=0: df.loc[t,'EXT_200'] = abs(float(s.iloc[-1]/sma200.iloc[-1]-1.0))
L389
L390             # --- 配当の詳細系 ---
L391             DIV_TTM_PS=DIV_VAR5=DIV_YOY=DIV_FCF_COVER=np.nan
L392             try:
L393                 divs = yf.Ticker(t).dividends.dropna()
L394                 if not divs.empty:
L395                     last_close = s.iloc[-1]; div_1y = float(divs[divs.index >= (divs.index.max()-pd.Timedelta(days=365))].sum())
L396                     DIV_TTM_PS = div_1y if div_1y>0 else np.nan
L397                     ann = divs.groupby(divs.index.year).sum()
L398                     if len(ann)>=2 and ann.iloc[-2]!=0: DIV_YOY = float(ann.iloc[-1]/ann.iloc[-2]-1.0)
L399                     tail = ann.iloc[-5:] if len(ann)>=5 else ann
L400                     if len(tail)>=3 and tail.mean()!=0: DIV_VAR5 = float(tail.std(ddof=1)/abs(tail.mean()))
L401                 so = d.get('sharesOutstanding',None)
L402                 if so and pd.notna(DIV_TTM_PS) and pd.notna(fcf_val) and fcf_val!=0:
L403                     DIV_FCF_COVER = float((fcf_val)/(DIV_TTM_PS*float(so)))
L404             except Exception: pass
L405             df.loc[t,'DIV_TTM_PS'], df.loc[t,'DIV_VAR5'], df.loc[t,'DIV_YOY'], df.loc[t,'DIV_FCF_COVER'] = DIV_TTM_PS, DIV_VAR5, DIV_YOY, DIV_FCF_COVER
L406
L407             # --- 財務安定性 ---
L408             df.loc[t,'DEBT2EQ'], df.loc[t,'CURR_RATIO'] = d.get('debtToEquity',np.nan), d.get('currentRatio',np.nan)
L409
L410             # --- EPS 変動 ---
L411             EPS_VAR_8Q = np.nan
L412             try:
L413                 qe, so = tickers_bulk.tickers[t].quarterly_earnings, d.get('sharesOutstanding',None)
L414                 if qe is not None and not qe.empty and so:
L415                     eps_q = (qe['Earnings'].dropna().astype(float)/float(so)).replace([np.inf,-np.inf],np.nan)
L416                     if len(eps_q)>=4: EPS_VAR_8Q = float(eps_q.iloc[-min(8,len(eps_q)):].std(ddof=1))
L417             except Exception: pass
L418             df.loc[t,'EPS_VAR_8Q'] = EPS_VAR_8Q
L419
L420             # --- サイズ/流動性 ---
L421             df.loc[t,'MARKET_CAP'] = d.get('marketCap',np.nan); adv60 = np.nan
L422             try:
L423                 vol_series = ib.data['Volume'][t].dropna()
L424                 if len(vol_series)>=5 and len(s)==len(vol_series):
L425                     dv = (vol_series*s).rolling(60).mean(); adv60 = float(dv.iloc[-1])
L426             except Exception: pass
L427             df.loc[t,'ADV60_USD'] = adv60
L428
L429             # --- 売上/利益の加速度等 ---
L430             REV_Q_YOY=EPS_Q_YOY=REV_YOY_ACC=REV_YOY_VAR=np.nan
L431             REV_ANNUAL_STREAK = np.nan
L432             try:
L433                 qe, so = tickers_bulk.tickers[t].quarterly_earnings, d.get('sharesOutstanding',None)
L434                 if qe is not None and not qe.empty:
L435                     if 'Revenue' in qe.columns:
L436                         rev = qe['Revenue'].dropna().astype(float)
L437                         if len(rev)>=5: REV_Q_YOY = _safe_div(rev.iloc[-1]-rev.iloc[-5], rev.iloc[-5])
L438                         if len(rev)>=6:
L439                             yoy_now = _safe_div(rev.iloc[-1]-rev.iloc[-5], rev.iloc[-5]); yoy_prev = _safe_div(rev.iloc[-2]-rev.iloc[-6], rev.iloc[-6])
L440                             if pd.notna(yoy_now) and pd.notna(yoy_prev): REV_YOY_ACC = yoy_now - yoy_prev
L441                         yoy_list=[]
L442                         for k in range(1,5):
L443                             if len(rev)>=4+k:
L444                                 y = _safe_div(rev.iloc[-k]-rev.iloc[-(k+4)], rev.iloc[-(k+4)])
L445                                 if pd.notna(y): yoy_list.append(y)
L446                         if len(yoy_list)>=2: REV_YOY_VAR = float(np.std(yoy_list, ddof=1))
L447                         # NEW: 年次の持続性（直近から遡って前年比プラスが何年連続か、四半期4本揃う完全年のみ）
L448                         try:
L449                             g = rev.groupby(rev.index.year)
L450                             ann_sum, cnt = g.sum(), g.count()
L451                             ann_sum = ann_sum[cnt >= 4]
L452                             if len(ann_sum) >= 3:
L453                                 yoy = ann_sum.pct_change().dropna()
L454                                 streak = 0
L455                                 for v in yoy.iloc[::-1]:
L456                                     if pd.isna(v) or v <= 0:
L457                                         break
L458                                     streak += 1
L459                                 REV_ANNUAL_STREAK = float(streak)
L460                         except Exception:
L461                             pass
L462                     if 'Earnings' in qe.columns and so:
L463                         eps_series = (qe['Earnings'].dropna().astype(float)/float(so)).replace([np.inf,-np.inf],np.nan)
L464                         if len(eps_series)>=5 and pd.notna(eps_series.iloc[-5]) and eps_series.iloc[-5]!=0:
L465                             EPS_Q_YOY = _safe_div(eps_series.iloc[-1]-eps_series.iloc[-5], eps_series.iloc[-5])
L466             except Exception: pass
L467             df.loc[t,'REV_Q_YOY'], df.loc[t,'EPS_Q_YOY'], df.loc[t,'REV_YOY_ACC'], df.loc[t,'REV_YOY_VAR'] = REV_Q_YOY, EPS_Q_YOY, REV_YOY_ACC, REV_YOY_VAR
L468             df.loc[t,'REV_ANN_STREAK'] = REV_ANNUAL_STREAK
L469
L470             # --- Rule of 40 や周辺 ---
L471             total_rev_ttm = d.get('totalRevenue',np.nan)
L472             FCF_MGN = _safe_div(fcf_val, total_rev_ttm)
L473             df.loc[t,'FCF_MGN'] = FCF_MGN
L474             rule40 = np.nan
L475             try:
L476                 r = df.loc[t,'REV']; rule40 = (r if pd.notna(r) else np.nan) + (FCF_MGN if pd.notna(FCF_MGN) else np.nan)
L477             except Exception: pass
L478             df.loc[t,'RULE40'] = rule40
L479
L480             # --- トレンド補助 ---
L481             sma50  = s.rolling(50).mean()
L482             sma150 = s.rolling(150).mean()
L483             sma200 = s.rolling(200).mean()
L484             p = _safe_last(s)
L485
L486             df.loc[t,'MA50_OVER_150'] = (_safe_last(sma50)/_safe_last(sma150) - 1
L487                 if pd.notna(_safe_last(sma50)) and pd.notna(_safe_last(sma150)) and _safe_last(sma150)!=0 else np.nan)
L488             df.loc[t,'MA150_OVER_200'] = (_safe_last(sma150)/_safe_last(sma200) - 1
L489                 if pd.notna(_safe_last(sma150)) and pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan)
L490
L491             lo52 = s[-252:].min() if len(s)>=252 else s.min()
L492             df.loc[t,'P_OVER_LOW52'] = (p/lo52 - 1) if (lo52 and lo52>0 and pd.notna(p)) else np.nan
L493
L494             df.loc[t,'MA200_SLOPE_1M'] = np.nan
L495             if len(sma200.dropna()) >= 21:
L496                 cur200 = _safe_last(sma200)
L497                 old2001 = float(sma200.iloc[-21])
L498                 if old2001:
L499                     df.loc[t,'MA200_SLOPE_1M'] = cur200/old2001 - 1
L500
L501             df.loc[t,'P_OVER_150'] = p/_safe_last(sma150)-1 if pd.notna(_safe_last(sma150)) and _safe_last(sma150)!=0 else np.nan
L502             df.loc[t,'P_OVER_200'] = p/_safe_last(sma200)-1 if pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L503             df.loc[t,'MA50_OVER_200'] = _safe_last(sma50)/_safe_last(sma200)-1 if pd.notna(_safe_last(sma50)) and pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L504             df.loc[t,'MA200_SLOPE_5M'] = np.nan
L505             if len(sma200.dropna())>=105:
L506                 cur200, old200 = _safe_last(sma200), float(sma200.iloc[-105])
L507                 if old200 and old200!=0: df.loc[t,'MA200_SLOPE_5M'] = cur200/old200 - 1
L508             # NEW: 200日線が連続で上向きの「日数」
L509             df.loc[t,'MA200_UP_STREAK_D'] = np.nan
L510             try:
L511                 s200 = sma200.dropna()
L512                 if len(s200) >= 2:
L513                     diff200 = s200.diff()
L514                     up = 0
L515                     for v in diff200.iloc[::-1]:
L516                         if pd.isna(v) or v <= 0:
L517                             break
L518                         up += 1
L519                     df.loc[t,'MA200_UP_STREAK_D'] = float(up)
L520             except Exception:
L521                 pass
L522             df.loc[t,'LOW52PCT25_EXCESS'] = np.nan if (lo52 is None or lo52<=0 or pd.isna(p)) else (p/(lo52*1.25)-1)
L523             hi52 = s[-252:].max() if len(s)>=252 else s.max(); df.loc[t,'NEAR_52W_HIGH'] = np.nan
L524             if hi52 and hi52>0 and pd.notna(p):
L525                 d_hi = (p/hi52)-1.0; df.loc[t,'NEAR_52W_HIGH'] = -abs(min(0.0, d_hi))
L526             df.loc[t,'RS_SLOPE_6W'] = self.rs_line_slope(s, ib.spx, 30)
L527             df.loc[t,'RS_SLOPE_13W'] = self.rs_line_slope(s, ib.spx, 65)
L528
L529             df.loc[t,'DIV_STREAK'] = self.div_streak(t)
L530
L531             # --- 欠損メモ ---
L532             fin_cols = ['REV','ROE','BETA','DIV','FCF']
L533             need_finnhub = [col for col in fin_cols if pd.isna(df.loc[t,col])]
L534             if need_finnhub:
L535                 fin_data = self.fetch_finnhub_metrics(t)
L536                 for col in need_finnhub:
L537                     val = fin_data.get(col)
L538                     if val is not None and not pd.isna(val): df.loc[t,col] = val
L539             for col in fin_cols + ['EPS','RS','TR_str','DIV_STREAK']:
L540                 if pd.isna(df.loc[t,col]):
L541                     if col=='DIV':
L542                         status = self.dividend_status(t)
L543                         if status!='none_confident': missing_logs.append({'Ticker':t,'Column':col,'Status':status})
L544                     else:
L545                         missing_logs.append({'Ticker':t,'Column':col})
L546
L547         def _trend_template_pass(row, rs_alpha_thresh=0.10):
L548             c1 = (row.get('P_OVER_150', np.nan) > 0) and (row.get('P_OVER_200', np.nan) > 0)
L549             c2 = (row.get('MA150_OVER_200', np.nan) > 0)
L550             c3 = (row.get('MA200_SLOPE_1M', np.nan) > 0)
L551             c4 = (row.get('MA50_OVER_150', np.nan) > 0) and (row.get('MA50_OVER_200', np.nan) > 0)
L552             c5 = (row.get('TR_str', np.nan) > 0)
L553             c6 = (row.get('P_OVER_LOW52', np.nan) >= 0.30)
L554             c7 = (row.get('NEAR_52W_HIGH', np.nan) >= -0.25)
L555             c8 = (row.get('RS', np.nan) >= 0.10)
L556             return bool(c1 and c2 and c3 and c4 and c5 and c6 and c7 and c8)
L557
L558         if 'trend_template' not in df.columns: df['trend_template'] = df.apply(_trend_template_pass, axis=1).fillna(False)
L559         assert 'trend_template' in df.columns
L560
L561         # === Z化と合成 ===
L562         for col in ['ROE','FCF','REV','EPS']: df[f'{col}_W'] = winsorize_s(df[col], 0.02)
L563
L564         for _c in ('DIV_TTM_PS', 'DIV_FCF_COVER'):
L565             if _c in df.columns:
L566                 df[_c] = df[_c].fillna(0.0)
L567
L568         df_z = pd.DataFrame(index=df.index)
L569         for col in ['EPS','REV','ROE','FCF','RS','TR_str','BETA','DIV','DIV_STREAK']:
L570             df_z[col] = robust_z(df[col])
L571         df_z['REV'], df_z['EPS'], df_z['TR'] = robust_z(df['REV_W']), robust_z(df['EPS_W']), robust_z(df['TR'])
L572         for col in ['P_OVER_150','P_OVER_200','MA50_OVER_200','MA200_SLOPE_5M','LOW52PCT25_EXCESS','NEAR_52W_HIGH','RS_SLOPE_6W','RS_SLOPE_13W','MA200_UP_STREAK_D']: df_z[col] = robust_z(df[col])
L573         for col in ['REV_Q_YOY','EPS_Q_YOY','REV_YOY_ACC','REV_YOY_VAR','FCF_MGN','RULE40','REV_ANN_STREAK']: df_z[col] = robust_z(df[col])
L574         for col in ['DOWNSIDE_DEV','MDD_1Y','RESID_VOL','DOWN_OUTPERF','EXT_200','DIV_TTM_PS','DIV_VAR5','DIV_YOY','DIV_FCF_COVER','DEBT2EQ','CURR_RATIO','EPS_VAR_8Q','MARKET_CAP','ADV60_USD']: df_z[col] = robust_z(df[col])
L575
L576         # --- Dividend handling: penalize non-payers ---
L577         df['HAS_DIV'] = (df['DIV_TTM_PS'] > 0).astype(int)
L578         y = df['DIV_TTM_PS'].where(df['HAS_DIV'] == 1, np.nan)
L579         y_non = y.dropna()
L580         z_yld = pd.Series(robust_z(y_non), index=y_non.index).reindex(df.index)
L581         penalty = (np.nanmin(z_yld) - 1.0) if len(z_yld.dropna()) else -1.0
L582         z_yld = z_yld.fillna(penalty)
L583         z_streak = pd.Series(robust_z(df['DIV_STREAK'].where(df['HAS_DIV'] == 1, 0)), index=df.index)
L584         z_cover = pd.Series(robust_z(df['DIV_FCF_COVER'].where(df['HAS_DIV'] == 1, 0)), index=df.index)
L585         z_var = pd.Series(robust_z(df['DIV_VAR5'].where(df['HAS_DIV'] == 1, df['DIV_VAR5'].max())), index=df.index)
L586         df['YLD'] = (
L587             0.30*z_yld +
L588             0.30*z_streak +
L589             0.25*z_cover -
L590             0.15*z_var
L591         )
L592         df_z['DIV'] = z_yld
L593         df_z['DIV_STREAK'] = z_streak
L594         df_z['DIV_FCF_COVER'] = z_cover
L595         df_z['DIV_VAR5'] = z_var
L596
L597         df_z['SIZE'], df_z['LIQ'] = robust_z(np.log1p(df['MARKET_CAP'])), robust_z(np.log1p(df['ADV60_USD']))
L598         df_z['QUALITY_F'] = robust_z(0.6*df['FCF_W'] + 0.4*df['ROE_W']).clip(-3.0,3.0)
L599         df_z['YIELD_F']   = 0.3*df_z['DIV'] + 0.7*df_z['DIV_STREAK']
L600         df_z['GROWTH_F']  = robust_z(0.25*df_z['REV']          # ↓0.30→0.25
L601             + 0.20*df_z['EPS_Q_YOY']
L602             + 0.15*df_z['REV_Q_YOY']
L603             + 0.15*df_z['REV_YOY_ACC']
L604             + 0.10*df_z['RULE40']
L605             + 0.10*df_z['FCF_MGN']
L606             + 0.10*df_z['EPS']          # ★追加：黒字優遇／赤字減点
L607             + 0.05*df_z['REV_ANN_STREAK']
L608             - 0.05*df_z['REV_YOY_VAR']).clip(-3.0,3.0)
L609         df_z['MOM_F'] = robust_z(0.40*df_z['RS']
L610             + 0.15*df_z['TR_str']
L611             + 0.15*df_z['RS_SLOPE_6W']
L612             + 0.15*df_z['RS_SLOPE_13W']
L613             + 0.10*df_z['MA200_SLOPE_5M']
L614             + 0.10*df_z['MA200_UP_STREAK_D']).clip(-3.0,3.0)
L615         df_z['VOL'] = robust_z(df['BETA'])
L616         df_z.rename(columns={'GROWTH_F':'GRW','MOM_F':'MOM','QUALITY_F':'QAL','YIELD_F':'YLD'}, inplace=True)
L617
L618         # --- EPS-only penalty: punish negative EPS (GAAP or nEPS) ---
L619         eps_any = (df.get('EPS', 0) > 0) | (df.get('nEPS_ttm', 0) > 0)
L620         PEN_EPS = float(os.getenv("PEN_EPS_FOR_GRW", "0.8"))
L621         if 'GRW' in df_z.columns:
L622             red_eps = (~eps_any).astype(float)
L623             # Adjust displayed GRW directly based on EPS penalty only (no FCF)
L624             df_z['GRW'] = (df_z['GRW'] - PEN_EPS * red_eps).clip(-3.0, 3.0)
L625
L626         # === begin: BIO LOSS PENALTY =====================================
L627         try:
L628             penalty_z = float(os.getenv("BIO_LOSS_PENALTY_Z", "0.8"))
L629         except Exception:
L630             penalty_z = 0.8
L631
L632         def _is_bio_like(t: str) -> bool:
L633             inf = info.get(t, {}) if isinstance(info, dict) else {}
L634             sec = str(inf.get("sector", "")).lower()
L635             ind = str(inf.get("industry", "")).lower()
L636             if "health" not in sec:
L637                 return False
L638             keys = ("biotech", "biopharma", "pharma")
L639             return any(k in ind for k in keys)
L640
L641         tickers_s = pd.Index(df_z.index)
L642         is_bio = pd.Series({t: _is_bio_like(t) for t in tickers_s})
L643         is_loss = pd.Series({t: (pd.notna(df.loc[t,"EPS"]) and df.loc[t,"EPS"] <= 0) for t in tickers_s})
L644         mask_bio_loss = (is_bio & is_loss).reindex(df_z.index).fillna(False)
L645
L646         if bool(mask_bio_loss.any()) and penalty_z > 0:
L647             df_z.loc[mask_bio_loss, "GRW"] = df_z.loc[mask_bio_loss, "GRW"] - penalty_z
L648             df_z["GRW"] = df_z["GRW"].clip(-3.0, 3.0)
L649         # === end: BIO LOSS PENALTY =======================================
L650
L651         df_z['TRD'] = 0.0  # TRDはスコア寄与から外し、テンプレ判定はフィルタで行う（列は表示互換のため残す）
L652         if 'BETA' not in df_z.columns: df_z['BETA'] = robust_z(df['BETA'])
L653
L654         df_z['D_VOL_RAW'] = robust_z(0.40*df_z['DOWNSIDE_DEV'] + 0.22*df_z['RESID_VOL'] + 0.18*df_z['MDD_1Y'] - 0.10*df_z['DOWN_OUTPERF'] - 0.05*df_z['EXT_200'] - 0.08*df_z['SIZE'] - 0.10*df_z['LIQ'] + 0.10*df_z['BETA'])
L655         df_z['D_QAL']     = robust_z(0.35*df_z['QAL'] + 0.20*df_z['FCF'] + 0.15*df_z['CURR_RATIO'] - 0.15*df_z['DEBT2EQ'] - 0.15*df_z['EPS_VAR_8Q'])
L656         df_z['D_YLD']     = df['YLD']
L657         df_z['D_TRD']     = robust_z(0.40*df_z.get('MA200_SLOPE_5M',0) - 0.30*df_z.get('EXT_200',0) + 0.15*df_z.get('NEAR_52W_HIGH',0) + 0.15*df_z['TR'])
L658
L659         # --- 重みは cfg を優先（外部があればそれを使用） ---
L660         # ① 全銘柄で G/D スコアを算出（unmasked）
L661         g_score_all = df_z.mul(pd.Series(cfg.weights.g)).sum(axis=1)
L662
L663         d_comp = pd.concat({
L664             'QAL': df_z['D_QAL'],
L665             'YLD': df_z['D_YLD'],
L666             'VOL': df_z['D_VOL_RAW'],
L667             'TRD': df_z['D_TRD']
L668         }, axis=1)
L669         dw = pd.Series(cfg.weights.d, dtype=float).reindex(['QAL','YLD','VOL','TRD']).fillna(0.0)
L670         globals()['D_WEIGHTS_EFF'] = dw.copy()
L671         d_score_all = d_comp.mul(dw, axis=1).sum(axis=1)
L672
L673         # ② テンプレ判定（既存ロジックそのまま）
L674         mask = df['trend_template']
L675         if not bool(mask.any()):
L676             mask = ((df.get('P_OVER_LOW52', np.nan) >= 0.25) &
L677                 (df.get('NEAR_52W_HIGH', np.nan) >= -0.30) &
L678                 (df.get('RS', np.nan) >= 0.08) &
L679                 (df.get('MA200_SLOPE_1M', np.nan) > 0) &
L680                 (df.get('P_OVER_150', np.nan) > 0) & (df.get('P_OVER_200', np.nan) > 0) &
L681                 (df.get('MA150_OVER_200', np.nan) > 0) &
L682                 (df.get('MA50_OVER_150', np.nan) > 0) & (df.get('MA50_OVER_200', np.nan) > 0) &
L683                 (df.get('TR_str', np.nan) > 0)).fillna(False)
L684             df['trend_template'] = mask
L685
L686         # ③ 採用用は mask、表示/分析用は列で全銘柄保存
L687         g_score = g_score_all.loc[mask]
L688         Scorer.g_score = g_score
L689         df_z['GSC'] = g_score_all
L690         df_z['DSC'] = d_score_all
L691
L692         try:
L693             current = (pd.read_csv("current_tickers.csv")
L694                   .iloc[:, 0]
L695                   .str.upper()
L696                   .tolist())
L697         except FileNotFoundError:
L698             warnings.warn("current_tickers.csv not found — bonus skipped")
L699             current = []
L700
L701         mask_bonus = g_score.index.isin(current)
L702         if mask_bonus.any():
L703             # 1) factor.BONUS_COEFF から k を決め、無ければ 0.4
L704             k = float(getattr(sys.modules.get("factor"), "BONUS_COEFF", 0.4))
L705             # 2) g 側の σ を取り、NaN なら 0 に丸める
L706             sigma_g = g_score.std()
L707             if pd.isna(sigma_g):
L708                 sigma_g = 0.0
L709             bonus_g = round(k * sigma_g, 3)
L710             g_score.loc[mask_bonus] += bonus_g
L711             Scorer.g_score = g_score
L712             # 3) D 側も同様に σ の NaN をケア
L713             sigma_d = d_score_all.std()
L714             if pd.isna(sigma_d):
L715                 sigma_d = 0.0
L716             bonus_d = round(k * sigma_d, 3)
L717             d_score_all.loc[d_score_all.index.isin(current)] += bonus_d
L718
L719         try:
L720             df = _apply_growth_entry_flags(df, ib, self, win_breakout=5, win_pullback=5)
L721         except Exception:
L722             pass
L723
L724         from factor import FeatureBundle  # type: ignore  # 実行時importなし（循環回避）
L725         return FeatureBundle(df=df,
L726             df_z=df_z,
L727             g_score=g_score,
L728             d_score_all=d_score_all,
L729             missing_logs=pd.DataFrame(missing_logs))
L730
L731 def _apply_growth_entry_flags(feature_df, bundle, self_obj, win_breakout=5, win_pullback=5):
L732     """
L733     G枠ユニバースに対し、ブレイクアウト確定/押し目反発の「直近N営業日内の発火」を判定し、
L734     次の列を feature_df に追加する（index=ticker）。
L735       - G_BREAKOUT_recent_5d : bool
L736       - G_BREAKOUT_last_date : str "YYYY-MM-DD"
L737       - G_PULLBACK_recent_5d : bool
L738       - G_PULLBACK_last_date : str "YYYY-MM-DD"
L739       - G_PIVOT_price        : float
L740     失敗しても例外は握り潰し、既存処理を阻害しない。
L741     """
L742     try:
L743         px   = bundle.px                      # 終値 DataFrame
L744         hi   = bundle.data['High']
L745         lo   = bundle.data['Low']
L746         vol  = bundle.data['Volume']
L747         bench= bundle.spx                     # ベンチマーク Series
L748
L749         # Gユニバース推定：self.g_universe 優先 → feature_df['group']=='G' → 全銘柄
L750         g_universe = getattr(self_obj, "g_universe", None)
L751         if g_universe is None:
L752             try:
L753                 g_universe = feature_df.index[feature_df['group'].astype(str).str.upper().eq('G')].tolist()
L754             except Exception:
L755                 g_universe = list(feature_df.index)
L756         if not g_universe:
L757             return feature_df
L758
L759         # 指標
L760         ema21 = px[g_universe].ewm(span=21, adjust=False).mean()
L761         ma50  = px[g_universe].rolling(50).mean()
L762         ma150 = px[g_universe].rolling(150).mean()
L763         ma200 = px[g_universe].rolling(200).mean()
L764         atr20 = (hi[g_universe] - lo[g_universe]).rolling(20).mean()
L765         vol20 = vol[g_universe].rolling(20).mean()
L766         vol50 = vol[g_universe].rolling(50).mean()
L767
L768         # トレンドテンプレート合格
L769         trend_template_ok = (px[g_universe] > ma50) & (px[g_universe] > ma150) & (px[g_universe] > ma200) \
L770                             & (ma150 > ma200) & (ma200.diff() > 0)
L771
L772         # 汎用ピボット：直近65営業日の高値（当日除外）
L773         pivot_price = hi[g_universe].rolling(65).max().shift(1)
L774
L775         # 相対力：年内高値更新
L776         bench_aligned = bench.reindex(px.index).ffill()
L777         rs = px[g_universe].div(bench_aligned, axis=0)
L778         rs_high = rs.rolling(252).max().shift(1)
L779
L780         # ブレイクアウト「発生日」：条件立ち上がり
L781         breakout_today = trend_template_ok & (px[g_universe] > pivot_price) \
L782                          & (vol[g_universe] >= 1.5 * vol50) & (rs > rs_high)
L783         breakout_event = breakout_today & ~breakout_today.shift(1).fillna(False)
L784
L785         # 押し目反発「発生日」：EMA21帯×出来高ドライアップ×前日高値越え×終値EMA21上
L786         near_ema21_band = px[g_universe].between(ema21 - atr20, ema21 + atr20)
L787         volume_dryup = (vol20 / vol50) <= 1.0
L788         pullback_bounce_confirmed = (px[g_universe] > hi[g_universe].shift(1)) & (px[g_universe] > ema21)
L789         pullback_today = trend_template_ok & near_ema21_band & volume_dryup & pullback_bounce_confirmed
L790         pullback_event = pullback_today & ~pullback_today.shift(1).fillna(False)
L791
L792         # 直近N営業日内の発火 / 最終発生日
L793         rows = []
L794         for t in g_universe:
L795             def _recent_and_date(s, win):
L796                 sw = s[t].iloc[-win:]
L797                 if sw.any():
L798                     d = sw[sw].index[-1]
L799                     return True, d.strftime("%Y-%m-%d")
L800                 return False, ""
L801             br_recent, br_date = _recent_and_date(breakout_event, win_breakout)
L802             pb_recent, pb_date = _recent_and_date(pullback_event, win_pullback)
L803             rows.append((t, {
L804                 "G_BREAKOUT_recent_5d": br_recent,
L805                 "G_BREAKOUT_last_date": br_date,
L806                 "G_PULLBACK_recent_5d": pb_recent,
L807                 "G_PULLBACK_last_date": pb_date,
L808                 "G_PIVOT_price": float(pivot_price[t].iloc[-1]) if t in pivot_price.columns else float('nan'),
L809             }))
L810         flags = pd.DataFrame({k: v for k, v in rows}).T
L811
L812         # 列を作成・上書き
L813         cols = ["G_BREAKOUT_recent_5d","G_BREAKOUT_last_date","G_PULLBACK_recent_5d","G_PULLBACK_last_date","G_PIVOT_price"]
L814         for c in cols:
L815             if c not in feature_df.columns:
L816                 feature_df[c] = np.nan
L817         feature_df.loc[flags.index, flags.columns] = flags
L818
L819     except Exception:
L820         pass
L821     return feature_df
L822
```

## <.github/workflows/weekly-report.yml>
```text
L1 name: Weekly Stock Report
L2
L3 on:
L4   push:
L5     branches: [ main ]
L6     paths-ignore:
L7       - 'CodeForChat/**'
L8   schedule:
L9     - cron: '0 0 * * 6'  # UTC 00:00 → JST 09:00（土）
L10   workflow_dispatch:
L11
L12 jobs:
L13   build-and-report:
L14     runs-on: ubuntu-latest
L15     permissions:
L16       contents: write
L17
L18     steps:
L19       - name: Debug start
L20         run: echo '🚀 DEBUGstarted'
L21               
L22       - name: Checkout repository
L23         uses: actions/checkout@v3
L24
L25       - name: Setup Python
L26         uses: actions/setup-python@v5
L27         with:
L28           python-version: '3.x'
L29           cache: 'pip'
L30           cache-dependency-path: requirements.txt
L31
L32       - name: Install dependencies
L33         run: pip install -r requirements.txt
L34
L35       - name: Prepare results directory
L36         run: mkdir -p results
L37
L38       - name: Run factor & scoring
L39         env:
L40           SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
L41           FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
L42           FIN_THREADS: "8"
L43         run: python factor.py
```

## <documents/README.md>
```text
L1 # 運用ルール
L2
L3 ## 基本構成
L4 - 20銘柄を均等配分（現金を除き1銘柄あたり5%）
L5 - moomoo証券で運用
L6 - **Growth枠 12銘柄 / Defense枠 8銘柄**（NORMAL 基準）
L7
L8 ## Barbell Growth-Defense方針
L9 - Growth枠 **12銘柄**：高成長で乖離源となる攻めの銘柄
L10 - Defense枠 **8銘柄**：低ボラで安定成長し配当を増やす守りの銘柄
L11 - 「猛烈に伸びる攻め × 着実に稼ぐ盾」の組合せで乖離→半戻しプレミアムを狙う
L12
L13 ## レジーム判定（trend_template 合格“本数”で判定）
L14 - 合格本数 = current+candidate 全体のうち、trend_template 条件を満たした銘柄の**本数(C)**（基準 N_G=12）
L15 - しきい値は過去~600営業日の分布から**毎回自動採用**（分位点と運用“床”のmax）
L16   - 緊急入り: `max(q05, 12本)`（= N_G）
L17   - 緊急解除: `max(q20, 18本)`（= ceil(1.5×12)）
L18   - 通常復帰: `max(q60, 36本)`（= 3×N_G）
L19 - ヒステリシス: 前回モードに依存（EMERG→解除は23本以上、CAUTION→通常は45本以上）
L20
L21 ## レジーム別の現金・ドリフト
L22  - **通常(NORMAL)** : 現金 **10%** / ドリフト閾値 **12%**
L23  - **警戒(CAUTION)** : 現金 **12.5%** / ドリフト閾値 **14%**
L24  - **緊急(EMERG)** : 現金 **20%** / **ドリフト売買停止**（20×5%に全戻しのみ）
L25
L26 ## モード別の推奨“保有銘柄数”（MMF≒現金）
L27 *各枠=5%（20銘柄均等）。モード移行時は**Gの枠数のみ**調整し、外した枠は現金として保持。*
L28
L29 - **NORMAL:** G **12** / D **8** / 現金化枠 **0**  
L30 - **CAUTION:** G **10** / D **8** / 現金化枠 **2**（= 10%）  
L31 - **EMERG:** G **8**  / D **8** / 現金化枠 **4**（= 20%）  
L32
L33 > 実運用：⭐️低スコアのGから順に外す。解除時はfactor上位から補充。
L34
L35 ## トレーリングストップ
L36 - **基本TS (モード別):** NORMAL **15%** / CAUTION **13%** / EMERG **10%**
L37 - 含み益が **+30% / +60% / +100%** 到達で、基本から **-3pt / -6pt / -8pt** 引き上げ
L38 - TS発動で減少した銘柄は翌日以降に補充（※緊急モード中は補充しない）
L39
L40 ## 半戻し（リバランス）手順
L41 ドリフトチェックで**アラート**が出た場合（合計|drift| がモード閾値を超過、EMERG除く）、翌営業日の米国寄付きで下記を実施する。
L42
L43 1. **売却（必須）**  
L44    Slackテーブルの **Δqty がマイナスの銘柄を売却** する（寄付き成行推奨）。  
L45    これは「半戻し」計算に基づく過重量の削減を意味する。
L46
L47 2. **購入（任意・半戻し目安）**  
L48    半戻し後の合計|drift|を**シミュレーション値（Slackヘッダに表示）**に近づけることを目安に、  
L49    **任意の銘柄を買い増し**してバランスを取る（Δqtyがプラスの銘柄を優先してもよい）。
L50
L51 3. **トレーリングストップの再設定（必須）**  
L52    すべての保有銘柄について、最新の評価額に合わせてTSを**再発注／更新**する。  
L53    ルールは下記（利益到達で段階的にタイト化）：  
L54    - **基本TS:** -15%  
L55    - **+30% 到達 → TS -12%**  
L56    - **+60% 到達 → TS -9%**  
L57    - **+100% 到達 → TS -7%**  
L58    ※ストップ価格の引き上げは許可、**引き下げは不可**（利益保全の原則）。
L59
L60 4. **例外（EMERGモード）**  
L61    緊急(EMERG)では**ドリフト由来の売買は停止（∞）**。20銘柄×各5%への**全戻し**のみ許容。
L62
L63 5. **実行タイミング**
L64    - 判定：米国市場終値直後
L65    - 執行：翌営業日の米国寄付き成行
L66
L67 ## モード移行の実務手順（超シンプル）
L68 モードが変わったら、**MMF≒現金**として扱い、**Gの枠数だけ**を調整する：
L69 1. **Gを削る**（CAUTION/EMERG）  
L70    - ⭐️低スコアのGから順に外す。  
L71    - **`current_tickers.csv` から外すG銘柄の行を削除**（＝その枠は現金化）。
L72 2. **現金として保持**  
L73    - 外した枠は現金（またはMMF相当）でプール。  
L74 3. **復帰時の補充**（NORMALへ）  
L75    - **`current_tickers.csv` に銘柄を追加**（factor上位から）。  
L76    - 以降は日次ドリフト/TSルールに従う。
L77
L78 > driftは `target_ratio = 1/銘柄数` を自動適用。行数に応じて自動で均等比率が再計算される。
L79
L80 ## 入替銘柄選定
L81 - Oxfordキャピタル／インカム、Alpha Investor、Motley Fool Stock Advisor、moomooスクリーニング等を参考にchatGPTで検討
L82 - 年間NISA枠はGrowth群の中から低ボラ銘柄を選定し利用。長期保持にはこだわらない。
L83
L84 ## 再エントリー（クールダウン）
L85 - TSヒット後の同銘柄再INは **8営業日** のクールダウンを設ける（期間中は再IN禁止）
L86
L87 ## 実行タイミング
L88 - 判定：米国市場終値直後
L89 - 執行：翌営業日の米国寄付き成行
```

## <documents/factor_design.md>
```text
L1 # factor.py 詳細設計書
L2
L3 ## 概要
L4 - 既存ポートフォリオの銘柄と検討中の銘柄群を同時に扱う銘柄選定パイプライン。
L5 - 価格・財務データを取り込みスコアリングとDRRS選定を行うことで、以下のアウトプットを得る。
L6   - 採用銘柄と惜しくも漏れた銘柄のスコア一覧
L7   - IN/OUTのティッカーリストとOUT側の低スコア銘柄
L8   - 新旧ポートフォリオの比較表
L9   - 検討中銘柄の低スコアランキング（整理用）
L10
L11 ## 全体フロー
L12 1. **Input** – `current_tickers.csv`と`candidate_tickers.csv`を読み込み、yfinanceやFinnhubのAPIから価格・財務データを収集して`InputBundle`を整備。
L13 2. **Score Calculation** – Scorerが特徴量を計算し因子スコアを合成して`FeatureBundle`を生成。
L14 3. **Correlation Reduction & Selection** – SelectorがDRRSロジックで相関を抑えつつG/D銘柄を選定し`SelectionBundle`を得る。
L15 4. **Output** – 採用結果と周辺情報を表・Slack通知として出力。
L16
L17 ```mermaid
L18 flowchart LR
L19   A[Input\nAPI & 前処理] --> B[Score Calculation\n特徴量・因子合成]
L20   B --> C[Correlation Reduction\nDRRS選定]
L21   C --> D[Output\nSlack通知]
L22 ```
L23
L24 ## 定数・設定
L25 | 変数 | 内容 | 主な用途 |
L26 | --- | --- | --- |
L27 | `exist` / `cand` | 現行ポートフォリオと検討中銘柄のティッカーリスト | スコア対象ユニバースの構成、候補整理 |
L28 | `bench` | ベンチマークティッカー | 相対強さ・β算出、ポート比較 |
L29 | `CAND_PRICE_MAX` | 候補銘柄の許容価格上限 | 高額銘柄の事前除外 |
L30 | `N_G` / `N_D` | G/D採用枠の件数（**既定: 12 / 8**） | 最終的に選ぶ銘柄数の制約 |
L31 | `g_weights` / `D_weights` | 各因子の重みdict | G/Dスコア合成 |
L32 | `D_BETA_MAX` | Dバケットの許容β上限 | 高β銘柄の除外フィルタ |
L33 | `FILTER_SPEC` | G/Dごとの前処理フィルタ | トレンドマスクやβ上限設定 |
L34 | `corrM` | DRRS初期プールの最大件数 | 相関行列サイズ制御 |
L35 | `DRRS_G` / `DRRS_D` | DRRSパラメータdict | バケット別の相関低減設定 |
L36 | `DRRS_SHRINK` | 残差相関の対角シュリンク率 | `residual_corr`の安定化 |
L37 | `CROSS_MU_GD` | G-D間クロス相関ペナルティμ | 2バケット同時最適化で相関抑制 |
L38 | `RESULTS_DIR` | 選定結果保存ディレクトリ | `_save_sel`/`_load_prev`の入出力 |
L39
L40 選定結果は`results/`配下にJSONとして保存し、次回実行時に`_load_prev`で読み込んで選定条件に反映。
L41
L42 ## DTO/Config
L43 各ステップ間で受け渡すデータ構造と設定値。変数の意味合いと利用箇所を以下に示す。
L44
L45 ### InputBundle（Input → Scorer）
L46 | 変数 | 内容 | 主な用途 |
L47 | --- | --- | --- |
L48 | `cand` | 候補銘柄ティッカーのリスト | OUTテーブルや低スコアランキング対象の母集団 |
L49 | `tickers` | 現行+候補を合わせたティッカー一覧 | 価格・出来高ダウンロード、リターン計算 |
L50 | `bench` | ベンチマークティッカー | 相対強さ・β算出、ポート比較 |
L51 | `data` | yfinanceのダウンロード結果（階層列） | `px`/`spx`/リターン等の基礎データ |
L52 | `px` | `data['Close']`だけを抜き出した価格系列 | 指標計算・リターン生成 |
L53 | `spx` | `data['Close'][bench]` のSeries | `rs`や`calc_beta`の基準指数 |
L54 | `tickers_bulk` | `yf.Tickers`オブジェクト | `info`等の一括取得 |
L55 | `info` | ティッカー別のyfinance情報dict | セクター判定やEPS補完 |
L56 | `eps_df` | EPS TTM/直近EPS等をまとめた表 | 成長指標の算出 |
L57 | `fcf_df` | CFO・CapEx・FCF TTMと情報源フラグ | FCF/EVや配当カバレッジ |
L58 | `returns` | `px.pct_change()`のリターン表 | 相関行列・DRRS計算 |
L59
L60 ### FeatureBundle（Scorer → Selector）
L61 | 変数 | 内容 | 主な用途 |
L62 | --- | --- | --- |
L63 | `df` | 計算済み指標の生値テーブル | デバッグ・出力表示 |
L64 | `df_z` | ウィンザー後Zスコア化した指標表 | 因子スコア合成、選定基準 |
L65 | `g_score` | Gバケット総合スコア | G選定、IN/OUT比較 |
L66 | `d_score_all` | Dバケット総合スコア（全銘柄） | D選定、低スコアランキング |
L67 | `missing_logs` | 欠損指標と補完状況のログ | データ品質チェック |
L68
L69 ### SelectionBundle（Selector → Output）
L70 | 変数 | 内容 | 主な用途 |
L71 | --- | --- | --- |
L72 | `resG` | G選定結果の詳細dict（`tickers`、目的値等） | 結果保存・平均相関などの指標表示 |
L73 | `resD` | D選定結果の詳細dict | 同上 |
L74 | `top_G` | 最終採用Gティッカー | 新ポートフォリオ構築 |
L75 | `top_D` | 最終採用Dティッカー | 同上 |
L76 | `init_G` | DRRS前のG初期候補 | 惜しくも外れた銘柄表示 |
L77 | `init_D` | DRRS前のD初期候補 | 同上 |
L78
L79 ### WeightsConfig
L80 | 変数 | 内容 | 主な用途 |
L81 | --- | --- | --- |
L82 | `g` | G因子（GRW/MOM/VOL）の重みdict | `g_score`合成 |
L83 | `d` | D因子（D_QAL/D_YLD/D_VOL_RAW/D_TRD）の重みdict | `d_score_all`合成 |
L84
L85 ### DRRSParams
L86 | 変数 | 内容 | 主な用途 |
L87 | --- | --- | --- |
L88 | `corrM` | DRRS初期プールの最大件数 | 相関行列サイズ制御 |
L89 | `shrink` | 残差相関のシュリンク率 | `residual_corr`の対角強調 |
L90 | `G` | Gバケット用パラメータdict（`lookback`等） | `select_bucket_drrs`設定 |
L91 | `D` | Dバケット用パラメータdict | 同上 |
L92 | `cross_mu_gd` | G-Dクロス相関ペナルティ係数μ | `select_buckets`の目的関数 |
L93
L94 ### PipelineConfig
L95 | 変数 | 内容 | 主な用途 |
L96 | --- | --- | --- |
L97 | `weights` | `WeightsConfig`のインスタンス | スコア合成の重み参照 |
L98 | `drrs` | `DRRSParams`のインスタンス | 選定ステップの設定値 |
L99 | `price_max` | 候補銘柄の許容価格上限 | Input段階でのフィルタ |
L100
L101 ## 共通ユーティリティ
L102 - `winsorize_s` / `robust_z` : 外れ値処理とZスコア化。
L103 - `_safe_div` / `_safe_last` : 例外を潰した分割・末尾取得。
L104 - `_load_prev` / `_save_sel` : 選定結果の読み書き。
L105
L106 ## クラス設計
L107 ### Step1: Input
L108 `current_tickers.csv`の現行銘柄と`candidate_tickers.csv`の検討中銘柄を起点にデータを集約する。外部I/Oと前処理を担当し、`prepare_data`で`InputBundle`を生成。価格・財務データの取得は**yfinanceを優先し、欠損がある指標のみFinnhub APIで補完**する。
L109 主なメソッド:
L110 - `impute_eps_ttm` : 四半期EPS×4でTTMを推定し欠損時のみ差し替え。
L111 - `fetch_cfo_capex_ttm_yf` : yfinanceの四半期/年次キャッシュフローからCFO・CapEx・FCF TTMを算出。
L112 - `fetch_cfo_capex_ttm_finnhub` : yfinanceで欠けた銘柄のみFinnhub APIで補完。
L113 - `compute_fcf_with_fallback` : yfinance値を基準にFinnhub値で穴埋めし、CFO/CapEx/FCFと情報源フラグを返す。
L114 - `_build_eps_df` : `info`や`quarterly_earnings`からEPS TTMと直近EPSを計算し、`impute_eps_ttm`で補完。
L115 - `prepare_data` :
L116     0. CSVから現行銘柄と候補銘柄のティッカー一覧を読み込む。
L117     1. 候補銘柄の現在値を取得し価格上限でフィルタ。
L118     2. 既存+候補から対象ティッカーを決定し、価格・出来高を一括ダウンロード（yfinance）。
L119     3. yfinance値を基にEPS/FCFテーブルやベンチマーク系列、リターンを構築し、欠損セルはFinnhub呼び出しで穴埋め。
L120     4. 上記を`InputBundle`に格納して返す。
L121
L122 ### Step2: Score Calculation (Scorer)
L123 特徴量計算とスコア合成を担当し、`FeatureBundle`を返す。
L124
L125 #### 補助関数
L126 - `trend(s)` : 50/150/200日移動平均や52週レンジから-0.5〜0.5で構成されたトレンド指標。
L127 - `rs(s,b)` / `tr_str(s)` / `rs_line_slope(s,b,win)` : 相対強さや短期トレンド、RS回帰傾きを算出。
L128 - `ev_fallback` : `enterpriseValue`欠損時に負債・現金からEVを推定。
L129 - `dividend_status` / `div_streak` : 配当未設定状況の判定と増配年数カウント。
L130 - `fetch_finnhub_metrics` : Finnhub APIからEPS成長・ROE・βなど不足指標を取得。
L131 - `calc_beta` : ベンチマークとの共分散からβを算出。
L132 - `spx_to_alpha` : S&P500の位置情報からDRRSで用いるαを推定。
L133 - `soft_cap_effective_scores` / `pick_top_softcap` : セクターソフトキャップ付きスコア調整と上位抽出。
L134
L135 **補助関数と生成指標**
L136
L137 | 補助関数 | 生成指標 | 略称 |
L138 | --- | --- | --- |
L139 | `trend` | トレンド総合値 | `TR` |
L140 | `rs` | 相対強さ | `RS` |
L141 | `tr_str` | 価格と50日線の乖離 | `TR_str` |
L142 | `rs_line_slope` | RS線の回帰傾き | `RS_SLOPE_*` |
L143 | `calc_beta` | β | `BETA` |
L144 | `div_streak` | 連続増配年数 | `DIV_STREAK` |
L145
L146 #### `aggregate_scores` 詳細
L147 1. 各銘柄の価格系列や`info`を基に以下を算出。
L148    - **トレンド/モメンタム**: `TR`、`RS`、`TR_str`、多様な移動平均比、`RS_SLOPE_*`など。
L149    - **リスク**: `BETA`、`DOWNSIDE_DEV`、`MDD_1Y`、`RESID_VOL`、`DOWN_OUTPERF`、`EXT_200`等。
L150    - **配当**: `DIV`、`DIV_TTM_PS`、`DIV_VAR5`、`DIV_YOY`、`DIV_FCF_COVER`、`DIV_STREAK`。
L151    - **財務・成長**: `EPS`、`REV`、`ROE`、`FCF/EV`、`REV_Q_YOY`、`EPS_Q_YOY`、`REV_YOY_ACC`、`REV_YOY_VAR`、`REV_ANN_STREAK`、`RULE40`、`FCF_MGN` 等。
L152    - **安定性/サイズ**: `DEBT2EQ`、`CURR_RATIO`、`MARKET_CAP`、`ADV60_USD`、`EPS_VAR_8Q`など。
L153 2. 指標欠損はFinnhub API等で補完し、未取得項目を`missing_logs`に記録。
L154 3. `winsorize_s`→`robust_z`で標準化し`df_z`へ保存。サイズ・流動性は対数変換。
L155 4. 正規化済指標から因子スコアを合成。
L156    - 各因子の構成と重みは以下の通り。
L157      - **GRW**: 0.30×`REV` + 0.20×`EPS_Q_YOY` + 0.15×`REV_Q_YOY` + 0.15×`REV_YOY_ACC` + 0.10×`RULE40` + 0.10×`FCF_MGN` + 0.10×`REV_ANN_STREAK` − 0.05×`REV_YOY_VAR`。
L158      - **MOM**: 0.40×`RS` + 0.15×`TR_str` + 0.15×`RS_SLOPE_6W` + 0.15×`RS_SLOPE_13W` + 0.10×`MA200_SLOPE_5M` + 0.10×`MA200_UP_STREAK_D`。
L159      - **VOL**: `BETA`単体を使用。
L160      - **QAL**: 0.60×`FCF_W` + 0.40×`ROE_W`で作成。
L161      - **YLD**: 0.30×`DIV` + 0.70×`DIV_STREAK`。
L162      - **D_QAL**: 0.35×`QAL` + 0.20×`FCF` + 0.15×`CURR_RATIO` − 0.15×`DEBT2EQ` − 0.15×`EPS_VAR_8Q`。
L163      - **D_YLD**: 0.45×`DIV` + 0.25×`DIV_STREAK` + 0.20×`DIV_FCF_COVER` − 0.10×`DIV_VAR5`。
L164      - **D_VOL_RAW**: 0.40×`DOWNSIDE_DEV` + 0.22×`RESID_VOL` + 0.18×`MDD_1Y` − 0.10×`DOWN_OUTPERF` − 0.05×`EXT_200` − 0.08×`SIZE` − 0.10×`LIQ` + 0.10×`BETA`。
L165      - **D_TRD**: 0.40×`MA200_SLOPE_5M` − 0.30×`EXT_200` + 0.15×`NEAR_52W_HIGH` + 0.15×`TR`。
L166     - 主な指標の略称と意味:
L167
L168       | 略称 | 補助関数 | 概要 |
L169       | --- | --- | --- |
L170       | TR | `trend` | 50/150/200日移動平均と52週レンジを組み合わせたトレンド総合値 |
L171       | RS | `rs` | ベンチマークに対する相対強さ（12M/1Mリターン差） |
L172       | TR_str | `tr_str` | 価格と50日移動平均の乖離 |
L173       | RS_SLOPE_6W | `rs_line_slope` | 相対強さ線の6週回帰傾き |
L174       | RS_SLOPE_13W | `rs_line_slope` | 相対強さ線の13週回帰傾き |
L175       | MA200_SLOPE_5M | - | 200日移動平均の5か月騰落率 |
L176       | MA200_UP_STREAK_D | - | 200日線が連続で上向いた日数 |
L177       | BETA | `calc_beta` | ベンチマークに対するβ |
L178       | DOWNSIDE_DEV | - | 下方リターンのみの年率化標準偏差 |
L179       | RESID_VOL | - | βで調整した残差リターンの年率化標準偏差 |
L180       | MDD_1Y | - | 過去1年の最大ドローダウン |
L181       | DOWN_OUTPERF | - | 市場下落日に対する平均超過リターン |
L182       | EXT_200 | - | 200日移動平均からの絶対乖離率 |
L183       | NEAR_52W_HIGH | - | 52週高値までの下方距離（0=高値） |
L184       | FCF_W | - | ウィンザー処理後のFCF/EV |
L185       | ROE_W | - | ウィンザー処理後のROE |
L186       | FCF | - | FCF/EV |
L187       | QAL | - | FCF_WとROE_Wを組み合わせた品質スコア |
L188       | CURR_RATIO | - | 流動比率 |
L189       | DEBT2EQ | - | 負債資本倍率 |
L190       | EPS_VAR_8Q | - | EPSの8四半期標準偏差 |
L191       | DIV | - | 年率換算配当利回り |
L192       | DIV_STREAK | `div_streak` | 連続増配年数 |
L193       | DIV_FCF_COVER | - | 配当のFCFカバレッジ |
L194       | DIV_VAR5 | - | 5年配当変動率 |
L195       | DIV_TTM_PS | - | 1株当たりTTM配当 |
L196       | DIV_YOY | - | 前年比配当成長率 |
L197       | REV | - | 売上成長率TTM |
L198       | EPS_Q_YOY | - | 四半期EPSの前年同期比 |
L199       | REV_Q_YOY | - | 四半期売上の前年同期比 |
L200       | REV_YOY_ACC | - | 売上成長率の加速分 |
L201       | RULE40 | - | 売上成長率とFCFマージンの合計 |
L202       | FCF_MGN | - | FCFマージン |
L203       | REV_ANN_STREAK | - | 年次売上成長の連続年数 |
L204       | REV_YOY_VAR | - | 年次売上成長率の変動性 |
L205       | SIZE | - | 時価総額の対数値 |
L206       | LIQ | - | 60日平均出来高ドルの対数値 |
L207    - Gバケット: `GRW`、`MOM`、`VOL`を`cfg.weights.g`（0.40/0.45/-0.15）で加重し`g_score`を得る。
L208    - Dバケット: `D_QAL`、`D_YLD`、`D_VOL_RAW`、`D_TRD`を`cfg.weights.d`（0.15/0.15/-0.45/0.25）で加重し`d_score_all`を算出。
L209    - セクターcapによる`soft_cap_effective_scores`を適用し、G採用銘柄にはトレンドテンプレートフィルタを適用。
L210 5. `_apply_growth_entry_flags`でブレイクアウト/押し目発火状況を付加し、`FeatureBundle`を返す。
L211
L212 ### Step3: Correlation Reduction & Selection (Selector)
L213 DRRSアルゴリズムで相関を抑えた銘柄選定を行い、`SelectionBundle`を返す。`results/`に保存された前回選定（`G_selection.json` / `D_selection.json`）を`_load_prev`で読み込み、目的値が大きく悪化しない限り維持する。新しい採用集合は`_save_sel`でJSONに書き出し次回以降の入力に備える。
L214 主なメソッド:
L215 - `residual_corr` : 収益率行列をZスコア化し、上位主成分を除去した残差から相関行列を求め、平均相関に応じてシュリンク。
L216 - `rrqr_like_det` : スコアを重み付けしたQR分解風の手順で初期候補をk件抽出し、スコアの高い非相関な集合を得る。
L217 - `swap_local_det` / `swap_local_det_cross` : `sum(score) - λ*within_corr - μ*cross_corr`を目的関数として、入れ替え探索で局所的に最適化。
L218 - `select_bucket_drrs` : プール銘柄とスコアから残差相関を計算し、上記2段階(初期選択→入れ替え)でk銘柄を決定。過去採用銘柄との比較で目的値が劣化しなければ維持する。
L219 - `select_buckets` : Gバケットを選定後、その結果を除いた候補からDバケットを選ぶ。D選定時はGとの相関ペナルティμを付与し、両バケットの分散を制御する。
L220
L221 #### 相関低減ロジック詳細
L222 1. **残差相関行列の構築 (`residual_corr`)**
L223    - リターン行列`R`をZスコア化。
L224    - SVDで上位`n_pc`主成分`F`を求め、最小二乗で係数`B`を算出し残差`E = Z - F@B`を得る。
L225    - `E`の相関行列`C`を計算し、平均絶対相関に応じてシュリンク量`shrink_eff`を補正して対角を強調。
L226 2. **初期候補の抽出 (`rrqr_like_det`)**
L227    - スコアを0-1正規化した重み`w`とし、`Z*(1+γw)`で列ノルムを強調。
L228    - 残差ノルム最大の列を逐次選び、QRライクなデフレーションを行って非相関かつ高スコアな`k`銘柄集合`S0`を得る。
L229 3. **局所探索 (`swap_local_det` / `swap_local_det_cross`)**
L230    - 目的関数`Σz_score − λ·within_corr − μ·cross_corr`を最大化。
L231    - 選択集合の各銘柄を他候補と入れ替え、改善がなくなるまでまたは`max_pass`回まで探索。
L232    - `swap_local_det_cross`はGバケットとのクロス相関行列`C_cross`を使用し、ペナルティ`μ`を付与。
L233 4. **過去採用の維持とクロスペナルティ (`select_bucket_drrs` / `select_buckets`)**
L234    - 局所探索結果`S`と過去集合`P`の目的値を比較し、`S`が`P`より`η`未満の改善なら`P`を維持。
L235    - `select_buckets`ではGを先に決定し、D選定時にGとの相関ペナルティ`μ`を加えてクロス分散を抑制。
L236
L237 ### Step4: Output
L238 選定結果を可視化し共有する工程。以下の内容をテーブル化して標準出力とSlackへ送る。
L239 - 採用銘柄と惜しくも選外となった銘柄のスコア一覧
L240 - IN/OUTリストとOUT銘柄のスコア（低得点銘柄を確認しやすく）
L241 - 新旧ポートフォリオの比較表（組入れ・除外、スコア変化）
L242 - 検討中銘柄の低スコアランキング
L243
L244 主なメソッド:
L245 - `display_results` : 上記テーブルに加えパフォーマンス指標や分散化指標を表示。
L246 - `notify_slack` : Slack Webhookへ同内容を送信。
L247 - 補助:`_avg_offdiag`、`_resid_avg_rho`、`_raw_avg_rho`、`_cross_block_raw_rho`。
L248
L249 ## エントリポイント
L250 1. `PipelineConfig`を構築。
L251 2. **Step1** `Input.prepare_data`で`InputBundle`を生成。
L252 3. **Step2** `Scorer.aggregate_scores`で`FeatureBundle`を取得。
L253 4. **Step3** `Selector.select_buckets`で`SelectionBundle`を算出。
L254 5. **Step4** `Output.display_results`と`notify_slack`で結果を出力。
```

# === Chat Paste Pack ===
# Repo: dakara32/GPT_Code @ main
# Files: config.py, factor.py, scorer.py, .github/workflows/weekly-report.yml, documents/README.md, documents/factor_design.md
# 作成日時: 2025-09-19 10:42:36 (JST)
# 使い方: 下のチャンクを順に貼ればこのチャットで全体把握できます。
# 注記: 各ファイルは個別に L1.. で行番号付与。
---

## <config.py>
```text
L1 # 共通設定（factor / drift から参照）
L2 from dataclasses import dataclass
L3
L4 TOTAL_TARGETS = 20
L5
L6 # 基準のバケット数（NORMAL）
L7 COUNTS_BASE = {"G": 12, "D": 8}
L8
L9 # モード別の推奨バケット数
L10 COUNTS_BY_MODE = {
L11     "NORMAL": {"G": 12, "D": 8},
L12     "CAUTION": {"G": 10, "D": 8},
L13     "EMERG": {"G": 8,  "D": 8},
L14 }
L15
L16 # モード別のドリフト閾値（%）
L17 DRIFT_THRESHOLD_BY_MODE = {"NORMAL": 12, "CAUTION": 14, "EMERG": float("inf")}
L18
L19 # モード別のTS（基本幅, 小数=割合）
L20 TS_BASE_BY_MODE = {"NORMAL": 0.15, "CAUTION": 0.13, "EMERG": 0.10}
L21 # 利益到達(+30/+60/+100%)時の段階タイト化（ポイント差）
L22 TS_STEP_DELTAS_PT = (3, 6, 8)
L23
L24 # Breadthの校正は N_G に連動（緊急解除=ceil(1.5*N_G), 通常復帰=3*N_G）
L25 N_G = COUNTS_BASE["G"]
L26 N_D = COUNTS_BASE["D"]
L27
```

## <factor.py>
```text
L1 '''ROLE: Orchestration ONLY（外部I/O・SSOT・Slack出力）, 計算は scorer.py'''
L2 # === NOTE: 機能・入出力・ログ文言・例外挙動は不変。安全な短縮（import統合/複数代入/内包表記/メソッドチェーン/一行化/空行圧縮など）のみ適用 ===
L3 BONUS_COEFF = 0.55  # 推奨: 攻め=0.45 / 中庸=0.55 / 守り=0.65
L4 SWAP_DELTA_Z = 0.15   # 僅差判定: σの15%。(緩め=0.10 / 標準=0.15 / 固め=0.20)
L5 SWAP_KEEP_BUFFER = 3  # n_target+この順位以内の現行は保持。(粘り弱=2 / 標準=3 / 粘り強=4〜5)
L6 import os, time, requests
L7 import logging
L8 from time import perf_counter
L9 from dataclasses import dataclass
L10 from typing import Any, Dict, List
L11 from concurrent.futures import ThreadPoolExecutor
L12 import numpy as np
L13 import pandas as pd
L14 import yfinance as yf
L15 from scipy.stats import zscore  # used via scorer
L16 from scorer import Scorer, ttm_div_yield_portfolio
L17 import config
L18
L19 # その他
L20 debug_mode, FINNHUB_API_KEY = True, os.environ.get("FINNHUB_API_KEY")
L21
L22 logger = logging.getLogger(__name__)
L23 if debug_mode:
L24     logging.basicConfig(level=logging.INFO, force=True)
L25 else:
L26     logging.basicConfig(level=logging.WARNING, force=True)
L27
L28 class T:
L29     t = perf_counter()
L30     log = staticmethod(lambda tag: (lambda now=perf_counter(): (print(f"[T] {tag}: {now - T.t:.2f}s"), setattr(T, "t", now))[-1])())
L31
L32 T.log("start")
L33
L34 # === ユニバースと定数（冒頭に固定） ===
L35 exist, cand = [pd.read_csv(f, header=None)[0].tolist() for f in ("current_tickers.csv","candidate_tickers.csv")]
L36 T.log(f"csv loaded: exist={len(exist)} cand={len(cand)}")
L37 CAND_PRICE_MAX, bench = 450, '^GSPC'  # 価格上限・ベンチマーク
L38 N_G, N_D = config.N_G, config.N_D  # G/D枠サイズ（NORMAL基準: G12/D8）
L39 g_weights = {'GROWTH_F':0.35,'MOM':0.55,'VOL':-0.10}
L40 D_BETA_MAX = float(os.environ.get("D_BETA_MAX", "0.8"))
L41 FILTER_SPEC = {"G":{"pre_mask":["trend_template"]},"D":{"pre_filter":{"beta_max":D_BETA_MAX}}}
L42 D_weights = {'QAL':0.1,'YLD':0.3,'VOL':-0.5,'TRD':0.1}
L43 _fmt_w = lambda w: " ".join(f"{k}{int(v*100)}" for k, v in w.items())
L44
L45 # DRRS 初期プール・各種パラメータ
L46 corrM = 45
L47 DRRS_G, DRRS_D = dict(lookback=252,n_pc=3,gamma=1.2,lam=0.68,eta=0.8), dict(lookback=504,n_pc=4,gamma=0.8,lam=0.85,eta=0.5)
L48 DRRS_SHRINK = 0.10  # 残差相関の対角シュリンク（基礎）
L49
L50 # クロス相関ペナルティ（未定義なら設定）
L51 try: CROSS_MU_GD
L52 except NameError: CROSS_MU_GD = 0.40  # 推奨 0.35–0.45（lam=0.85想定）
L53
L54 # 出力関連
L55 RESULTS_DIR = "results"
L56 os.makedirs(RESULTS_DIR, exist_ok=True)
L57
L58 # === 共有DTO（クラス間I/O契約）＋ Config ===
L59 @dataclass(frozen=True)
L60 class InputBundle:
L61     # Input → Scorer で受け渡す素材（I/O禁止の生データ）
L62     cand: List[str]
L63     tickers: List[str]
L64     bench: str
L65     data: pd.DataFrame              # yfinance download結果（'Close','Volume'等の階層列）
L66     px: pd.DataFrame                # data['Close']
L67     spx: pd.Series                  # data['Close'][bench]
L68     tickers_bulk: object            # yfinance.Tickers
L69     info: Dict[str, dict]           # yfinance info per ticker
L70     eps_df: pd.DataFrame            # ['eps_ttm','eps_q_recent',...]
L71     fcf_df: pd.DataFrame            # ['fcf_ttm', ...]
L72     returns: pd.DataFrame           # px[tickers].pct_change()
L73
L74 @dataclass(frozen=True)
L75 class FeatureBundle:
L76     df: pd.DataFrame
L77     df_z: pd.DataFrame
L78     g_score: pd.Series
L79     d_score_all: pd.Series
L80     missing_logs: pd.DataFrame
L81     df_full: pd.DataFrame | None = None
L82     df_full_z: pd.DataFrame | None = None
L83     scaler: Any | None = None
L84
L85 @dataclass(frozen=True)
L86 class SelectionBundle:
L87     resG: dict
L88     resD: dict
L89     top_G: List[str]
L90     top_D: List[str]
L91     init_G: List[str]
L92     init_D: List[str]
L93
L94 @dataclass(frozen=True)
L95 class WeightsConfig:
L96     g: Dict[str,float]
L97     d: Dict[str,float]
L98
L99 @dataclass(frozen=True)
L100 class DRRSParams:
L101     corrM: int
L102     shrink: float
L103     G: Dict[str,float]   # lookback, n_pc, gamma, lam, eta
L104     D: Dict[str,float]
L105     cross_mu_gd: float
L106
L107 @dataclass(frozen=True)
L108 class PipelineConfig:
L109     weights: WeightsConfig
L110     drrs: DRRSParams
L111     price_max: float
L112     debug_mode: bool = False
L113
L114 # === 共通ユーティリティ（複数クラスで使用） ===
L115 # (unused local utils removed – use scorer.py versions if needed)
L116
L117 _env_true = lambda name, default=False: (os.getenv(name) or str(default)).strip().lower() == "true"
L118
L119 def _post_slack(payload: dict):
L120     url = os.getenv("SLACK_WEBHOOK_URL")
L121     if not url: print("⚠️ SLACK_WEBHOOK_URL 未設定"); return
L122     try:
L123         requests.post(url, json=payload).raise_for_status()
L124     except Exception as e:
L125         print(f"⚠️ Slack通知エラー: {e}")
L126
L127 def _slack_send_text_chunks(url: str, text: str, chunk: int = 2800) -> None:
L128     """Slackへテキストを分割送信（コードブロック形式）。"""
L129
L130     def _post_text(payload: str) -> None:
L131         try:
L132             resp = requests.post(url, json={"text": payload})
L133             print(f"[DBG] debug_post status={getattr(resp, 'status_code', None)} size={len(payload)}")
L134             if resp is not None:
L135                 resp.raise_for_status()
L136         except Exception as e:
L137             print(f"[ERR] debug_post_failed: {e}")
L138
L139     body = str(text or "").strip()
L140     if not body:
L141         print("[DBG] skip debug send: empty body")
L142         return
L143
L144     lines = body.splitlines()
L145     block: list[str] = []
L146     block_len = 0
L147
L148     def _flush() -> None:
L149         nonlocal block, block_len
L150         if not block:
L151             return
L152         payload = "```" + "\n".join(block) + "```"
L153         _post_text(payload)
L154         block, block_len = [], 0
L155
L156     for raw in lines:
L157         line = raw or ""
L158         while len(line) > chunk:
L159             head, line = line[:chunk], line[chunk:]
L160             _flush()
L161             _post_text("```" + head + "```")
L162         add_len = len(line) if not block else len(line) + 1
L163         if block and block_len + add_len > chunk:
L164             _flush()
L165             add_len = len(line)
L166         block.append(line)
L167         block_len += add_len
L168     _flush()
L169
L170 def _disjoint_keepG(top_G, top_D, poolD):
L171     """G重複をDから除去し、poolDで順次補充（枯渇時は元銘柄維持）。"""
L172     used, D, i = set(top_G), list(top_D), 0
L173     for j, t in enumerate(D):
L174         if t in used:
L175             while i < len(poolD) and (poolD[i] in used or poolD[i] in D):
L176                 i += 1
L177             if i < len(poolD):
L178                 D[j] = poolD[i]; used.add(D[j]); i += 1
L179     return top_G, D
L180
L181
L182 def _sticky_keep_current(agg: pd.Series, pick: list[str], incumbents: list[str],
L183                          n_target: int, delta_z: float, keep_buffer: int) -> list[str]:
L184     import pandas as pd, numpy as np
L185     sel = list(pick)
L186     if not sel: return sel
L187     ranked_sel = agg.reindex(sel).sort_values(ascending=False)
L188     kth = ranked_sel.iloc[min(len(sel), n_target)-1]
L189     sigma = float(agg.std()) if pd.notna(agg.std()) else 0.0
L190     thresh = kth - delta_z * sigma
L191     ranked_all = agg.sort_values(ascending=False)
L192     cand = [t for t in incumbents if (t not in sel) and (t in agg.index)]
L193     for t in cand:
L194         within_score = (pd.notna(agg[t]) and agg[t] >= thresh)
L195         within_rank  = (t in ranked_all.index) and (ranked_all.index.get_loc(t) < n_target + keep_buffer)
L196         if within_score or within_rank:
L197             non_inc = [x for x in sel if x not in incumbents]
L198             if not non_inc: break
L199             weakest = min(non_inc, key=lambda x: agg.get(x, -np.inf))
L200             if weakest in sel and agg.get(t, -np.inf) >= agg.get(weakest, -np.inf):
L201                 sel.remove(weakest); sel.append(t)
L202     if len(sel) > n_target:
L203         sel = sorted(sel, key=lambda x: agg.get(x, -1e9), reverse=True)[:n_target]
L204     return sel
L205
L206
L207 # === Input：外部I/Oと前処理（CSV/API・欠損補完） ===
L208 class Input:
L209     def __init__(self, cand, exist, bench, price_max, finnhub_api_key=None):
L210         self.cand, self.exist, self.bench, self.price_max = cand, exist, bench, price_max
L211         self.api_key = finnhub_api_key or os.environ.get("FINNHUB_API_KEY")
L212
L213     # ---- （Input専用）EPS補完・FCF算出系 ----
L214     @staticmethod
L215     def impute_eps_ttm(df: pd.DataFrame, ttm_col: str="eps_ttm", q_col: str="eps_q_recent", out_col: str|None=None) -> pd.DataFrame:
L216         out_col = out_col or ttm_col; df = df.copy(); df["eps_imputed"] = False
L217         cand = df[q_col]*4; ok = df[ttm_col].isna() & cand.replace([np.inf,-np.inf], np.nan).notna()
L218         df.loc[ok, out_col], df.loc[ok,"eps_imputed"] = cand[ok], True; return df
L219
L220     _CF_ALIASES = {"cfo":["Operating Cash Flow","Total Cash From Operating Activities"], "capex":["Capital Expenditure","Capital Expenditures"]}
L221
L222     @staticmethod
L223     def _pick_row(df: pd.DataFrame, names: list[str]) -> pd.Series|None:
L224         if df is None or df.empty: return None
L225         idx_lower={str(i).lower():i for i in df.index}
L226         for n in names:
L227             k=n.lower()
L228             if k in idx_lower: return df.loc[idx_lower[k]]
L229         return None
L230
L231     @staticmethod
L232     def _sum_last_n(s: pd.Series|None, n: int) -> float|None:
L233         if s is None or s.empty: return None
L234         v=s.dropna().astype(float); return None if v.empty else v.iloc[:n].sum()
L235
L236     @staticmethod
L237     def _latest(s: pd.Series|None) -> float|None:
L238         if s is None or s.empty: return None
L239         v=s.dropna().astype(float); return v.iloc[0] if not v.empty else None
L240
L241     def fetch_cfo_capex_ttm_yf(self, tickers: list[str]) -> pd.DataFrame:
L242         from concurrent.futures import ThreadPoolExecutor, as_completed
L243         pick, sumn, latest, aliases = self._pick_row, self._sum_last_n, self._latest, self._CF_ALIASES
L244
L245         def one(t: str):
L246             try:
L247                 tk = yf.Ticker(t)  # ★ セッションは渡さない（YFがcurl_cffiで管理）
L248                 qcf = tk.quarterly_cashflow
L249                 cfo_q, capex_q = pick(qcf, aliases["cfo"]), pick(qcf, aliases["capex"])
L250                 fcf_q = pick(qcf, ["Free Cash Flow","FreeCashFlow","Free cash flow"])
L251                 cfo, capex, fcf = sumn(cfo_q,4), sumn(capex_q,4), sumn(fcf_q,4)
L252                 if any(v is None for v in (cfo, capex, fcf)):
L253                     acf = tk.cashflow
L254                     if cfo   is None: cfo   = latest(pick(acf, aliases["cfo"]))
L255                     if capex is None: capex = latest(pick(acf, aliases["capex"]))
L256                     if fcf   is None: fcf   = latest(pick(acf, ["Free Cash Flow","FreeCashFlow","Free cash flow"]))
L257             except Exception as e:
L258                 print(f"[warn] yf financials error: {t}: {e}"); cfo=capex=fcf=None
L259             n=np.nan
L260             return {"ticker":t,
L261                     "cfo_ttm_yf":   n if cfo   is None else cfo,
L262                     "capex_ttm_yf": n if capex is None else capex,
L263                     "fcf_ttm_yf_direct": n if fcf is None else fcf}
L264
L265         rows, mw = [], int(os.getenv("FIN_THREADS","8"))
L266         with ThreadPoolExecutor(max_workers=mw) as ex:
L267             rows=[f.result() for f in as_completed(ex.submit(one,t) for t in tickers)]
L268         return pd.DataFrame(rows).set_index("ticker")
L269
L270     _FINN_CFO_KEYS = ["netCashProvidedByOperatingActivities","netCashFromOperatingActivities","cashFlowFromOperatingActivities","operatingCashFlow"]
L271     _FINN_CAPEX_KEYS = ["capitalExpenditure","capitalExpenditures","purchaseOfPPE","investmentsInPropertyPlantAndEquipment"]
L272
L273     @staticmethod
L274     def _first_key(d: dict, keys: list[str]):
L275         for k in keys:
L276             if k in d and d[k] is not None: return d[k]
L277         return None
L278
L279     @staticmethod
L280     def _finn_get(session: requests.Session, url: str, params: dict, retries: int=3, sleep_s: float=0.5):
L281         for i in range(retries):
L282             r = session.get(url, params=params, timeout=15)
L283             if r.status_code==429: time.sleep(min(2**i*sleep_s,4.0)); continue
L284             r.raise_for_status(); return r.json()
L285         r.raise_for_status()
L286
L287     def fetch_cfo_capex_ttm_finnhub(self, tickers: list[str], api_key: str|None=None) -> pd.DataFrame:
L288         api_key = api_key or os.getenv("FINNHUB_API_KEY")
L289         if not api_key: raise ValueError("Finnhub API key not provided. Set FINNHUB_API_KEY or pass api_key=")
L290         base, s, rows = "https://finnhub.io/api/v1", requests.Session(), []
L291         for sym in tickers:
L292             cfo_ttm = capex_ttm = None
L293             try:
L294                 j = self._finn_get(s, f"{base}/stock/cash-flow", {"symbol":sym,"frequency":"quarterly","limit":8,"token":api_key})
L295                 arr = j.get("cashFlow") or []; cfo_vals, capex_vals = [], []
L296                 for item in arr[:4]:
L297                     cfo_vals.append(self._first_key(item,self._FINN_CFO_KEYS)); capex_vals.append(self._first_key(item,self._FINN_CAPEX_KEYS))
L298                 if any(v is not None for v in cfo_vals): cfo_ttm = float(np.nansum([np.nan if v is None else float(v) for v in cfo_vals]))
L299                 if any(v is not None for v in capex_vals): capex_ttm = float(np.nansum([np.nan if v is None else float(v) for v in capex_vals]))
L300             except Exception: pass
L301             if cfo_ttm is None or capex_ttm is None:
L302                 try:
L303                     j = self._finn_get(s, f"{base}/stock/cash-flow", {"symbol":sym,"frequency":"annual","limit":1,"token":api_key})
L304                     arr = j.get("cashFlow") or []
L305                     if arr:
L306                         item0 = arr[0]
L307                         if cfo_ttm is None:
L308                             v = self._first_key(item0,self._FINN_CFO_KEYS)
L309                             if v is not None: cfo_ttm = float(v)
L310                         if capex_ttm is None:
L311                             v = self._first_key(item0,self._FINN_CAPEX_KEYS)
L312                             if v is not None: capex_ttm = float(v)
L313                 except Exception: pass
L314             rows.append({"ticker":sym,"cfo_ttm_fh":np.nan if cfo_ttm is None else cfo_ttm,"capex_ttm_fh":np.nan if capex_ttm is None else capex_ttm})
L315         return pd.DataFrame(rows).set_index("ticker")
L316
L317     def compute_fcf_with_fallback(self, tickers: list[str], finnhub_api_key: str|None=None) -> pd.DataFrame:
L318         yf_df = self.fetch_cfo_capex_ttm_yf(tickers)
L319         T.log("financials (yf) done")
L320         miss_mask = yf_df[["cfo_ttm_yf","capex_ttm_yf","fcf_ttm_yf_direct"]].isna().any(axis=1)
L321         need = yf_df.index[miss_mask].tolist(); print(f"[T] yf financials missing: {len(need)} {need[:10]}{'...' if len(need)>10 else ''}")
L322         if need:
L323             fh_df = self.fetch_cfo_capex_ttm_finnhub(need, api_key=finnhub_api_key)
L324             df = yf_df.join(fh_df, how="left")
L325             for col_yf, col_fh in [("cfo_ttm_yf","cfo_ttm_fh"),("capex_ttm_yf","capex_ttm_fh")]:
L326                 df[col_yf] = df[col_yf].fillna(df[col_fh])
L327             print("[T] financials (finnhub) done (fallback only)")
L328         else:
L329             df = yf_df.assign(cfo_ttm_fh=np.nan, capex_ttm_fh=np.nan)
L330             print("[T] financials (finnhub) skipped (no missing)")
L331         df["cfo_ttm"]  = df["cfo_ttm_yf"].where(df["cfo_ttm_yf"].notna(), df["cfo_ttm_fh"])
L332         df["capex_ttm"] = df["capex_ttm_yf"].where(df["capex_ttm_yf"].notna(), df["capex_ttm_fh"])
L333         cfo, capex = pd.to_numeric(df["cfo_ttm"], errors="coerce"), pd.to_numeric(df["capex_ttm"], errors="coerce").abs()
L334         fcf_calc = cfo - capex
L335         fcf_direct = pd.to_numeric(df.get("fcf_ttm_yf_direct"), errors="coerce")
L336         df["fcf_ttm"] = fcf_calc.where(fcf_calc.notna(), fcf_direct)
L337         df["cfo_source"]  = np.where(df["cfo_ttm_yf"].notna(),"yfinance",np.where(df["cfo_ttm_fh"].notna(),"finnhub",""))
L338         df["capex_source"] = np.where(df["capex_ttm_yf"].notna(),"yfinance",np.where(df["capex_ttm_fh"].notna(),"finnhub",""))
L339         df["fcf_imputed"] = df[["cfo_ttm","capex_ttm"]].isna().any(axis=1) & df["fcf_ttm"].notna()
L340         cols = ["cfo_ttm_yf","capex_ttm_yf","cfo_ttm_fh","capex_ttm_fh","cfo_ttm","capex_ttm","fcf_ttm","fcf_ttm_yf_direct","cfo_source","capex_source","fcf_imputed"]
L341         return df[cols].sort_index()
L342
L343     def _build_eps_df(self, tickers, tickers_bulk, info):
L344         eps_rows=[]
L345         for t in tickers:
L346             info_t, eps_ttm, eps_q = info[t], info[t].get("trailingEps", np.nan), np.nan
L347             try:
L348                 qearn, so = tickers_bulk.tickers[t].quarterly_earnings, info_t.get("sharesOutstanding")
L349                 if so and qearn is not None and not qearn.empty and "Earnings" in qearn.columns:
L350                     eps_ttm_q = qearn["Earnings"].head(4).sum()/so
L351                     if pd.notna(eps_ttm_q) and (pd.isna(eps_ttm) or (abs(eps_ttm)>0 and abs(eps_ttm/eps_ttm_q)>3)): eps_ttm = eps_ttm_q
L352                     eps_q = qearn["Earnings"].iloc[-1]/so
L353             except Exception: pass
L354             eps_rows.append({"ticker":t,"eps_ttm":eps_ttm,"eps_q_recent":eps_q})
L355         return self.impute_eps_ttm(pd.DataFrame(eps_rows).set_index("ticker"))
L356
L357     def prepare_data(self):
L358         """Fetch price and fundamental data for all tickers."""
L359         cand_info = yf.Tickers(" ".join(self.cand)); cand_prices = {}
L360         for t in self.cand:
L361             try: cand_prices[t] = cand_info.tickers[t].fast_info.get("lastPrice", np.inf)
L362             except Exception as e: print(f"{t}: price fetch failed ({e})"); cand_prices[t] = np.inf
L363         cand_f = [t for t,p in cand_prices.items() if p<=self.price_max]
L364         T.log("price cap filter done (CAND_PRICE_MAX)")
L365         tickers = sorted(set(self.exist + cand_f))
L366         T.log(f"universe prepared: unique={len(tickers)} bench={self.bench}")
L367         data = yf.download(tickers + [self.bench], period="600d",
L368                            auto_adjust=True, progress=False, threads=False)
L369         T.log("yf.download done")
L370         px = data["Close"].dropna(how="all", axis=1).ffill(limit=2)
L371         spx = data["Close"][self.bench].reindex(px.index).ffill()
L372         clip_days = int(os.getenv("PRICE_CLIP_DAYS", "0"))   # 0なら無効（既定）
L373         if clip_days > 0:
L374             px  = px.tail(clip_days + 1)
L375             spx = spx.tail(clip_days + 1)
L376             logger.info("[T] price window clipped by env: %d rows (PRICE_CLIP_DAYS=%d)", len(px), clip_days)
L377         else:
L378             logger.info("[T] price window clip skipped; rows=%d", len(px))
L379         tickers_bulk, info = yf.Tickers(" ".join(tickers)), {}
L380         for t in tickers:
L381             try:
L382                 info[t] = tickers_bulk.tickers[t].info
L383             except Exception as e:
L384                 logger.info("[warn] %s: info fetch failed (%s)", t, e)
L385                 info[t] = {}
L386         eps_df = self._build_eps_df(tickers, tickers_bulk, info)
L387         fcf_df = self.compute_fcf_with_fallback(tickers, finnhub_api_key=self.api_key)
L388         T.log("eps/fcf prep done")
L389         returns = px[tickers].pct_change()
L390         T.log("price prep/returns done")
L391         return dict(cand=cand_f, tickers=tickers, data=data, px=px, spx=spx, tickers_bulk=tickers_bulk, info=info, eps_df=eps_df, fcf_df=fcf_df, returns=returns)
L392
L393 # === Selector：相関低減・選定（スコア＆リターンだけ読む） ===
L394 class Selector:
L395     # ---- DRRS helpers（Selector専用） ----
L396     @staticmethod
L397     def _z_np(X: np.ndarray) -> np.ndarray:
L398         X = np.asarray(X, dtype=np.float32); m = np.nanmean(X, axis=0, keepdims=True); s = np.nanstd(X, axis=0, keepdims=True)+1e-9
L399         return (np.nan_to_num(X)-m)/s
L400
L401     @classmethod
L402     def residual_corr(cls, R: np.ndarray, n_pc: int=3, shrink: float=0.1) -> np.ndarray:
L403         Z = cls._z_np(R); U,S,_ = np.linalg.svd(Z, full_matrices=False); F = U[:,:n_pc]*S[:n_pc]; B = np.linalg.lstsq(F, Z, rcond=None)[0]
L404         E = Z - F@B; C = np.corrcoef(E, rowvar=False)
L405         off = C - np.diag(np.diag(C)); iu = np.triu_indices_from(off,1); avg_abs = np.nanmean(np.abs(off[iu])) if iu[0].size else 0.0
L406         shrink_eff = float(np.clip(shrink + 0.5*avg_abs, 0.1, 0.6)); N = C.shape[0]
L407         return (1.0 - shrink_eff)*C + shrink_eff*np.eye(N, dtype=C.dtype)
L408
L409     @classmethod
L410     def rrqr_like_det(cls, R: np.ndarray, score: np.ndarray, k: int, gamma: float=1.0):
L411         Z, w = cls._z_np(R), (score-score.min())/(np.ptp(score)+1e-12); X = Z*(1.0 + gamma*w)
L412         N, k = X.shape[1], max(0, min(k, X.shape[1]))
L413         if k==0: return []
L414         S, selected, Rres = [], np.zeros(N, dtype=bool), X.copy()
L415         for _ in range(k):
L416             norms = (Rres*Rres).sum(axis=0); cand = np.where(~selected)[0]
L417             j = sorted(cand, key=lambda c:(-norms[c], -w[c], c))[0]
L418             S.append(j); selected[j]=True; u = X[:,j:j+1]; u/=(np.linalg.norm(u)+1e-12); Rres = Rres - u @ (u.T @ Rres)
L419         return sorted(S)
L420
L421     @staticmethod
L422     def _obj(corrM: np.ndarray, score: np.ndarray, idx, lam: float) -> float:
L423         idx = list(idx); P = corrM[np.ix_(idx, idx)]; s = (score-score.mean())/(score.std()+1e-9)
L424         return float(s[idx].sum() - lam*((P.sum()-np.trace(P))/2.0))
L425
L426     @classmethod
L427     def swap_local_det(cls, corrM: np.ndarray, score: np.ndarray, idx, lam: float=0.6, max_pass: int=15):
L428         S, best, improved, passes = sorted(idx), cls._obj(corrM, score, idx, lam), True, 0
L429         while improved and passes<max_pass:
L430             improved, passes = False, passes+1
L431             for i,out in enumerate(list(S)):
L432                 for inn in range(len(score)):
L433                     if inn in S: continue
L434                     cand = sorted(S[:i]+[inn]+S[i+1:]); v = cls._obj(corrM, score, cand, lam)
L435                     if v>best+1e-10: S, best, improved = cand, v, True; break
L436                 if improved: break
L437         return S, best
L438
L439     @staticmethod
L440     def _obj_with_cross(C_within: np.ndarray, C_cross: np.ndarray|None, score: np.ndarray, idx, lam: float, mu: float) -> float:
L441         idx = list(idx); P = C_within[np.ix_(idx, idx)]; s = (score-score.mean())/(score.std()+1e-9)
L442         within = (P.sum()-np.trace(P))/2.0; cross = 0.0
L443         if C_cross is not None and C_cross.size>0: cross = C_cross[idx,:].sum()
L444         return float(s[idx].sum() - lam*within - mu*cross)
L445
L446     @classmethod
L447     def swap_local_det_cross(cls, C_within: np.ndarray, C_cross: np.ndarray|None, score: np.ndarray, idx, lam: float=0.6, mu: float=0.3, max_pass: int=15):
L448         S, best, improved, passes, N = sorted(idx), cls._obj_with_cross(C_within,C_cross,score,idx,lam,mu), True, 0, len(score)
L449         while improved and passes<max_pass:
L450             improved, passes = False, passes+1
L451             for i,out in enumerate(list(S)):
L452                 for inn in range(N):
L453                     if inn in S: continue
L454                     cand = sorted(S[:i]+[inn]+S[i+1:]); v = cls._obj_with_cross(C_within,C_cross,score,cand,lam,mu)
L455                     if v>best+1e-10: S, best, improved = cand, v, True; break
L456                 if improved: break
L457         return S, best
L458
L459     @staticmethod
L460     def avg_corr(C: np.ndarray, idx) -> float:
L461         k = len(idx); P = C[np.ix_(idx, idx)]
L462         return float((P.sum()-np.trace(P))/(k*(k-1)+1e-12))
L463
L464     @classmethod
L465     def select_bucket_drrs(cls, returns_df: pd.DataFrame, score_ser: pd.Series, pool_tickers: list[str], k: int, *, n_pc: int, gamma: float, lam: float, lookback: int, shrink: float=0.10, g_fixed_tickers: list[str]|None=None, mu: float=0.0):
L466         g_fixed = [t for t in (g_fixed_tickers or []) if t in returns_df.columns]
L467         union = [t for t in pool_tickers if t in returns_df.columns]
L468         for t in g_fixed:
L469             if t not in union: union.append(t)
L470         Rdf_all = returns_df[union]; Rdf_all = Rdf_all.iloc[-lookback:] if len(Rdf_all)>=lookback else Rdf_all; Rdf_all = Rdf_all.dropna()
L471         pool_eff, g_eff = [t for t in pool_tickers if t in Rdf_all.columns], [t for t in g_fixed if t in Rdf_all.columns]
L472         if len(pool_eff)==0: return dict(idx=[], tickers=[], avg_res_corr=np.nan, sum_score=0.0, objective=-np.inf)
L473         score = score_ser.reindex(pool_eff).to_numpy(dtype=np.float32)
L474         C_all = cls.residual_corr(Rdf_all.to_numpy(), n_pc=n_pc, shrink=shrink)
L475         col_pos = {c:i for i,c in enumerate(Rdf_all.columns)}; pool_pos = [col_pos[t] for t in pool_eff]
L476         C_within, C_cross = C_all[np.ix_(pool_pos,pool_pos)], None
L477         if len(g_eff)>0 and mu>0.0:
L478             g_pos = [col_pos[t] for t in g_eff]; C_cross = C_all[np.ix_(pool_pos,g_pos)]
L479         R_pool = Rdf_all[pool_eff].to_numpy(); S0 = cls.rrqr_like_det(R_pool, score, k, gamma=gamma)
L480         S, Jn = (cls.swap_local_det_cross(C_within, C_cross, score, S0, lam=lam, mu=mu, max_pass=15) if C_cross is not None else cls.swap_local_det(C_within, score, S0, lam=lam, max_pass=15))
L481         selected_tickers = [pool_eff[i] for i in S]
L482         return dict(idx=S, tickers=selected_tickers, avg_res_corr=cls.avg_corr(C_within,S), sum_score=float(score[S].sum()), objective=float(Jn))
L483
L484     # ---- 選定（スコア Series / returns だけを受ける）----
L485 # === Output：出力整形と送信（表示・Slack） ===
L486 class Output:
L487
L488     def __init__(self, debug=None):
L489         # self.debug は使わない（互換のため引数は受けるが無視）
L490         self.miss_df = self.g_table = self.d_table = self.io_table = self.df_metrics_fmt = self.debug_table = None
L491         self.g_title = self.d_title = ""
L492         self.g_formatters = self.d_formatters = {}
L493         # 低スコア（GSC+DSC）Top10 表示/送信用
L494         self.low10_table = None
L495         self.debug_text = ""   # デバッグ用本文はここに一本化
L496         self._debug_logged = False
L497
L498     # --- 表示（元 display_results のロジックそのまま） ---
L499     def display_results(self, *, exist, bench, df_z, g_score, d_score_all,
L500                         init_G, init_D, top_G, top_D, **kwargs):
L501         logger.info("📌 reached display_results")
L502         pd.set_option('display.float_format','{:.3f}'.format)
L503         print("📈 ファクター分散最適化の結果")
L504         if self.miss_df is not None and not self.miss_df.empty:
L505             print("Missing Data:")
L506             print(self.miss_df.to_string(index=False))
L507
L508         # ---- 表示用：Changes/Near-Miss のスコア源を“最終集計”に統一するプロキシ ----
L509         try:
L510             sc = getattr(self, "_sc", None)
L511             agg_G = getattr(sc, "_agg_G", None)
L512             agg_D = getattr(sc, "_agg_D", None)
L513         except Exception:
L514             sc = agg_G = agg_D = None
L515         class _SeriesProxy:
L516             __slots__ = ("primary", "fallback")
L517             def __init__(self, primary, fallback): self.primary, self.fallback = primary, fallback
L518             def get(self, key, default=None):
L519                 try:
L520                     v = self.primary.get(key) if hasattr(self.primary, "get") else None
L521                     if v is not None and not (isinstance(v, float) and v != v):
L522                         return v
L523                 except Exception:
L524                     pass
L525                 try:
L526                     return self.fallback.get(key) if hasattr(self.fallback, "get") else default
L527                 except Exception:
L528                     return default
L529         g_score = _SeriesProxy(agg_G, g_score)
L530         d_score_all = _SeriesProxy(agg_D, d_score_all)
L531         near_G = getattr(sc, "_near_G", []) if sc else []
L532         near_D = getattr(sc, "_near_D", []) if sc else []
L533
L534         extra_G = [t for t in init_G if t not in top_G][:5]; G_UNI = top_G + extra_G
L535         gsc_series = pd.Series({t: g_score.get(t) for t in G_UNI}, name='GSC')
L536         self.g_table = pd.concat([df_z.loc[G_UNI,['GROWTH_F','MOM','TRD','VOL']], gsc_series], axis=1)
L537         self.g_table.index = [t + ("⭐️" if t in top_G else "") for t in G_UNI]
L538         self.g_formatters = {col:"{:.2f}".format for col in ['GROWTH_F','MOM','TRD','VOL']}; self.g_formatters['GSC'] = "{:.3f}".format
L539         self.g_title = (f"[G枠 / {N_G} / {_fmt_w(g_weights)} / corrM={corrM} / "
L540                         f"LB={DRRS_G['lookback']} nPC={DRRS_G['n_pc']} γ={DRRS_G['gamma']} λ={DRRS_G['lam']} η={DRRS_G['eta']} shrink={DRRS_SHRINK}]")
L541         if near_G:
L542             add = [t for t in near_G if t not in set(G_UNI)][:10]
L543             if len(add) < 10:
L544                 try:
L545                     aggG = getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False)
L546                     out_now = sorted(set(exist) - set(top_G + top_D))  # 今回 OUT
L547                     used = set(G_UNI + add)
L548                     def _push(lst):
L549                         nonlocal add, used
L550                         for t in lst:
L551                             if len(add) == 10: break
L552                             if t in aggG.index and t not in used:
L553                                 add.append(t); used.add(t)
L554                     _push(out_now)           # ① 今回 OUT を優先
L555                     _push(list(aggG.index))  # ② まだ足りなければ上位で充填
L556                 except Exception:
L557                     pass
L558             if add:
L559                 near_tbl = pd.concat([df_z.loc[add,['GROWTH_F','MOM','TRD','VOL']], pd.Series({t: g_score.get(t) for t in add}, name='GSC')], axis=1)
L560                 self.g_table = pd.concat([self.g_table, near_tbl], axis=0)
L561         print(self.g_title); print(self.g_table.to_string(formatters=self.g_formatters))
L562
L563         extra_D = [t for t in init_D if t not in top_D][:5]; D_UNI = top_D + extra_D
L564         cols_D = ['QAL','YLD','VOL','TRD']; d_disp = pd.DataFrame(index=D_UNI)
L565         d_disp['QAL'], d_disp['YLD'], d_disp['VOL'], d_disp['TRD'] = df_z.loc[D_UNI,'D_QAL'], df_z.loc[D_UNI,'D_YLD'], df_z.loc[D_UNI,'D_VOL_RAW'], df_z.loc[D_UNI,'D_TRD']
L566         dsc_series = pd.Series({t: d_score_all.get(t) for t in D_UNI}, name='DSC')
L567         self.d_table = pd.concat([d_disp, dsc_series], axis=1); self.d_table.index = [t + ("⭐️" if t in top_D else "") for t in D_UNI]
L568         self.d_formatters = {col:"{:.2f}".format for col in cols_D}; self.d_formatters['DSC']="{:.3f}".format
L569         import scorer
L570         dw_eff = scorer.D_WEIGHTS_EFF
L571         self.d_title = (f"[D枠 / {N_D} / {_fmt_w(dw_eff)} / corrM={corrM} / "
L572                         f"LB={DRRS_D['lookback']} nPC={DRRS_D['n_pc']} γ={DRRS_D['gamma']} λ={DRRS_D['lam']} μ={CROSS_MU_GD} η={DRRS_D['eta']} shrink={DRRS_SHRINK}]")
L573         if near_D:
L574             add = [t for t in near_D if t not in set(D_UNI)][:10]
L575             if add:
L576                 d_disp2 = pd.DataFrame(index=add)
L577                 d_disp2['QAL'], d_disp2['YLD'], d_disp2['VOL'], d_disp2['TRD'] = df_z.loc[add,'D_QAL'], df_z.loc[add,'D_YLD'], df_z.loc[add,'D_VOL_RAW'], df_z.loc[add,'D_TRD']
L578                 near_tbl = pd.concat([d_disp2, pd.Series({t: d_score_all.get(t) for t in add}, name='DSC')], axis=1)
L579                 self.d_table = pd.concat([self.d_table, near_tbl], axis=0)
L580         print(self.d_title); print(self.d_table.to_string(formatters=self.d_formatters))
L581
L582         # === Changes（IN の GSC/DSC を表示。OUT は銘柄名のみ） ===
L583         in_list = sorted(set(list(top_G)+list(top_D)) - set(exist))
L584         out_list = sorted(set(exist) - set(list(top_G)+list(top_D)))
L585
L586         self.io_table = pd.DataFrame({
L587             'IN': pd.Series(in_list),
L588             '/ OUT': pd.Series(out_list)
L589         })
L590         g_list = [f"{g_score.get(t):.3f}" if pd.notna(g_score.get(t)) else '—' for t in out_list]
L591         d_list = [f"{d_score_all.get(t):.3f}" if pd.notna(d_score_all.get(t)) else '—' for t in out_list]
L592         self.io_table['GSC'] = pd.Series(g_list)
L593         self.io_table['DSC'] = pd.Series(d_list)
L594
L595         print("Changes:")
L596         print(self.io_table.to_string(index=False))
L597
L598         all_tickers = list(set(exist + list(top_G) + list(top_D) + [bench])); prices = yf.download(all_tickers, period='1y', auto_adjust=True, progress=False, threads=False)['Close'].ffill(limit=2)
L599         ret = prices.pct_change(); portfolios = {'CUR':exist,'NEW':list(top_G)+list(top_D)}; metrics={}
L600         for name,ticks in portfolios.items():
L601             pr = ret[ticks].mean(axis=1, skipna=True).dropna(); cum = (1+pr).cumprod()-1; n = len(pr)
L602             if n>=252: ann_ret, ann_vol = (1+cum.iloc[-1])**(252/n)-1, pr.std()*np.sqrt(252)
L603             else: ann_ret, ann_vol = cum.iloc[-1], pr.std()*np.sqrt(n)
L604             sharpe, drawdown = ann_ret/ann_vol, (cum - cum.cummax()).min()
L605             if len(ticks)>=2:
L606                 C_raw = ret[ticks].corr(); RAW_rho = C_raw.mask(np.eye(len(ticks), dtype=bool)).stack().mean()
L607                 R = ret[ticks].dropna().to_numpy(); C_resid = Selector.residual_corr(R, n_pc=3, shrink=DRRS_SHRINK)
L608                 RESID_rho = float((C_resid.sum()-np.trace(C_resid))/(C_resid.shape[0]*(C_resid.shape[0]-1)))
L609             else: RAW_rho = RESID_rho = np.nan
L610             divy = ttm_div_yield_portfolio(ticks); metrics[name] = {'RET':ann_ret,'VOL':ann_vol,'SHP':sharpe,'MDD':drawdown,'RAWρ':RAW_rho,'RESIDρ':RESID_rho,'DIVY':divy}
L611         df_metrics = pd.DataFrame(metrics).T; df_metrics_pct = df_metrics.copy(); self.df_metrics = df_metrics
L612         for col in ['RET','VOL','MDD','DIVY']: df_metrics_pct[col] = df_metrics_pct[col]*100
L613         cols_order = ['RET','VOL','SHP','MDD','RAWρ','RESIDρ','DIVY']; df_metrics_pct = df_metrics_pct.reindex(columns=cols_order)
L614         def _fmt_row(s):
L615             return pd.Series({'RET':f"{s['RET']:.1f}%",'VOL':f"{s['VOL']:.1f}%",'SHP':f"{s['SHP']:.1f}",'MDD':f"{s['MDD']:.1f}%",'RAWρ':(f"{s['RAWρ']:.2f}" if pd.notna(s['RAWρ']) else "NaN"),'RESIDρ':(f"{s['RESIDρ']:.2f}" if pd.notna(s['RESIDρ']) else "NaN"),'DIVY':f"{s['DIVY']:.1f}%"})
L616         self.df_metrics_fmt = df_metrics_pct.apply(_fmt_row, axis=1); print("Performance Comparison:"); print(self.df_metrics_fmt.to_string())
L617         # === 追加: GSC+DSC が低い順 TOP10 ===
L618         try:
L619             all_scores = pd.DataFrame({'GSC': df_z['GSC'], 'DSC': df_z['DSC']}).copy()
L620             all_scores['G_plus_D'] = all_scores['GSC'] + all_scores['DSC']
L621             all_scores = all_scores.dropna(subset=['G_plus_D'])
L622             self.low10_table = all_scores.sort_values('G_plus_D', ascending=True).head(10).round(3)
L623             print("Low Score Candidates (GSC+DSC bottom 10):")
L624             print(self.low10_table.to_string())
L625         except Exception as e:
L626             print(f"[warn] low-score ranking failed: {e}")
L627             self.low10_table = None
L628         self.debug_text = ""
L629         if debug_mode:
L630             logger.info("debug_mode=True: df_z dump handled in scorer; skipping factor-side debug output")
L631         else:
L632             logger.debug(
L633                 "skip debug log: debug_mode=%s debug_text_empty=%s",
L634                 debug_mode, True
L635             )
L636         self._debug_logged = True
L637
L638     # --- Slack送信（元 notify_slack のロジックそのまま） ---
L639     def notify_slack(self):
L640         SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")
L641
L642         if not SLACK_WEBHOOK_URL:
L643             print("⚠️ SLACK_WEBHOOK_URL not set (main report skipped)")
L644             return
L645
L646         def _filter_suffix_from(spec: dict, group: str) -> str:
L647             g = spec.get(group, {})
L648             parts = [str(m) for m in g.get("pre_mask", [])]
L649             for k, v in (g.get("pre_filter", {}) or {}).items():
L650                 base, op = (k[:-4], "<") if k.endswith("_max") else ((k[:-4], ">") if k.endswith("_min") else (k, "="))
L651                 name = {"beta": "β"}.get(base, base)
L652                 try:
L653                     val = f"{float(v):g}"
L654                 except Exception:
L655                     val = str(v)
L656                 parts.append(f"{name}{op}{val}")
L657             return "" if not parts else " / filter:" + " & ".join(parts)
L658
L659         def _inject_filter_suffix(title: str, group: str) -> str:
L660             suf = _filter_suffix_from(FILTER_SPEC, group)
L661             return f"{title[:-1]}{suf}]" if suf and title.endswith("]") else (title + suf)
L662
L663         def _blk(title, tbl, fmt=None, drop=()):
L664             if tbl is None or getattr(tbl, 'empty', False):
L665                 return f"{title}\n(選定なし)\n"
L666             if drop and hasattr(tbl, 'columns'):
L667                 keep = [c for c in tbl.columns if c not in drop]
L668                 tbl, fmt = tbl[keep], {k: v for k, v in (fmt or {}).items() if k in keep}
L669             return f"{title}\n```{tbl.to_string(formatters=fmt)}```\n"
L670
L671         message = "📈 ファクター分散最適化の結果\n"
L672         if self.miss_df is not None and not self.miss_df.empty:
L673             message += "Missing Data\n```" + self.miss_df.to_string(index=False) + "```\n"
L674         message += _blk(_inject_filter_suffix(self.g_title, "G"), self.g_table, self.g_formatters, drop=("TRD",))
L675         message += _blk(_inject_filter_suffix(self.d_title, "D"), self.d_table, self.d_formatters)
L676         message += "Changes\n" + ("(変更なし)\n" if self.io_table is None or getattr(self.io_table, 'empty', False) else f"```{self.io_table.to_string(index=False)}```\n")
L677         message += "Performance Comparison:\n```" + self.df_metrics_fmt.to_string() + "```"
L678
L679         try:
L680             r = requests.post(SLACK_WEBHOOK_URL, json={"text": message})
L681             print(f"[DBG] main_post status={getattr(r, 'status_code', None)} size={len(message)}")
L682             if r is not None:
L683                 r.raise_for_status()
L684         except Exception as e:
L685             print(f"[ERR] main_post_failed: {e}")
L686
L687 def _infer_g_universe(feature_df, selected12=None, near5=None):
L688     try:
L689         out = feature_df.index[feature_df['group'].astype(str).str.upper().eq('G')].tolist()
L690         if out: return out
L691     except Exception:
L692         pass
L693     base = set()
L694     for lst in (selected12 or []), (near5 or []):
L695         for x in (lst or []): base.add(x)
L696     return list(base) if base else list(feature_df.index)
L697
L698 def _fmt_with_fire_mark(tickers, feature_df):
L699     out = []
L700     for t in tickers or []:
L701         try:
L702             br = bool(feature_df.at[t, "G_BREAKOUT_recent_5d"])
L703             pb = bool(feature_df.at[t, "G_PULLBACK_recent_5d"])
L704             out.append(f"{t}{' 🔥' if (br or pb) else ''}")
L705         except Exception:
L706             out.append(t)
L707     return out
L708
L709 def _label_recent_event(t, feature_df):
L710     try:
L711         br = bool(feature_df.at[t, "G_BREAKOUT_recent_5d"]); dbr = str(feature_df.at[t, "G_BREAKOUT_last_date"]) if br else ""
L712         pb = bool(feature_df.at[t, "G_PULLBACK_recent_5d"]); dpb = str(feature_df.at[t, "G_PULLBACK_last_date"]) if pb else ""
L713         if   br and not pb: return f"{t}（ブレイクアウト確定 {dbr}）"
L714         elif pb and not br: return f"{t}（押し目反発 {dpb}）"
L715         elif br and pb:     return f"{t}（ブレイクアウト確定 {dbr}／押し目反発 {dpb}）"
L716     except Exception:
L717         pass
L718     return t
L719
L720 # === パイプライン可視化：G/D共通フロー（出力は不変） ===
L721
L722 def io_build_input_bundle() -> InputBundle:
L723     """
L724     既存の『データ取得→前処理』を実行し、InputBundle を返す。
L725     処理内容・列名・丸め・例外・ログ文言は現行どおり（変更禁止）。
L726     """
L727     state = Input(cand=cand, exist=exist, bench=bench, price_max=CAND_PRICE_MAX, finnhub_api_key=FINNHUB_API_KEY).prepare_data()
L728     return InputBundle(cand=state["cand"], tickers=state["tickers"], bench=bench, data=state["data"], px=state["px"], spx=state["spx"], tickers_bulk=state["tickers_bulk"], info=state["info"], eps_df=state["eps_df"], fcf_df=state["fcf_df"], returns=state["returns"])
L729
L730 def run_group(sc: Scorer, group: str, inb: InputBundle, cfg: PipelineConfig,
L731               n_target: int) -> tuple[list, float, float, float]:
L732     """
L733     G/Dを同一手順で処理：採点→フィルター→選定（相関低減込み）。
L734     戻り値：(pick, avg_res_corr, sum_score, objective)
L735     JSON保存は既存フォーマット（キー名・丸め桁・順序）を踏襲。
L736     """
L737     sc.cfg = cfg
L738
L739     if hasattr(sc, "score_build_features"):
L740         feat = sc.score_build_features(inb)
L741         if not hasattr(sc, "_feat_logged"):
L742             T.log("features built (scorer)")
L743             sc._feat_logged = True
L744         agg = sc.score_aggregate(feat, group, cfg) if hasattr(sc, "score_aggregate") else feat
L745     else:
L746         fb = sc.aggregate_scores(inb, cfg)
L747         if not hasattr(sc, "_feat_logged"):
L748             T.log("features built (scorer)")
L749             sc._feat_logged = True
L750         sc._feat = fb
L751         agg = fb.g_score if group == "G" else fb.d_score_all
L752         if group == "D" and hasattr(fb, "df"):
L753             agg = agg[fb.df['BETA'] < D_BETA_MAX]
L754
L755     if hasattr(sc, "filter_candidates"):
L756         agg = agg[sc.filter_candidates(inb, agg, group, cfg)]
L757
L758     selector = Selector()
L759     if hasattr(sc, "select_diversified"):
L760         pick, avg_r, sum_sc, obj = sc.select_diversified(agg, group, cfg, n_target,
L761             selector=selector, prev_tickers=None,
L762             corrM=cfg.drrs.corrM, shrink=cfg.drrs.shrink,
L763             cross_mu=cfg.drrs.cross_mu_gd)
L764     else:
L765         if group == "G":
L766             init = agg.nlargest(min(cfg.drrs.corrM, len(agg))).index.tolist()
L767             res = selector.select_bucket_drrs(returns_df=inb.returns, score_ser=agg, pool_tickers=init, k=n_target,
L768                 n_pc=cfg.drrs.G.get("n_pc", 3), gamma=cfg.drrs.G.get("gamma", 1.2),
L769                 lam=cfg.drrs.G.get("lam", 0.68),
L770                 lookback=cfg.drrs.G.get("lookback", 252),
L771                 shrink=cfg.drrs.shrink, g_fixed_tickers=None, mu=0.0)
L772         else:
L773             init = agg.nlargest(min(cfg.drrs.corrM, len(agg))).index.tolist()
L774             g_fixed = getattr(sc, "_top_G", None)
L775             res = selector.select_bucket_drrs(returns_df=inb.returns, score_ser=agg, pool_tickers=init, k=n_target,
L776                 n_pc=cfg.drrs.D.get("n_pc", 4), gamma=cfg.drrs.D.get("gamma", 0.8),
L777                 lam=cfg.drrs.D.get("lam", 0.85),
L778                 lookback=cfg.drrs.D.get("lookback", 504),
L779                 shrink=cfg.drrs.shrink, g_fixed_tickers=g_fixed,
L780                 mu=cfg.drrs.cross_mu_gd)
L781         pick = res["tickers"]; avg_r = res["avg_res_corr"]
L782         sum_sc = res["sum_score"]; obj = res["objective"]
L783         if group == "D":
L784             _, pick = _disjoint_keepG(getattr(sc, "_top_G", []), pick, init)
L785             T.log("selection finalized (G/D)")
L786     try:
L787         inc = [t for t in exist if t in agg.index]
L788         pick = _sticky_keep_current(
L789             agg=agg, pick=pick, incumbents=inc, n_target=n_target,
L790             delta_z=SWAP_DELTA_Z, keep_buffer=SWAP_KEEP_BUFFER
L791         )
L792     except Exception as _e:
L793         print(f"[warn] sticky_keep_current skipped: {str(_e)}")
L794     # --- Near-Miss: 惜しくも選ばれなかった上位10を保持（Slack表示用） ---
L795     # 5) Near-Miss と最終集計Seriesを保持（表示専用。計算へ影響なし）
L796     try:
L797         pool = agg.drop(index=[t for t in pick if t in agg.index], errors="ignore")
L798         near10 = list(pool.sort_values(ascending=False).head(10).index)
L799         setattr(sc, f"_near_{group}", near10)
L800         setattr(sc, f"_agg_{group}", agg)
L801     except Exception:
L802         pass
L803
L804     if group == "D":
L805         T.log("save done")
L806     if group == "G":
L807         sc._top_G = pick
L808     return pick, avg_r, sum_sc, obj
L809
L810 def run_pipeline() -> SelectionBundle:
L811     """
L812     G/D共通フローの入口。I/Oはここだけで実施し、計算はScorerに委譲。
L813     Slack文言・丸め・順序は既存の Output を用いて変更しない。
L814     """
L815     inb = io_build_input_bundle()
L816     cfg = PipelineConfig(
L817         weights=WeightsConfig(g=g_weights, d=D_weights),
L818         drrs=DRRSParams(
L819             corrM=corrM, shrink=DRRS_SHRINK,
L820             G=DRRS_G, D=DRRS_D, cross_mu_gd=CROSS_MU_GD
L821         ),
L822         price_max=CAND_PRICE_MAX,
L823         debug_mode=debug_mode
L824     )
L825     sc = Scorer()
L826     top_G, avgG, sumG, objG = run_group(sc, "G", inb, cfg, N_G)
L827     poolG = list(getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False).index)
L828     alpha = Scorer.spx_to_alpha(inb.spx)
L829     sectors = {t:(inb.info.get(t,{}).get("sector") or "U") for t in poolG}; scores = {t:Scorer.g_score.get(t,0.0) for t in poolG}
L830     top_G = Scorer.pick_top_softcap(scores, sectors, N=N_G, cap=2, alpha=alpha, hard=5)
L831     sc._top_G = top_G
L832     try:
L833         aggG = getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False)
L834         sc._near_G = [t for t in aggG.index if t not in set(top_G)][:10]
L835     except Exception:
L836         pass
L837     base = sum(Scorer.g_score.get(t,0.0) for t in poolG[:N_G])
L838     effs = sum(Scorer.g_score.get(t,0.0) for t in top_G)
L839     print(f"[soft_cap2] score_cost={(base-effs)/max(1e-9,abs(base)):.2%}, alpha={alpha:.3f}")
L840     top_D, avgD, sumD, objD = run_group(sc, "D", inb, cfg, N_D)
L841     fb = getattr(sc, "_feat", None)
L842     near_G = getattr(sc, "_near_G", [])
L843     selected12 = list(top_G)
L844     df = fb.df if fb is not None else pd.DataFrame()
L845     guni = _infer_g_universe(df, selected12, near_G)
L846     try:
L847         fire_recent = [t for t in guni
L848                        if (str(df.at[t, "G_BREAKOUT_recent_5d"]) == "True") or
L849                           (str(df.at[t, "G_PULLBACK_recent_5d"]) == "True")]
L850     except Exception: fire_recent = []
L851
L852     lines = [
L853         "【G枠レポート｜週次モニタ（直近5営業日）】",
L854         "【凡例】🔥=直近5営業日内に「ブレイクアウト確定」または「押し目反発」を検知",
L855         f"選定{N_G}: {', '.join(_fmt_with_fire_mark(selected12, df))}" if selected12 else f"選定{N_G}: なし",
L856         f"次点10: {', '.join(_fmt_with_fire_mark(near_G, df))}" if near_G else "次点10: なし",]
L857
L858     if fire_recent:
L859         fire_list = ", ".join([_label_recent_event(t, df) for t in fire_recent])
L860         lines.append(f"過去5営業日の検知: {fire_list}")
L861     else:
L862         lines.append("過去5営業日の検知: なし")
L863
L864     try:
L865         webhook = os.environ.get("SLACK_WEBHOOK_URL", "")
L866         if webhook:
L867             requests.post(webhook, json={"text": "\n".join([s for s in lines if s != ""])}, timeout=10)
L868     except Exception:
L869         pass
L870
L871     out = Output()
L872     # 表示側から選定時の集計へアクセスできるように保持（表示専用・副作用なし）
L873     try: out._sc = sc
L874     except Exception: pass
L875     if hasattr(sc, "_feat"):
L876         try:
L877             fb = sc._feat
L878             out.miss_df = fb.missing_logs
L879             out.display_results(
L880                 exist=exist,
L881                 bench=bench,
L882                 df_z=fb.df_z,
L883                 g_score=fb.g_score,
L884                 d_score_all=fb.d_score_all,
L885                 init_G=top_G,
L886                 init_D=top_D,
L887                 top_G=top_G,
L888                 top_D=top_D,
L889                 df_full_z=getattr(fb, "df_full_z", None),
L890                 prev_G=getattr(sc, "_prev_G", exist),
L891                 prev_D=getattr(sc, "_prev_D", exist),
L892             )
L893         except Exception:
L894             pass
L895     out.notify_slack()
L896     sb = SelectionBundle(resG={"tickers": top_G, "avg_res_corr": avgG,
L897               "sum_score": sumG, "objective": objG},
L898         resD={"tickers": top_D, "avg_res_corr": avgD,
L899               "sum_score": sumD, "objective": objD},
L900         top_G=top_G, top_D=top_D, init_G=top_G, init_D=top_D)
L901
L902     # --- Low Score Candidates (GSC+DSC bottom 10) : send before debug dump ---
L903     try:
L904         _low_df = (pd.DataFrame({"GSC": fb.g_score, "DSC": fb.d_score_all})
L905               .assign(G_plus_D=lambda x: x["GSC"] + x["DSC"])
L906               .sort_values("G_plus_D")
L907               .head(10)
L908               .round(3))
L909         low_msg = "Low Score Candidates (GSC+DSC bottom 10)\n" + _low_df.to_string(index=True, index_names=False)
L910         _post_slack({"text": f"```{low_msg}```"})
L911     except Exception as _e:
L912         _post_slack({"text": f"```Low Score Candidates: 作成失敗: {_e}```"})
L913
L914     return sb
L915
L916 if __name__ == "__main__":
L917     run_pipeline()
```

## <scorer.py>
```text
L1 # scorer.py
L2 # kawatest
L3 # =============================================================================
L4 # Scorer: ファクター/指標の生成と合成スコア算出を担う純粋層
L5 #
L6 # 【このファイルだけ読めば分かるポイント】
L7 # - 入力(InputBundle)は「価格/出来高/ベンチ/基本情報/EPS/FCF/リターン」を含むDTO
L8 # - 出力(FeatureBundle)は「raw特徴量 df」「標準化 df_z」「G/D スコア」「欠損ログ」
L9 # - 重み等のコンフィグ(PipelineConfig)は factor から渡す（cfg 必須）
L10 # - 旧カラム名は Scorer 内で自動リネームして受け入れ（後方互換）
L11 #   例) eps_ttm -> EPS_TTM, eps_q_recent -> EPS_Q_LastQ, fcf_ttm -> FCF_TTM
L12 #
L13 # 【I/O契約（Scorerが参照するInputBundleフィールド）】
L14 #   - cand: List[str]    … 候補銘柄（単体実行では未使用）
L15 #   - tickers: List[str] … 対象銘柄リスト
L16 #   - bench: str         … ベンチマークティッカー（例 '^GSPC'）
L17 #   - data: pd.DataFrame … yfinance download結果 ('Close','Volume' 等の階層列)
L18 #   - px: pd.DataFrame   … data['Close'] 相当（終値）
L19 #   - spx: pd.Series     … ベンチマークの終値
L20 #   - tickers_bulk: object         … yfinance.Tickers
L21 #   - info: Dict[str, dict]        … yfinance info per ticker
L22 #   - eps_df: pd.DataFrame         … 必須列: EPS_TTM, EPS_Q_LastQ（旧名も可）
L23 #   - fcf_df: pd.DataFrame         … 必須列: FCF_TTM（旧名も可）
L24 #   - returns: pd.DataFrame        … px[tickers].pct_change() 相当
L25 #
L26 # ※入出力の形式・例外文言は既存実装を変えません（安全な短縮のみ）
L27 # =============================================================================
L28
L29 import logging
L30 import math
L31 import os, sys, warnings
L32 import requests
L33 import numpy as np
L34 import pandas as pd
L35 import yfinance as yf
L36 from typing import Any, TYPE_CHECKING
L37 from scipy.stats import zscore
L38
L39 if TYPE_CHECKING:
L40     from factor import PipelineConfig  # type: ignore  # 実行時importなし（循環回避）
L41
L42 logger = logging.getLogger(__name__)
L43
L44 # ---- Dividend Helpers -------------------------------------------------------
L45 def _last_close(t, price_map=None):
L46     if price_map and (c := price_map.get(t)) is not None: return float(c)
L47     try:
L48         h = yf.Ticker(t).history(period="5d")["Close"].dropna()
L49         return float(h.iloc[-1]) if len(h) else np.nan
L50     except Exception:
L51         return np.nan
L52
L53 def _ttm_div_sum(t, lookback_days=400):
L54     try:
L55         div = yf.Ticker(t).dividends
L56         if div is None or len(div) == 0: return 0.0
L57         cutoff = pd.Timestamp.utcnow().tz_localize(None) - pd.Timedelta(days=lookback_days)
L58         ttm = float(div[div.index.tz_localize(None) >= cutoff].sum())
L59         return ttm if ttm > 0 else float(div.tail(4).sum())
L60     except Exception:
L61         return 0.0
L62
L63 def ttm_div_yield_portfolio(tickers, price_map=None):
L64     ys = [(lambda c, s: (s/c) if (np.isfinite(c) and c>0 and s>0) else 0.0)(_last_close(t, price_map), _ttm_div_sum(t)) for t in tickers]
L65     return float(np.mean(ys)) if ys else 0.0
L66
L67 # ---- 簡易ユーティリティ（安全な短縮のみ） -----------------------------------
L68 def winsorize_s(s: pd.Series, p=0.02):
L69     if s is None or s.dropna().empty: return s
L70     lo, hi = np.nanpercentile(s.astype(float), [100*p, 100*(1-p)]); return s.clip(lo, hi)
L71
L72 def robust_z(s: pd.Series, p=0.02):
L73     s2 = winsorize_s(s,p); return np.nan_to_num(zscore(s2.fillna(s2.mean())))
L74
L75 def robust_z_keepnan(s: pd.Series) -> pd.Series:
L76     """robust_z variant that preserves NaNs and falls back to rank-z when needed."""
L77     if s is None:
L78         return pd.Series(dtype=float)
L79     v = pd.to_numeric(s, errors="coerce")
L80     m = np.nanmedian(v)
L81     mad = np.nanmedian(np.abs(v - m))
L82     z = (v - m) / (1.4826 * mad + 1e-9)
L83     if np.nanstd(z) < 1e-9:
L84         r = v.rank(method="average", na_option="keep")
L85         z = (r - np.nanmean(r)) / (np.nanstd(r) + 1e-9)
L86     return pd.Series(z, index=v.index, dtype=float)
L87
L88
L89 def _fetch_revenue_quarterly_via_finnhub(symbol: str):
L90     """Finnhub financials-reported endpoint から四半期売上を取得する."""
L91
L92     api_key = os.getenv("FINNHUB_API_KEY", "").strip()
L93     if not api_key:
L94         return None
L95
L96     try:
L97         url = "https://finnhub.io/api/v1/stock/financials-reported"
L98         r = requests.get(url, params={"symbol": symbol, "token": api_key}, timeout=10)
L99         r.raise_for_status()
L100         data = (r.json() or {}).get("data", []) or []
L101
L102         cand_keys = [
L103             "revenue",
L104             "totalRevenue",
L105             "netSales",
L106             "salesRevenueNet",
L107             "operatingRevenue",
L108             "totalOperatingRevenue",
L109             "totalRevenueFromOperations",
L110             "turnover",
L111         ]
L112
L113         def _num(v):
L114             try:
L115                 if isinstance(v, dict):
L116                     v = v.get("value")
L117                 if v is None:
L118                     return None
L119                 x = float(v)
L120                 return x if math.isfinite(x) else None
L121             except Exception:
L122                 return None
L123
L124         rows = []
L125         for item in data:
L126             rep = (item or {}).get("report", {})
L127             ic = rep.get("ic", {}) if isinstance(rep, dict) else {}
L128             if isinstance(ic, list):
L129                 ic = {
L130                     u.get("concept"): {"value": u.get("value")}
L131                     for u in ic
L132                     if isinstance(u, dict) and "concept" in u
L133                 }
L134
L135             val = None
L136             if isinstance(ic, dict):
L137                 for key in cand_keys:
L138                     if key in ic:
L139                         val = _num(ic.get(key))
L140                         if val is not None:
L141                             break
L142
L143             year = item.get("year")
L144             quarter = item.get("quarter")
L145             if val is None or year is None or quarter is None:
L146                 continue
L147
L148             try:
L149                 rows.append((int(year), int(quarter), float(val)))
L150             except Exception:
L151                 continue
L152
L153         if not rows:
L154             return None
L155
L156         rows.sort(key=lambda x: (x[0], x[1]))
L157         idx = [f"{y}Q{q}" for y, q, _ in rows]
L158         vals = [v for _, _, v in rows]
L159         s = pd.Series(vals, index=idx, name="Revenue")
L160         return None if s.dropna().empty else s
L161     except Exception:
L162         return None
L163
L164
L165 def _dump_dfz(df_z: pd.DataFrame, debug_mode: bool, max_rows: int = 400, ndigits: int = 3) -> None:
L166     """df_z を System log(INFO) へダンプする簡潔なユーティリティ."""
L167     if not debug_mode:
L168         return
L169     try:
L170         view = df_z.copy()
L171         view = view.apply(
L172             lambda s: s.round(ndigits)
L173             if getattr(getattr(s, "dtype", None), "kind", "") in ("f", "i")
L174             else s
L175         )
L176         if len(view) > max_rows:
L177             view = view.iloc[:max_rows]
L178
L179         # === NaNサマリ（列ごとの欠損件数 上位20） ===
L180         try:
L181             nan_counts = df_z.isna().sum().sort_values(ascending=False)
L182             top_nan = nan_counts[nan_counts > 0].head(20)
L183             if len(top_nan) > 0:
L184                 logger.info("NaN columns (top20):\n%s", top_nan.to_string())
L185             else:
L186                 logger.info("NaN columns: none")
L187         except Exception as exc:
L188             logger.warning("nan summary failed: %s", exc)
L189
L190         # === Zeroサマリ（列ごとのゼロ比率 上位20） ===
L191         try:
L192             zero_counts = ((df_z == 0) & (~df_z.isna())).sum()
L193             nonnull_counts = (~df_z.isna()).sum()
L194             zero_ratio = (zero_counts / nonnull_counts).sort_values(ascending=False)
L195             top_zero = zero_ratio[zero_ratio > 0].head(20)
L196             if len(top_zero) > 0:
L197                 logger.info(
L198                     "Zero-dominated columns (top20):\n%s",
L199                     top_zero.to_string(float_format=lambda x: f"{x:.2%}"),
L200                 )
L201             else:
L202                 logger.info("Zero-dominated columns: none")
L203         except Exception as exc:
L204             logger.warning("zero summary failed: %s", exc)
L205
L206         logger.info("===== DF_Z DUMP START =====")
L207         logger.info("\n%s", view.to_string(max_rows=None, max_cols=None))
L208         logger.info("===== DF_Z DUMP END =====")
L209     except Exception as exc:
L210         logger.warning("df_z dump failed: %s", exc)
L211
L212 def _safe_div(a, b):
L213     try: return np.nan if (b is None or float(b)==0 or pd.isna(b)) else float(a)/float(b)
L214     except Exception: return np.nan
L215
L216 def _safe_last(series: pd.Series, default=np.nan):
L217     try: return float(series.iloc[-1])
L218     except Exception: return default
L219
L220 D_WEIGHTS_EFF = None  # 出力表示互換のため
L221
L222 # ---- Scorer 本体 -------------------------------------------------------------
L223 class Scorer:
L224     """
L225     - factor.py からは `aggregate_scores(ib, cfg)` を呼ぶだけでOK。
L226     - cfg は必須（factor.PipelineConfig を渡す）。
L227     - 旧カラム名を自動リネームして新スキーマに吸収します。
L228     """
L229
L230     # === 先頭で旧→新カラム名マップ（移行用） ===
L231     EPS_RENAME = {"eps_ttm":"EPS_TTM", "eps_q_recent":"EPS_Q_LastQ"}
L232     FCF_RENAME = {"fcf_ttm":"FCF_TTM"}
L233
L234     # === スキーマ簡易チェック（最低限） ===
L235     @staticmethod
L236     def _validate_ib_for_scorer(ib: Any):
L237         miss = [a for a in ["tickers","bench","data","px","spx","tickers_bulk","info","eps_df","fcf_df","returns"] if not hasattr(ib,a) or getattr(ib,a) is None]
L238         if miss: raise ValueError(f"InputBundle is missing required attributes for Scorer: {miss}")
L239         if any(c in ib.eps_df.columns for c in Scorer.EPS_RENAME): ib.eps_df.rename(columns=Scorer.EPS_RENAME, inplace=True)
L240         if any(c in ib.fcf_df.columns for c in Scorer.FCF_RENAME): ib.fcf_df.rename(columns=Scorer.FCF_RENAME, inplace=True)
L241         need_eps, need_fcf = {"EPS_TTM","EPS_Q_LastQ"},{"FCF_TTM"}
L242         if not need_eps.issubset(ib.eps_df.columns): raise ValueError(f"eps_df must contain columns {need_eps} (accepts old names via auto-rename). Got: {list(ib.eps_df.columns)}")
L243         if not need_fcf.issubset(ib.fcf_df.columns): raise ValueError(f"fcf_df must contain columns {need_fcf} (accepts old names via auto-rename). Got: {list(ib.fcf_df.columns)}")
L244
L245     # ----（Scorer専用）テクニカル・指標系 ----
L246     @staticmethod
L247     def trend(s: pd.Series):
L248         if len(s)<200: return np.nan
L249         sma50, sma150, sma200 = s.rolling(50).mean().iloc[-1], s.rolling(150).mean().iloc[-1], s.rolling(200).mean().iloc[-1]
L250         prev200, p = s.rolling(200).mean().iloc[-21], s.iloc[-1]
L251         lo_52 = s[-252:].min() if len(s)>=252 else s.min(); hi_52 = s[-252:].max() if len(s)>=252 else s.max()
L252         rng = (hi_52 - lo_52) if hi_52>lo_52 else np.nan
L253         clip = lambda x,lo,hi: (np.nan if pd.isna(x) else max(lo,min(hi,x)))
L254         a = clip(p/(s.rolling(50).mean().iloc[-1]) - 1, -0.5, 0.5)
L255         b = clip(sma50/sma150 - 1, -0.5, 0.5)
L256         c = clip(sma150/sma200 - 1, -0.5, 0.5)
L257         d = clip(sma200/prev200 - 1, -0.2, 0.2)
L258         e = clip((p - lo_52) / (rng if rng and rng>0 else np.nan) - 0.5, -0.5, 0.5)
L259         parts = [0.0 if pd.isna(x) else x for x in (a,b,c,d,e)]
L260         return 0.30*parts[0] + 0.20*parts[1] + 0.15*parts[2] + 0.15*parts[3] + 0.20*parts[4]
L261
L262     @staticmethod
L263     def rs(s, b):
L264         n, nb = len(s), len(b)
L265         if n<60 or nb<60: return np.nan
L266         L12 = 252 if n>=252 and nb>=252 else min(n,nb)-1; L1 = 22 if n>=22 and nb>=22 else max(5, min(n,nb)//3)
L267         r12, r1, br12, br1 = s.iloc[-1]/s.iloc[-L12]-1, s.iloc[-1]/s.iloc[-L1]-1, b.iloc[-1]/b.iloc[-L12]-1, b.iloc[-1]/b.iloc[-L1]-1
L268         return (r12 - br12)*0.7 + (r1 - br1)*0.3
L269
L270     @staticmethod
L271     def tr_str(s):
L272         if s is None:
L273             return np.nan
L274         s = s.ffill(limit=2).dropna()
L275         if len(s) < 50:
L276             return np.nan
L277         ma50 = s.rolling(50, min_periods=50).mean()
L278         last_ma = ma50.iloc[-1]
L279         last_px = s.iloc[-1]
L280         return float(last_px/last_ma - 1.0) if pd.notna(last_ma) and pd.notna(last_px) else np.nan
L281
L282     @staticmethod
L283     def rs_line_slope(s: pd.Series, b: pd.Series, win: int) -> float:
L284         r = (s/b).dropna()
L285         if len(r) < win: return np.nan
L286         y, x = np.log(r.iloc[-win:]), np.arange(win, dtype=float)
L287         try: return float(np.polyfit(x, y, 1)[0])
L288         except Exception: return np.nan
L289
L290     @staticmethod
L291     def ev_fallback(info_t: dict, tk: yf.Ticker) -> float:
L292         ev = info_t.get('enterpriseValue', np.nan)
L293         if pd.notna(ev) and ev>0: return float(ev)
L294         mc, debt, cash = info_t.get('marketCap', np.nan), np.nan, np.nan
L295         try:
L296             bs = tk.quarterly_balance_sheet
L297             if bs is not None and not bs.empty:
L298                 c = bs.columns[0]
L299                 for k in ("Total Debt","Long Term Debt","Short Long Term Debt"):
L300                     if k in bs.index: debt = float(bs.loc[k,c]); break
L301                 for k in ("Cash And Cash Equivalents","Cash And Cash Equivalents And Short Term Investments","Cash"):
L302                     if k in bs.index: cash = float(bs.loc[k,c]); break
L303         except Exception: pass
L304         if pd.notna(mc): return float(mc + (0 if pd.isna(debt) else debt) - (0 if pd.isna(cash) else cash))
L305         return np.nan
L306
L307     @staticmethod
L308     def dividend_status(ticker: str) -> str:
L309         t = yf.Ticker(ticker)
L310         try:
L311             if not t.dividends.empty: return "has"
L312         except Exception: return "unknown"
L313         try:
L314             a = t.actions
L315             if (a is not None and not a.empty and "Stock Splits" in a.columns and a["Stock Splits"].abs().sum()>0): return "none_confident"
L316         except Exception: pass
L317         try:
L318             fi = t.fast_info
L319             if any(getattr(fi,k,None) for k in ("last_dividend_date","dividend_rate","dividend_yield")): return "maybe_missing"
L320         except Exception: pass
L321         return "unknown"
L322
L323     @staticmethod
L324     def div_streak(t):
L325         try:
L326             divs = yf.Ticker(t).dividends.dropna(); ann = divs.groupby(divs.index.year).sum(); ann = ann[ann.index<pd.Timestamp.today().year]
L327             years, streak = sorted(ann.index), 0
L328             for i in range(len(years)-1,0,-1):
L329                 if ann[years[i]] > ann[years[i-1]]: streak += 1
L330                 else: break
L331             return streak
L332         except Exception: return 0
L333
L334     @staticmethod
L335     def fetch_finnhub_metrics(symbol):
L336         api_key = os.environ.get("FINNHUB_API_KEY")
L337         if not api_key: return {}
L338         url, params = "https://finnhub.io/api/v1/stock/metric", {"symbol":symbol,"metric":"all","token":api_key}
L339         try:
L340             r = requests.get(url, params=params, timeout=10); r.raise_for_status(); m = r.json().get("metric",{})
L341             return {'EPS':m.get('epsGrowthTTMYoy'),'REV':m.get('revenueGrowthTTMYoy'),'ROE':m.get('roeTTM'),'BETA':m.get('beta'),'DIV':m.get('dividendYieldIndicatedAnnual'),'FCF':(m.get('freeCashFlowTTM')/m.get('enterpriseValue')) if m.get('freeCashFlowTTM') and m.get('enterpriseValue') else None}
L342         except Exception: return {}
L343
L344     @staticmethod
L345     def calc_beta(series: pd.Series, market: pd.Series, lookback=252):
L346         r, m = series.pct_change().dropna(), market.pct_change().dropna()
L347         n = min(len(r), len(m), lookback)
L348         if n<60: return np.nan
L349         r, m = r.iloc[-n:], m.iloc[-n:]; cov, var = np.cov(r, m)[0,1], np.var(m)
L350         return np.nan if var==0 else cov/var
L351
L352     @staticmethod
L353     def spx_to_alpha(spx: pd.Series, bands=(0.03,0.10), w=(0.6,0.4),
L354                      span=5, q=(0.20,0.40), alphas=(0.05,0.08,0.10)) -> float:
L355         """
L356         S&P500指数のみから擬似breadthを作り、履歴分位でαを段階決定。
L357         bands=(±3%, ±10%), w=(50DMA,200DMA), 分位q=(20%,40%), alphas=(低,中,高)
L358         """
L359         ma50, ma200 = spx.rolling(50).mean(), spx.rolling(200).mean()
L360         b50, b200 = ((spx/ma50 - 1)+bands[0])/(2*bands[0]), ((spx/ma200 - 1)+bands[1])/(2*bands[1])
L361         hist = (w[0]*b50 + w[1]*b200).clip(0,1).ewm(span=span).mean()
L362         b, (lo, mid) = float(hist.iloc[-1]), (float(hist.quantile(q[0])), float(hist.quantile(q[1])))
L363         return alphas[0] if b < lo else alphas[1] if b < mid else alphas[2]
L364
L365     @staticmethod
L366     def soft_cap_effective_scores(scores: pd.Series|dict, sectors: dict, cap=2, alpha=0.08) -> pd.Series:
L367         """
L368         同一セクターcap超過（3本目以降）に α×段階減点を課した“有効スコア”Seriesを返す。
L369         戻り値は降順ソート済み。
L370         """
L371         s = pd.Series(scores, dtype=float); order = s.sort_values(ascending=False).index
L372         cnt, pen = {}, {}
L373         for t in order:
L374             sec = sectors.get(t, "U"); cnt[sec] = cnt.get(sec,0) + 1; pen[t] = alpha*max(0, cnt[sec]-cap)
L375         return (s - pd.Series(pen)).sort_values(ascending=False)
L376
L377     @staticmethod
L378     def pick_top_softcap(scores: pd.Series|dict, sectors: dict, N: int, cap=2, alpha=0.08, hard: int|None=5) -> list[str]:
L379         """
L380         soft-cap適用後の上位Nティッカーを返す。hard>0なら非常用ハード上限で同一セクター超過を間引く（既定=5）。
L381         """
L382         eff = Scorer.soft_cap_effective_scores(scores, sectors, cap, alpha)
L383         if not hard:
L384             return list(eff.head(N).index)
L385         pick, used = [], {}
L386         for t in eff.index:
L387             s = sectors.get(t, "U")
L388             if used.get(s,0) < hard:
L389                 pick.append(t); used[s] = used.get(s,0) + 1
L390             if len(pick) == N: break
L391         return pick
L392
L393     @staticmethod
L394     def trend_template_breadth_series(px: pd.DataFrame, spx: pd.Series, win_days: int | None = None) -> pd.Series:
L395         """
L396         各営業日の trend_template 合格本数（合格“本数”=C）を返す。
L397         - px: 列=ticker（ベンチは含めない）
L398         - spx: ベンチマーク Series（px.index に整列）
L399         - win_days: 末尾の計算対象営業日数（None→全体、既定600は呼び出し側指定）
L400         ベクトル化＆rollingのみで軽量。欠損は False 扱い。
L401         """
L402         import numpy as np, pandas as pd
L403         if px is None or px.empty:
L404             return pd.Series(dtype=int)
L405         px = px.dropna(how="all", axis=1)
L406         if win_days and win_days > 0:
L407             px = px.tail(win_days)
L408         if px.empty:
L409             return pd.Series(dtype=int)
L410         spx = spx.reindex(px.index).ffill()
L411
L412         ma50  = px.rolling(50).mean()
L413         ma150 = px.rolling(150).mean()
L414         ma200 = px.rolling(200).mean()
L415
L416         tt = (px > ma150)
L417         tt &= (px > ma200)
L418         tt &= (ma150 > ma200)
L419         tt &= (ma200 - ma200.shift(21) > 0)
L420         tt &= (ma50  > ma150)
L421         tt &= (ma50  > ma200)
L422         tt &= (px    > ma50)
L423
L424         lo252 = px.rolling(252).min()
L425         hi252 = px.rolling(252).max()
L426         tt &= (px.divide(lo252).sub(1.0) >= 0.30)   # P_OVER_LOW52 >= 0.30
L427         tt &= (px >= (0.75 * hi252))                # NEAR_52W_HIGH >= -0.25
L428
L429         r12  = px.divide(px.shift(252)).sub(1.0)
L430         br12 = spx.divide(spx.shift(252)).sub(1.0)
L431         r1   = px.divide(px.shift(22)).sub(1.0)
L432         br1  = spx.divide(spx.shift(22)).sub(1.0)
L433         rs   = 0.7*(r12.sub(br12, axis=0)) + 0.3*(r1.sub(br1, axis=0))
L434         tt &= (rs >= 0.10)
L435
L436         return tt.fillna(False).sum(axis=1).astype(int)
L437
L438     # ---- スコア集計（DTO/Configを受け取り、FeatureBundleを返す） ----
L439     def aggregate_scores(self, ib: Any, cfg):
L440         if cfg is None:
L441             raise ValueError("cfg is required; pass factor.PipelineConfig")
L442         self._validate_ib_for_scorer(ib)
L443
L444         px, spx, tickers = ib.px, ib.spx, ib.tickers
L445         tickers_bulk, info, eps_df, fcf_df = ib.tickers_bulk, ib.info, ib.eps_df, ib.fcf_df
L446
L447         debug_mode = bool(getattr(cfg, "debug_mode", False))
L448
L449         df, missing_logs = pd.DataFrame(index=tickers), []
L450         for t in tickers:
L451             d, s = info[t], px[t]; ev = self.ev_fallback(d, tickers_bulk.tickers[t])
L452             # --- 基本特徴 ---
L453             df.loc[t,'TR']   = self.trend(s)
L454             df.loc[t,'EPS']  = eps_df.loc[t,'EPS_TTM'] if t in eps_df.index else np.nan
L455             df.loc[t,'REV']  = d.get('revenueGrowth',np.nan)
L456             df.loc[t,'ROE']  = d.get('returnOnEquity',np.nan)
L457             df.loc[t,'BETA'] = self.calc_beta(s, spx, lookback=252)
L458
L459             # --- 配当（欠損補完含む） ---
L460             div = d.get('dividendYield') if d.get('dividendYield') is not None else d.get('trailingAnnualDividendYield')
L461             if div is None or pd.isna(div):
L462                 try:
L463                     divs = yf.Ticker(t).dividends
L464                     if divs is not None and not divs.empty:
L465                         last_close = s.iloc[-1]; div_1y = divs[divs.index >= (divs.index.max() - pd.Timedelta(days=365))].sum()
L466                         if last_close and last_close>0: div = float(div_1y/last_close)
L467                 except Exception: pass
L468             df.loc[t,'DIV'] = 0.0 if (div is None or pd.isna(div)) else float(div)
L469
L470             # --- FCF/EV ---
L471             fcf_val = fcf_df.loc[t,'FCF_TTM'] if t in fcf_df.index else np.nan
L472             df.loc[t,'FCF'] = (fcf_val/ev) if (pd.notna(fcf_val) and pd.notna(ev) and ev>0) else np.nan
L473
L474             # --- モメンタム・ボラ関連 ---
L475             df.loc[t,'RS'], df.loc[t,'TR_str'] = self.rs(s, spx), self.tr_str(s)
L476             r, rm = s.pct_change().dropna(), spx.pct_change().dropna()
L477             n = int(min(len(r), len(rm)))
L478
L479             DOWNSIDE_DEV = np.nan
L480             if n>=60:
L481                 r6 = r.iloc[-min(len(r),126):]; neg = r6[r6<0]
L482                 if len(neg)>=10: DOWNSIDE_DEV = float(neg.std(ddof=0)*np.sqrt(252))
L483             df.loc[t,'DOWNSIDE_DEV'] = DOWNSIDE_DEV
L484
L485             MDD_1Y = np.nan
L486             try:
L487                 w = s.iloc[-min(len(s),252):].dropna()
L488                 if len(w)>=30:
L489                     roll_max = w.cummax(); MDD_1Y = float((w/roll_max - 1.0).min())
L490             except Exception: pass
L491             df.loc[t,'MDD_1Y'] = MDD_1Y
L492
L493             RESID_VOL = np.nan
L494             if n>=120:
L495                 rr, rrm = r.iloc[-n:].align(rm.iloc[-n:], join='inner')
L496                 if len(rr)==len(rrm) and len(rr)>=120 and rrm.var()>0:
L497                     beta = float(np.cov(rr, rrm)[0,1]/np.var(rrm)); resid = rr - beta*rrm
L498                     RESID_VOL = float(resid.std(ddof=0)*np.sqrt(252))
L499             df.loc[t,'RESID_VOL'] = RESID_VOL
L500
L501             DOWN_OUTPERF = np.nan
L502             if n>=60:
L503                 m, x = rm.iloc[-n:], r.iloc[-n:]; mask = m<0
L504                 if mask.sum()>=10:
L505                     mr, sr = float(m[mask].mean()), float(x[mask].mean())
L506                     DOWN_OUTPERF = (sr - mr)/abs(mr) if mr!=0 else np.nan
L507             df.loc[t,'DOWN_OUTPERF'] = DOWN_OUTPERF
L508
L509             # --- 長期移動平均/位置 ---
L510             sma200 = s.rolling(200).mean(); df.loc[t,'EXT_200'] = np.nan
L511             if pd.notna(sma200.iloc[-1]) and sma200.iloc[-1]!=0: df.loc[t,'EXT_200'] = abs(float(s.iloc[-1]/sma200.iloc[-1]-1.0))
L512
L513             # --- 配当の詳細系 ---
L514             DIV_TTM_PS=DIV_VAR5=DIV_YOY=DIV_FCF_COVER=np.nan
L515             try:
L516                 divs = yf.Ticker(t).dividends.dropna()
L517                 if not divs.empty:
L518                     last_close = s.iloc[-1]; div_1y = float(divs[divs.index >= (divs.index.max()-pd.Timedelta(days=365))].sum())
L519                     DIV_TTM_PS = div_1y if div_1y>0 else np.nan
L520                     ann = divs.groupby(divs.index.year).sum()
L521                     if len(ann)>=2 and ann.iloc[-2]!=0: DIV_YOY = float(ann.iloc[-1]/ann.iloc[-2]-1.0)
L522                     tail = ann.iloc[-5:] if len(ann)>=5 else ann
L523                     if len(tail)>=3 and tail.mean()!=0: DIV_VAR5 = float(tail.std(ddof=1)/abs(tail.mean()))
L524                 so = d.get('sharesOutstanding',None)
L525                 if so and pd.notna(DIV_TTM_PS) and pd.notna(fcf_val) and fcf_val!=0:
L526                     DIV_FCF_COVER = float((fcf_val)/(DIV_TTM_PS*float(so)))
L527             except Exception: pass
L528             df.loc[t,'DIV_TTM_PS'], df.loc[t,'DIV_VAR5'], df.loc[t,'DIV_YOY'], df.loc[t,'DIV_FCF_COVER'] = DIV_TTM_PS, DIV_VAR5, DIV_YOY, DIV_FCF_COVER
L529
L530             # --- 財務安定性 ---
L531             df.loc[t,'DEBT2EQ'], df.loc[t,'CURR_RATIO'] = d.get('debtToEquity',np.nan), d.get('currentRatio',np.nan)
L532
L533             # --- EPS 変動 ---
L534             EPS_VAR_8Q = np.nan
L535             try:
L536                 qe, so = tickers_bulk.tickers[t].quarterly_earnings, d.get('sharesOutstanding',None)
L537                 if qe is not None and not qe.empty and so:
L538                     eps_q = (qe['Earnings'].dropna().astype(float)/float(so)).replace([np.inf,-np.inf],np.nan)
L539                     if len(eps_q)>=4: EPS_VAR_8Q = float(eps_q.iloc[-min(8,len(eps_q)):].std(ddof=1))
L540             except Exception: pass
L541             df.loc[t,'EPS_VAR_8Q'] = EPS_VAR_8Q
L542
L543             # --- サイズ/流動性 ---
L544             df.loc[t,'MARKET_CAP'] = d.get('marketCap',np.nan); adv60 = np.nan
L545             try:
L546                 vol_series = ib.data['Volume'][t].dropna()
L547                 if len(vol_series)>=5 and len(s)==len(vol_series):
L548                     dv = (vol_series*s).rolling(60).mean(); adv60 = float(dv.iloc[-1])
L549             except Exception: pass
L550             df.loc[t,'ADV60_USD'] = adv60
L551
L552             # --- 売上/利益の加速度等 ---
L553             REV_Q_YOY=EPS_Q_YOY=REV_YOY_ACC=REV_YOY_VAR=np.nan
L554             REV_YOY = REV_ANNUAL_STREAK = np.nan
L555             EPS_YOY = np.nan
L556             try:
L557                 qe, so = tickers_bulk.tickers[t].quarterly_earnings, d.get('sharesOutstanding',None)
L558                 rev_series = None
L559                 if qe is not None and not qe.empty and 'Revenue' in qe.columns:
L560                     rev_series = pd.to_numeric(qe['Revenue'], errors='coerce')
L561                 else:
L562                     qf = tickers_bulk.tickers[t].quarterly_financials
L563                     if qf is not None and not qf.empty and 'Total Revenue' in qf.index:
L564                         rev_series = pd.to_numeric(qf.loc['Total Revenue'], errors='coerce')
L565                         try:
L566                             rev_series = rev_series.sort_index()
L567                         except Exception:
L568                             pass
L569                     needs_fallback = True
L570                     if isinstance(rev_series, pd.Series):
L571                         needs_fallback = rev_series.dropna().empty
L572                     if needs_fallback:
L573                         fallback = _fetch_revenue_quarterly_via_finnhub(t)
L574                         if isinstance(fallback, pd.Series) and not fallback.dropna().empty:
L575                             rev_series = fallback
L576
L577                 if rev_series is not None and rev_series.dropna().shape[0] >= 2:
L578                     r = rev_series.dropna().astype(float)
L579                     yoy = r.pct_change(4).replace([np.inf, -np.inf], np.nan)
L580                     yoy_valid = yoy.dropna()
L581                     if not yoy_valid.empty:
L582                         REV_Q_YOY = float(yoy_valid.iloc[-1])
L583                         if len(yoy_valid) >= 2:
L584                             yoy_delta = yoy_valid.diff().dropna()
L585                             if not yoy_delta.empty:
L586                                 REV_YOY_ACC = float(yoy_delta.iloc[-1])
L587                             tail_len = min(4, len(yoy_valid))
L588                             tail = yoy_valid.iloc[-tail_len:]
L589                             if len(tail) >= 2:
L590                                 REV_YOY_VAR = float(tail.std(ddof=1))
L591                     if len(r) >= 8:
L592                         annual = r.rolling(4).sum().dropna()
L593                         if len(annual) >= 2:
L594                             prev = annual.iloc[-2]
L595                             if prev not in (None, 0) and not pd.isna(prev):
L596                                 REV_YOY = float((annual.iloc[-1] - prev) / prev)
L597                             streak_series = (annual.diff() > 0).astype(int).rolling(4, min_periods=1).sum()
L598                             if not streak_series.empty and pd.notna(streak_series.iloc[-1]):
L599                                 REV_ANNUAL_STREAK = int(streak_series.iloc[-1])
L600                 if qe is not None and not getattr(qe, "empty", True) and 'Earnings' in qe.columns and so:
L601                     eps_series = (qe['Earnings'].dropna().astype(float)/float(so)).replace([np.inf,-np.inf],np.nan)
L602                     if len(eps_series)>=5 and pd.notna(eps_series.iloc[-5]) and eps_series.iloc[-5]!=0:
L603                         EPS_Q_YOY = _safe_div(eps_series.iloc[-1]-eps_series.iloc[-5], eps_series.iloc[-5])
L604                         try:
L605                             g_eps = eps_series.groupby(eps_series.index.year)
L606                             ann_eps, cnt_eps = g_eps.sum(), g_eps.count()
L607                             ann_eps = ann_eps[cnt_eps >= 4]
L608                             if len(ann_eps) >= 2:
L609                                 eps_yoy = ann_eps.pct_change().dropna()
L610                                 if not eps_yoy.empty:
L611                                     EPS_YOY = float(eps_yoy.iloc[-1])
L612                         except Exception:
L613                             pass
L614             except Exception: pass
L615             df.loc[t,'REV_Q_YOY'], df.loc[t,'EPS_Q_YOY'] = REV_Q_YOY, EPS_Q_YOY
L616             df.loc[t,'REV_YOY_ACC'], df.loc[t,'REV_YOY_VAR'] = REV_YOY_ACC, REV_YOY_VAR
L617             df.loc[t,'REV_YOY'] = REV_YOY
L618             df.loc[t,'REV_ANN_STREAK'] = REV_ANNUAL_STREAK
L619             df.loc[t,'EPS_YOY'] = EPS_YOY
L620
L621             # --- Rule of 40 や周辺 ---
L622             total_rev_ttm = d.get('totalRevenue',np.nan)
L623             FCF_MGN = _safe_div(fcf_val, total_rev_ttm)
L624             df.loc[t,'FCF_MGN'] = FCF_MGN
L625             rule40 = np.nan
L626             try:
L627                 r = df.loc[t,'REV']; rule40 = (r if pd.notna(r) else np.nan) + (FCF_MGN if pd.notna(FCF_MGN) else np.nan)
L628             except Exception: pass
L629             df.loc[t,'RULE40'] = rule40
L630
L631             # --- トレンド補助 ---
L632             sma50  = s.rolling(50).mean()
L633             sma150 = s.rolling(150).mean()
L634             sma200 = s.rolling(200).mean()
L635             p = _safe_last(s)
L636
L637             df.loc[t,'MA50_OVER_150'] = (_safe_last(sma50)/_safe_last(sma150) - 1
L638                 if pd.notna(_safe_last(sma50)) and pd.notna(_safe_last(sma150)) and _safe_last(sma150)!=0 else np.nan)
L639             df.loc[t,'MA150_OVER_200'] = (_safe_last(sma150)/_safe_last(sma200) - 1
L640                 if pd.notna(_safe_last(sma150)) and pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan)
L641
L642             lo52 = s[-252:].min() if len(s)>=252 else s.min()
L643             df.loc[t,'P_OVER_LOW52'] = (p/lo52 - 1) if (lo52 and lo52>0 and pd.notna(p)) else np.nan
L644
L645             df.loc[t,'MA200_SLOPE_1M'] = np.nan
L646             if len(sma200.dropna()) >= 21:
L647                 cur200 = _safe_last(sma200)
L648                 old2001 = float(sma200.iloc[-21])
L649                 if old2001:
L650                     df.loc[t,'MA200_SLOPE_1M'] = cur200/old2001 - 1
L651
L652             df.loc[t,'P_OVER_150'] = p/_safe_last(sma150)-1 if pd.notna(_safe_last(sma150)) and _safe_last(sma150)!=0 else np.nan
L653             df.loc[t,'P_OVER_200'] = p/_safe_last(sma200)-1 if pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L654             df.loc[t,'MA50_OVER_200'] = _safe_last(sma50)/_safe_last(sma200)-1 if pd.notna(_safe_last(sma50)) and pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L655             df.loc[t,'MA200_SLOPE_5M'] = np.nan
L656             if len(sma200.dropna())>=105:
L657                 cur200, old200 = _safe_last(sma200), float(sma200.iloc[-105])
L658                 if old200 and old200!=0: df.loc[t,'MA200_SLOPE_5M'] = cur200/old200 - 1
L659             # NEW: 200日線が連続で上向きの「日数」
L660             df.loc[t,'MA200_UP_STREAK_D'] = np.nan
L661             try:
L662                 s200 = sma200.dropna()
L663                 if len(s200) >= 2:
L664                     diff200 = s200.diff()
L665                     up = 0
L666                     for v in diff200.iloc[::-1]:
L667                         if pd.isna(v) or v <= 0:
L668                             break
L669                         up += 1
L670                     df.loc[t,'MA200_UP_STREAK_D'] = float(up)
L671             except Exception:
L672                 pass
L673             df.loc[t,'LOW52PCT25_EXCESS'] = np.nan if (lo52 is None or lo52<=0 or pd.isna(p)) else (p/(lo52*1.25)-1)
L674             hi52 = s[-252:].max() if len(s)>=252 else s.max(); df.loc[t,'NEAR_52W_HIGH'] = np.nan
L675             if hi52 and hi52>0 and pd.notna(p):
L676                 d_hi = (p/hi52)-1.0; df.loc[t,'NEAR_52W_HIGH'] = -abs(min(0.0, d_hi))
L677             df.loc[t,'RS_SLOPE_6W'] = self.rs_line_slope(s, ib.spx, 30)
L678             df.loc[t,'RS_SLOPE_13W'] = self.rs_line_slope(s, ib.spx, 65)
L679
L680             df.loc[t,'DIV_STREAK'] = self.div_streak(t)
L681
L682             # --- 欠損メモ ---
L683             fin_cols = ['REV','ROE','BETA','DIV','FCF']
L684             need_finnhub = [col for col in fin_cols if pd.isna(df.loc[t,col])]
L685             if need_finnhub:
L686                 fin_data = self.fetch_finnhub_metrics(t)
L687                 for col in need_finnhub:
L688                     val = fin_data.get(col)
L689                     if val is not None and not pd.isna(val): df.loc[t,col] = val
L690             for col in fin_cols + ['EPS','RS','TR_str','DIV_STREAK']:
L691                 if pd.isna(df.loc[t,col]):
L692                     if col=='DIV':
L693                         status = self.dividend_status(t)
L694                         if status!='none_confident': missing_logs.append({'Ticker':t,'Column':col,'Status':status})
L695                     else:
L696                         missing_logs.append({'Ticker':t,'Column':col})
L697
L698         if debug_mode:
L699             rev_cols = [c for c in df.columns if c.startswith("REV_")]
L700             if rev_cols:
L701                 total = len(df.index)
L702                 if total:
L703                     parts = []
L704                     for col in sorted(rev_cols):
L705                         series = pd.to_numeric(df[col], errors="coerce")
L706                         valid = int(series.notna().sum())
L707                         nan_cnt = total - valid
L708                         nan_ratio = nan_cnt / total if total else float("nan")
L709                         parts.append(
L710                             f"{col}:ok={valid} nan={nan_cnt} nan%={nan_ratio:.2f}"
L711                         )
L712                     logger.info("[REV Summary] n=%d %s", total, " ".join(parts))
L713
L714         def _trend_template_pass(row, rs_alpha_thresh=0.10):
L715             c1 = (row.get('P_OVER_150', np.nan) > 0) and (row.get('P_OVER_200', np.nan) > 0)
L716             c2 = (row.get('MA150_OVER_200', np.nan) > 0)
L717             c3 = (row.get('MA200_SLOPE_1M', np.nan) > 0)
L718             c4 = (row.get('MA50_OVER_150', np.nan) > 0) and (row.get('MA50_OVER_200', np.nan) > 0)
L719             c5 = (row.get('TR_str', np.nan) > 0)
L720             c6 = (row.get('P_OVER_LOW52', np.nan) >= 0.30)
L721             c7 = (row.get('NEAR_52W_HIGH', np.nan) >= -0.25)
L722             c8 = (row.get('RS', np.nan) >= 0.10)
L723             return bool(c1 and c2 and c3 and c4 and c5 and c6 and c7 and c8)
L724
L725         if 'trend_template' not in df.columns: df['trend_template'] = df.apply(_trend_template_pass, axis=1).fillna(False)
L726         assert 'trend_template' in df.columns
L727
L728         # === Z化と合成 ===
L729         for col in ['ROE','FCF','REV','EPS']: df[f'{col}_W'] = winsorize_s(df[col], 0.02)
L730
L731         df_z = pd.DataFrame(index=df.index)
L732         for col in ['EPS','REV','ROE','FCF','RS','TR_str','BETA','DIV','DIV_STREAK']: df_z[col] = robust_z(df[col])
L733         df_z['REV'], df_z['EPS'], df_z['TR'] = robust_z(df['REV_W']), robust_z(df['EPS_W']), robust_z(df['TR'])
L734         for col in ['P_OVER_150','P_OVER_200','MA50_OVER_200','MA200_SLOPE_5M','LOW52PCT25_EXCESS','NEAR_52W_HIGH','RS_SLOPE_6W','RS_SLOPE_13W','MA200_UP_STREAK_D']: df_z[col] = robust_z(df[col])
L735
L736         # === Growth深掘り系（欠損保持z + RAW併載） ===
L737         grw_cols = ['REV_Q_YOY','EPS_Q_YOY','REV_YOY','EPS_YOY','REV_YOY_ACC','REV_YOY_VAR','FCF_MGN','RULE40','REV_ANN_STREAK']
L738         for col in grw_cols:
L739             if col in df.columns:
L740                 raw = pd.to_numeric(df[col], errors="coerce")
L741                 df_z[col] = robust_z_keepnan(raw)
L742                 df_z[f'{col}_RAW'] = raw
L743         for k in ("TREND_SLOPE_EPS", "TREND_SLOPE_REV"):
L744             if k in df.columns and k not in df_z.columns:
L745                 raw = pd.to_numeric(df[k], errors="coerce")
L746                 df_z[k] = robust_z_keepnan(raw)
L747                 df_z[f'{k}_RAW'] = raw
L748         for col in ['DOWNSIDE_DEV','MDD_1Y','RESID_VOL','DOWN_OUTPERF','EXT_200','DIV_TTM_PS','DIV_VAR5','DIV_YOY','DIV_FCF_COVER','DEBT2EQ','CURR_RATIO','EPS_VAR_8Q','MARKET_CAP','ADV60_USD']: df_z[col] = robust_z(df[col])
L749
L750         df_z['SIZE'], df_z['LIQ'] = robust_z(np.log1p(df['MARKET_CAP'])), robust_z(np.log1p(df['ADV60_USD']))
L751         df_z['QUALITY_F'] = robust_z(0.6*df['FCF_W'] + 0.4*df['ROE_W']).clip(-3.0,3.0)
L752         df_z['YIELD_F']   = 0.3*df_z['DIV'] + 0.7*df_z['DIV_STREAK']
L753
L754         # EPSが赤字でもFCFが黒字なら実質黒字とみなす
L755         eps_pos_mask = (df['EPS'] > 0) | (df['FCF_MGN'] > 0)
L756         df_z['EPS_POS'] = df_z['EPS'].where(eps_pos_mask, 0.0)
L757
L758         # ===== トレンドスロープ算出 =====
L759         def zpos(x):
L760             arr = robust_z(x)
L761             idx = getattr(x, 'index', df_z.index)
L762             return pd.Series(arr, index=idx).fillna(0.0)
L763
L764         def relu(x):
L765             ser = x if isinstance(x, pd.Series) else pd.Series(x, index=df_z.index)
L766             return ser.clip(lower=0).fillna(0.0)
L767
L768         # 売上トレンドスロープ（四半期）
L769         slope_rev = 0.70*zpos(df_z['REV_Q_YOY']) + 0.30*zpos(df_z['REV_YOY_ACC'])
L770         noise_rev = relu(robust_z(df_z['REV_YOY_VAR']) - 0.8)
L771         slope_rev_combo = slope_rev - 0.25*noise_rev
L772         df_z['TREND_SLOPE_REV_RAW'] = slope_rev_combo
L773         df_z['TREND_SLOPE_REV'] = slope_rev_combo.clip(-3.0, 3.0)
L774
L775         # EPSトレンドスロープ（四半期）
L776         slope_eps = 0.60*zpos(df_z['EPS_Q_YOY']) + 0.40*zpos(df_z['EPS_POS'])
L777         df_z['TREND_SLOPE_EPS_RAW'] = slope_eps
L778         df_z['TREND_SLOPE_EPS'] = slope_eps.clip(-3.0, 3.0)
L779
L780         # 年次トレンド（サブ）
L781         slope_rev_yr = zpos(df_z['REV_YOY'])
L782         slope_eps_yr = zpos(df_z.get('EPS_YOY', pd.Series(0.0, index=df.index)))
L783         streak_base = df['REV_ANN_STREAK'].clip(lower=0).fillna(0)
L784         streak_yr = streak_base / (streak_base.abs() + 1.0)
L785         slope_rev_yr_combo = 0.7*slope_rev_yr + 0.3*streak_yr
L786         df_z['TREND_SLOPE_REV_YR_RAW'] = slope_rev_yr_combo
L787         df_z['TREND_SLOPE_REV_YR'] = slope_rev_yr_combo.clip(-3.0, 3.0)
L788         df_z['TREND_SLOPE_EPS_YR_RAW'] = slope_eps_yr
L789         df_z['TREND_SLOPE_EPS_YR'] = slope_eps_yr.clip(-3.0, 3.0)
L790
L791         if debug_mode:
L792             slope_cols = [c for c in df_z.columns if c.startswith("TREND_SLOPE_")]
L793             if slope_cols:
L794                 total = len(df_z.index)
L795                 parts = []
L796                 for col in sorted(slope_cols):
L797                     series = pd.to_numeric(df_z[col], errors="coerce")
L798                     nan_cnt = int(series.isna().sum())
L799                     zero_cnt = int((series == 0).sum())
L800                     parts.append(f"{col}:0={zero_cnt} nan={nan_cnt}")
L801                 logger.info("[SLOPE Summary] n=%d %s", total, " ".join(parts))
L802
L803         # ===== 新GRW合成式（SEPA寄りシフト） =====
L804         _nz = lambda name: df_z.get(name, pd.Series(0.0, index=df_z.index)).fillna(0.0)
L805         grw_combo = (
L806               0.20*_nz('REV_Q_YOY')
L807             + 0.10*_nz('REV_YOY_ACC')
L808             + 0.10*_nz('REV_ANN_STREAK')
L809             - 0.05*_nz('REV_YOY_VAR')
L810             + 0.10*_nz('TREND_SLOPE_REV')
L811             + 0.15*_nz('EPS_Q_YOY')
L812             + 0.05*_nz('EPS_POS')
L813             + 0.20*_nz('TREND_SLOPE_EPS')
L814             + 0.05*_nz('TREND_SLOPE_REV_YR')
L815             + 0.03*_nz('TREND_SLOPE_EPS_YR')
L816             + 0.10*_nz('FCF_MGN')
L817             + 0.05*_nz('RULE40')
L818         )
L819         df_z['GROWTH_F_RAW'] = grw_combo
L820         df_z['GROWTH_F'] = robust_z(grw_combo).clip(-3.0, 3.0)
L821
L822         # Debug dump for GRW composition (console OFF by default; enable only with env)
L823         if bool(os.getenv("GRW_CONSOLE_DEBUG")):
L824             try:
L825                 i = df_z[['GROWTH_F', 'GROWTH_F_RAW']].copy()
L826                 i.sort_values('GROWTH_F', ascending=False, inplace=True)
L827                 limit = max(0, min(40, len(i)))
L828                 print("[DEBUG: GRW]")
L829                 for t in i.index[:limit]:
L830                     row = i.loc[t]
L831                     parts = [f"GROWTH_F={row['GROWTH_F']:.3f}"]
L832                     if pd.notna(row.get('GROWTH_F_RAW')):
L833                         parts.append(f"GROWTH_F_RAW={row['GROWTH_F_RAW']:.3f}")
L834                     print(f"Ticker: {t} | " + " ".join(parts))
L835                 print()
L836             except Exception as exc:
L837                 print(f"[ERR] GRW debug dump failed: {exc}")
L838
L839         df_z['MOM_F'] = robust_z(0.40*df_z['RS']
L840             + 0.15*df_z['TR_str']
L841             + 0.15*df_z['RS_SLOPE_6W']
L842             + 0.15*df_z['RS_SLOPE_13W']
L843             + 0.10*df_z['MA200_SLOPE_5M']
L844             + 0.10*df_z['MA200_UP_STREAK_D']).clip(-3.0,3.0)
L845         df_z['VOL'] = robust_z(df['BETA'])
L846         df_z['QAL'], df_z['YLD'], df_z['MOM'] = df_z['QUALITY_F'], df_z['YIELD_F'], df_z['MOM_F']
L847         df_z.drop(columns=['QUALITY_F','YIELD_F','MOM_F'], inplace=True, errors='ignore')
L848
L849         _dump_dfz(df_z=df_z, debug_mode=getattr(cfg, "debug_mode", False))
L850
L851         # === begin: BIO LOSS PENALTY =====================================
L852         try:
L853             penalty_z = float(os.getenv("BIO_LOSS_PENALTY_Z", "0.8"))
L854         except Exception:
L855             penalty_z = 0.8
L856
L857         def _is_bio_like(t: str) -> bool:
L858             inf = info.get(t, {}) if isinstance(info, dict) else {}
L859             sec = str(inf.get("sector", "")).lower()
L860             ind = str(inf.get("industry", "")).lower()
L861             if "health" not in sec:
L862                 return False
L863             keys = ("biotech", "biopharma", "pharma")
L864             return any(k in ind for k in keys)
L865
L866         tickers_s = pd.Index(df_z.index)
L867         is_bio = pd.Series({t: _is_bio_like(t) for t in tickers_s})
L868         is_loss = pd.Series({t: (pd.notna(df.loc[t,"EPS"]) and df.loc[t,"EPS"] <= 0) for t in tickers_s})
L869         mask_bio_loss = (is_bio & is_loss).reindex(df_z.index).fillna(False)
L870
L871         if bool(mask_bio_loss.any()) and penalty_z > 0:
L872             df_z.loc[mask_bio_loss, "GROWTH_F"] = df_z.loc[mask_bio_loss, "GROWTH_F"] - penalty_z
L873             df_z["GROWTH_F"] = df_z["GROWTH_F"].clip(-3.0, 3.0)
L874         # === end: BIO LOSS PENALTY =======================================
L875
L876         df_z['TRD'] = 0.0  # TRDはスコア寄与から外し、テンプレ判定はフィルタで行う（列は表示互換のため残す）
L877         if 'BETA' not in df_z.columns: df_z['BETA'] = robust_z(df['BETA'])
L878
L879         df_z['D_VOL_RAW'] = robust_z(0.40*df_z['DOWNSIDE_DEV'] + 0.22*df_z['RESID_VOL'] + 0.18*df_z['MDD_1Y'] - 0.10*df_z['DOWN_OUTPERF'] - 0.05*df_z['EXT_200'] - 0.08*df_z['SIZE'] - 0.10*df_z['LIQ'] + 0.10*df_z['BETA'])
L880         df_z['D_QAL']     = robust_z(0.35*df_z['QAL'] + 0.20*df_z['FCF'] + 0.15*df_z['CURR_RATIO'] - 0.15*df_z['DEBT2EQ'] - 0.15*df_z['EPS_VAR_8Q'])
L881         df_z['D_YLD']     = robust_z(0.45*df_z['DIV'] + 0.25*df_z['DIV_STREAK'] + 0.20*df_z['DIV_FCF_COVER'] - 0.10*df_z['DIV_VAR5'])
L882         df_z['D_TRD']     = robust_z(0.40*df_z.get('MA200_SLOPE_5M',0) - 0.30*df_z.get('EXT_200',0) + 0.15*df_z.get('NEAR_52W_HIGH',0) + 0.15*df_z['TR'])
L883
L884         # --- 重みは cfg を優先（外部があればそれを使用） ---
L885         # ① 全銘柄で G/D スコアを算出（unmasked）
L886         g_score_all = df_z.mul(pd.Series(cfg.weights.g)).sum(axis=1)
L887
L888         d_comp = pd.concat({
L889             'QAL': df_z['D_QAL'],
L890             'YLD': df_z['D_YLD'],
L891             'VOL': df_z['D_VOL_RAW'],
L892             'TRD': df_z['D_TRD']
L893         }, axis=1)
L894         dw = pd.Series(cfg.weights.d, dtype=float).reindex(['QAL','YLD','VOL','TRD']).fillna(0.0)
L895         globals()['D_WEIGHTS_EFF'] = dw.copy()
L896         d_score_all = d_comp.mul(dw, axis=1).sum(axis=1)
L897
L898         # ② テンプレ判定（既存ロジックそのまま）
L899         mask = df['trend_template']
L900         if not bool(mask.any()):
L901             mask = ((df.get('P_OVER_LOW52', np.nan) >= 0.25) &
L902                 (df.get('NEAR_52W_HIGH', np.nan) >= -0.30) &
L903                 (df.get('RS', np.nan) >= 0.08) &
L904                 (df.get('MA200_SLOPE_1M', np.nan) > 0) &
L905                 (df.get('P_OVER_150', np.nan) > 0) & (df.get('P_OVER_200', np.nan) > 0) &
L906                 (df.get('MA150_OVER_200', np.nan) > 0) &
L907                 (df.get('MA50_OVER_150', np.nan) > 0) & (df.get('MA50_OVER_200', np.nan) > 0) &
L908                 (df.get('TR_str', np.nan) > 0)).fillna(False)
L909             df['trend_template'] = mask
L910
L911         # ③ 採用用は mask、表示/分析用は列で全銘柄保存
L912         g_score = g_score_all.loc[mask]
L913         Scorer.g_score = g_score
L914         df_z['GSC'] = g_score_all
L915         df_z['DSC'] = d_score_all
L916
L917         try:
L918             current = (pd.read_csv("current_tickers.csv")
L919                   .iloc[:, 0]
L920                   .str.upper()
L921                   .tolist())
L922         except FileNotFoundError:
L923             warnings.warn("current_tickers.csv not found — bonus skipped")
L924             current = []
L925
L926         mask_bonus = g_score.index.isin(current)
L927         if mask_bonus.any():
L928             # 1) factor.BONUS_COEFF から k を決め、無ければ 0.4
L929             k = float(getattr(sys.modules.get("factor"), "BONUS_COEFF", 0.4))
L930             # 2) g 側の σ を取り、NaN なら 0 に丸める
L931             sigma_g = g_score.std()
L932             if pd.isna(sigma_g):
L933                 sigma_g = 0.0
L934             bonus_g = round(k * sigma_g, 3)
L935             g_score.loc[mask_bonus] += bonus_g
L936             Scorer.g_score = g_score
L937             # 3) D 側も同様に σ の NaN をケア
L938             sigma_d = d_score_all.std()
L939             if pd.isna(sigma_d):
L940                 sigma_d = 0.0
L941             bonus_d = round(k * sigma_d, 3)
L942             d_score_all.loc[d_score_all.index.isin(current)] += bonus_d
L943
L944         try:
L945             df = _apply_growth_entry_flags(df, ib, self, win_breakout=5, win_pullback=5)
L946         except Exception:
L947             pass
L948
L949         df_full = df.copy()
L950         df_full_z = df_z.copy()
L951
L952         from factor import FeatureBundle  # type: ignore  # 実行時importなし（循環回避）
L953         return FeatureBundle(df=df,
L954             df_z=df_z,
L955             g_score=g_score,
L956             d_score_all=d_score_all,
L957             missing_logs=pd.DataFrame(missing_logs),
L958             df_full=df_full,
L959             df_full_z=df_full_z,
L960             scaler=None)
L961
L962 def _apply_growth_entry_flags(feature_df, bundle, self_obj, win_breakout=5, win_pullback=5):
L963     """
L964     G枠ユニバースに対し、ブレイクアウト確定/押し目反発の「直近N営業日内の発火」を判定し、
L965     次の列を feature_df に追加する（index=ticker）。
L966       - G_BREAKOUT_recent_5d : bool
L967       - G_BREAKOUT_last_date : str "YYYY-MM-DD"
L968       - G_PULLBACK_recent_5d : bool
L969       - G_PULLBACK_last_date : str "YYYY-MM-DD"
L970       - G_PIVOT_price        : float
L971     失敗しても例外は握り潰し、既存処理を阻害しない。
L972     """
L973     try:
L974         px   = bundle.px                      # 終値 DataFrame
L975         hi   = bundle.data['High']
L976         lo   = bundle.data['Low']
L977         vol  = bundle.data['Volume']
L978         bench= bundle.spx                     # ベンチマーク Series
L979
L980         # Gユニバース推定：self.g_universe 優先 → feature_df['group']=='G' → 全銘柄
L981         g_universe = getattr(self_obj, "g_universe", None)
L982         if g_universe is None:
L983             try:
L984                 g_universe = feature_df.index[feature_df['group'].astype(str).str.upper().eq('G')].tolist()
L985             except Exception:
L986                 g_universe = list(feature_df.index)
L987         if not g_universe:
L988             return feature_df
L989
L990         # 指標
L991         px = px.ffill(limit=2)
L992         ema21 = px[g_universe].ewm(span=21, adjust=False).mean()
L993         ma50  = px[g_universe].rolling(50).mean()
L994         ma150 = px[g_universe].rolling(150).mean()
L995         ma200 = px[g_universe].rolling(200).mean()
L996         atr20 = (hi[g_universe] - lo[g_universe]).rolling(20).mean()
L997         vol20 = vol[g_universe].rolling(20).mean()
L998         vol50 = vol[g_universe].rolling(50).mean()
L999
L1000         # トレンドテンプレート合格
L1001         trend_template_ok = (px[g_universe] > ma50) & (px[g_universe] > ma150) & (px[g_universe] > ma200) \
L1002                             & (ma150 > ma200) & (ma200.diff() > 0)
L1003
L1004         # 汎用ピボット：直近65営業日の高値（当日除外）
L1005         pivot_price = hi[g_universe].rolling(65).max().shift(1)
L1006
L1007         # 相対力：年内高値更新
L1008         bench_aligned = bench.reindex(px.index).ffill()
L1009         rs = px[g_universe].div(bench_aligned, axis=0)
L1010         rs_high = rs.rolling(252).max().shift(1)
L1011
L1012         # ブレイクアウト「発生日」：条件立ち上がり
L1013         breakout_today = trend_template_ok & (px[g_universe] > pivot_price) \
L1014                          & (vol[g_universe] >= 1.5 * vol50) & (rs > rs_high)
L1015         breakout_event = breakout_today & ~breakout_today.shift(1).fillna(False)
L1016
L1017         # 押し目反発「発生日」：EMA21帯×出来高ドライアップ×前日高値越え×終値EMA21上
L1018         near_ema21_band = px[g_universe].between(ema21 - atr20, ema21 + atr20)
L1019         volume_dryup = (vol20 / vol50) <= 1.0
L1020         pullback_bounce_confirmed = (px[g_universe] > hi[g_universe].shift(1)) & (px[g_universe] > ema21)
L1021         pullback_today = trend_template_ok & near_ema21_band & volume_dryup & pullback_bounce_confirmed
L1022         pullback_event = pullback_today & ~pullback_today.shift(1).fillna(False)
L1023
L1024         # 直近N営業日内の発火 / 最終発生日
L1025         rows = []
L1026         for t in g_universe:
L1027             def _recent_and_date(s, win):
L1028                 sw = s[t].iloc[-win:]
L1029                 if sw.any():
L1030                     d = sw[sw].index[-1]
L1031                     return True, d.strftime("%Y-%m-%d")
L1032                 return False, ""
L1033             br_recent, br_date = _recent_and_date(breakout_event, win_breakout)
L1034             pb_recent, pb_date = _recent_and_date(pullback_event, win_pullback)
L1035             rows.append((t, {
L1036                 "G_BREAKOUT_recent_5d": br_recent,
L1037                 "G_BREAKOUT_last_date": br_date,
L1038                 "G_PULLBACK_recent_5d": pb_recent,
L1039                 "G_PULLBACK_last_date": pb_date,
L1040                 "G_PIVOT_price": float(pivot_price[t].iloc[-1]) if t in pivot_price.columns else float('nan'),
L1041             }))
L1042         flags = pd.DataFrame({k: v for k, v in rows}).T
L1043
L1044         # 列を作成・上書き
L1045         cols = ["G_BREAKOUT_recent_5d","G_BREAKOUT_last_date","G_PULLBACK_recent_5d","G_PULLBACK_last_date","G_PIVOT_price"]
L1046         for c in cols:
L1047             if c not in feature_df.columns:
L1048                 feature_df[c] = np.nan
L1049         feature_df.loc[flags.index, flags.columns] = flags
L1050
L1051     except Exception:
L1052         pass
L1053     return feature_df
L1054
```

## <.github/workflows/weekly-report.yml>
```text
L1 name: Weekly Stock Report
L2
L3 on:
L4   push:
L5     branches: [ main ]
L6     paths-ignore:
L7       - 'CodeForChat/**'
L8   schedule:
L9     - cron: '0 0 * * 6'  # UTC 00:00 → JST 09:00（土）
L10   workflow_dispatch:
L11
L12 jobs:
L13   build-and-report:
L14     runs-on: ubuntu-latest
L15     permissions:
L16       contents: write
L17
L18     steps:
L19       - name: Debug start
L20         run: echo '🚀 DEBUGstarted'
L21               
L22       - name: Checkout repository
L23         uses: actions/checkout@v3
L24
L25       - name: Setup Python
L26         uses: actions/setup-python@v5
L27         with:
L28           python-version: '3.x'
L29           cache: 'pip'
L30           cache-dependency-path: requirements.txt
L31
L32       - name: Install dependencies
L33         run: pip install -r requirements.txt
L34
L35       - name: Prepare results directory
L36         run: mkdir -p results
L37
L38       - name: Run factor & scoring
L39         env:
L40           SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
L41           FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
L42           FIN_THREADS: "8"
L43         run: python factor.py
```

## <documents/README.md>
```text
L1 # 運用ルール
L2
L3 ## 基本構成
L4 - 20銘柄を均等配分（現金を除き1銘柄あたり5%）
L5 - moomoo証券で運用
L6 - **Growth枠 12銘柄 / Defense枠 8銘柄**（NORMAL 基準）
L7
L8 ## Barbell Growth-Defense方針
L9 - Growth枠 **12銘柄**：高成長で乖離源となる攻めの銘柄
L10 - Defense枠 **8銘柄**：低ボラで安定成長し配当を増やす守りの銘柄
L11 - 「猛烈に伸びる攻め × 着実に稼ぐ盾」の組合せで乖離→半戻しプレミアムを狙う
L12
L13 ## レジーム判定（trend_template 合格“本数”で判定）
L14 - 合格本数 = current+candidate 全体のうち、trend_template 条件を満たした銘柄の**本数(C)**（基準 N_G=12）
L15 - しきい値は過去~600営業日の分布から**毎回自動採用**（分位点と運用“床”のmax）
L16   - 緊急入り: `max(q05, 12本)`（= N_G）
L17   - 緊急解除: `max(q20, 18本)`（= ceil(1.5×12)）
L18   - 通常復帰: `max(q60, 36本)`（= 3×N_G）
L19 - ヒステリシス: 前回モードに依存（EMERG→解除は23本以上、CAUTION→通常は45本以上）
L20
L21 ## レジーム別の現金・ドリフト
L22  - **通常(NORMAL)** : 現金 **10%** / ドリフト閾値 **12%**
L23  - **警戒(CAUTION)** : 現金 **12.5%** / ドリフト閾値 **14%**
L24  - **緊急(EMERG)** : 現金 **20%** / **ドリフト売買停止**（20×5%に全戻しのみ）
L25
L26 ## モード別の推奨“保有銘柄数”（MMF≒現金）
L27 *各枠=5%（20銘柄均等）。モード移行時は**Gの枠数のみ**調整し、外した枠は現金として保持。*
L28
L29 - **NORMAL:** G **12** / D **8** / 現金化枠 **0**  
L30 - **CAUTION:** G **10** / D **8** / 現金化枠 **2**（= 10%）  
L31 - **EMERG:** G **8**  / D **8** / 現金化枠 **4**（= 20%）  
L32
L33 > 実運用：⭐️低スコアのGから順に外す。解除時はfactor上位から補充。
L34
L35 ## トレーリングストップ
L36 - **基本TS (モード別):** NORMAL **15%** / CAUTION **13%** / EMERG **10%**
L37 - 含み益が **+30% / +60% / +100%** 到達で、基本から **-3pt / -6pt / -8pt** 引き上げ
L38 - TS発動で減少した銘柄は翌日以降に補充（※緊急モード中は補充しない）
L39
L40 ## 半戻し（リバランス）手順
L41 ドリフトチェックで**アラート**が出た場合（合計|drift| がモード閾値を超過、EMERG除く）、翌営業日の米国寄付きで下記を実施する。
L42
L43 1. **売却（必須）**  
L44    Slackテーブルの **Δqty がマイナスの銘柄を売却** する（寄付き成行推奨）。  
L45    これは「半戻し」計算に基づく過重量の削減を意味する。
L46
L47 2. **購入（任意・半戻し目安）**  
L48    半戻し後の合計|drift|を**シミュレーション値（Slackヘッダに表示）**に近づけることを目安に、  
L49    **任意の銘柄を買い増し**してバランスを取る（Δqtyがプラスの銘柄を優先してもよい）。
L50
L51 3. **トレーリングストップの再設定（必須）**  
L52    すべての保有銘柄について、最新の評価額に合わせてTSを**再発注／更新**する。  
L53    ルールは下記（利益到達で段階的にタイト化）：  
L54    - **基本TS:** -15%  
L55    - **+30% 到達 → TS -12%**  
L56    - **+60% 到達 → TS -9%**  
L57    - **+100% 到達 → TS -7%**  
L58    ※ストップ価格の引き上げは許可、**引き下げは不可**（利益保全の原則）。
L59
L60 4. **例外（EMERGモード）**  
L61    緊急(EMERG)では**ドリフト由来の売買は停止（∞）**。20銘柄×各5%への**全戻し**のみ許容。
L62
L63 5. **実行タイミング**
L64    - 判定：米国市場終値直後
L65    - 執行：翌営業日の米国寄付き成行
L66
L67 ## モード移行の実務手順（超シンプル）
L68 モードが変わったら、**MMF≒現金**として扱い、**Gの枠数だけ**を調整する：
L69 1. **Gを削る**（CAUTION/EMERG）  
L70    - ⭐️低スコアのGから順に外す。  
L71    - **`current_tickers.csv` から外すG銘柄の行を削除**（＝その枠は現金化）。
L72 2. **現金として保持**  
L73    - 外した枠は現金（またはMMF相当）でプール。  
L74 3. **復帰時の補充**（NORMALへ）  
L75    - **`current_tickers.csv` に銘柄を追加**（factor上位から）。  
L76    - 以降は日次ドリフト/TSルールに従う。
L77
L78 > driftは `target_ratio = 1/銘柄数` を自動適用。行数に応じて自動で均等比率が再計算される。
L79
L80 ## 入替銘柄選定
L81 - Oxfordキャピタル／インカム、Alpha Investor、Motley Fool Stock Advisor、moomooスクリーニング等を参考にchatGPTで検討
L82 - 年間NISA枠はGrowth群の中から低ボラ銘柄を選定し利用。長期保持にはこだわらない。
L83
L84 ## 再エントリー（クールダウン）
L85 - TSヒット後の同銘柄再INは **8営業日** のクールダウンを設ける（期間中は再IN禁止）
L86
L87 ## 実行タイミング
L88 - 判定：米国市場終値直後
L89 - 執行：翌営業日の米国寄付き成行
```

## <documents/factor_design.md>
```text
L1 # factor.py 詳細設計書
L2
L3 ## 概要
L4 - 既存ポートフォリオの銘柄と検討中の銘柄群を同時に扱う銘柄選定パイプライン。
L5 - 価格・財務データを取り込みスコアリングとDRRS選定を行うことで、以下のアウトプットを得る。
L6   - 採用銘柄と惜しくも漏れた銘柄のスコア一覧
L7   - IN/OUTのティッカーリストとOUT側の低スコア銘柄
L8   - 新旧ポートフォリオの比較表
L9   - 検討中銘柄の低スコアランキング（整理用）
L10
L11 ## 全体フロー
L12 1. **Input** – `current_tickers.csv`と`candidate_tickers.csv`を読み込み、yfinanceやFinnhubのAPIから価格・財務データを収集して`InputBundle`を整備。
L13 2. **Score Calculation** – Scorerが特徴量を計算し因子スコアを合成して`FeatureBundle`を生成。
L14 3. **Correlation Reduction & Selection** – SelectorがDRRSロジックで相関を抑えつつG/D銘柄を選定し`SelectionBundle`を得る。
L15 4. **Output** – 採用結果と周辺情報を表・Slack通知として出力。
L16
L17 ```mermaid
L18 flowchart LR
L19   A[Input\nAPI & 前処理] --> B[Score Calculation\n特徴量・因子合成]
L20   B --> C[Correlation Reduction\nDRRS選定]
L21   C --> D[Output\nSlack通知]
L22 ```
L23
L24 ## 定数・設定
L25 | 変数 | 内容 | 主な用途 |
L26 | --- | --- | --- |
L27 | `exist` / `cand` | 現行ポートフォリオと検討中銘柄のティッカーリスト | スコア対象ユニバースの構成、候補整理 |
L28 | `bench` | ベンチマークティッカー | 相対強さ・β算出、ポート比較 |
L29 | `CAND_PRICE_MAX` | 候補銘柄の許容価格上限 | 高額銘柄の事前除外 |
L30 | `N_G` / `N_D` | G/D採用枠の件数（**既定: 12 / 8**） | 最終的に選ぶ銘柄数の制約 |
L31 | `g_weights` / `D_weights` | 各因子の重みdict | G/Dスコア合成 |
L32 | `D_BETA_MAX` | Dバケットの許容β上限 | 高β銘柄の除外フィルタ |
L33 | `FILTER_SPEC` | G/Dごとの前処理フィルタ | トレンドマスクやβ上限設定 |
L34 | `corrM` | DRRS初期プールの最大件数 | 相関行列サイズ制御 |
L35 | `DRRS_G` / `DRRS_D` | DRRSパラメータdict | バケット別の相関低減設定 |
L36 | `DRRS_SHRINK` | 残差相関の対角シュリンク率 | `residual_corr`の安定化 |
L37 | `CROSS_MU_GD` | G-D間クロス相関ペナルティμ | 2バケット同時最適化で相関抑制 |
L38 | `RESULTS_DIR` | 選定結果保存ディレクトリ | `_save_sel`/`_load_prev`の入出力 |
L39
L40 選定結果は`results/`配下にJSONとして保存し、次回実行時に`_load_prev`で読み込んで選定条件に反映。
L41
L42 ## DTO/Config
L43 各ステップ間で受け渡すデータ構造と設定値。変数の意味合いと利用箇所を以下に示す。
L44
L45 ### InputBundle（Input → Scorer）
L46 | 変数 | 内容 | 主な用途 |
L47 | --- | --- | --- |
L48 | `cand` | 候補銘柄ティッカーのリスト | OUTテーブルや低スコアランキング対象の母集団 |
L49 | `tickers` | 現行+候補を合わせたティッカー一覧 | 価格・出来高ダウンロード、リターン計算 |
L50 | `bench` | ベンチマークティッカー | 相対強さ・β算出、ポート比較 |
L51 | `data` | yfinanceのダウンロード結果（階層列） | `px`/`spx`/リターン等の基礎データ |
L52 | `px` | `data['Close']`だけを抜き出した価格系列 | 指標計算・リターン生成 |
L53 | `spx` | `data['Close'][bench]` のSeries | `rs`や`calc_beta`の基準指数 |
L54 | `tickers_bulk` | `yf.Tickers`オブジェクト | `info`等の一括取得 |
L55 | `info` | ティッカー別のyfinance情報dict | セクター判定やEPS補完 |
L56 | `eps_df` | EPS TTM/直近EPS等をまとめた表 | 成長指標の算出 |
L57 | `fcf_df` | CFO・CapEx・FCF TTMと情報源フラグ | FCF/EVや配当カバレッジ |
L58 | `returns` | `px.pct_change()`のリターン表 | 相関行列・DRRS計算 |
L59
L60 ### FeatureBundle（Scorer → Selector）
L61 | 変数 | 内容 | 主な用途 |
L62 | --- | --- | --- |
L63 | `df` | 計算済み指標の生値テーブル | デバッグ・出力表示 |
L64 | `df_z` | ウィンザー後Zスコア化した指標表 | 因子スコア合成、選定基準 |
L65 | `g_score` | Gバケット総合スコア | G選定、IN/OUT比較 |
L66 | `d_score_all` | Dバケット総合スコア（全銘柄） | D選定、低スコアランキング |
L67 | `missing_logs` | 欠損指標と補完状況のログ | データ品質チェック |
L68
L69 ### SelectionBundle（Selector → Output）
L70 | 変数 | 内容 | 主な用途 |
L71 | --- | --- | --- |
L72 | `resG` | G選定結果の詳細dict（`tickers`、目的値等） | 結果保存・平均相関などの指標表示 |
L73 | `resD` | D選定結果の詳細dict | 同上 |
L74 | `top_G` | 最終採用Gティッカー | 新ポートフォリオ構築 |
L75 | `top_D` | 最終採用Dティッカー | 同上 |
L76 | `init_G` | DRRS前のG初期候補 | 惜しくも外れた銘柄表示 |
L77 | `init_D` | DRRS前のD初期候補 | 同上 |
L78
L79 ### WeightsConfig
L80 | 変数 | 内容 | 主な用途 |
L81 | --- | --- | --- |
L82 | `g` | G因子（GRW/MOM/VOL）の重みdict | `g_score`合成 |
L83 | `d` | D因子（D_QAL/D_YLD/D_VOL_RAW/D_TRD）の重みdict | `d_score_all`合成 |
L84
L85 ### DRRSParams
L86 | 変数 | 内容 | 主な用途 |
L87 | --- | --- | --- |
L88 | `corrM` | DRRS初期プールの最大件数 | 相関行列サイズ制御 |
L89 | `shrink` | 残差相関のシュリンク率 | `residual_corr`の対角強調 |
L90 | `G` | Gバケット用パラメータdict（`lookback`等） | `select_bucket_drrs`設定 |
L91 | `D` | Dバケット用パラメータdict | 同上 |
L92 | `cross_mu_gd` | G-Dクロス相関ペナルティ係数μ | `select_buckets`の目的関数 |
L93
L94 ### PipelineConfig
L95 | 変数 | 内容 | 主な用途 |
L96 | --- | --- | --- |
L97 | `weights` | `WeightsConfig`のインスタンス | スコア合成の重み参照 |
L98 | `drrs` | `DRRSParams`のインスタンス | 選定ステップの設定値 |
L99 | `price_max` | 候補銘柄の許容価格上限 | Input段階でのフィルタ |
L100
L101 ## 共通ユーティリティ
L102 - `winsorize_s` / `robust_z` : 外れ値処理とZスコア化。
L103 - `_safe_div` / `_safe_last` : 例外を潰した分割・末尾取得。
L104 - `_load_prev` / `_save_sel` : 選定結果の読み書き。
L105
L106 ## クラス設計
L107 ### Step1: Input
L108 `current_tickers.csv`の現行銘柄と`candidate_tickers.csv`の検討中銘柄を起点にデータを集約する。外部I/Oと前処理を担当し、`prepare_data`で`InputBundle`を生成。価格・財務データの取得は**yfinanceを優先し、欠損がある指標のみFinnhub APIで補完**する。
L109 主なメソッド:
L110 - `impute_eps_ttm` : 四半期EPS×4でTTMを推定し欠損時のみ差し替え。
L111 - `fetch_cfo_capex_ttm_yf` : yfinanceの四半期/年次キャッシュフローからCFO・CapEx・FCF TTMを算出。
L112 - `fetch_cfo_capex_ttm_finnhub` : yfinanceで欠けた銘柄のみFinnhub APIで補完。
L113 - `compute_fcf_with_fallback` : yfinance値を基準にFinnhub値で穴埋めし、CFO/CapEx/FCFと情報源フラグを返す。
L114 - `_build_eps_df` : `info`や`quarterly_earnings`からEPS TTMと直近EPSを計算し、`impute_eps_ttm`で補完。
L115 - `prepare_data` :
L116     0. CSVから現行銘柄と候補銘柄のティッカー一覧を読み込む。
L117     1. 候補銘柄の現在値を取得し価格上限でフィルタ。
L118     2. 既存+候補から対象ティッカーを決定し、価格・出来高を一括ダウンロード（yfinance）。
L119     3. yfinance値を基にEPS/FCFテーブルやベンチマーク系列、リターンを構築し、欠損セルはFinnhub呼び出しで穴埋め。
L120     4. 上記を`InputBundle`に格納して返す。
L121
L122 ### Step2: Score Calculation (Scorer)
L123 特徴量計算とスコア合成を担当し、`FeatureBundle`を返す。
L124
L125 #### 補助関数
L126 - `trend(s)` : 50/150/200日移動平均や52週レンジから-0.5〜0.5で構成されたトレンド指標。
L127 - `rs(s,b)` / `tr_str(s)` / `rs_line_slope(s,b,win)` : 相対強さや短期トレンド、RS回帰傾きを算出。
L128 - `ev_fallback` : `enterpriseValue`欠損時に負債・現金からEVを推定。
L129 - `dividend_status` / `div_streak` : 配当未設定状況の判定と増配年数カウント。
L130 - `fetch_finnhub_metrics` : Finnhub APIからEPS成長・ROE・βなど不足指標を取得。
L131 - `calc_beta` : ベンチマークとの共分散からβを算出。
L132 - `spx_to_alpha` : S&P500の位置情報からDRRSで用いるαを推定。
L133 - `soft_cap_effective_scores` / `pick_top_softcap` : セクターソフトキャップ付きスコア調整と上位抽出。
L134
L135 **補助関数と生成指標**
L136
L137 | 補助関数 | 生成指標 | 略称 |
L138 | --- | --- | --- |
L139 | `trend` | トレンド総合値 | `TR` |
L140 | `rs` | 相対強さ | `RS` |
L141 | `tr_str` | 価格と50日線の乖離 | `TR_str` |
L142 | `rs_line_slope` | RS線の回帰傾き | `RS_SLOPE_*` |
L143 | `calc_beta` | β | `BETA` |
L144 | `div_streak` | 連続増配年数 | `DIV_STREAK` |
L145
L146 #### `aggregate_scores` 詳細
L147 1. 各銘柄の価格系列や`info`を基に以下を算出。
L148    - **トレンド/モメンタム**: `TR`、`RS`、`TR_str`、多様な移動平均比、`RS_SLOPE_*`など。
L149    - **リスク**: `BETA`、`DOWNSIDE_DEV`、`MDD_1Y`、`RESID_VOL`、`DOWN_OUTPERF`、`EXT_200`等。
L150    - **配当**: `DIV`、`DIV_TTM_PS`、`DIV_VAR5`、`DIV_YOY`、`DIV_FCF_COVER`、`DIV_STREAK`。
L151    - **財務・成長**: `EPS`、`REV`、`ROE`、`FCF/EV`、`REV_Q_YOY`、`EPS_Q_YOY`、`REV_YOY_ACC`、`REV_YOY_VAR`、`REV_ANN_STREAK`、`RULE40`、`FCF_MGN` 等。
L152    - **安定性/サイズ**: `DEBT2EQ`、`CURR_RATIO`、`MARKET_CAP`、`ADV60_USD`、`EPS_VAR_8Q`など。
L153 2. 指標欠損はFinnhub API等で補完し、未取得項目を`missing_logs`に記録。
L154 3. `winsorize_s`→`robust_z`で標準化し`df_z`へ保存。サイズ・流動性は対数変換。
L155 4. 正規化済指標から因子スコアを合成。
L156    - 各因子の構成と重みは以下の通り。
L157      - **GRW**: 0.30×`REV` + 0.20×`EPS_Q_YOY` + 0.15×`REV_Q_YOY` + 0.15×`REV_YOY_ACC` + 0.10×`RULE40` + 0.10×`FCF_MGN` + 0.10×`REV_ANN_STREAK` − 0.05×`REV_YOY_VAR`。
L158      - **MOM**: 0.40×`RS` + 0.15×`TR_str` + 0.15×`RS_SLOPE_6W` + 0.15×`RS_SLOPE_13W` + 0.10×`MA200_SLOPE_5M` + 0.10×`MA200_UP_STREAK_D`。
L159      - **VOL**: `BETA`単体を使用。
L160      - **QAL**: 0.60×`FCF_W` + 0.40×`ROE_W`で作成。
L161      - **YLD**: 0.30×`DIV` + 0.70×`DIV_STREAK`。
L162      - **D_QAL**: 0.35×`QAL` + 0.20×`FCF` + 0.15×`CURR_RATIO` − 0.15×`DEBT2EQ` − 0.15×`EPS_VAR_8Q`。
L163      - **D_YLD**: 0.45×`DIV` + 0.25×`DIV_STREAK` + 0.20×`DIV_FCF_COVER` − 0.10×`DIV_VAR5`。
L164      - **D_VOL_RAW**: 0.40×`DOWNSIDE_DEV` + 0.22×`RESID_VOL` + 0.18×`MDD_1Y` − 0.10×`DOWN_OUTPERF` − 0.05×`EXT_200` − 0.08×`SIZE` − 0.10×`LIQ` + 0.10×`BETA`。
L165      - **D_TRD**: 0.40×`MA200_SLOPE_5M` − 0.30×`EXT_200` + 0.15×`NEAR_52W_HIGH` + 0.15×`TR`。
L166     - 主な指標の略称と意味:
L167
L168       | 略称 | 補助関数 | 概要 |
L169       | --- | --- | --- |
L170       | TR | `trend` | 50/150/200日移動平均と52週レンジを組み合わせたトレンド総合値 |
L171       | RS | `rs` | ベンチマークに対する相対強さ（12M/1Mリターン差） |
L172       | TR_str | `tr_str` | 価格と50日移動平均の乖離 |
L173       | RS_SLOPE_6W | `rs_line_slope` | 相対強さ線の6週回帰傾き |
L174       | RS_SLOPE_13W | `rs_line_slope` | 相対強さ線の13週回帰傾き |
L175       | MA200_SLOPE_5M | - | 200日移動平均の5か月騰落率 |
L176       | MA200_UP_STREAK_D | - | 200日線が連続で上向いた日数 |
L177       | BETA | `calc_beta` | ベンチマークに対するβ |
L178       | DOWNSIDE_DEV | - | 下方リターンのみの年率化標準偏差 |
L179       | RESID_VOL | - | βで調整した残差リターンの年率化標準偏差 |
L180       | MDD_1Y | - | 過去1年の最大ドローダウン |
L181       | DOWN_OUTPERF | - | 市場下落日に対する平均超過リターン |
L182       | EXT_200 | - | 200日移動平均からの絶対乖離率 |
L183       | NEAR_52W_HIGH | - | 52週高値までの下方距離（0=高値） |
L184       | FCF_W | - | ウィンザー処理後のFCF/EV |
L185       | ROE_W | - | ウィンザー処理後のROE |
L186       | FCF | - | FCF/EV |
L187       | QAL | - | FCF_WとROE_Wを組み合わせた品質スコア |
L188       | CURR_RATIO | - | 流動比率 |
L189       | DEBT2EQ | - | 負債資本倍率 |
L190       | EPS_VAR_8Q | - | EPSの8四半期標準偏差 |
L191       | DIV | - | 年率換算配当利回り |
L192       | DIV_STREAK | `div_streak` | 連続増配年数 |
L193       | DIV_FCF_COVER | - | 配当のFCFカバレッジ |
L194       | DIV_VAR5 | - | 5年配当変動率 |
L195       | DIV_TTM_PS | - | 1株当たりTTM配当 |
L196       | DIV_YOY | - | 前年比配当成長率 |
L197       | REV | - | 売上成長率TTM |
L198       | EPS_Q_YOY | - | 四半期EPSの前年同期比 |
L199       | REV_Q_YOY | - | 四半期売上の前年同期比 |
L200       | REV_YOY_ACC | - | 売上成長率の加速分 |
L201       | RULE40 | - | 売上成長率とFCFマージンの合計 |
L202       | FCF_MGN | - | FCFマージン |
L203       | REV_ANN_STREAK | - | 年次売上成長の連続年数 |
L204       | REV_YOY_VAR | - | 年次売上成長率の変動性 |
L205       | SIZE | - | 時価総額の対数値 |
L206       | LIQ | - | 60日平均出来高ドルの対数値 |
L207    - Gバケット: `GRW`、`MOM`、`VOL`を`cfg.weights.g`（0.40/0.45/-0.15）で加重し`g_score`を得る。
L208    - Dバケット: `D_QAL`、`D_YLD`、`D_VOL_RAW`、`D_TRD`を`cfg.weights.d`（0.15/0.15/-0.45/0.25）で加重し`d_score_all`を算出。
L209    - セクターcapによる`soft_cap_effective_scores`を適用し、G採用銘柄にはトレンドテンプレートフィルタを適用。
L210 5. `_apply_growth_entry_flags`でブレイクアウト/押し目発火状況を付加し、`FeatureBundle`を返す。
L211
L212 ### Step3: Correlation Reduction & Selection (Selector)
L213 DRRSアルゴリズムで相関を抑えた銘柄選定を行い、`SelectionBundle`を返す。`results/`に保存された前回選定（`G_selection.json` / `D_selection.json`）を`_load_prev`で読み込み、目的値が大きく悪化しない限り維持する。新しい採用集合は`_save_sel`でJSONに書き出し次回以降の入力に備える。
L214 主なメソッド:
L215 - `residual_corr` : 収益率行列をZスコア化し、上位主成分を除去した残差から相関行列を求め、平均相関に応じてシュリンク。
L216 - `rrqr_like_det` : スコアを重み付けしたQR分解風の手順で初期候補をk件抽出し、スコアの高い非相関な集合を得る。
L217 - `swap_local_det` / `swap_local_det_cross` : `sum(score) - λ*within_corr - μ*cross_corr`を目的関数として、入れ替え探索で局所的に最適化。
L218 - `select_bucket_drrs` : プール銘柄とスコアから残差相関を計算し、上記2段階(初期選択→入れ替え)でk銘柄を決定。過去採用銘柄との比較で目的値が劣化しなければ維持する。
L219 - `select_buckets` : Gバケットを選定後、その結果を除いた候補からDバケットを選ぶ。D選定時はGとの相関ペナルティμを付与し、両バケットの分散を制御する。
L220
L221 #### 相関低減ロジック詳細
L222 1. **残差相関行列の構築 (`residual_corr`)**
L223    - リターン行列`R`をZスコア化。
L224    - SVDで上位`n_pc`主成分`F`を求め、最小二乗で係数`B`を算出し残差`E = Z - F@B`を得る。
L225    - `E`の相関行列`C`を計算し、平均絶対相関に応じてシュリンク量`shrink_eff`を補正して対角を強調。
L226 2. **初期候補の抽出 (`rrqr_like_det`)**
L227    - スコアを0-1正規化した重み`w`とし、`Z*(1+γw)`で列ノルムを強調。
L228    - 残差ノルム最大の列を逐次選び、QRライクなデフレーションを行って非相関かつ高スコアな`k`銘柄集合`S0`を得る。
L229 3. **局所探索 (`swap_local_det` / `swap_local_det_cross`)**
L230    - 目的関数`Σz_score − λ·within_corr − μ·cross_corr`を最大化。
L231    - 選択集合の各銘柄を他候補と入れ替え、改善がなくなるまでまたは`max_pass`回まで探索。
L232    - `swap_local_det_cross`はGバケットとのクロス相関行列`C_cross`を使用し、ペナルティ`μ`を付与。
L233 4. **過去採用の維持とクロスペナルティ (`select_bucket_drrs` / `select_buckets`)**
L234    - 局所探索結果`S`と過去集合`P`の目的値を比較し、`S`が`P`より`η`未満の改善なら`P`を維持。
L235    - `select_buckets`ではGを先に決定し、D選定時にGとの相関ペナルティ`μ`を加えてクロス分散を抑制。
L236
L237 ### Step4: Output
L238 選定結果を可視化し共有する工程。以下の内容をテーブル化して標準出力とSlackへ送る。
L239 - 採用銘柄と惜しくも選外となった銘柄のスコア一覧
L240 - IN/OUTリストとOUT銘柄のスコア（低得点銘柄を確認しやすく）
L241 - 新旧ポートフォリオの比較表（組入れ・除外、スコア変化）
L242 - 検討中銘柄の低スコアランキング
L243
L244 主なメソッド:
L245 - `display_results` : 上記テーブルに加えパフォーマンス指標や分散化指標を表示。
L246 - `notify_slack` : Slack Webhookへ同内容を送信。
L247 - 補助:`_avg_offdiag`、`_resid_avg_rho`、`_raw_avg_rho`、`_cross_block_raw_rho`。
L248
L249 ## エントリポイント
L250 1. `PipelineConfig`を構築。
L251 2. **Step1** `Input.prepare_data`で`InputBundle`を生成。
L252 3. **Step2** `Scorer.aggregate_scores`で`FeatureBundle`を取得。
L253 4. **Step3** `Selector.select_buckets`で`SelectionBundle`を算出。
L254 5. **Step4** `Output.display_results`と`notify_slack`で結果を出力。
```

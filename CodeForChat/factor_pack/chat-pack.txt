# === Chat Paste Pack ===
# Repo: dakara32/GPT_Code @ main
# Files: config.py, factor.py, scorer.py, .github/workflows/weekly-report.yml, documents/README.md, documents/factor_design.md
# ä½œæˆæ—¥æ™‚: 2025-09-19 17:54:32 (JST)
# ä½¿ã„æ–¹: ä¸‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’é †ã«è²¼ã‚Œã°ã“ã®ãƒãƒ£ãƒƒãƒˆã§å…¨ä½“æŠŠæ¡ã§ãã¾ã™ã€‚
# æ³¨è¨˜: å„ãƒ•ã‚¡ã‚¤ãƒ«ã¯å€‹åˆ¥ã« L1.. ã§è¡Œç•ªå·ä»˜ä¸ã€‚
---

## <config.py>
```text
L1 # å…±é€šè¨­å®šï¼ˆfactor / drift ã‹ã‚‰å‚ç…§ï¼‰
L2 from dataclasses import dataclass
L3
L4 TOTAL_TARGETS = 20
L5
L6 # åŸºæº–ã®ãƒã‚±ãƒƒãƒˆæ•°ï¼ˆNORMALï¼‰
L7 COUNTS_BASE = {"G": 12, "D": 8}
L8
L9 # ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®æ¨å¥¨ãƒã‚±ãƒƒãƒˆæ•°
L10 COUNTS_BY_MODE = {
L11     "NORMAL": {"G": 12, "D": 8},
L12     "CAUTION": {"G": 10, "D": 8},
L13     "EMERG": {"G": 8,  "D": 8},
L14 }
L15
L16 # ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®ãƒ‰ãƒªãƒ•ãƒˆé–¾å€¤ï¼ˆ%ï¼‰
L17 DRIFT_THRESHOLD_BY_MODE = {"NORMAL": 12, "CAUTION": 14, "EMERG": float("inf")}
L18
L19 # ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®TSï¼ˆåŸºæœ¬å¹…, å°æ•°=å‰²åˆï¼‰
L20 TS_BASE_BY_MODE = {"NORMAL": 0.15, "CAUTION": 0.13, "EMERG": 0.10}
L21 # åˆ©ç›Šåˆ°é”(+30/+60/+100%)æ™‚ã®æ®µéšã‚¿ã‚¤ãƒˆåŒ–ï¼ˆãƒã‚¤ãƒ³ãƒˆå·®ï¼‰
L22 TS_STEP_DELTAS_PT = (3, 6, 8)
L23
L24 # Breadthã®æ ¡æ­£ã¯ N_G ã«é€£å‹•ï¼ˆç·Šæ€¥è§£é™¤=ceil(1.5*N_G), é€šå¸¸å¾©å¸°=3*N_Gï¼‰
L25 N_G = COUNTS_BASE["G"]
L26 N_D = COUNTS_BASE["D"]
L27
```

## <factor.py>
```text
L1 '''ROLE: Orchestration ONLYï¼ˆå¤–éƒ¨I/Oãƒ»SSOTãƒ»Slackå‡ºåŠ›ï¼‰, è¨ˆç®—ã¯ scorer.py'''
L2 # === NOTE: æ©Ÿèƒ½ãƒ»å…¥å‡ºåŠ›ãƒ»ãƒ­ã‚°æ–‡è¨€ãƒ»ä¾‹å¤–æŒ™å‹•ã¯ä¸å¤‰ã€‚å®‰å…¨ãªçŸ­ç¸®ï¼ˆimportçµ±åˆ/è¤‡æ•°ä»£å…¥/å†…åŒ…è¡¨è¨˜/ãƒ¡ã‚½ãƒƒãƒ‰ãƒã‚§ãƒ¼ãƒ³/ä¸€è¡ŒåŒ–/ç©ºè¡Œåœ§ç¸®ãªã©ï¼‰ã®ã¿é©ç”¨ ===
L3 BONUS_COEFF = 0.55  # æ¨å¥¨: æ”»ã‚=0.45 / ä¸­åº¸=0.55 / å®ˆã‚Š=0.65
L4 SWAP_DELTA_Z = 0.15   # åƒ…å·®åˆ¤å®š: Ïƒã®15%ã€‚(ç·©ã‚=0.10 / æ¨™æº–=0.15 / å›ºã‚=0.20)
L5 SWAP_KEEP_BUFFER = 3  # n_target+ã“ã®é †ä½ä»¥å†…ã®ç¾è¡Œã¯ä¿æŒã€‚(ç²˜ã‚Šå¼±=2 / æ¨™æº–=3 / ç²˜ã‚Šå¼·=4ã€œ5)
L6 import os, time, requests
L7 import logging
L8 from time import perf_counter
L9 from dataclasses import dataclass
L10 from typing import Any, Dict, List, Tuple
L11 from concurrent.futures import ThreadPoolExecutor
L12 import numpy as np
L13 import pandas as pd
L14 import yfinance as yf
L15 from scipy.stats import zscore  # used via scorer
L16 from scorer import Scorer, ttm_div_yield_portfolio
L17 import config
L18
L19 # ãã®ä»–
L20 debug_mode, FINNHUB_API_KEY = True, os.environ.get("FINNHUB_API_KEY")
L21
L22 logger = logging.getLogger(__name__)
L23 if debug_mode:
L24     logging.basicConfig(level=logging.INFO, force=True)
L25 else:
L26     logging.basicConfig(level=logging.WARNING, force=True)
L27
L28 class T:
L29     t = perf_counter()
L30     log = staticmethod(lambda tag: (lambda now=perf_counter(): (print(f"[T] {tag}: {now - T.t:.2f}s"), setattr(T, "t", now))[-1])())
L31
L32 T.log("start")
L33
L34 # === ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã¨å®šæ•°ï¼ˆå†’é ­ã«å›ºå®šï¼‰ ===
L35 exist, cand = [pd.read_csv(f, header=None)[0].tolist() for f in ("current_tickers.csv","candidate_tickers.csv")]
L36 T.log(f"csv loaded: exist={len(exist)} cand={len(cand)}")
L37 CAND_PRICE_MAX, bench = 450, '^GSPC'  # ä¾¡æ ¼ä¸Šé™ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
L38 N_G, N_D = config.N_G, config.N_D  # G/Dæ ã‚µã‚¤ã‚ºï¼ˆNORMALåŸºæº–: G12/D8ï¼‰
L39 g_weights = {'GROWTH_F':0.35,'MOM':0.55,'VOL':-0.10}
L40 D_BETA_MAX = float(os.environ.get("D_BETA_MAX", "0.8"))
L41 FILTER_SPEC = {"G":{"pre_mask":["trend_template"]},"D":{"pre_filter":{"beta_max":D_BETA_MAX}}}
L42 D_weights = {'QAL':0.1,'YLD':0.3,'VOL':-0.5,'TRD':0.1}
L43 _fmt_w = lambda w: " ".join(f"{k}{int(v*100)}" for k, v in w.items())
L44
L45 # DRRS åˆæœŸãƒ—ãƒ¼ãƒ«ãƒ»å„ç¨®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
L46 corrM = 45
L47 DRRS_G, DRRS_D = dict(lookback=252,n_pc=3,gamma=1.2,lam=0.68,eta=0.8), dict(lookback=504,n_pc=4,gamma=0.8,lam=0.85,eta=0.5)
L48 DRRS_SHRINK = 0.10  # æ®‹å·®ç›¸é–¢ã®å¯¾è§’ã‚·ãƒ¥ãƒªãƒ³ã‚¯ï¼ˆåŸºç¤ï¼‰
L49
L50 # ã‚¯ãƒ­ã‚¹ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆæœªå®šç¾©ãªã‚‰è¨­å®šï¼‰
L51 try: CROSS_MU_GD
L52 except NameError: CROSS_MU_GD = 0.40  # æ¨å¥¨ 0.35â€“0.45ï¼ˆlam=0.85æƒ³å®šï¼‰
L53
L54 # å‡ºåŠ›é–¢é€£
L55 RESULTS_DIR = "results"
L56 os.makedirs(RESULTS_DIR, exist_ok=True)
L57
L58 # === å…±æœ‰DTOï¼ˆã‚¯ãƒ©ã‚¹é–“I/Oå¥‘ç´„ï¼‰ï¼‹ Config ===
L59 @dataclass(frozen=True)
L60 class InputBundle:
L61     # Input â†’ Scorer ã§å—ã‘æ¸¡ã™ç´ æï¼ˆI/Oç¦æ­¢ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ï¼‰
L62     cand: List[str]
L63     tickers: List[str]
L64     bench: str
L65     data: pd.DataFrame              # yfinance downloadçµæœï¼ˆ'Close','Volume'ç­‰ã®éšå±¤åˆ—ï¼‰
L66     px: pd.DataFrame                # data['Close']
L67     spx: pd.Series                  # data['Close'][bench]
L68     tickers_bulk: object            # yfinance.Tickers
L69     info: Dict[str, dict]           # yfinance info per ticker
L70     eps_df: pd.DataFrame            # ['eps_ttm','eps_q_recent',...]
L71     fcf_df: pd.DataFrame            # ['fcf_ttm', ...]
L72     returns: pd.DataFrame           # px[tickers].pct_change()
L73
L74 @dataclass(frozen=True)
L75 class FeatureBundle:
L76     df: pd.DataFrame
L77     df_z: pd.DataFrame
L78     g_score: pd.Series
L79     d_score_all: pd.Series
L80     missing_logs: pd.DataFrame
L81     df_full: pd.DataFrame | None = None
L82     df_full_z: pd.DataFrame | None = None
L83     scaler: Any | None = None
L84
L85 @dataclass(frozen=True)
L86 class SelectionBundle:
L87     resG: dict
L88     resD: dict
L89     top_G: List[str]
L90     top_D: List[str]
L91     init_G: List[str]
L92     init_D: List[str]
L93
L94 @dataclass(frozen=True)
L95 class WeightsConfig:
L96     g: Dict[str,float]
L97     d: Dict[str,float]
L98
L99 @dataclass(frozen=True)
L100 class DRRSParams:
L101     corrM: int
L102     shrink: float
L103     G: Dict[str,float]   # lookback, n_pc, gamma, lam, eta
L104     D: Dict[str,float]
L105     cross_mu_gd: float
L106
L107 @dataclass(frozen=True)
L108 class PipelineConfig:
L109     weights: WeightsConfig
L110     drrs: DRRSParams
L111     price_max: float
L112     debug_mode: bool = False
L113
L114 # === å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆè¤‡æ•°ã‚¯ãƒ©ã‚¹ã§ä½¿ç”¨ï¼‰ ===
L115 # (unused local utils removed â€“ use scorer.py versions if needed)
L116
L117 _env_true = lambda name, default=False: (os.getenv(name) or str(default)).strip().lower() == "true"
L118
L119 def _post_slack(payload: dict):
L120     url = os.getenv("SLACK_WEBHOOK_URL")
L121     if not url: print("âš ï¸ SLACK_WEBHOOK_URL æœªè¨­å®š"); return
L122     try:
L123         requests.post(url, json=payload).raise_for_status()
L124     except Exception as e:
L125         print(f"âš ï¸ Slacké€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
L126
L127 def _slack_send_text_chunks(url: str, text: str, chunk: int = 2800) -> None:
L128     """Slackã¸ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²é€ä¿¡ï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å½¢å¼ï¼‰ã€‚"""
L129
L130     def _post_text(payload: str) -> None:
L131         try:
L132             resp = requests.post(url, json={"text": payload})
L133             print(f"[DBG] debug_post status={getattr(resp, 'status_code', None)} size={len(payload)}")
L134             if resp is not None:
L135                 resp.raise_for_status()
L136         except Exception as e:
L137             print(f"[ERR] debug_post_failed: {e}")
L138
L139     body = str(text or "").strip()
L140     if not body:
L141         print("[DBG] skip debug send: empty body")
L142         return
L143
L144     lines = body.splitlines()
L145     block: list[str] = []
L146     block_len = 0
L147
L148     def _flush() -> None:
L149         nonlocal block, block_len
L150         if not block:
L151             return
L152         payload = "```" + "\n".join(block) + "```"
L153         _post_text(payload)
L154         block, block_len = [], 0
L155
L156     for raw in lines:
L157         line = raw or ""
L158         while len(line) > chunk:
L159             head, line = line[:chunk], line[chunk:]
L160             _flush()
L161             _post_text("```" + head + "```")
L162         add_len = len(line) if not block else len(line) + 1
L163         if block and block_len + add_len > chunk:
L164             _flush()
L165             add_len = len(line)
L166         block.append(line)
L167         block_len += add_len
L168     _flush()
L169
L170 def _disjoint_keepG(top_G, top_D, poolD):
L171     """Gé‡è¤‡ã‚’Dã‹ã‚‰é™¤å»ã—ã€poolDã§é †æ¬¡è£œå……ï¼ˆæ¯æ¸‡æ™‚ã¯å…ƒéŠ˜æŸ„ç¶­æŒï¼‰ã€‚"""
L172     used, D, i = set(top_G), list(top_D), 0
L173     for j, t in enumerate(D):
L174         if t in used:
L175             while i < len(poolD) and (poolD[i] in used or poolD[i] in D):
L176                 i += 1
L177             if i < len(poolD):
L178                 D[j] = poolD[i]; used.add(D[j]); i += 1
L179     return top_G, D
L180
L181
L182 def _sticky_keep_current(agg: pd.Series, pick: list[str], incumbents: list[str],
L183                          n_target: int, delta_z: float, keep_buffer: int) -> list[str]:
L184     import pandas as pd, numpy as np
L185     sel = list(pick)
L186     if not sel: return sel
L187     ranked_sel = agg.reindex(sel).sort_values(ascending=False)
L188     kth = ranked_sel.iloc[min(len(sel), n_target)-1]
L189     sigma = float(agg.std()) if pd.notna(agg.std()) else 0.0
L190     thresh = kth - delta_z * sigma
L191     ranked_all = agg.sort_values(ascending=False)
L192     cand = [t for t in incumbents if (t not in sel) and (t in agg.index)]
L193     for t in cand:
L194         within_score = (pd.notna(agg[t]) and agg[t] >= thresh)
L195         within_rank  = (t in ranked_all.index) and (ranked_all.index.get_loc(t) < n_target + keep_buffer)
L196         if within_score or within_rank:
L197             non_inc = [x for x in sel if x not in incumbents]
L198             if not non_inc: break
L199             weakest = min(non_inc, key=lambda x: agg.get(x, -np.inf))
L200             if weakest in sel and agg.get(t, -np.inf) >= agg.get(weakest, -np.inf):
L201                 sel.remove(weakest); sel.append(t)
L202     if len(sel) > n_target:
L203         sel = sorted(sel, key=lambda x: agg.get(x, -1e9), reverse=True)[:n_target]
L204     return sel
L205
L206
L207 # === Inputï¼šå¤–éƒ¨I/Oã¨å‰å‡¦ç†ï¼ˆCSV/APIãƒ»æ¬ æè£œå®Œï¼‰ ===
L208 class Input:
L209     def __init__(self, cand, exist, bench, price_max, finnhub_api_key=None):
L210         self.cand, self.exist, self.bench, self.price_max = cand, exist, bench, price_max
L211         self.api_key = finnhub_api_key or os.environ.get("FINNHUB_API_KEY")
L212
L213     # ---- ï¼ˆInputå°‚ç”¨ï¼‰EPSè£œå®Œãƒ»FCFç®—å‡ºç³» ----
L214     @staticmethod
L215     def _sec_headers():
L216         mail = (os.getenv("SEC_CONTACT_EMAIL") or "yasonba55@gmail.com").strip()
L217         app = (os.getenv("SEC_APP_NAME") or "FactorBot/1.0").strip()
L218         return {
L219             "User-Agent": f"{app} ({mail})",
L220             "From": mail,
L221             "Accept": "application/json",
L222         }
L223
L224     @staticmethod
L225     def _sec_get(url: str, retries: int = 3, backoff: float = 0.5):
L226         for i in range(retries):
L227             r = requests.get(url, headers=Input._sec_headers(), timeout=20)
L228             if r.status_code in (429, 503, 403):
L229                 time.sleep(min(2 ** i * backoff, 8.0))
L230                 continue
L231             r.raise_for_status()
L232             return r.json()
L233         r.raise_for_status()
L234
L235     @staticmethod
L236     def _sec_ticker_map():
L237         j = Input._sec_get("https://data.sec.gov/api/xbrl/company_tickers.json")
L238         mp = {}
L239         for _, v in (j or {}).items():
L240             try:
L241                 mp[str(v["ticker"]).upper()] = f"{int(v['cik_str']):010d}"
L242             except Exception:
L243                 pass
L244         return mp
L245
L246     # --- è¿½åŠ : ADR/OTCå‘ã‘ã®ç°¡æ˜“æ­£è¦åŒ–ï¼ˆæœ«å°¾Y/F, ãƒ‰ãƒƒãƒˆç­‰ï¼‰ ---
L247     @staticmethod
L248     def _normalize_ticker(sym: str) -> list[str]:
L249         s = (sym or "").upper().strip()
L250         # è¿½åŠ : å…ˆé ­ã®$ã‚„å…¨è§’ã®è¨˜å·ã‚’é™¤å»
L251         s = s.lstrip("$").replace("ï¼„", "").replace("ï¼", ".").replace("ï¼", "-")
L252         cand: list[str] = []
L253
L254         def add(x: str) -> None:
L255             if x and x not in cand:
L256                 cand.append(x)
L257
L258         # 1) åŸæ–‡ã‚’æœ€å„ªå…ˆï¼ˆSECã¯ BRK.B, BF.B ãªã© . ã‚’æ­£å¼æ¡ç”¨ï¼‰
L259         add(s)
L260         # 2) Yahooç³»ãƒãƒªã‚¢ãƒ³ãƒˆï¼ˆ. ã¨ - ã®æºã‚Œã‚’ç›¸äº’ã«ï¼‰
L261         if "." in s:
L262             add(s.replace(".", "-"))
L263             add(s.replace(".", ""))
L264         if "-" in s:
L265             add(s.replace("-", "."))
L266             add(s.replace("-", ""))
L267         # 3) ãƒ‰ãƒƒãƒˆãƒ»ãƒã‚¤ãƒ•ãƒ³ãƒ»ãƒ”ãƒªã‚ªãƒ‰ç„¡ã—ç‰ˆï¼ˆæœ€å¾Œã®ä¿é™ºï¼‰
L268         add(s.replace("-", "").replace(".", ""))
L269         # 4) ADRç°¡æ˜“ï¼šæœ«å°¾Y/Fã®é™¤å»ï¼ˆSECãƒãƒƒãƒ—ã¯æœ¬ä½“ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚’æŒã¤ã“ã¨ãŒã‚ã‚‹ï¼‰
L270         if len(s) >= 2 and s[-1] in {"Y", "F"}:
L271             add(s[:-1])
L272         return cand
L273
L274     @staticmethod
L275     def _sec_companyfacts(cik: str):
L276         return Input._sec_get(f"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json")
L277
L278     @staticmethod
L279     def _units_for_tags(facts: dict, namespaces: list[str], tags: list[str]) -> list[dict]:
L280         """facts ã‹ã‚‰ namespace/tag ã‚’æ¨ªæ–­ã—ã¦ units é…åˆ—ã‚’åé›†ï¼ˆå­˜åœ¨é †ã«é€£çµï¼‰ã€‚"""
L281         out: list[dict] = []
L282         facts = facts or {}
L283         for ns in namespaces:
L284             try:
L285                 node = facts.get("facts", {}).get(ns, {})
L286             except Exception:
L287                 node = {}
L288             for tg in tags:
L289                 try:
L290                     units = node[tg]["units"]
L291                 except Exception:
L292                     continue
L293                 picks: list[dict] = []
L294                 if "USD/shares" in units:
L295                     picks.extend(list(units["USD/shares"]))
L296                 if "USD" in units:
L297                     picks.extend(list(units["USD"]))
L298                 if not picks:
L299                     for arr in units.values():
L300                         picks.extend(list(arr))
L301                 out.extend(picks)
L302         return out
L303
L304     @staticmethod
L305     def _only_quarterly(arr: list[dict]) -> list[dict]:
L306         """companyfactsã®æ··åœ¨é…åˆ—ã‹ã‚‰ã€å››åŠæœŸã€ã ã‘ã‚’æŠ½å‡ºã€‚
L307
L308         - frame ã« "Q" ã‚’å«ã‚€ï¼ˆä¾‹: CY2024Q2Iï¼‰
L309         - fp ãŒ Q1/Q2/Q3/Q4
L310         - form ãŒ 10-Q/10-Q/A/6-K
L311         """
L312         if not arr:
L313             return []
L314         q_forms = {"10-Q", "10-Q/A", "6-K"}
L315
L316         def is_q(x: dict) -> bool:
L317             frame = (x.get("frame") or "").upper()
L318             fp = (x.get("fp") or "").upper()
L319             form = (x.get("form") or "").upper()
L320             return ("Q" in frame) or (fp in {"Q1", "Q2", "Q3", "Q4"}) or (form in q_forms)
L321
L322         out = [x for x in arr if is_q(x)]
L323         out.sort(key=lambda x: (x.get("end") or ""), reverse=True)
L324         return out
L325
L326     @staticmethod
L327     def _series_from_facts_with_dates(arr, key_val="val", key_dt="end", normalize=float):
L328         """companyfactsã‚¢ã‚¤ãƒ†ãƒ é…åˆ—ã‹ã‚‰ (date,value) ã‚’è¿”ã™ã€‚dateã¯YYYY-MM-DDã‚’æƒ³å®šã€‚"""
L329         out: List[Tuple[str, float]] = []
L330         for x in (arr or []):
L331             try:
L332                 v = x.get(key_val)
L333                 d = x.get(key_dt)
L334                 if d is None:
L335                     continue
L336                 out.append((str(d), normalize(v) if v is not None else float("nan")))
L337             except Exception:
L338                 continue
L339         # end(=æ—¥ä»˜)ã®é™é †ã«ã‚½ãƒ¼ãƒˆï¼ˆæœ€æ–°â†’å¤ã„ï¼‰
L340         out.sort(key=lambda t: t[0], reverse=True)
L341         return out
L342
L343     def fetch_eps_rev_from_sec(self, tickers: list[str]) -> dict:
L344         out = {}
L345         t2cik = self._sec_ticker_map()
L346         n_map = n_rev = n_eps = 0
L347         miss_map: list[str] = []
L348         miss_facts: list[str] = []
L349         for t in tickers:
L350             candidates: list[str] = []
L351
L352             def add(key: str) -> None:
L353                 if key and key not in candidates:
L354                     candidates.append(key)
L355
L356             add((t or "").upper())
L357             for key in self._normalize_ticker(t):
L358                 add(key)
L359
L360             cik = None
L361             for key in candidates:
L362                 cik = t2cik.get(key)
L363                 if cik:
L364                     break
L365             if not cik:
L366                 out[t] = {}
L367                 miss_map.append(t)
L368                 continue
L369             try:
L370                 j = self._sec_companyfacts(cik)
L371                 facts = j or {}
L372                 rev_tags = [
L373                     "Revenues",
L374                     "RevenueFromContractWithCustomerExcludingAssessedTax",
L375                     "SalesRevenueNet",
L376                     "SalesRevenueGoodsNet",
L377                     "SalesRevenueServicesNet",
L378                     "Revenue",
L379                 ]
L380                 eps_tags = [
L381                     "EarningsPerShareDiluted",
L382                     "EarningsPerShareBasicAndDiluted",
L383                     "EarningsPerShare",
L384                     "EarningsPerShareBasic",
L385                 ]
L386                 rev_arr = self._units_for_tags(facts, ["us-gaap", "ifrs-full"], rev_tags)
L387                 eps_arr = self._units_for_tags(facts, ["us-gaap", "ifrs-full"], eps_tags)
L388                 rev_q_items = self._only_quarterly(rev_arr)
L389                 eps_q_items = self._only_quarterly(eps_arr)
L390                 # (date,value) ã§å–å¾—
L391                 rev_pairs = self._series_from_facts_with_dates(rev_q_items)
L392                 eps_pairs = self._series_from_facts_with_dates(eps_q_items)
L393                 rev_vals = [v for (_d, v) in rev_pairs]
L394                 eps_vals = [v for (_d, v) in eps_pairs]
L395                 rev_q = float(rev_vals[0]) if rev_vals else float("nan")
L396                 eps_q = float(eps_vals[0]) if eps_vals else float("nan")
L397                 rev_ttm = float(sum([v for v in rev_vals[:4] if v == v])) if rev_vals else float("nan")
L398                 eps_ttm = float(sum([v for v in eps_vals[:4] if v == v])) if eps_vals else float("nan")
L399                 out[t] = {
L400                     "eps_q_recent": eps_q,
L401                     "eps_ttm": eps_ttm,
L402                     "rev_q_recent": rev_q,
L403                     "rev_ttm": rev_ttm,
L404                     # å¾Œæ®µã§DatetimeIndexåŒ–ã§ãã‚‹ã‚ˆã† (date,value) ã‚’ä¿æŒã€‚å€¤ã ã‘ã®äº’æ›ã‚­ãƒ¼ã‚‚æ®‹ã™ã€‚
L405                     "eps_q_series_pairs": eps_pairs[:16],
L406                     "rev_q_series_pairs": rev_pairs[:16],
L407                     "eps_q_series": eps_vals[:16],
L408                     "rev_q_series": rev_vals[:16],
L409                 }
L410                 n_map += 1
L411                 if rev_vals:
L412                     n_rev += 1
L413                 if eps_vals:
L414                     n_eps += 1
L415             except Exception:
L416                 out[t] = {}
L417                 miss_facts.append(t)
L418             time.sleep(0.30)
L419         # å–å¾—ã‚µãƒãƒªã‚’ãƒ­ã‚°ï¼ˆActionsã§ç¢ºèªã—ã‚„ã™ã„ã‚ˆã† printï¼‰
L420         try:
L421             total = len(tickers)
L422             print(f"[SEC] map={n_map}/{total}  rev_q_hit={n_rev}  eps_q_hit={n_eps}")
L423             # ãƒ‡ãƒãƒƒã‚°: å–å¾—æœ¬æ•°ã®åˆ†å¸ƒï¼ˆå…ˆé ­ã®ã¿ï¼‰
L424             try:
L425                 lens = [len((out.get(t, {}) or {}).get("rev_q_series", [])) for t in tickers]
L426                 print(f"[SEC] rev_q_series length: min={min(lens) if lens else 0} "
L427                       f"p25={np.percentile(lens,25) if lens else 0} median={np.median(lens) if lens else 0} "
L428                       f"p75={np.percentile(lens,75) if lens else 0} max={max(lens) if lens else 0}")
L429             except Exception:
L430                 pass
L431             if miss_map:
L432                 print(f"[SEC] no CIK map: {len(miss_map)} (ã‚µãƒ³ãƒ—ãƒ«ä¾‹) {miss_map[:20]}")
L433             if miss_facts:
L434                 print(f"[SEC] CIKã‚ã‚Š ã ãŒå¯¾è±¡factãªã—: {len(miss_facts)} (ã‚µãƒ³ãƒ—ãƒ«ä¾‹) {miss_facts[:20]}")
L435         except Exception:
L436             pass
L437         return out
L438
L439     def sec_dryrun_sample(self, tickers: list[str] | None = None) -> None:
L440         if not _env_true("SEC_DRYRUN_SAMPLE", False):
L441             return
L442         sample = tickers or ["BRK.B", "BF.B", "GOOGL", "META", "UBER", "PBR.A", "TSM", "NARI", "EVBN", "SWAV"]
L443         print(f"[SEC-DRYRUN] sample tickers: {sample}")
L444         try:
L445             t2cik = self._sec_ticker_map()
L446             hits = 0
L447             for sym in sample:
L448                 candidates: list[str] = []
L449
L450                 def add(key: str) -> None:
L451                     if key and key not in candidates:
L452                         candidates.append(key)
L453
L454                 add((sym or "").upper())
L455                 for alt in self._normalize_ticker(sym):
L456                     add(alt)
L457                 if any(t2cik.get(key) for key in candidates):
L458                     hits += 1
L459             sec_data = self.fetch_eps_rev_from_sec(sample)
L460             rev_hits = sum(1 for v in sec_data.values() if v.get("rev_q_series"))
L461             eps_hits = sum(1 for v in sec_data.values() if v.get("eps_q_series"))
L462             total = len(sample)
L463             print(f"[SEC-DRYRUN] CIK map hit: {hits}/{total}  rev_q_series hits: {rev_hits}  eps_q_series hits: {eps_hits}")
L464         except Exception as e:
L465             print(f"[SEC-DRYRUN] error: {e}")
L466     @staticmethod
L467     def impute_eps_ttm(df: pd.DataFrame, ttm_col: str="eps_ttm", q_col: str="eps_q_recent", out_col: str|None=None) -> pd.DataFrame:
L468         out_col = out_col or ttm_col; df = df.copy(); df["eps_imputed"] = False
L469         cand = df[q_col]*4; ok = df[ttm_col].isna() & cand.replace([np.inf,-np.inf], np.nan).notna()
L470         df.loc[ok, out_col], df.loc[ok,"eps_imputed"] = cand[ok], True; return df
L471
L472     _CF_ALIASES = {"cfo":["Operating Cash Flow","Total Cash From Operating Activities"], "capex":["Capital Expenditure","Capital Expenditures"]}
L473
L474     @staticmethod
L475     def _pick_row(df: pd.DataFrame, names: list[str]) -> pd.Series|None:
L476         if df is None or df.empty: return None
L477         idx_lower={str(i).lower():i for i in df.index}
L478         for n in names:
L479             k=n.lower()
L480             if k in idx_lower: return df.loc[idx_lower[k]]
L481         return None
L482
L483     @staticmethod
L484     def _sum_last_n(s: pd.Series|None, n: int) -> float|None:
L485         if s is None or s.empty: return None
L486         v=s.dropna().astype(float); return None if v.empty else v.iloc[:n].sum()
L487
L488     @staticmethod
L489     def _latest(s: pd.Series|None) -> float|None:
L490         if s is None or s.empty: return None
L491         v=s.dropna().astype(float); return v.iloc[0] if not v.empty else None
L492
L493     def fetch_cfo_capex_ttm_yf(self, tickers: list[str]) -> pd.DataFrame:
L494         from concurrent.futures import ThreadPoolExecutor, as_completed
L495         pick, sumn, latest, aliases = self._pick_row, self._sum_last_n, self._latest, self._CF_ALIASES
L496
L497         def one(t: str):
L498             try:
L499                 tk = yf.Ticker(t)  # â˜… ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¯æ¸¡ã•ãªã„ï¼ˆYFãŒcurl_cffiã§ç®¡ç†ï¼‰
L500                 qcf = tk.quarterly_cashflow
L501                 cfo_q, capex_q = pick(qcf, aliases["cfo"]), pick(qcf, aliases["capex"])
L502                 fcf_q = pick(qcf, ["Free Cash Flow","FreeCashFlow","Free cash flow"])
L503                 cfo, capex, fcf = sumn(cfo_q,4), sumn(capex_q,4), sumn(fcf_q,4)
L504                 if any(v is None for v in (cfo, capex, fcf)):
L505                     acf = tk.cashflow
L506                     if cfo   is None: cfo   = latest(pick(acf, aliases["cfo"]))
L507                     if capex is None: capex = latest(pick(acf, aliases["capex"]))
L508                     if fcf   is None: fcf   = latest(pick(acf, ["Free Cash Flow","FreeCashFlow","Free cash flow"]))
L509             except Exception as e:
L510                 print(f"[warn] yf financials error: {t}: {e}"); cfo=capex=fcf=None
L511             n=np.nan
L512             return {"ticker":t,
L513                     "cfo_ttm_yf":   n if cfo   is None else cfo,
L514                     "capex_ttm_yf": n if capex is None else capex,
L515                     "fcf_ttm_yf_direct": n if fcf is None else fcf}
L516
L517         rows, mw = [], int(os.getenv("FIN_THREADS","8"))
L518         with ThreadPoolExecutor(max_workers=mw) as ex:
L519             rows=[f.result() for f in as_completed(ex.submit(one,t) for t in tickers)]
L520         return pd.DataFrame(rows).set_index("ticker")
L521
L522     _FINN_CFO_KEYS = ["netCashProvidedByOperatingActivities","netCashFromOperatingActivities","cashFlowFromOperatingActivities","operatingCashFlow"]
L523     _FINN_CAPEX_KEYS = ["capitalExpenditure","capitalExpenditures","purchaseOfPPE","investmentsInPropertyPlantAndEquipment"]
L524
L525     @staticmethod
L526     def _first_key(d: dict, keys: list[str]):
L527         for k in keys:
L528             if k in d and d[k] is not None: return d[k]
L529         return None
L530
L531     @staticmethod
L532     def _finn_get(session: requests.Session, url: str, params: dict, retries: int=3, sleep_s: float=0.5):
L533         for i in range(retries):
L534             r = session.get(url, params=params, timeout=15)
L535             if r.status_code==429: time.sleep(min(2**i*sleep_s,4.0)); continue
L536             r.raise_for_status(); return r.json()
L537         r.raise_for_status()
L538
L539     def fetch_cfo_capex_ttm_finnhub(self, tickers: list[str], api_key: str|None=None) -> pd.DataFrame:
L540         api_key = api_key or os.getenv("FINNHUB_API_KEY")
L541         if not api_key: raise ValueError("Finnhub API key not provided. Set FINNHUB_API_KEY or pass api_key=")
L542         base, s, rows = "https://finnhub.io/api/v1", requests.Session(), []
L543         for sym in tickers:
L544             cfo_ttm = capex_ttm = None
L545             try:
L546                 j = self._finn_get(s, f"{base}/stock/cash-flow", {"symbol":sym,"frequency":"quarterly","limit":8,"token":api_key})
L547                 arr = j.get("cashFlow") or []; cfo_vals, capex_vals = [], []
L548                 for item in arr[:4]:
L549                     cfo_vals.append(self._first_key(item,self._FINN_CFO_KEYS)); capex_vals.append(self._first_key(item,self._FINN_CAPEX_KEYS))
L550                 if any(v is not None for v in cfo_vals): cfo_ttm = float(np.nansum([np.nan if v is None else float(v) for v in cfo_vals]))
L551                 if any(v is not None for v in capex_vals): capex_ttm = float(np.nansum([np.nan if v is None else float(v) for v in capex_vals]))
L552             except Exception: pass
L553             if cfo_ttm is None or capex_ttm is None:
L554                 try:
L555                     j = self._finn_get(s, f"{base}/stock/cash-flow", {"symbol":sym,"frequency":"annual","limit":1,"token":api_key})
L556                     arr = j.get("cashFlow") or []
L557                     if arr:
L558                         item0 = arr[0]
L559                         if cfo_ttm is None:
L560                             v = self._first_key(item0,self._FINN_CFO_KEYS)
L561                             if v is not None: cfo_ttm = float(v)
L562                         if capex_ttm is None:
L563                             v = self._first_key(item0,self._FINN_CAPEX_KEYS)
L564                             if v is not None: capex_ttm = float(v)
L565                 except Exception: pass
L566             rows.append({"ticker":sym,"cfo_ttm_fh":np.nan if cfo_ttm is None else cfo_ttm,"capex_ttm_fh":np.nan if capex_ttm is None else capex_ttm})
L567         return pd.DataFrame(rows).set_index("ticker")
L568
L569     def compute_fcf_with_fallback(self, tickers: list[str], finnhub_api_key: str|None=None) -> pd.DataFrame:
L570         yf_df = self.fetch_cfo_capex_ttm_yf(tickers)
L571         T.log("financials (yf) done")
L572         miss_mask = yf_df[["cfo_ttm_yf","capex_ttm_yf","fcf_ttm_yf_direct"]].isna().any(axis=1)
L573         need = yf_df.index[miss_mask].tolist(); print(f"[T] yf financials missing: {len(need)} {need[:10]}{'...' if len(need)>10 else ''}")
L574         if need:
L575             fh_df = self.fetch_cfo_capex_ttm_finnhub(need, api_key=finnhub_api_key)
L576             df = yf_df.join(fh_df, how="left")
L577             for col_yf, col_fh in [("cfo_ttm_yf","cfo_ttm_fh"),("capex_ttm_yf","capex_ttm_fh")]:
L578                 df[col_yf] = df[col_yf].fillna(df[col_fh])
L579             print("[T] financials (finnhub) done (fallback only)")
L580         else:
L581             df = yf_df.assign(cfo_ttm_fh=np.nan, capex_ttm_fh=np.nan)
L582             print("[T] financials (finnhub) skipped (no missing)")
L583         df["cfo_ttm"]  = df["cfo_ttm_yf"].where(df["cfo_ttm_yf"].notna(), df["cfo_ttm_fh"])
L584         df["capex_ttm"] = df["capex_ttm_yf"].where(df["capex_ttm_yf"].notna(), df["capex_ttm_fh"])
L585         cfo, capex = pd.to_numeric(df["cfo_ttm"], errors="coerce"), pd.to_numeric(df["capex_ttm"], errors="coerce").abs()
L586         fcf_calc = cfo - capex
L587         fcf_direct = pd.to_numeric(df.get("fcf_ttm_yf_direct"), errors="coerce")
L588         df["fcf_ttm"] = fcf_calc.where(fcf_calc.notna(), fcf_direct)
L589         df["cfo_source"]  = np.where(df["cfo_ttm_yf"].notna(),"yfinance",np.where(df["cfo_ttm_fh"].notna(),"finnhub",""))
L590         df["capex_source"] = np.where(df["capex_ttm_yf"].notna(),"yfinance",np.where(df["capex_ttm_fh"].notna(),"finnhub",""))
L591         df["fcf_imputed"] = df[["cfo_ttm","capex_ttm"]].isna().any(axis=1) & df["fcf_ttm"].notna()
L592         cols = ["cfo_ttm_yf","capex_ttm_yf","cfo_ttm_fh","capex_ttm_fh","cfo_ttm","capex_ttm","fcf_ttm","fcf_ttm_yf_direct","cfo_source","capex_source","fcf_imputed"]
L593         return df[cols].sort_index()
L594
L595     def _build_eps_df(self, tickers, tickers_bulk, info, sec_map: dict | None = None):
L596         eps_rows=[]
L597         for t in tickers:
L598             info_t = info[t]
L599             sec_t = (sec_map or {}).get(t, {})
L600             eps_ttm = sec_t.get("eps_ttm", info_t.get("trailingEps", np.nan))
L601             eps_q = sec_t.get("eps_q_recent", np.nan)
L602             try:
L603                 qearn, so = tickers_bulk.tickers[t].quarterly_earnings, info_t.get("sharesOutstanding")
L604                 if so and qearn is not None and not qearn.empty and "Earnings" in qearn.columns:
L605                     eps_ttm_q = qearn["Earnings"].head(4).sum()/so
L606                     if pd.notna(eps_ttm_q) and (pd.isna(eps_ttm) or (abs(eps_ttm)>0 and abs(eps_ttm/eps_ttm_q)>3)): eps_ttm = eps_ttm_q
L607                     if pd.isna(eps_q):
L608                         eps_q = qearn["Earnings"].iloc[-1]/so
L609             except Exception: pass
L610             rev_ttm = sec_t.get("rev_ttm", np.nan)
L611             rev_q = sec_t.get("rev_q_recent", np.nan)
L612             if (not sec_t) or pd.isna(rev_ttm):
L613                 try:
L614                     tk = tickers_bulk.tickers[t]
L615                     qfin = getattr(tk, "quarterly_financials", None)
L616                     if qfin is not None and not qfin.empty:
L617                         idx_lower = {str(i).lower(): i for i in qfin.index}
L618                         rev_idx = None
L619                         for name in ("Total Revenue", "TotalRevenue"):
L620                             key = name.lower()
L621                             if key in idx_lower:
L622                                 rev_idx = idx_lower[key]
L623                                 break
L624                         if rev_idx is not None:
L625                             rev_series = pd.to_numeric(qfin.loc[rev_idx], errors="coerce").dropna()
L626                             if not rev_series.empty:
L627                                 rev_ttm_yf = float(rev_series.head(4).sum())
L628                                 if pd.isna(rev_ttm):
L629                                     rev_ttm = rev_ttm_yf
L630                                 if pd.isna(rev_q):
L631                                     rev_q = float(rev_series.iloc[0])
L632                 except Exception:
L633                     pass
L634             eps_rows.append({"ticker":t,"eps_ttm":eps_ttm,"eps_q_recent":eps_q,"rev_ttm":rev_ttm,"rev_q_recent":rev_q})
L635         return self.impute_eps_ttm(pd.DataFrame(eps_rows).set_index("ticker"))
L636
L637     def prepare_data(self):
L638         """Fetch price and fundamental data for all tickers."""
L639         self.sec_dryrun_sample()
L640         cand_info = yf.Tickers(" ".join(self.cand)); cand_prices = {}
L641         for t in self.cand:
L642             try: cand_prices[t] = cand_info.tickers[t].fast_info.get("lastPrice", np.inf)
L643             except Exception as e: print(f"{t}: price fetch failed ({e})"); cand_prices[t] = np.inf
L644         cand_f = [t for t,p in cand_prices.items() if p<=self.price_max]
L645         T.log("price cap filter done (CAND_PRICE_MAX)")
L646         # å…¥åŠ›ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã®é‡è¤‡ã‚’é™¤å»ã—ã€ç¾è¡Œâ†’å€™è£œã®é †åºã‚’ç¶­æŒ
L647         tickers = list(dict.fromkeys(self.exist + cand_f))
L648         T.log(f"universe prepared: unique={len(tickers)} bench={self.bench}")
L649         data = yf.download(tickers + [self.bench], period="600d",
L650                            auto_adjust=True, progress=False, threads=False)
L651         T.log("yf.download done")
L652         px = data["Close"].dropna(how="all", axis=1).ffill(limit=2)
L653         spx = data["Close"][self.bench].reindex(px.index).ffill()
L654         clip_days = int(os.getenv("PRICE_CLIP_DAYS", "0"))   # 0ãªã‚‰ç„¡åŠ¹ï¼ˆæ—¢å®šï¼‰
L655         if clip_days > 0:
L656             px  = px.tail(clip_days + 1)
L657             spx = spx.tail(clip_days + 1)
L658             logger.info("[T] price window clipped by env: %d rows (PRICE_CLIP_DAYS=%d)", len(px), clip_days)
L659         else:
L660             logger.info("[T] price window clip skipped; rows=%d", len(px))
L661         tickers_bulk, info = yf.Tickers(" ".join(tickers)), {}
L662         for t in tickers:
L663             try:
L664                 info[t] = tickers_bulk.tickers[t].info
L665             except Exception as e:
L666                 logger.info("[warn] %s: info fetch failed (%s)", t, e)
L667                 info[t] = {}
L668         try:
L669             sec_map = self.fetch_eps_rev_from_sec(tickers)
L670             for t in tickers:
L671                 if t in info and sec_map.get(t):
L672                     # (date,value) ã‚’å„ªå…ˆæ¡ç”¨ã€‚ãªã‘ã‚Œã°å¾“æ¥ã®å€¤ã ã‘é…åˆ—ã€‚
L673                     pairs_r = sec_map[t].get("rev_q_series_pairs") or []
L674                     pairs_e = sec_map[t].get("eps_q_series_pairs") or []
L675                     if pairs_r:
L676                         idx = pd.to_datetime([d for (d, _v) in pairs_r], errors="coerce")
L677                         val = pd.to_numeric([v for (_d, v) in pairs_r], errors="coerce")
L678                         s = pd.Series(val, index=idx).sort_index()  # å¤ã„â†’æ–°ã—ã„ï¼ˆYoYç­‰ã®ç›´æ„Ÿã«åˆã‚ã›ã‚‹ï¼‰
L679                         info[t]["SEC_REV_Q_SERIES"] = s
L680                     else:
L681                         info[t]["SEC_REV_Q_SERIES"] = sec_map[t].get("rev_q_series") or []
L682                     if pairs_e:
L683                         idx = pd.to_datetime([d for (d, _v) in pairs_e], errors="coerce")
L684                         val = pd.to_numeric([v for (_d, v) in pairs_e], errors="coerce")
L685                         s = pd.Series(val, index=idx).sort_index()
L686                         info[t]["SEC_EPS_Q_SERIES"] = s
L687                     else:
L688                         info[t]["SEC_EPS_Q_SERIES"] = sec_map[t].get("eps_q_series") or []
L689         except Exception:
L690             sec_map = None
L691         eps_df = self._build_eps_df(tickers, tickers_bulk, info, sec_map=sec_map)
L692         # index é‡è¤‡ãŒã‚ã‚‹ã¨ .loc[t, col] ãŒ Series ã«ãªã‚Šä»£å…¥æ™‚ã« ValueError ã‚’èª˜ç™ºã™ã‚‹
L693         if not eps_df.index.is_unique:
L694             eps_df = eps_df[~eps_df.index.duplicated(keep="last")]
L695         eps_df = eps_df.assign(
L696             EPS_TTM=eps_df["eps_ttm"],
L697             EPS_Q_LastQ=eps_df["eps_q_recent"],
L698             REV_TTM=eps_df["rev_ttm"],
L699             REV_Q_LastQ=eps_df["rev_q_recent"],
L700         )
L701         # ã“ã“ã§éNaNä»¶æ•°ã‚’ã‚µãƒãƒªè¡¨ç¤ºï¼ˆæ¬ æçŠ¶æ³ã®å³æ™‚æŠŠæ¡ç”¨ï¼‰
L702         try:
L703             n = len(eps_df)
L704             c_eps = int(eps_df["EPS_TTM"].notna().sum())
L705             c_rev = int(eps_df["REV_TTM"].notna().sum())
L706             print(f"[SEC] eps_ttm non-NaN: {c_eps}/{n}  rev_ttm non-NaN: {c_rev}/{n}")
L707         except Exception:
L708             pass
L709         fcf_df = self.compute_fcf_with_fallback(tickers, finnhub_api_key=self.api_key)
L710         T.log("eps/fcf prep done")
L711         returns = px[tickers].pct_change()
L712         T.log("price prep/returns done")
L713         return dict(cand=cand_f, tickers=tickers, data=data, px=px, spx=spx, tickers_bulk=tickers_bulk, info=info, eps_df=eps_df, fcf_df=fcf_df, returns=returns)
L714
L715 # === Selectorï¼šç›¸é–¢ä½æ¸›ãƒ»é¸å®šï¼ˆã‚¹ã‚³ã‚¢ï¼†ãƒªã‚¿ãƒ¼ãƒ³ã ã‘èª­ã‚€ï¼‰ ===
L716 class Selector:
L717     # ---- DRRS helpersï¼ˆSelectorå°‚ç”¨ï¼‰ ----
L718     @staticmethod
L719     def _z_np(X: np.ndarray) -> np.ndarray:
L720         X = np.asarray(X, dtype=np.float32); m = np.nanmean(X, axis=0, keepdims=True); s = np.nanstd(X, axis=0, keepdims=True)+1e-9
L721         return (np.nan_to_num(X)-m)/s
L722
L723     @classmethod
L724     def residual_corr(cls, R: np.ndarray, n_pc: int=3, shrink: float=0.1) -> np.ndarray:
L725         Z = cls._z_np(R); U,S,_ = np.linalg.svd(Z, full_matrices=False); F = U[:,:n_pc]*S[:n_pc]; B = np.linalg.lstsq(F, Z, rcond=None)[0]
L726         E = Z - F@B; C = np.corrcoef(E, rowvar=False)
L727         off = C - np.diag(np.diag(C)); iu = np.triu_indices_from(off,1); avg_abs = np.nanmean(np.abs(off[iu])) if iu[0].size else 0.0
L728         shrink_eff = float(np.clip(shrink + 0.5*avg_abs, 0.1, 0.6)); N = C.shape[0]
L729         return (1.0 - shrink_eff)*C + shrink_eff*np.eye(N, dtype=C.dtype)
L730
L731     @classmethod
L732     def rrqr_like_det(cls, R: np.ndarray, score: np.ndarray, k: int, gamma: float=1.0):
L733         Z, w = cls._z_np(R), (score-score.min())/(np.ptp(score)+1e-12); X = Z*(1.0 + gamma*w)
L734         N, k = X.shape[1], max(0, min(k, X.shape[1]))
L735         if k==0: return []
L736         S, selected, Rres = [], np.zeros(N, dtype=bool), X.copy()
L737         for _ in range(k):
L738             norms = (Rres*Rres).sum(axis=0); cand = np.where(~selected)[0]
L739             j = sorted(cand, key=lambda c:(-norms[c], -w[c], c))[0]
L740             S.append(j); selected[j]=True; u = X[:,j:j+1]; u/=(np.linalg.norm(u)+1e-12); Rres = Rres - u @ (u.T @ Rres)
L741         return sorted(S)
L742
L743     @staticmethod
L744     def _obj(corrM: np.ndarray, score: np.ndarray, idx, lam: float) -> float:
L745         idx = list(idx); P = corrM[np.ix_(idx, idx)]; s = (score-score.mean())/(score.std()+1e-9)
L746         return float(s[idx].sum() - lam*((P.sum()-np.trace(P))/2.0))
L747
L748     @classmethod
L749     def swap_local_det(cls, corrM: np.ndarray, score: np.ndarray, idx, lam: float=0.6, max_pass: int=15):
L750         S, best, improved, passes = sorted(idx), cls._obj(corrM, score, idx, lam), True, 0
L751         while improved and passes<max_pass:
L752             improved, passes = False, passes+1
L753             for i,out in enumerate(list(S)):
L754                 for inn in range(len(score)):
L755                     if inn in S: continue
L756                     cand = sorted(S[:i]+[inn]+S[i+1:]); v = cls._obj(corrM, score, cand, lam)
L757                     if v>best+1e-10: S, best, improved = cand, v, True; break
L758                 if improved: break
L759         return S, best
L760
L761     @staticmethod
L762     def _obj_with_cross(C_within: np.ndarray, C_cross: np.ndarray|None, score: np.ndarray, idx, lam: float, mu: float) -> float:
L763         idx = list(idx); P = C_within[np.ix_(idx, idx)]; s = (score-score.mean())/(score.std()+1e-9)
L764         within = (P.sum()-np.trace(P))/2.0; cross = 0.0
L765         if C_cross is not None and C_cross.size>0: cross = C_cross[idx,:].sum()
L766         return float(s[idx].sum() - lam*within - mu*cross)
L767
L768     @classmethod
L769     def swap_local_det_cross(cls, C_within: np.ndarray, C_cross: np.ndarray|None, score: np.ndarray, idx, lam: float=0.6, mu: float=0.3, max_pass: int=15):
L770         S, best, improved, passes, N = sorted(idx), cls._obj_with_cross(C_within,C_cross,score,idx,lam,mu), True, 0, len(score)
L771         while improved and passes<max_pass:
L772             improved, passes = False, passes+1
L773             for i,out in enumerate(list(S)):
L774                 for inn in range(N):
L775                     if inn in S: continue
L776                     cand = sorted(S[:i]+[inn]+S[i+1:]); v = cls._obj_with_cross(C_within,C_cross,score,cand,lam,mu)
L777                     if v>best+1e-10: S, best, improved = cand, v, True; break
L778                 if improved: break
L779         return S, best
L780
L781     @staticmethod
L782     def avg_corr(C: np.ndarray, idx) -> float:
L783         k = len(idx); P = C[np.ix_(idx, idx)]
L784         return float((P.sum()-np.trace(P))/(k*(k-1)+1e-12))
L785
L786     @classmethod
L787     def select_bucket_drrs(cls, returns_df: pd.DataFrame, score_ser: pd.Series, pool_tickers: list[str], k: int, *, n_pc: int, gamma: float, lam: float, lookback: int, shrink: float=0.10, g_fixed_tickers: list[str]|None=None, mu: float=0.0):
L788         g_fixed = [t for t in (g_fixed_tickers or []) if t in returns_df.columns]
L789         union = [t for t in pool_tickers if t in returns_df.columns]
L790         for t in g_fixed:
L791             if t not in union: union.append(t)
L792         Rdf_all = returns_df[union]; Rdf_all = Rdf_all.iloc[-lookback:] if len(Rdf_all)>=lookback else Rdf_all; Rdf_all = Rdf_all.dropna()
L793         pool_eff, g_eff = [t for t in pool_tickers if t in Rdf_all.columns], [t for t in g_fixed if t in Rdf_all.columns]
L794         if len(pool_eff)==0: return dict(idx=[], tickers=[], avg_res_corr=np.nan, sum_score=0.0, objective=-np.inf)
L795         score = score_ser.reindex(pool_eff).to_numpy(dtype=np.float32)
L796         C_all = cls.residual_corr(Rdf_all.to_numpy(), n_pc=n_pc, shrink=shrink)
L797         col_pos = {c:i for i,c in enumerate(Rdf_all.columns)}; pool_pos = [col_pos[t] for t in pool_eff]
L798         C_within, C_cross = C_all[np.ix_(pool_pos,pool_pos)], None
L799         if len(g_eff)>0 and mu>0.0:
L800             g_pos = [col_pos[t] for t in g_eff]; C_cross = C_all[np.ix_(pool_pos,g_pos)]
L801         R_pool = Rdf_all[pool_eff].to_numpy(); S0 = cls.rrqr_like_det(R_pool, score, k, gamma=gamma)
L802         S, Jn = (cls.swap_local_det_cross(C_within, C_cross, score, S0, lam=lam, mu=mu, max_pass=15) if C_cross is not None else cls.swap_local_det(C_within, score, S0, lam=lam, max_pass=15))
L803         selected_tickers = [pool_eff[i] for i in S]
L804         return dict(idx=S, tickers=selected_tickers, avg_res_corr=cls.avg_corr(C_within,S), sum_score=float(score[S].sum()), objective=float(Jn))
L805
L806     # ---- é¸å®šï¼ˆã‚¹ã‚³ã‚¢ Series / returns ã ã‘ã‚’å—ã‘ã‚‹ï¼‰----
L807 # === Outputï¼šå‡ºåŠ›æ•´å½¢ã¨é€ä¿¡ï¼ˆè¡¨ç¤ºãƒ»Slackï¼‰ ===
L808 class Output:
L809
L810     def __init__(self, debug=None):
L811         # self.debug ã¯ä½¿ã‚ãªã„ï¼ˆäº’æ›ã®ãŸã‚å¼•æ•°ã¯å—ã‘ã‚‹ãŒç„¡è¦–ï¼‰
L812         self.miss_df = self.g_table = self.d_table = self.io_table = self.df_metrics_fmt = self.debug_table = None
L813         self.g_title = self.d_title = ""
L814         self.g_formatters = self.d_formatters = {}
L815         # ä½ã‚¹ã‚³ã‚¢ï¼ˆGSC+DSCï¼‰Top10 è¡¨ç¤º/é€ä¿¡ç”¨
L816         self.low10_table = None
L817         self.debug_text = ""   # ãƒ‡ãƒãƒƒã‚°ç”¨æœ¬æ–‡ã¯ã“ã“ã«ä¸€æœ¬åŒ–
L818         self._debug_logged = False
L819
L820     # --- è¡¨ç¤ºï¼ˆå…ƒ display_results ã®ãƒ­ã‚¸ãƒƒã‚¯ãã®ã¾ã¾ï¼‰ ---
L821     def display_results(self, *, exist, bench, df_z, g_score, d_score_all,
L822                         init_G, init_D, top_G, top_D, **kwargs):
L823         logger.info("ğŸ“Œ reached display_results")
L824         pd.set_option('display.float_format','{:.3f}'.format)
L825         print("ğŸ“ˆ ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼åˆ†æ•£æœ€é©åŒ–ã®çµæœ")
L826         if self.miss_df is not None and not self.miss_df.empty:
L827             print("Missing Data:")
L828             print(self.miss_df.to_string(index=False))
L829
L830         # ---- è¡¨ç¤ºç”¨ï¼šChanges/Near-Miss ã®ã‚¹ã‚³ã‚¢æºã‚’â€œæœ€çµ‚é›†è¨ˆâ€ã«çµ±ä¸€ã™ã‚‹ãƒ—ãƒ­ã‚­ã‚· ----
L831         try:
L832             sc = getattr(self, "_sc", None)
L833             agg_G = getattr(sc, "_agg_G", None)
L834             agg_D = getattr(sc, "_agg_D", None)
L835         except Exception:
L836             sc = agg_G = agg_D = None
L837         class _SeriesProxy:
L838             __slots__ = ("primary", "fallback")
L839             def __init__(self, primary, fallback): self.primary, self.fallback = primary, fallback
L840             def get(self, key, default=None):
L841                 try:
L842                     v = self.primary.get(key) if hasattr(self.primary, "get") else None
L843                     if v is not None and not (isinstance(v, float) and v != v):
L844                         return v
L845                 except Exception:
L846                     pass
L847                 try:
L848                     return self.fallback.get(key) if hasattr(self.fallback, "get") else default
L849                 except Exception:
L850                     return default
L851         g_score = _SeriesProxy(agg_G, g_score)
L852         d_score_all = _SeriesProxy(agg_D, d_score_all)
L853         near_G = getattr(sc, "_near_G", []) if sc else []
L854         near_D = getattr(sc, "_near_D", []) if sc else []
L855
L856         extra_G = [t for t in init_G if t not in top_G][:5]; G_UNI = top_G + extra_G
L857         gsc_series = pd.Series({t: g_score.get(t) for t in G_UNI}, name='GSC')
L858         self.g_table = pd.concat([df_z.loc[G_UNI,['GROWTH_F','MOM','TRD','VOL']], gsc_series], axis=1)
L859         self.g_table.index = [t + ("â­ï¸" if t in top_G else "") for t in G_UNI]
L860         self.g_formatters = {col:"{:.2f}".format for col in ['GROWTH_F','MOM','TRD','VOL']}; self.g_formatters['GSC'] = "{:.3f}".format
L861         self.g_title = (f"[Gæ  / {N_G} / {_fmt_w(g_weights)} / corrM={corrM} / "
L862                         f"LB={DRRS_G['lookback']} nPC={DRRS_G['n_pc']} Î³={DRRS_G['gamma']} Î»={DRRS_G['lam']} Î·={DRRS_G['eta']} shrink={DRRS_SHRINK}]")
L863         if near_G:
L864             add = [t for t in near_G if t not in set(G_UNI)][:10]
L865             if len(add) < 10:
L866                 try:
L867                     aggG = getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False)
L868                     out_now = sorted(set(exist) - set(top_G + top_D))  # ä»Šå› OUT
L869                     used = set(G_UNI + add)
L870                     def _push(lst):
L871                         nonlocal add, used
L872                         for t in lst:
L873                             if len(add) == 10: break
L874                             if t in aggG.index and t not in used:
L875                                 add.append(t); used.add(t)
L876                     _push(out_now)           # â‘  ä»Šå› OUT ã‚’å„ªå…ˆ
L877                     _push(list(aggG.index))  # â‘¡ ã¾ã è¶³ã‚Šãªã‘ã‚Œã°ä¸Šä½ã§å……å¡«
L878                 except Exception:
L879                     pass
L880             if add:
L881                 near_tbl = pd.concat([df_z.loc[add,['GROWTH_F','MOM','TRD','VOL']], pd.Series({t: g_score.get(t) for t in add}, name='GSC')], axis=1)
L882                 self.g_table = pd.concat([self.g_table, near_tbl], axis=0)
L883         print(self.g_title); print(self.g_table.to_string(formatters=self.g_formatters))
L884
L885         extra_D = [t for t in init_D if t not in top_D][:5]; D_UNI = top_D + extra_D
L886         cols_D = ['QAL','YLD','VOL','TRD']; d_disp = pd.DataFrame(index=D_UNI)
L887         d_disp['QAL'], d_disp['YLD'], d_disp['VOL'], d_disp['TRD'] = df_z.loc[D_UNI,'D_QAL'], df_z.loc[D_UNI,'D_YLD'], df_z.loc[D_UNI,'D_VOL_RAW'], df_z.loc[D_UNI,'D_TRD']
L888         dsc_series = pd.Series({t: d_score_all.get(t) for t in D_UNI}, name='DSC')
L889         self.d_table = pd.concat([d_disp, dsc_series], axis=1); self.d_table.index = [t + ("â­ï¸" if t in top_D else "") for t in D_UNI]
L890         self.d_formatters = {col:"{:.2f}".format for col in cols_D}; self.d_formatters['DSC']="{:.3f}".format
L891         import scorer
L892         dw_eff = scorer.D_WEIGHTS_EFF
L893         self.d_title = (f"[Dæ  / {N_D} / {_fmt_w(dw_eff)} / corrM={corrM} / "
L894                         f"LB={DRRS_D['lookback']} nPC={DRRS_D['n_pc']} Î³={DRRS_D['gamma']} Î»={DRRS_D['lam']} Î¼={CROSS_MU_GD} Î·={DRRS_D['eta']} shrink={DRRS_SHRINK}]")
L895         if near_D:
L896             add = [t for t in near_D if t not in set(D_UNI)][:10]
L897             if add:
L898                 d_disp2 = pd.DataFrame(index=add)
L899                 d_disp2['QAL'], d_disp2['YLD'], d_disp2['VOL'], d_disp2['TRD'] = df_z.loc[add,'D_QAL'], df_z.loc[add,'D_YLD'], df_z.loc[add,'D_VOL_RAW'], df_z.loc[add,'D_TRD']
L900                 near_tbl = pd.concat([d_disp2, pd.Series({t: d_score_all.get(t) for t in add}, name='DSC')], axis=1)
L901                 self.d_table = pd.concat([self.d_table, near_tbl], axis=0)
L902         print(self.d_title); print(self.d_table.to_string(formatters=self.d_formatters))
L903
L904         # === Changesï¼ˆIN ã® GSC/DSC ã‚’è¡¨ç¤ºã€‚OUT ã¯éŠ˜æŸ„åã®ã¿ï¼‰ ===
L905         in_list = sorted(set(list(top_G)+list(top_D)) - set(exist))
L906         out_list = sorted(set(exist) - set(list(top_G)+list(top_D)))
L907
L908         self.io_table = pd.DataFrame({
L909             'IN': pd.Series(in_list),
L910             '/ OUT': pd.Series(out_list)
L911         })
L912         g_list = [f"{g_score.get(t):.3f}" if pd.notna(g_score.get(t)) else 'â€”' for t in out_list]
L913         d_list = [f"{d_score_all.get(t):.3f}" if pd.notna(d_score_all.get(t)) else 'â€”' for t in out_list]
L914         self.io_table['GSC'] = pd.Series(g_list)
L915         self.io_table['DSC'] = pd.Series(d_list)
L916
L917         print("Changes:")
L918         print(self.io_table.to_string(index=False))
L919
L920         all_tickers = list(set(exist + list(top_G) + list(top_D) + [bench])); prices = yf.download(all_tickers, period='1y', auto_adjust=True, progress=False, threads=False)['Close'].ffill(limit=2)
L921         ret = prices.pct_change(); portfolios = {'CUR':exist,'NEW':list(top_G)+list(top_D)}; metrics={}
L922         for name,ticks in portfolios.items():
L923             pr = ret[ticks].mean(axis=1, skipna=True).dropna(); cum = (1+pr).cumprod()-1; n = len(pr)
L924             if n>=252: ann_ret, ann_vol = (1+cum.iloc[-1])**(252/n)-1, pr.std()*np.sqrt(252)
L925             else: ann_ret, ann_vol = cum.iloc[-1], pr.std()*np.sqrt(n)
L926             sharpe, drawdown = ann_ret/ann_vol, (cum - cum.cummax()).min()
L927             if len(ticks)>=2:
L928                 C_raw = ret[ticks].corr(); RAW_rho = C_raw.mask(np.eye(len(ticks), dtype=bool)).stack().mean()
L929                 R = ret[ticks].dropna().to_numpy(); C_resid = Selector.residual_corr(R, n_pc=3, shrink=DRRS_SHRINK)
L930                 RESID_rho = float((C_resid.sum()-np.trace(C_resid))/(C_resid.shape[0]*(C_resid.shape[0]-1)))
L931             else: RAW_rho = RESID_rho = np.nan
L932             divy = ttm_div_yield_portfolio(ticks); metrics[name] = {'RET':ann_ret,'VOL':ann_vol,'SHP':sharpe,'MDD':drawdown,'RAWÏ':RAW_rho,'RESIDÏ':RESID_rho,'DIVY':divy}
L933         df_metrics = pd.DataFrame(metrics).T; df_metrics_pct = df_metrics.copy(); self.df_metrics = df_metrics
L934         for col in ['RET','VOL','MDD','DIVY']: df_metrics_pct[col] = df_metrics_pct[col]*100
L935         cols_order = ['RET','VOL','SHP','MDD','RAWÏ','RESIDÏ','DIVY']; df_metrics_pct = df_metrics_pct.reindex(columns=cols_order)
L936         def _fmt_row(s):
L937             return pd.Series({'RET':f"{s['RET']:.1f}%",'VOL':f"{s['VOL']:.1f}%",'SHP':f"{s['SHP']:.1f}",'MDD':f"{s['MDD']:.1f}%",'RAWÏ':(f"{s['RAWÏ']:.2f}" if pd.notna(s['RAWÏ']) else "NaN"),'RESIDÏ':(f"{s['RESIDÏ']:.2f}" if pd.notna(s['RESIDÏ']) else "NaN"),'DIVY':f"{s['DIVY']:.1f}%"})
L938         self.df_metrics_fmt = df_metrics_pct.apply(_fmt_row, axis=1); print("Performance Comparison:"); print(self.df_metrics_fmt.to_string())
L939         # === è¿½åŠ : GSC+DSC ãŒä½ã„é † TOP10 ===
L940         try:
L941             all_scores = pd.DataFrame({'GSC': df_z['GSC'], 'DSC': df_z['DSC']}).copy()
L942             all_scores['G_plus_D'] = all_scores['GSC'] + all_scores['DSC']
L943             all_scores = all_scores.dropna(subset=['G_plus_D'])
L944             self.low10_table = all_scores.sort_values('G_plus_D', ascending=True).head(10).round(3)
L945             print("Low Score Candidates (GSC+DSC bottom 10):")
L946             print(self.low10_table.to_string())
L947         except Exception as e:
L948             print(f"[warn] low-score ranking failed: {e}")
L949             self.low10_table = None
L950         self.debug_text = ""
L951         if debug_mode:
L952             logger.info("debug_mode=True: df_z dump handled in scorer; skipping factor-side debug output")
L953         else:
L954             logger.debug(
L955                 "skip debug log: debug_mode=%s debug_text_empty=%s",
L956                 debug_mode, True
L957             )
L958         self._debug_logged = True
L959
L960     # --- Slacké€ä¿¡ï¼ˆå…ƒ notify_slack ã®ãƒ­ã‚¸ãƒƒã‚¯ãã®ã¾ã¾ï¼‰ ---
L961     def notify_slack(self):
L962         SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")
L963
L964         if not SLACK_WEBHOOK_URL:
L965             print("âš ï¸ SLACK_WEBHOOK_URL not set (main report skipped)")
L966             return
L967
L968         def _filter_suffix_from(spec: dict, group: str) -> str:
L969             g = spec.get(group, {})
L970             parts = [str(m) for m in g.get("pre_mask", [])]
L971             for k, v in (g.get("pre_filter", {}) or {}).items():
L972                 base, op = (k[:-4], "<") if k.endswith("_max") else ((k[:-4], ">") if k.endswith("_min") else (k, "="))
L973                 name = {"beta": "Î²"}.get(base, base)
L974                 try:
L975                     val = f"{float(v):g}"
L976                 except Exception:
L977                     val = str(v)
L978                 parts.append(f"{name}{op}{val}")
L979             return "" if not parts else " / filter:" + " & ".join(parts)
L980
L981         def _inject_filter_suffix(title: str, group: str) -> str:
L982             suf = _filter_suffix_from(FILTER_SPEC, group)
L983             return f"{title[:-1]}{suf}]" if suf and title.endswith("]") else (title + suf)
L984
L985         def _blk(title, tbl, fmt=None, drop=()):
L986             if tbl is None or getattr(tbl, 'empty', False):
L987                 return f"{title}\n(é¸å®šãªã—)\n"
L988             if drop and hasattr(tbl, 'columns'):
L989                 keep = [c for c in tbl.columns if c not in drop]
L990                 tbl, fmt = tbl[keep], {k: v for k, v in (fmt or {}).items() if k in keep}
L991             return f"{title}\n```{tbl.to_string(formatters=fmt)}```\n"
L992
L993         message = "ğŸ“ˆ ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼åˆ†æ•£æœ€é©åŒ–ã®çµæœ\n"
L994         if self.miss_df is not None and not self.miss_df.empty:
L995             message += "Missing Data\n```" + self.miss_df.to_string(index=False) + "```\n"
L996         message += _blk(_inject_filter_suffix(self.g_title, "G"), self.g_table, self.g_formatters, drop=("TRD",))
L997         message += _blk(_inject_filter_suffix(self.d_title, "D"), self.d_table, self.d_formatters)
L998         message += "Changes\n" + ("(å¤‰æ›´ãªã—)\n" if self.io_table is None or getattr(self.io_table, 'empty', False) else f"```{self.io_table.to_string(index=False)}```\n")
L999         message += "Performance Comparison:\n```" + self.df_metrics_fmt.to_string() + "```"
L1000
L1001         try:
L1002             r = requests.post(SLACK_WEBHOOK_URL, json={"text": message})
L1003             print(f"[DBG] main_post status={getattr(r, 'status_code', None)} size={len(message)}")
L1004             if r is not None:
L1005                 r.raise_for_status()
L1006         except Exception as e:
L1007             print(f"[ERR] main_post_failed: {e}")
L1008
L1009 def _infer_g_universe(feature_df, selected12=None, near5=None):
L1010     try:
L1011         out = feature_df.index[feature_df['group'].astype(str).str.upper().eq('G')].tolist()
L1012         if out: return out
L1013     except Exception:
L1014         pass
L1015     base = set()
L1016     for lst in (selected12 or []), (near5 or []):
L1017         for x in (lst or []): base.add(x)
L1018     return list(base) if base else list(feature_df.index)
L1019
L1020 def _fmt_with_fire_mark(tickers, feature_df):
L1021     out = []
L1022     for t in tickers or []:
L1023         try:
L1024             br = bool(feature_df.at[t, "G_BREAKOUT_recent_5d"])
L1025             pb = bool(feature_df.at[t, "G_PULLBACK_recent_5d"])
L1026             out.append(f"{t}{' ğŸ”¥' if (br or pb) else ''}")
L1027         except Exception:
L1028             out.append(t)
L1029     return out
L1030
L1031 def _label_recent_event(t, feature_df):
L1032     try:
L1033         br = bool(feature_df.at[t, "G_BREAKOUT_recent_5d"]); dbr = str(feature_df.at[t, "G_BREAKOUT_last_date"]) if br else ""
L1034         pb = bool(feature_df.at[t, "G_PULLBACK_recent_5d"]); dpb = str(feature_df.at[t, "G_PULLBACK_last_date"]) if pb else ""
L1035         if   br and not pb: return f"{t}ï¼ˆãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®š {dbr}ï¼‰"
L1036         elif pb and not br: return f"{t}ï¼ˆæŠ¼ã—ç›®åç™º {dpb}ï¼‰"
L1037         elif br and pb:     return f"{t}ï¼ˆãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®š {dbr}ï¼æŠ¼ã—ç›®åç™º {dpb}ï¼‰"
L1038     except Exception:
L1039         pass
L1040     return t
L1041
L1042 # === ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å¯è¦–åŒ–ï¼šG/Då…±é€šãƒ•ãƒ­ãƒ¼ï¼ˆå‡ºåŠ›ã¯ä¸å¤‰ï¼‰ ===
L1043
L1044 def io_build_input_bundle() -> InputBundle:
L1045     """
L1046     æ—¢å­˜ã®ã€ãƒ‡ãƒ¼ã‚¿å–å¾—â†’å‰å‡¦ç†ã€ã‚’å®Ÿè¡Œã—ã€InputBundle ã‚’è¿”ã™ã€‚
L1047     å‡¦ç†å†…å®¹ãƒ»åˆ—åãƒ»ä¸¸ã‚ãƒ»ä¾‹å¤–ãƒ»ãƒ­ã‚°æ–‡è¨€ã¯ç¾è¡Œã©ãŠã‚Šï¼ˆå¤‰æ›´ç¦æ­¢ï¼‰ã€‚
L1048     """
L1049     state = Input(cand=cand, exist=exist, bench=bench, price_max=CAND_PRICE_MAX, finnhub_api_key=FINNHUB_API_KEY).prepare_data()
L1050     return InputBundle(cand=state["cand"], tickers=state["tickers"], bench=bench, data=state["data"], px=state["px"], spx=state["spx"], tickers_bulk=state["tickers_bulk"], info=state["info"], eps_df=state["eps_df"], fcf_df=state["fcf_df"], returns=state["returns"])
L1051
L1052 def run_group(sc: Scorer, group: str, inb: InputBundle, cfg: PipelineConfig,
L1053               n_target: int) -> tuple[list, float, float, float]:
L1054     """
L1055     G/Dã‚’åŒä¸€æ‰‹é †ã§å‡¦ç†ï¼šæ¡ç‚¹â†’ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼â†’é¸å®šï¼ˆç›¸é–¢ä½æ¸›è¾¼ã¿ï¼‰ã€‚
L1056     æˆ»ã‚Šå€¤ï¼š(pick, avg_res_corr, sum_score, objective)
L1057     JSONä¿å­˜ã¯æ—¢å­˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆã‚­ãƒ¼åãƒ»ä¸¸ã‚æ¡ãƒ»é †åºï¼‰ã‚’è¸è¥²ã€‚
L1058     """
L1059     sc.cfg = cfg
L1060
L1061     if hasattr(sc, "score_build_features"):
L1062         feat = sc.score_build_features(inb)
L1063         if not hasattr(sc, "_feat_logged"):
L1064             T.log("features built (scorer)")
L1065             sc._feat_logged = True
L1066         agg = sc.score_aggregate(feat, group, cfg) if hasattr(sc, "score_aggregate") else feat
L1067     else:
L1068         fb = sc.aggregate_scores(inb, cfg)
L1069         if not hasattr(sc, "_feat_logged"):
L1070             T.log("features built (scorer)")
L1071             sc._feat_logged = True
L1072         sc._feat = fb
L1073         agg = fb.g_score if group == "G" else fb.d_score_all
L1074         if group == "D" and hasattr(fb, "df"):
L1075             agg = agg[fb.df['BETA'] < D_BETA_MAX]
L1076
L1077     if hasattr(sc, "filter_candidates"):
L1078         agg = agg[sc.filter_candidates(inb, agg, group, cfg)]
L1079
L1080     selector = Selector()
L1081     if hasattr(sc, "select_diversified"):
L1082         pick, avg_r, sum_sc, obj = sc.select_diversified(agg, group, cfg, n_target,
L1083             selector=selector, prev_tickers=None,
L1084             corrM=cfg.drrs.corrM, shrink=cfg.drrs.shrink,
L1085             cross_mu=cfg.drrs.cross_mu_gd)
L1086     else:
L1087         if group == "G":
L1088             init = agg.nlargest(min(cfg.drrs.corrM, len(agg))).index.tolist()
L1089             res = selector.select_bucket_drrs(returns_df=inb.returns, score_ser=agg, pool_tickers=init, k=n_target,
L1090                 n_pc=cfg.drrs.G.get("n_pc", 3), gamma=cfg.drrs.G.get("gamma", 1.2),
L1091                 lam=cfg.drrs.G.get("lam", 0.68),
L1092                 lookback=cfg.drrs.G.get("lookback", 252),
L1093                 shrink=cfg.drrs.shrink, g_fixed_tickers=None, mu=0.0)
L1094         else:
L1095             init = agg.nlargest(min(cfg.drrs.corrM, len(agg))).index.tolist()
L1096             g_fixed = getattr(sc, "_top_G", None)
L1097             res = selector.select_bucket_drrs(returns_df=inb.returns, score_ser=agg, pool_tickers=init, k=n_target,
L1098                 n_pc=cfg.drrs.D.get("n_pc", 4), gamma=cfg.drrs.D.get("gamma", 0.8),
L1099                 lam=cfg.drrs.D.get("lam", 0.85),
L1100                 lookback=cfg.drrs.D.get("lookback", 504),
L1101                 shrink=cfg.drrs.shrink, g_fixed_tickers=g_fixed,
L1102                 mu=cfg.drrs.cross_mu_gd)
L1103         pick = res["tickers"]; avg_r = res["avg_res_corr"]
L1104         sum_sc = res["sum_score"]; obj = res["objective"]
L1105         if group == "D":
L1106             _, pick = _disjoint_keepG(getattr(sc, "_top_G", []), pick, init)
L1107             T.log("selection finalized (G/D)")
L1108     try:
L1109         inc = [t for t in exist if t in agg.index]
L1110         pick = _sticky_keep_current(
L1111             agg=agg, pick=pick, incumbents=inc, n_target=n_target,
L1112             delta_z=SWAP_DELTA_Z, keep_buffer=SWAP_KEEP_BUFFER
L1113         )
L1114     except Exception as _e:
L1115         print(f"[warn] sticky_keep_current skipped: {str(_e)}")
L1116     # --- Near-Miss: æƒœã—ãã‚‚é¸ã°ã‚Œãªã‹ã£ãŸä¸Šä½10ã‚’ä¿æŒï¼ˆSlackè¡¨ç¤ºç”¨ï¼‰ ---
L1117     # 5) Near-Miss ã¨æœ€çµ‚é›†è¨ˆSeriesã‚’ä¿æŒï¼ˆè¡¨ç¤ºå°‚ç”¨ã€‚è¨ˆç®—ã¸å½±éŸ¿ãªã—ï¼‰
L1118     try:
L1119         pool = agg.drop(index=[t for t in pick if t in agg.index], errors="ignore")
L1120         near10 = list(pool.sort_values(ascending=False).head(10).index)
L1121         setattr(sc, f"_near_{group}", near10)
L1122         setattr(sc, f"_agg_{group}", agg)
L1123     except Exception:
L1124         pass
L1125
L1126     if group == "D":
L1127         T.log("save done")
L1128     if group == "G":
L1129         sc._top_G = pick
L1130     return pick, avg_r, sum_sc, obj
L1131
L1132 def run_pipeline() -> SelectionBundle:
L1133     """
L1134     G/Då…±é€šãƒ•ãƒ­ãƒ¼ã®å…¥å£ã€‚I/Oã¯ã“ã“ã ã‘ã§å®Ÿæ–½ã—ã€è¨ˆç®—ã¯Scorerã«å§”è­²ã€‚
L1135     Slackæ–‡è¨€ãƒ»ä¸¸ã‚ãƒ»é †åºã¯æ—¢å­˜ã® Output ã‚’ç”¨ã„ã¦å¤‰æ›´ã—ãªã„ã€‚
L1136     """
L1137     inb = io_build_input_bundle()
L1138     cfg = PipelineConfig(
L1139         weights=WeightsConfig(g=g_weights, d=D_weights),
L1140         drrs=DRRSParams(
L1141             corrM=corrM, shrink=DRRS_SHRINK,
L1142             G=DRRS_G, D=DRRS_D, cross_mu_gd=CROSS_MU_GD
L1143         ),
L1144         price_max=CAND_PRICE_MAX,
L1145         debug_mode=debug_mode
L1146     )
L1147     sc = Scorer()
L1148     top_G, avgG, sumG, objG = run_group(sc, "G", inb, cfg, N_G)
L1149     poolG = list(getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False).index)
L1150     alpha = Scorer.spx_to_alpha(inb.spx)
L1151     sectors = {t:(inb.info.get(t,{}).get("sector") or "U") for t in poolG}; scores = {t:Scorer.g_score.get(t,0.0) for t in poolG}
L1152     top_G = Scorer.pick_top_softcap(scores, sectors, N=N_G, cap=2, alpha=alpha, hard=5)
L1153     sc._top_G = top_G
L1154     try:
L1155         aggG = getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False)
L1156         sc._near_G = [t for t in aggG.index if t not in set(top_G)][:10]
L1157     except Exception:
L1158         pass
L1159     base = sum(Scorer.g_score.get(t,0.0) for t in poolG[:N_G])
L1160     effs = sum(Scorer.g_score.get(t,0.0) for t in top_G)
L1161     print(f"[soft_cap2] score_cost={(base-effs)/max(1e-9,abs(base)):.2%}, alpha={alpha:.3f}")
L1162     top_D, avgD, sumD, objD = run_group(sc, "D", inb, cfg, N_D)
L1163     fb = getattr(sc, "_feat", None)
L1164     near_G = getattr(sc, "_near_G", [])
L1165     selected12 = list(top_G)
L1166     df = fb.df if fb is not None else pd.DataFrame()
L1167     guni = _infer_g_universe(df, selected12, near_G)
L1168     try:
L1169         fire_recent = [t for t in guni
L1170                        if (str(df.at[t, "G_BREAKOUT_recent_5d"]) == "True") or
L1171                           (str(df.at[t, "G_PULLBACK_recent_5d"]) == "True")]
L1172     except Exception: fire_recent = []
L1173
L1174     lines = [
L1175         "ã€Gæ ãƒ¬ãƒãƒ¼ãƒˆï½œé€±æ¬¡ãƒ¢ãƒ‹ã‚¿ï¼ˆç›´è¿‘5å–¶æ¥­æ—¥ï¼‰ã€‘",
L1176         "ã€å‡¡ä¾‹ã€‘ğŸ”¥=ç›´è¿‘5å–¶æ¥­æ—¥å†…ã«ã€Œãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®šã€ã¾ãŸã¯ã€ŒæŠ¼ã—ç›®åç™ºã€ã‚’æ¤œçŸ¥",
L1177         f"é¸å®š{N_G}: {', '.join(_fmt_with_fire_mark(selected12, df))}" if selected12 else f"é¸å®š{N_G}: ãªã—",
L1178         f"æ¬¡ç‚¹10: {', '.join(_fmt_with_fire_mark(near_G, df))}" if near_G else "æ¬¡ç‚¹10: ãªã—",]
L1179
L1180     if fire_recent:
L1181         fire_list = ", ".join([_label_recent_event(t, df) for t in fire_recent])
L1182         lines.append(f"éå»5å–¶æ¥­æ—¥ã®æ¤œçŸ¥: {fire_list}")
L1183     else:
L1184         lines.append("éå»5å–¶æ¥­æ—¥ã®æ¤œçŸ¥: ãªã—")
L1185
L1186     try:
L1187         webhook = os.environ.get("SLACK_WEBHOOK_URL", "")
L1188         if webhook:
L1189             requests.post(webhook, json={"text": "\n".join([s for s in lines if s != ""])}, timeout=10)
L1190     except Exception:
L1191         pass
L1192
L1193     out = Output()
L1194     # è¡¨ç¤ºå´ã‹ã‚‰é¸å®šæ™‚ã®é›†è¨ˆã¸ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†ã«ä¿æŒï¼ˆè¡¨ç¤ºå°‚ç”¨ãƒ»å‰¯ä½œç”¨ãªã—ï¼‰
L1195     try: out._sc = sc
L1196     except Exception: pass
L1197     if hasattr(sc, "_feat"):
L1198         try:
L1199             fb = sc._feat
L1200             out.miss_df = fb.missing_logs
L1201             out.display_results(
L1202                 exist=exist,
L1203                 bench=bench,
L1204                 df_z=fb.df_z,
L1205                 g_score=fb.g_score,
L1206                 d_score_all=fb.d_score_all,
L1207                 init_G=top_G,
L1208                 init_D=top_D,
L1209                 top_G=top_G,
L1210                 top_D=top_D,
L1211                 df_full_z=getattr(fb, "df_full_z", None),
L1212                 prev_G=getattr(sc, "_prev_G", exist),
L1213                 prev_D=getattr(sc, "_prev_D", exist),
L1214             )
L1215         except Exception:
L1216             pass
L1217     out.notify_slack()
L1218     sb = SelectionBundle(resG={"tickers": top_G, "avg_res_corr": avgG,
L1219               "sum_score": sumG, "objective": objG},
L1220         resD={"tickers": top_D, "avg_res_corr": avgD,
L1221               "sum_score": sumD, "objective": objD},
L1222         top_G=top_G, top_D=top_D, init_G=top_G, init_D=top_D)
L1223
L1224     # --- Low Score Candidates (GSC+DSC bottom 10) : send before debug dump ---
L1225     try:
L1226         _low_df = (pd.DataFrame({"GSC": fb.g_score, "DSC": fb.d_score_all})
L1227               .assign(G_plus_D=lambda x: x["GSC"] + x["DSC"])
L1228               .sort_values("G_plus_D")
L1229               .head(10)
L1230               .round(3))
L1231         low_msg = "Low Score Candidates (GSC+DSC bottom 10)\n" + _low_df.to_string(index=True, index_names=False)
L1232         _post_slack({"text": f"```{low_msg}```"})
L1233     except Exception as _e:
L1234         _post_slack({"text": f"```Low Score Candidates: ä½œæˆå¤±æ•—: {_e}```"})
L1235
L1236     return sb
L1237
L1238 if __name__ == "__main__":
L1239     run_pipeline()
```

## <scorer.py>
```text
L1 # scorer.py
L2 # kawatest
L3 # =============================================================================
L4 # Scorer: ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼/æŒ‡æ¨™ã®ç”Ÿæˆã¨åˆæˆã‚¹ã‚³ã‚¢ç®—å‡ºã‚’æ‹…ã†ç´”ç²‹å±¤
L5 #
L6 # ã€ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã ã‘èª­ã‚ã°åˆ†ã‹ã‚‹ãƒã‚¤ãƒ³ãƒˆã€‘
L7 # - å…¥åŠ›(InputBundle)ã¯ã€Œä¾¡æ ¼/å‡ºæ¥é«˜/ãƒ™ãƒ³ãƒ/åŸºæœ¬æƒ…å ±/EPS/FCF/ãƒªã‚¿ãƒ¼ãƒ³ã€ã‚’å«ã‚€DTO
L8 # - å‡ºåŠ›(FeatureBundle)ã¯ã€Œrawç‰¹å¾´é‡ dfã€ã€Œæ¨™æº–åŒ– df_zã€ã€ŒG/D ã‚¹ã‚³ã‚¢ã€ã€Œæ¬ æãƒ­ã‚°ã€
L9 # - é‡ã¿ç­‰ã®ã‚³ãƒ³ãƒ•ã‚£ã‚°(PipelineConfig)ã¯ factor ã‹ã‚‰æ¸¡ã™ï¼ˆcfg å¿…é ˆï¼‰
L10 # - æ—§ã‚«ãƒ©ãƒ åã¯ Scorer å†…ã§è‡ªå‹•ãƒªãƒãƒ¼ãƒ ã—ã¦å—ã‘å…¥ã‚Œï¼ˆå¾Œæ–¹äº’æ›ï¼‰
L11 #   ä¾‹) eps_ttm -> EPS_TTM, eps_q_recent -> EPS_Q_LastQ, fcf_ttm -> FCF_TTM
L12 #
L13 # ã€I/Oå¥‘ç´„ï¼ˆScorerãŒå‚ç…§ã™ã‚‹InputBundleãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰ã€‘
L14 #   - cand: List[str]    â€¦ å€™è£œéŠ˜æŸ„ï¼ˆå˜ä½“å®Ÿè¡Œã§ã¯æœªä½¿ç”¨ï¼‰
L15 #   - tickers: List[str] â€¦ å¯¾è±¡éŠ˜æŸ„ãƒªã‚¹ãƒˆ
L16 #   - bench: str         â€¦ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼ï¼ˆä¾‹ '^GSPC'ï¼‰
L17 #   - data: pd.DataFrame â€¦ yfinance downloadçµæœ ('Close','Volume' ç­‰ã®éšå±¤åˆ—)
L18 #   - px: pd.DataFrame   â€¦ data['Close'] ç›¸å½“ï¼ˆçµ‚å€¤ï¼‰
L19 #   - spx: pd.Series     â€¦ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®çµ‚å€¤
L20 #   - tickers_bulk: object         â€¦ yfinance.Tickers
L21 #   - info: Dict[str, dict]        â€¦ yfinance info per ticker
L22 #   - eps_df: pd.DataFrame         â€¦ å¿…é ˆåˆ—: EPS_TTM, EPS_Q_LastQï¼ˆæ—§åã‚‚å¯ï¼‰
L23 #   - fcf_df: pd.DataFrame         â€¦ å¿…é ˆåˆ—: FCF_TTMï¼ˆæ—§åã‚‚å¯ï¼‰
L24 #   - returns: pd.DataFrame        â€¦ px[tickers].pct_change() ç›¸å½“
L25 #
L26 # â€»å…¥å‡ºåŠ›ã®å½¢å¼ãƒ»ä¾‹å¤–æ–‡è¨€ã¯æ—¢å­˜å®Ÿè£…ã‚’å¤‰ãˆã¾ã›ã‚“ï¼ˆå®‰å…¨ãªçŸ­ç¸®ã®ã¿ï¼‰
L27 # =============================================================================
L28
L29 import logging
L30 import os, sys, warnings
L31 import json
L32 import requests
L33 import numpy as np
L34 import pandas as pd
L35 import yfinance as yf
L36 from typing import Any, TYPE_CHECKING
L37 from scipy.stats import zscore
L38
L39 if TYPE_CHECKING:
L40     from factor import PipelineConfig  # type: ignore  # å®Ÿè¡Œæ™‚importãªã—ï¼ˆå¾ªç’°å›é¿ï¼‰
L41
L42 logger = logging.getLogger(__name__)
L43
L44 # ---- Dividend Helpers -------------------------------------------------------
L45 def _last_close(t, price_map=None):
L46     if price_map and (c := price_map.get(t)) is not None: return float(c)
L47     try:
L48         h = yf.Ticker(t).history(period="5d")["Close"].dropna()
L49         return float(h.iloc[-1]) if len(h) else np.nan
L50     except Exception:
L51         return np.nan
L52
L53 def _ttm_div_sum(t, lookback_days=400):
L54     try:
L55         div = yf.Ticker(t).dividends
L56         if div is None or len(div) == 0: return 0.0
L57         cutoff = pd.Timestamp.utcnow().tz_localize(None) - pd.Timedelta(days=lookback_days)
L58         ttm = float(div[div.index.tz_localize(None) >= cutoff].sum())
L59         return ttm if ttm > 0 else float(div.tail(4).sum())
L60     except Exception:
L61         return 0.0
L62
L63 def ttm_div_yield_portfolio(tickers, price_map=None):
L64     ys = [(lambda c, s: (s/c) if (np.isfinite(c) and c>0 and s>0) else 0.0)(_last_close(t, price_map), _ttm_div_sum(t)) for t in tickers]
L65     return float(np.mean(ys)) if ys else 0.0
L66
L67 # ---- ç°¡æ˜“ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆå®‰å…¨ãªçŸ­ç¸®ã®ã¿ï¼‰ -----------------------------------
L68 def winsorize_s(s: pd.Series, p=0.02):
L69     if s is None or s.dropna().empty: return s
L70     lo, hi = np.nanpercentile(s.astype(float), [100*p, 100*(1-p)]); return s.clip(lo, hi)
L71
L72 def robust_z(s: pd.Series, p=0.02):
L73     s2 = winsorize_s(s,p); return np.nan_to_num(zscore(s2.fillna(s2.mean())))
L74
L75 def robust_z_keepnan(s: pd.Series) -> pd.Series:
L76     """robust_z variant that preserves NaNs and falls back to rank-z when needed."""
L77     if s is None:
L78         return pd.Series(dtype=float)
L79     v = pd.to_numeric(s, errors="coerce")
L80     m = np.nanmedian(v)
L81     mad = np.nanmedian(np.abs(v - m))
L82     z = (v - m) / (1.4826 * mad + 1e-9)
L83     if np.nanstd(z) < 1e-9:
L84         r = v.rank(method="average", na_option="keep")
L85         z = (r - np.nanmean(r)) / (np.nanstd(r) + 1e-9)
L86     return pd.Series(z, index=v.index, dtype=float)
L87
L88
L89 def _dump_dfz(df_z: pd.DataFrame, debug_mode: bool, max_rows: int = 400, ndigits: int = 3) -> None:
L90     """df_z ã‚’ System log(INFO) ã¸ãƒ€ãƒ³ãƒ—ã™ã‚‹ç°¡æ½”ãªãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£."""
L91     if not debug_mode:
L92         return
L93     try:
L94         view = df_z.copy()
L95         view = view.apply(
L96             lambda s: s.round(ndigits)
L97             if getattr(getattr(s, "dtype", None), "kind", "") in ("f", "i")
L98             else s
L99         )
L100         if len(view) > max_rows:
L101             view = view.iloc[:max_rows]
L102
L103         # === NaNã‚µãƒãƒªï¼ˆåˆ—ã”ã¨ã®æ¬ æä»¶æ•° ä¸Šä½20ï¼‰ ===
L104         try:
L105             nan_counts = df_z.isna().sum().sort_values(ascending=False)
L106             top_nan = nan_counts[nan_counts > 0].head(20)
L107             if len(top_nan) > 0:
L108                 logger.info("NaN columns (top20):\n%s", top_nan.to_string())
L109             else:
L110                 logger.info("NaN columns: none")
L111         except Exception as exc:
L112             logger.warning("nan summary failed: %s", exc)
L113
L114         # === Zeroã‚µãƒãƒªï¼ˆåˆ—ã”ã¨ã®ã‚¼ãƒ­æ¯”ç‡ ä¸Šä½20ï¼‰ ===
L115         try:
L116             zero_counts = ((df_z == 0) & (~df_z.isna())).sum()
L117             nonnull_counts = (~df_z.isna()).sum()
L118             zero_ratio = (zero_counts / nonnull_counts).sort_values(ascending=False)
L119             top_zero = zero_ratio[zero_ratio > 0].head(20)
L120             if len(top_zero) > 0:
L121                 logger.info(
L122                     "Zero-dominated columns (top20):\n%s",
L123                     top_zero.to_string(float_format=lambda x: f"{x:.2%}"),
L124                 )
L125             else:
L126                 logger.info("Zero-dominated columns: none")
L127         except Exception as exc:
L128             logger.warning("zero summary failed: %s", exc)
L129
L130         logger.info("===== DF_Z DUMP START =====")
L131         logger.info("\n%s", view.to_string(max_rows=None, max_cols=None))
L132         logger.info("===== DF_Z DUMP END =====")
L133     except Exception as exc:
L134         logger.warning("df_z dump failed: %s", exc)
L135
L136 def _safe_div(a, b):
L137     try: return np.nan if (b is None or float(b)==0 or pd.isna(b)) else float(a)/float(b)
L138     except Exception: return np.nan
L139
L140 def _safe_last(series: pd.Series, default=np.nan):
L141     try: return float(series.iloc[-1])
L142     except Exception: return default
L143
L144
L145 def _ensure_series(data) -> pd.Series:
L146     if isinstance(data, pd.Series):
L147         return data.dropna()
L148     if data is None:
L149         return pd.Series(dtype=float)
L150     if isinstance(data, (list, tuple, np.ndarray)):
L151         try:
L152             return pd.Series(list(data), dtype=float).dropna()
L153         except Exception:
L154             return pd.Series(dtype=float)
L155     try:
L156         if hasattr(data, "values") and hasattr(data, "index"):
L157             return pd.Series(data).dropna()
L158     except Exception:
L159         pass
L160     return pd.Series(dtype=float)
L161
L162
L163 def _nz(x) -> float:
L164     if x is None:
L165         return 0.0
L166     try:
L167         value = float(x)
L168     except Exception:
L169         return 0.0
L170     if not np.isfinite(value):
L171         return 0.0
L172     return value
L173
L174
L175 def _winsor(x, lo=-2.0, hi=2.0) -> float:
L176     v = _nz(x)
L177     if v < lo:
L178         return float(lo)
L179     if v > hi:
L180         return float(hi)
L181     return float(v)
L182
L183
L184 def _round_debug(x, ndigits: int = 4):
L185     try:
L186         value = float(x)
L187     except Exception:
L188         return None
L189     if not np.isfinite(value):
L190         return None
L191     return round(value, ndigits)
L192
L193
L194 def _calc_grw_flexible(
L195     ticker: str,
L196     info_entry: dict | None,
L197     close_series: pd.Series | None,
L198     volume_series: pd.Series | None,
L199 ):
L200     info_entry = info_entry if isinstance(info_entry, dict) else {}
L201
L202     s_rev_q = _ensure_series(info_entry.get("SEC_REV_Q_SERIES"))
L203     s_eps_q = _ensure_series(info_entry.get("SEC_EPS_Q_SERIES"))
L204     s_rev_y = _ensure_series(info_entry.get("SEC_REV_Y_SERIES"))
L205
L206     nQ = int(getattr(s_rev_q, "size", 0))
L207     nY = int(getattr(s_rev_y, "size", 0))
L208
L209     parts: dict[str, Any] = {"nQ": nQ, "nY": nY}
L210     path = "NONE"
L211     w = 0.0
L212
L213     def _valid_ratio(a, b):
L214         try:
L215             na, nb = float(a), float(b)
L216         except Exception:
L217             return None
L218         if not np.isfinite(na) or not np.isfinite(nb) or nb == 0:
L219             return None
L220         return na, nb
L221
L222     def yoy_q(series: pd.Series) -> float | None:
L223         s = _ensure_series(series)
L224         if s.empty:
L225             return None
L226         s = s.sort_index()
L227         if isinstance(s.index, pd.DatetimeIndex):
L228             last_idx = s.index[-1]
L229             window_start = last_idx - pd.DateOffset(months=15)
L230             window_end = last_idx - pd.DateOffset(months=9)
L231             candidates = s.loc[(s.index >= window_start) & (s.index <= window_end)]
L232             if candidates.empty:
L233                 candidates = s.loc[s.index <= window_end]
L234             if candidates.empty:
L235                 return None
L236             v1 = candidates.iloc[-1]
L237             v0 = s.iloc[-1]
L238         else:
L239             if s.size < 5:
L240                 return None
L241             v0 = s.iloc[-1]
L242             v1 = s.iloc[-5]
L243         pair = _valid_ratio(v0, v1)
L244         if pair is None:
L245             return None
L246         a, b = pair
L247         return float(a / b - 1.0)
L248
L249     def qoq(series: pd.Series) -> float | None:
L250         s = _ensure_series(series)
L251         if s.size < 2:
L252             return None
L253         s = s.sort_index()
L254         v0, v1 = s.iloc[-1], s.iloc[-2]
L255         pair = _valid_ratio(v0, v1)
L256         if pair is None:
L257             return None
L258         a, b = pair
L259         return float(a / b - 1.0)
L260
L261     def ttm_delta(series: pd.Series) -> float | None:
L262         s = _ensure_series(series)
L263         if s.size < 2:
L264             return None
L265         s = s.sort_index()
L266         k = int(min(4, s.size))
L267         cur_slice = s.iloc[-k:]
L268         prev_slice = s.iloc[:-k]
L269         if prev_slice.empty:
L270             return None
L271         prev_k = int(min(k, prev_slice.size))
L272         cur_sum = float(cur_slice.sum())
L273         prev_sum = float(prev_slice.iloc[-prev_k:].sum())
L274         pair = _valid_ratio(cur_sum, prev_sum)
L275         if pair is None:
L276             return None
L277         a, b = pair
L278         return float(a / b - 1.0)
L279
L280     def yoy_y(series: pd.Series) -> float | None:
L281         s = _ensure_series(series)
L282         if s.size < 2:
L283             return None
L284         s = s.sort_index()
L285         v0, v1 = s.iloc[-1], s.iloc[-2]
L286         pair = _valid_ratio(v0, v1)
L287         if pair is None:
L288             return None
L289         a, b = pair
L290         return float(a / b - 1.0)
L291
L292     def price_proxy_growth() -> float | None:
L293         if not isinstance(close_series, pd.Series):
L294             return None
L295         close = close_series.sort_index().dropna()
L296         if close.empty:
L297             return None
L298         hh_window = int(min(126, len(close)))
L299         if hh_window < 20:
L300             return None
L301         hh = close.rolling(hh_window).max().iloc[-1]
L302         prox = None
L303         if np.isfinite(hh) and hh > 0:
L304             prox = float(close.iloc[-1] / hh)
L305         rs6 = None
L306         if len(close) >= 63:
L307             rs6 = float(close.pct_change(63).iloc[-1])
L308         rs12 = None
L309         if len(close) >= 126:
L310             rs12 = float(close.pct_change(126).iloc[-1])
L311         vexp = None
L312         if isinstance(volume_series, pd.Series):
L313             vol = volume_series.reindex(close.index).dropna()
L314             if len(vol) >= 50:
L315                 v20 = vol.rolling(20).mean().iloc[-1]
L316                 v50 = vol.rolling(50).mean().iloc[-1]
L317                 if np.isfinite(v20) and np.isfinite(v50) and v50 > 0:
L318                     vexp = float(v20 / v50 - 1.0)
L319         prox = 0.0 if prox is None or not np.isfinite(prox) else prox
L320         rs6 = 0.0 if rs6 is None or not np.isfinite(rs6) else rs6
L321         rs12 = 0.0 if rs12 is None or not np.isfinite(rs12) else rs12
L322         vexp = 0.0 if vexp is None or not np.isfinite(vexp) else vexp
L323         return 0.5 * prox + 0.3 * rs6 + 0.2 * rs12 + 0.2 * vexp
L324
L325     price_alt = price_proxy_growth() or 0.0
L326     core = 0.0
L327     core_raw = 0.0
L328     price_raw = price_alt
L329
L330     if nQ >= 5:
L331         path = "P5"
L332         yq = yoy_q(s_rev_q)
L333         parts["rev_yoy_q"] = yq
L334         tmp_prev = s_rev_q.iloc[:-1] if s_rev_q.size > 1 else s_rev_q
L335         acc = None
L336         if tmp_prev.size >= 5 and yq is not None:
L337             yq_prev = yoy_q(tmp_prev)
L338             if yq_prev is not None:
L339                 acc = float(yq - yq_prev)
L340         parts["rev_acc_q"] = acc
L341         eps_yoy = yoy_q(s_eps_q) if s_eps_q.size >= 5 else None
L342         parts["eps_yoy_q"] = eps_yoy
L343         eps_acc = None
L344         if eps_yoy is not None and s_eps_q.size > 5:
L345             eps_prev = s_eps_q.iloc[:-1]
L346             if eps_prev.size >= 5:
L347                 eps_prev_yoy = yoy_q(eps_prev)
L348                 if eps_prev_yoy is not None:
L349                     eps_acc = float(eps_yoy - eps_prev_yoy)
L350         parts["eps_acc_q"] = eps_acc
L351         w = 1.0
L352         core_raw = (
L353             0.60 * _nz(yq)
L354             + 0.20 * _nz(acc)
L355             + 0.15 * _nz(eps_yoy)
L356             + 0.05 * _nz(eps_acc)
L357         )
L358         price_alt = 0.0
L359     elif 2 <= nQ <= 4:
L360         path = "P24"
L361         rev_qoq = qoq(s_rev_q)
L362         rev_ttm2 = ttm_delta(s_rev_q)
L363         parts["rev_qoq"] = rev_qoq
L364         parts["rev_ttm2"] = rev_ttm2
L365         eps_qoq = qoq(s_eps_q) if s_eps_q.size >= 2 else None
L366         parts["eps_qoq"] = eps_qoq
L367         w = min(1.0, nQ / 5.0)
L368         core_raw = 0.6 * _nz(rev_qoq) + 0.3 * _nz(rev_ttm2) + 0.1 * _nz(eps_qoq)
L369     else:
L370         path = "P1Y"
L371         rev_yoy_y = yoy_y(s_rev_y) if nY >= 2 else None
L372         parts["rev_yoy_y"] = rev_yoy_y
L373         w = 0.6 * min(1.0, nY / 3.0) if nY >= 2 else 0.4
L374         core_raw = _nz(rev_yoy_y)
L375         if nQ <= 1 and nY < 2 and price_alt == 0.0:
L376             price_alt = price_proxy_growth() or 0.0
L377
L378     core = _winsor(core_raw, lo=-1.5, hi=1.5)
L379     price_alt = _winsor(price_alt, lo=-1.5, hi=1.5)
L380     grw = _winsor(w * core + (1.0 - w) * (0.5 * _nz(price_alt)), lo=-2.0, hi=2.0)
L381
L382     parts.update(
L383         {
L384             "core_raw": core_raw,
L385             "core": core,
L386             "price_proxy_raw": price_raw,
L387             "price_proxy": price_alt,
L388             "weight": w,
L389             "score": grw,
L390         }
L391     )
L392
L393     parts_out: dict[str, Any] = {
L394         "nQ": nQ,
L395         "nY": nY,
L396     }
L397     for key, value in parts.items():
L398         if key in ("nQ", "nY"):
L399             continue
L400         rounded = _round_debug(value)
L401         parts_out[key] = rounded
L402
L403     info_entry["DEBUG_GRW_PATH"] = path
L404     info_entry["DEBUG_GRW_PARTS"] = json.dumps(parts_out, ensure_ascii=False, sort_keys=True)
L405     info_entry["GRW_SCORE"] = grw
L406     info_entry["GRW_WEIGHT"] = w
L407     info_entry["GRW_CORE"] = core
L408     info_entry["GRW_PRICE_PROXY"] = price_alt
L409
L410     return {
L411         "score": grw,
L412         "path": path,
L413         "parts": info_entry["DEBUG_GRW_PARTS"],
L414         "weight": w,
L415         "core": core,
L416         "price_proxy": price_alt,
L417     }
L418
L419
L420 D_WEIGHTS_EFF = None  # å‡ºåŠ›è¡¨ç¤ºäº’æ›ã®ãŸã‚
L421
L422
L423 def _scalar(v):
L424     """å˜ä¸€ã‚»ãƒ«ä»£å…¥ç”¨ã«å€¤ã‚’ã‚¹ã‚«ãƒ©ãƒ¼ã¸æ­£è¦åŒ–ã™ã‚‹ã€‚
L425
L426     - pandas Series -> .iloc[-1]ï¼ˆæœ€å¾Œã‚’æ¡ç”¨ï¼‰
L427     - list/tuple/ndarray -> æœ€å¾Œã®è¦ç´ 
L428     - ãã‚Œä»¥å¤–          -> ãã®ã¾ã¾
L429     å–å¾—å¤±æ•—æ™‚ã¯ np.nan ã‚’è¿”ã™ã€‚
L430     """
L431     import numpy as _np
L432     import pandas as _pd
L433     try:
L434         if isinstance(v, _pd.Series):
L435             return v.iloc[-1] if len(v) else _np.nan
L436         if isinstance(v, (list, tuple, _np.ndarray)):
L437             return v[-1] if len(v) else _np.nan
L438         return v
L439     except Exception:
L440         return _np.nan
L441
L442
L443 # ---- Scorer æœ¬ä½“ -------------------------------------------------------------
L444 class Scorer:
L445     """
L446     - factor.py ã‹ã‚‰ã¯ `aggregate_scores(ib, cfg)` ã‚’å‘¼ã¶ã ã‘ã§OKã€‚
L447     - cfg ã¯å¿…é ˆï¼ˆfactor.PipelineConfig ã‚’æ¸¡ã™ï¼‰ã€‚
L448     - æ—§ã‚«ãƒ©ãƒ åã‚’è‡ªå‹•ãƒªãƒãƒ¼ãƒ ã—ã¦æ–°ã‚¹ã‚­ãƒ¼ãƒã«å¸åã—ã¾ã™ã€‚
L449     """
L450
L451     # === å…ˆé ­ã§æ—§â†’æ–°ã‚«ãƒ©ãƒ åãƒãƒƒãƒ—ï¼ˆç§»è¡Œç”¨ï¼‰ ===
L452     EPS_RENAME = {"eps_ttm":"EPS_TTM", "eps_q_recent":"EPS_Q_LastQ"}
L453     FCF_RENAME = {"fcf_ttm":"FCF_TTM"}
L454
L455     # === ã‚¹ã‚­ãƒ¼ãƒç°¡æ˜“ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€ä½é™ï¼‰ ===
L456     @staticmethod
L457     def _validate_ib_for_scorer(ib: Any):
L458         miss = [a for a in ["tickers","bench","data","px","spx","tickers_bulk","info","eps_df","fcf_df","returns"] if not hasattr(ib,a) or getattr(ib,a) is None]
L459         if miss: raise ValueError(f"InputBundle is missing required attributes for Scorer: {miss}")
L460         if any(c in ib.eps_df.columns for c in Scorer.EPS_RENAME): ib.eps_df.rename(columns=Scorer.EPS_RENAME, inplace=True)
L461         if any(c in ib.fcf_df.columns for c in Scorer.FCF_RENAME): ib.fcf_df.rename(columns=Scorer.FCF_RENAME, inplace=True)
L462         need_eps, need_fcf = {"EPS_TTM","EPS_Q_LastQ"},{"FCF_TTM"}
L463         if not need_eps.issubset(ib.eps_df.columns): raise ValueError(f"eps_df must contain columns {need_eps} (accepts old names via auto-rename). Got: {list(ib.eps_df.columns)}")
L464         if not need_fcf.issubset(ib.fcf_df.columns): raise ValueError(f"fcf_df must contain columns {need_fcf} (accepts old names via auto-rename). Got: {list(ib.fcf_df.columns)}")
L465
L466     # ----ï¼ˆScorerå°‚ç”¨ï¼‰ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ»æŒ‡æ¨™ç³» ----
L467     @staticmethod
L468     def trend(s: pd.Series):
L469         if len(s)<200: return np.nan
L470         sma50, sma150, sma200 = s.rolling(50).mean().iloc[-1], s.rolling(150).mean().iloc[-1], s.rolling(200).mean().iloc[-1]
L471         prev200, p = s.rolling(200).mean().iloc[-21], s.iloc[-1]
L472         lo_52 = s[-252:].min() if len(s)>=252 else s.min(); hi_52 = s[-252:].max() if len(s)>=252 else s.max()
L473         rng = (hi_52 - lo_52) if hi_52>lo_52 else np.nan
L474         clip = lambda x,lo,hi: (np.nan if pd.isna(x) else max(lo,min(hi,x)))
L475         a = clip(p/(s.rolling(50).mean().iloc[-1]) - 1, -0.5, 0.5)
L476         b = clip(sma50/sma150 - 1, -0.5, 0.5)
L477         c = clip(sma150/sma200 - 1, -0.5, 0.5)
L478         d = clip(sma200/prev200 - 1, -0.2, 0.2)
L479         e = clip((p - lo_52) / (rng if rng and rng>0 else np.nan) - 0.5, -0.5, 0.5)
L480         parts = [0.0 if pd.isna(x) else x for x in (a,b,c,d,e)]
L481         return 0.30*parts[0] + 0.20*parts[1] + 0.15*parts[2] + 0.15*parts[3] + 0.20*parts[4]
L482
L483     @staticmethod
L484     def rs(s, b):
L485         n, nb = len(s), len(b)
L486         if n<60 or nb<60: return np.nan
L487         L12 = 252 if n>=252 and nb>=252 else min(n,nb)-1; L1 = 22 if n>=22 and nb>=22 else max(5, min(n,nb)//3)
L488         r12, r1, br12, br1 = s.iloc[-1]/s.iloc[-L12]-1, s.iloc[-1]/s.iloc[-L1]-1, b.iloc[-1]/b.iloc[-L12]-1, b.iloc[-1]/b.iloc[-L1]-1
L489         return (r12 - br12)*0.7 + (r1 - br1)*0.3
L490
L491     @staticmethod
L492     def tr_str(s):
L493         if s is None:
L494             return np.nan
L495         s = s.ffill(limit=2).dropna()
L496         if len(s) < 50:
L497             return np.nan
L498         ma50 = s.rolling(50, min_periods=50).mean()
L499         last_ma = ma50.iloc[-1]
L500         last_px = s.iloc[-1]
L501         return float(last_px/last_ma - 1.0) if pd.notna(last_ma) and pd.notna(last_px) else np.nan
L502
L503     @staticmethod
L504     def rs_line_slope(s: pd.Series, b: pd.Series, win: int) -> float:
L505         r = (s/b).dropna()
L506         if len(r) < win: return np.nan
L507         y, x = np.log(r.iloc[-win:]), np.arange(win, dtype=float)
L508         try: return float(np.polyfit(x, y, 1)[0])
L509         except Exception: return np.nan
L510
L511     @staticmethod
L512     def ev_fallback(info_t: dict, tk: yf.Ticker) -> float:
L513         ev = info_t.get('enterpriseValue', np.nan)
L514         if pd.notna(ev) and ev>0: return float(ev)
L515         mc, debt, cash = info_t.get('marketCap', np.nan), np.nan, np.nan
L516         try:
L517             bs = tk.quarterly_balance_sheet
L518             if bs is not None and not bs.empty:
L519                 c = bs.columns[0]
L520                 for k in ("Total Debt","Long Term Debt","Short Long Term Debt"):
L521                     if k in bs.index: debt = float(bs.loc[k,c]); break
L522                 for k in ("Cash And Cash Equivalents","Cash And Cash Equivalents And Short Term Investments","Cash"):
L523                     if k in bs.index: cash = float(bs.loc[k,c]); break
L524         except Exception: pass
L525         if pd.notna(mc): return float(mc + (0 if pd.isna(debt) else debt) - (0 if pd.isna(cash) else cash))
L526         return np.nan
L527
L528     @staticmethod
L529     def dividend_status(ticker: str) -> str:
L530         t = yf.Ticker(ticker)
L531         try:
L532             if not t.dividends.empty: return "has"
L533         except Exception: return "unknown"
L534         try:
L535             a = t.actions
L536             if (a is not None and not a.empty and "Stock Splits" in a.columns and a["Stock Splits"].abs().sum()>0): return "none_confident"
L537         except Exception: pass
L538         try:
L539             fi = t.fast_info
L540             if any(getattr(fi,k,None) for k in ("last_dividend_date","dividend_rate","dividend_yield")): return "maybe_missing"
L541         except Exception: pass
L542         return "unknown"
L543
L544     @staticmethod
L545     def div_streak(t):
L546         try:
L547             divs = yf.Ticker(t).dividends.dropna(); ann = divs.groupby(divs.index.year).sum(); ann = ann[ann.index<pd.Timestamp.today().year]
L548             years, streak = sorted(ann.index), 0
L549             for i in range(len(years)-1,0,-1):
L550                 if ann[years[i]] > ann[years[i-1]]: streak += 1
L551                 else: break
L552             return streak
L553         except Exception: return 0
L554
L555     @staticmethod
L556     def fetch_finnhub_metrics(symbol):
L557         api_key = os.environ.get("FINNHUB_API_KEY")
L558         if not api_key: return {}
L559         url, params = "https://finnhub.io/api/v1/stock/metric", {"symbol":symbol,"metric":"all","token":api_key}
L560         try:
L561             r = requests.get(url, params=params, timeout=10); r.raise_for_status(); m = r.json().get("metric",{})
L562             return {'EPS':m.get('epsGrowthTTMYoy'),'REV':m.get('revenueGrowthTTMYoy'),'ROE':m.get('roeTTM'),'BETA':m.get('beta'),'DIV':m.get('dividendYieldIndicatedAnnual'),'FCF':(m.get('freeCashFlowTTM')/m.get('enterpriseValue')) if m.get('freeCashFlowTTM') and m.get('enterpriseValue') else None}
L563         except Exception: return {}
L564
L565     @staticmethod
L566     def calc_beta(series: pd.Series, market: pd.Series, lookback=252):
L567         r, m = series.pct_change().dropna(), market.pct_change().dropna()
L568         n = min(len(r), len(m), lookback)
L569         if n<60: return np.nan
L570         r, m = r.iloc[-n:], m.iloc[-n:]; cov, var = np.cov(r, m)[0,1], np.var(m)
L571         return np.nan if var==0 else cov/var
L572
L573     @staticmethod
L574     def spx_to_alpha(spx: pd.Series, bands=(0.03,0.10), w=(0.6,0.4),
L575                      span=5, q=(0.20,0.40), alphas=(0.05,0.08,0.10)) -> float:
L576         """
L577         S&P500æŒ‡æ•°ã®ã¿ã‹ã‚‰æ“¬ä¼¼breadthã‚’ä½œã‚Šã€å±¥æ­´åˆ†ä½ã§Î±ã‚’æ®µéšæ±ºå®šã€‚
L578         bands=(Â±3%, Â±10%), w=(50DMA,200DMA), åˆ†ä½q=(20%,40%), alphas=(ä½,ä¸­,é«˜)
L579         """
L580         ma50, ma200 = spx.rolling(50).mean(), spx.rolling(200).mean()
L581         b50, b200 = ((spx/ma50 - 1)+bands[0])/(2*bands[0]), ((spx/ma200 - 1)+bands[1])/(2*bands[1])
L582         hist = (w[0]*b50 + w[1]*b200).clip(0,1).ewm(span=span).mean()
L583         b, (lo, mid) = float(hist.iloc[-1]), (float(hist.quantile(q[0])), float(hist.quantile(q[1])))
L584         return alphas[0] if b < lo else alphas[1] if b < mid else alphas[2]
L585
L586     @staticmethod
L587     def soft_cap_effective_scores(scores: pd.Series|dict, sectors: dict, cap=2, alpha=0.08) -> pd.Series:
L588         """
L589         åŒä¸€ã‚»ã‚¯ã‚¿ãƒ¼capè¶…éï¼ˆ3æœ¬ç›®ä»¥é™ï¼‰ã« Î±Ã—æ®µéšæ¸›ç‚¹ã‚’èª²ã—ãŸâ€œæœ‰åŠ¹ã‚¹ã‚³ã‚¢â€Seriesã‚’è¿”ã™ã€‚
L590         æˆ»ã‚Šå€¤ã¯é™é †ã‚½ãƒ¼ãƒˆæ¸ˆã¿ã€‚
L591         """
L592         s = pd.Series(scores, dtype=float); order = s.sort_values(ascending=False).index
L593         cnt, pen = {}, {}
L594         for t in order:
L595             sec = sectors.get(t, "U"); cnt[sec] = cnt.get(sec,0) + 1; pen[t] = alpha*max(0, cnt[sec]-cap)
L596         return (s - pd.Series(pen)).sort_values(ascending=False)
L597
L598     @staticmethod
L599     def pick_top_softcap(scores: pd.Series|dict, sectors: dict, N: int, cap=2, alpha=0.08, hard: int|None=5) -> list[str]:
L600         """
L601         soft-capé©ç”¨å¾Œã®ä¸Šä½Nãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚’è¿”ã™ã€‚hard>0ãªã‚‰éå¸¸ç”¨ãƒãƒ¼ãƒ‰ä¸Šé™ã§åŒä¸€ã‚»ã‚¯ã‚¿ãƒ¼è¶…éã‚’é–“å¼•ãï¼ˆæ—¢å®š=5ï¼‰ã€‚
L602         """
L603         eff = Scorer.soft_cap_effective_scores(scores, sectors, cap, alpha)
L604         if not hard:
L605             return list(eff.head(N).index)
L606         pick, used = [], {}
L607         for t in eff.index:
L608             s = sectors.get(t, "U")
L609             if used.get(s,0) < hard:
L610                 pick.append(t); used[s] = used.get(s,0) + 1
L611             if len(pick) == N: break
L612         return pick
L613
L614     @staticmethod
L615     def trend_template_breadth_series(px: pd.DataFrame, spx: pd.Series, win_days: int | None = None) -> pd.Series:
L616         """
L617         å„å–¶æ¥­æ—¥ã® trend_template åˆæ ¼æœ¬æ•°ï¼ˆåˆæ ¼â€œæœ¬æ•°â€=Cï¼‰ã‚’è¿”ã™ã€‚
L618         - px: åˆ—=tickerï¼ˆãƒ™ãƒ³ãƒã¯å«ã‚ãªã„ï¼‰
L619         - spx: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ Seriesï¼ˆpx.index ã«æ•´åˆ—ï¼‰
L620         - win_days: æœ«å°¾ã®è¨ˆç®—å¯¾è±¡å–¶æ¥­æ—¥æ•°ï¼ˆNoneâ†’å…¨ä½“ã€æ—¢å®š600ã¯å‘¼ã³å‡ºã—å´æŒ‡å®šï¼‰
L621         ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼†rollingã®ã¿ã§è»½é‡ã€‚æ¬ æã¯ False æ‰±ã„ã€‚
L622         """
L623         import numpy as np, pandas as pd
L624         if px is None or px.empty:
L625             return pd.Series(dtype=int)
L626         px = px.dropna(how="all", axis=1)
L627         if win_days and win_days > 0:
L628             px = px.tail(win_days)
L629         if px.empty:
L630             return pd.Series(dtype=int)
L631         spx = spx.reindex(px.index).ffill()
L632
L633         ma50  = px.rolling(50).mean()
L634         ma150 = px.rolling(150).mean()
L635         ma200 = px.rolling(200).mean()
L636
L637         tt = (px > ma150)
L638         tt &= (px > ma200)
L639         tt &= (ma150 > ma200)
L640         tt &= (ma200 - ma200.shift(21) > 0)
L641         tt &= (ma50  > ma150)
L642         tt &= (ma50  > ma200)
L643         tt &= (px    > ma50)
L644
L645         lo252 = px.rolling(252).min()
L646         hi252 = px.rolling(252).max()
L647         tt &= (px.divide(lo252).sub(1.0) >= 0.30)   # P_OVER_LOW52 >= 0.30
L648         tt &= (px >= (0.75 * hi252))                # NEAR_52W_HIGH >= -0.25
L649
L650         r12  = px.divide(px.shift(252)).sub(1.0)
L651         br12 = spx.divide(spx.shift(252)).sub(1.0)
L652         r1   = px.divide(px.shift(22)).sub(1.0)
L653         br1  = spx.divide(spx.shift(22)).sub(1.0)
L654         rs   = 0.7*(r12.sub(br12, axis=0)) + 0.3*(r1.sub(br1, axis=0))
L655         tt &= (rs >= 0.10)
L656
L657         return tt.fillna(False).sum(axis=1).astype(int)
L658
L659     # ---- ã‚¹ã‚³ã‚¢é›†è¨ˆï¼ˆDTO/Configã‚’å—ã‘å–ã‚Šã€FeatureBundleã‚’è¿”ã™ï¼‰ ----
L660     def aggregate_scores(self, ib: Any, cfg):
L661         if cfg is None:
L662             raise ValueError("cfg is required; pass factor.PipelineConfig")
L663         self._validate_ib_for_scorer(ib)
L664
L665         px, spx, tickers = ib.px, ib.spx, ib.tickers
L666         tickers_bulk, info, eps_df, fcf_df = ib.tickers_bulk, ib.info, ib.eps_df, ib.fcf_df
L667
L668         df, missing_logs = pd.DataFrame(index=tickers), []
L669         for t in tickers:
L670             d, s = info[t], px[t]; ev = self.ev_fallback(d, tickers_bulk.tickers[t])
L671             try:
L672                 volume_series_full = ib.data['Volume'][t]
L673             except Exception:
L674                 volume_series_full = None
L675
L676             grw_result = _calc_grw_flexible(t, d, s, volume_series_full)
L677             df.loc[t,'GRW_FLEX_SCORE'] = grw_result.get('score')
L678             df.loc[t,'GRW_FLEX_WEIGHT'] = grw_result.get('weight')
L679             df.loc[t,'GRW_FLEX_CORE'] = grw_result.get('core')
L680             df.loc[t,'GRW_FLEX_PRICE'] = grw_result.get('price_proxy')
L681             df.loc[t,'DEBUG_GRW_PATH'] = grw_result.get('path')
L682             df.loc[t,'DEBUG_GRW_PARTS'] = grw_result.get('parts')
L683
L684             # --- åŸºæœ¬ç‰¹å¾´ ---
L685             df.loc[t,'TR']   = self.trend(s)
L686             df.loc[t,'EPS']  = _scalar(eps_df.loc[t,'EPS_TTM']) if t in eps_df.index else np.nan
L687             df.loc[t,'EPS_Q'] = _scalar(eps_df.loc[t,'EPS_Q_LastQ']) if t in eps_df.index else np.nan
L688             df.loc[t,'REV_TTM'] = _scalar(eps_df.loc[t,'REV_TTM']) if t in eps_df.index else np.nan
L689             df.loc[t,'REV_Q']   = _scalar(eps_df.loc[t,'REV_Q_LastQ']) if t in eps_df.index else np.nan
L690             df.loc[t,'REV']  = d.get('revenueGrowth',np.nan)
L691             df.loc[t,'ROE']  = d.get('returnOnEquity',np.nan)
L692             df.loc[t,'BETA'] = self.calc_beta(s, spx, lookback=252)
L693
L694             # --- é…å½“ï¼ˆæ¬ æè£œå®Œå«ã‚€ï¼‰ ---
L695             div = d.get('dividendYield') if d.get('dividendYield') is not None else d.get('trailingAnnualDividendYield')
L696             if div is None or pd.isna(div):
L697                 try:
L698                     divs = yf.Ticker(t).dividends
L699                     if divs is not None and not divs.empty:
L700                         last_close = s.iloc[-1]; div_1y = divs[divs.index >= (divs.index.max() - pd.Timedelta(days=365))].sum()
L701                         if last_close and last_close>0: div = float(div_1y/last_close)
L702                 except Exception: pass
L703             df.loc[t,'DIV'] = 0.0 if (div is None or pd.isna(div)) else float(div)
L704
L705             # --- FCF/EV ---
L706             fcf_val = fcf_df.loc[t,'FCF_TTM'] if t in fcf_df.index else np.nan
L707             df.loc[t,'FCF'] = (fcf_val/ev) if (pd.notna(fcf_val) and pd.notna(ev) and ev>0) else np.nan
L708
L709             # --- ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ãƒ»ãƒœãƒ©é–¢é€£ ---
L710             df.loc[t,'RS'], df.loc[t,'TR_str'] = self.rs(s, spx), self.tr_str(s)
L711             r, rm = s.pct_change().dropna(), spx.pct_change().dropna()
L712             n = int(min(len(r), len(rm)))
L713
L714             DOWNSIDE_DEV = np.nan
L715             if n>=60:
L716                 r6 = r.iloc[-min(len(r),126):]; neg = r6[r6<0]
L717                 if len(neg)>=10: DOWNSIDE_DEV = float(neg.std(ddof=0)*np.sqrt(252))
L718             df.loc[t,'DOWNSIDE_DEV'] = DOWNSIDE_DEV
L719
L720             MDD_1Y = np.nan
L721             try:
L722                 w = s.iloc[-min(len(s),252):].dropna()
L723                 if len(w)>=30:
L724                     roll_max = w.cummax(); MDD_1Y = float((w/roll_max - 1.0).min())
L725             except Exception: pass
L726             df.loc[t,'MDD_1Y'] = MDD_1Y
L727
L728             RESID_VOL = np.nan
L729             if n>=120:
L730                 rr, rrm = r.iloc[-n:].align(rm.iloc[-n:], join='inner')
L731                 if len(rr)==len(rrm) and len(rr)>=120 and rrm.var()>0:
L732                     beta = float(np.cov(rr, rrm)[0,1]/np.var(rrm)); resid = rr - beta*rrm
L733                     RESID_VOL = float(resid.std(ddof=0)*np.sqrt(252))
L734             df.loc[t,'RESID_VOL'] = RESID_VOL
L735
L736             DOWN_OUTPERF = np.nan
L737             if n>=60:
L738                 m, x = rm.iloc[-n:], r.iloc[-n:]; mask = m<0
L739                 if mask.sum()>=10:
L740                     mr, sr = float(m[mask].mean()), float(x[mask].mean())
L741                     DOWN_OUTPERF = (sr - mr)/abs(mr) if mr!=0 else np.nan
L742             df.loc[t,'DOWN_OUTPERF'] = DOWN_OUTPERF
L743
L744             # --- é•·æœŸç§»å‹•å¹³å‡/ä½ç½® ---
L745             sma200 = s.rolling(200).mean(); df.loc[t,'EXT_200'] = np.nan
L746             if pd.notna(sma200.iloc[-1]) and sma200.iloc[-1]!=0: df.loc[t,'EXT_200'] = abs(float(s.iloc[-1]/sma200.iloc[-1]-1.0))
L747
L748             # --- é…å½“ã®è©³ç´°ç³» ---
L749             DIV_TTM_PS=DIV_VAR5=DIV_YOY=DIV_FCF_COVER=np.nan
L750             try:
L751                 divs = yf.Ticker(t).dividends.dropna()
L752                 if not divs.empty:
L753                     last_close = s.iloc[-1]; div_1y = float(divs[divs.index >= (divs.index.max()-pd.Timedelta(days=365))].sum())
L754                     DIV_TTM_PS = div_1y if div_1y>0 else np.nan
L755                     ann = divs.groupby(divs.index.year).sum()
L756                     if len(ann)>=2 and ann.iloc[-2]!=0: DIV_YOY = float(ann.iloc[-1]/ann.iloc[-2]-1.0)
L757                     tail = ann.iloc[-5:] if len(ann)>=5 else ann
L758                     if len(tail)>=3 and tail.mean()!=0: DIV_VAR5 = float(tail.std(ddof=1)/abs(tail.mean()))
L759                 so = d.get('sharesOutstanding',None)
L760                 if so and pd.notna(DIV_TTM_PS) and pd.notna(fcf_val) and fcf_val!=0:
L761                     DIV_FCF_COVER = float((fcf_val)/(DIV_TTM_PS*float(so)))
L762             except Exception: pass
L763             df.loc[t,'DIV_TTM_PS'], df.loc[t,'DIV_VAR5'], df.loc[t,'DIV_YOY'], df.loc[t,'DIV_FCF_COVER'] = DIV_TTM_PS, DIV_VAR5, DIV_YOY, DIV_FCF_COVER
L764
L765             # --- è²¡å‹™å®‰å®šæ€§ ---
L766             df.loc[t,'DEBT2EQ'], df.loc[t,'CURR_RATIO'] = d.get('debtToEquity',np.nan), d.get('currentRatio',np.nan)
L767
L768             # --- EPS å¤‰å‹• ---
L769             EPS_VAR_8Q = np.nan
L770             try:
L771                 qe, so = tickers_bulk.tickers[t].quarterly_earnings, d.get('sharesOutstanding',None)
L772                 if qe is not None and not qe.empty and so:
L773                     eps_q = (qe['Earnings'].dropna().astype(float)/float(so)).replace([np.inf,-np.inf],np.nan)
L774                     if len(eps_q)>=4: EPS_VAR_8Q = float(eps_q.iloc[-min(8,len(eps_q)):].std(ddof=1))
L775             except Exception: pass
L776             df.loc[t,'EPS_VAR_8Q'] = EPS_VAR_8Q
L777
L778             # --- ã‚µã‚¤ã‚º/æµå‹•æ€§ ---
L779             df.loc[t,'MARKET_CAP'] = d.get('marketCap',np.nan); adv60 = np.nan
L780             try:
L781                 if isinstance(volume_series_full, pd.Series):
L782                     vol_series = volume_series_full.reindex(s.index).dropna()
L783                     if len(vol_series) >= 5:
L784                         aligned_px = s.reindex(vol_series.index).dropna()
L785                         if len(aligned_px) == len(vol_series):
L786                             dv = (vol_series*aligned_px).rolling(60).mean()
L787                             if not dv.dropna().empty:
L788                                 adv60 = float(dv.dropna().iloc[-1])
L789             except Exception:
L790                 pass
L791             df.loc[t,'ADV60_USD'] = adv60
L792
L793             # --- å£²ä¸Š/åˆ©ç›Šã®åŠ é€Ÿåº¦ç­‰ ---
L794             REV_Q_YOY=EPS_Q_YOY=REV_YOY_ACC=REV_YOY_VAR=np.nan
L795             REV_ANNUAL_STREAK = REV_YOY = np.nan
L796             EPS_YOY = np.nan
L797             try:
L798                 qe, so = tickers_bulk.tickers[t].quarterly_earnings, d.get('sharesOutstanding',None)
L799                 # å‹ã«ä¾å­˜ã—ãªã„å®‰å…¨ãªå–ã‚Šå‡ºã—ï¼ˆlist/np.array/pd.Series ã©ã‚Œã§ã‚‚OKï¼‰
L800                 sec_rev_series = d.get('SEC_REV_Q_SERIES')
L801                 rev = _ensure_series(sec_rev_series)
L802                 if rev.empty and qe is not None and not qe.empty and 'Revenue' in qe.columns:
L803                     rev = pd.to_numeric(qe['Revenue'], errors='coerce').dropna()
L804                 if not rev.empty:
L805                     if len(rev)>=5: REV_Q_YOY = _safe_div(rev.iloc[-1]-rev.iloc[-5], rev.iloc[-5])
L806                     if len(rev)>=6:
L807                         yoy_now = _safe_div(rev.iloc[-1]-rev.iloc[-5], rev.iloc[-5]); yoy_prev = _safe_div(rev.iloc[-2]-rev.iloc[-6], rev.iloc[-6])
L808                         if pd.notna(yoy_now) and pd.notna(yoy_prev): REV_YOY_ACC = yoy_now - yoy_prev
L809                     yoy_list=[]
L810                     for k in range(1,5):
L811                         if len(rev)>=4+k:
L812                             y = _safe_div(rev.iloc[-k]-rev.iloc[-(k+4)], rev.iloc[-(k+4)])
L813                             if pd.notna(y): yoy_list.append(y)
L814                     if len(yoy_list)>=2: REV_YOY_VAR = float(np.std(yoy_list, ddof=1))
L815                     # NEW: å¹´æ¬¡ã®æŒç¶šæ€§ï¼ˆç›´è¿‘ã‹ã‚‰é¡ã£ã¦å‰å¹´æ¯”ãƒ—ãƒ©ã‚¹ãŒä½•å¹´é€£ç¶šã‹ã€å››åŠæœŸ4æœ¬æƒã†å®Œå…¨å¹´ã®ã¿ï¼‰
L816                     try:
L817                         if isinstance(rev.index, pd.DatetimeIndex):
L818                             g = rev.groupby(rev.index.year)
L819                             ann_sum, cnt = g.sum(), g.count()
L820                             ann_sum = ann_sum[cnt >= 4]
L821                             if len(ann_sum) >= 2:
L822                                 yoy = ann_sum.pct_change().dropna()
L823                                 if not yoy.empty:
L824                                     REV_YOY = float(yoy.iloc[-1])
L825                                 streak = 0
L826                                 for v in yoy.iloc[::-1]:
L827                                     if pd.isna(v) or v <= 0:
L828                                         break
L829                                     streak += 1
L830                                 REV_ANNUAL_STREAK = float(streak)
L831                     except Exception:
L832                         pass
L833                 sec_eps_series = d.get('SEC_EPS_Q_SERIES')
L834                 eps_series = _ensure_series(sec_eps_series).replace([np.inf,-np.inf], np.nan)
L835                 if eps_series.empty and qe is not None and not qe.empty and 'Earnings' in qe.columns and so:
L836                     eps_series = (pd.to_numeric(qe['Earnings'], errors='coerce')/float(so)).replace([np.inf,-np.inf],np.nan).dropna()
L837                 if not eps_series.empty:
L838                     if len(eps_series)>=5 and pd.notna(eps_series.iloc[-5]) and eps_series.iloc[-5]!=0:
L839                         EPS_Q_YOY = _safe_div(eps_series.iloc[-1]-eps_series.iloc[-5], eps_series.iloc[-5])
L840                     try:
L841                         if isinstance(eps_series.index, pd.DatetimeIndex):
L842                             g_eps = eps_series.groupby(eps_series.index.year)
L843                             ann_eps, cnt_eps = g_eps.sum(), g_eps.count()
L844                             ann_eps = ann_eps[cnt_eps >= 4]
L845                             if len(ann_eps) >= 2:
L846                                 eps_yoy = ann_eps.pct_change().dropna()
L847                                 if not eps_yoy.empty:
L848                                     EPS_YOY = float(eps_yoy.iloc[-1])
L849                     except Exception:
L850                         pass
L851             except Exception as e:
L852                 # å‹ãƒã‚°ã‚’è¦‹é€ƒã•ãªã„ãŸã‚ã€ã“ã“ã ã‘ã¯åŸå› ã‚’ä¸€è¡Œã§å¯è¦–åŒ–
L853                 print(f"[WARN][{t}] growth-derivatives skipped: {type(e).__name__}: {e}")
L854             df.loc[t,'REV_Q_YOY'], df.loc[t,'EPS_Q_YOY'] = REV_Q_YOY, EPS_Q_YOY
L855             df.loc[t,'REV_YOY_ACC'], df.loc[t,'REV_YOY_VAR'] = REV_YOY_ACC, REV_YOY_VAR
L856             df.loc[t,'REV_YOY'] = REV_YOY
L857             df.loc[t,'REV_ANN_STREAK'] = REV_ANNUAL_STREAK
L858             df.loc[t,'EPS_YOY'] = EPS_YOY
L859
L860             # --- Rule of 40 ã‚„å‘¨è¾º ---
L861             total_rev_ttm = d.get('totalRevenue',np.nan)
L862             FCF_MGN = _safe_div(fcf_val, total_rev_ttm)
L863             df.loc[t,'FCF_MGN'] = FCF_MGN
L864             rule40 = np.nan
L865             try:
L866                 r = df.loc[t,'REV']; rule40 = (r if pd.notna(r) else np.nan) + (FCF_MGN if pd.notna(FCF_MGN) else np.nan)
L867             except Exception: pass
L868             df.loc[t,'RULE40'] = rule40
L869
L870             # --- ãƒˆãƒ¬ãƒ³ãƒ‰è£œåŠ© ---
L871             sma50  = s.rolling(50).mean()
L872             sma150 = s.rolling(150).mean()
L873             sma200 = s.rolling(200).mean()
L874             p = _safe_last(s)
L875
L876             df.loc[t,'MA50_OVER_150'] = (_safe_last(sma50)/_safe_last(sma150) - 1
L877                 if pd.notna(_safe_last(sma50)) and pd.notna(_safe_last(sma150)) and _safe_last(sma150)!=0 else np.nan)
L878             df.loc[t,'MA150_OVER_200'] = (_safe_last(sma150)/_safe_last(sma200) - 1
L879                 if pd.notna(_safe_last(sma150)) and pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan)
L880
L881             lo52 = s[-252:].min() if len(s)>=252 else s.min()
L882             df.loc[t,'P_OVER_LOW52'] = (p/lo52 - 1) if (lo52 and lo52>0 and pd.notna(p)) else np.nan
L883
L884             df.loc[t,'MA200_SLOPE_1M'] = np.nan
L885             if len(sma200.dropna()) >= 21:
L886                 cur200 = _safe_last(sma200)
L887                 old2001 = float(sma200.iloc[-21])
L888                 if old2001:
L889                     df.loc[t,'MA200_SLOPE_1M'] = cur200/old2001 - 1
L890
L891             df.loc[t,'P_OVER_150'] = p/_safe_last(sma150)-1 if pd.notna(_safe_last(sma150)) and _safe_last(sma150)!=0 else np.nan
L892             df.loc[t,'P_OVER_200'] = p/_safe_last(sma200)-1 if pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L893             df.loc[t,'MA50_OVER_200'] = _safe_last(sma50)/_safe_last(sma200)-1 if pd.notna(_safe_last(sma50)) and pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L894             df.loc[t,'MA200_SLOPE_5M'] = np.nan
L895             if len(sma200.dropna())>=105:
L896                 cur200, old200 = _safe_last(sma200), float(sma200.iloc[-105])
L897                 if old200 and old200!=0: df.loc[t,'MA200_SLOPE_5M'] = cur200/old200 - 1
L898             # NEW: 200æ—¥ç·šãŒé€£ç¶šã§ä¸Šå‘ãã®ã€Œæ—¥æ•°ã€
L899             df.loc[t,'MA200_UP_STREAK_D'] = np.nan
L900             try:
L901                 s200 = sma200.dropna()
L902                 if len(s200) >= 2:
L903                     diff200 = s200.diff()
L904                     up = 0
L905                     for v in diff200.iloc[::-1]:
L906                         if pd.isna(v) or v <= 0:
L907                             break
L908                         up += 1
L909                     df.loc[t,'MA200_UP_STREAK_D'] = float(up)
L910             except Exception:
L911                 pass
L912             df.loc[t,'LOW52PCT25_EXCESS'] = np.nan if (lo52 is None or lo52<=0 or pd.isna(p)) else (p/(lo52*1.25)-1)
L913             hi52 = s[-252:].max() if len(s)>=252 else s.max(); df.loc[t,'NEAR_52W_HIGH'] = np.nan
L914             if hi52 and hi52>0 and pd.notna(p):
L915                 d_hi = (p/hi52)-1.0; df.loc[t,'NEAR_52W_HIGH'] = -abs(min(0.0, d_hi))
L916             df.loc[t,'RS_SLOPE_6W'] = self.rs_line_slope(s, ib.spx, 30)
L917             df.loc[t,'RS_SLOPE_13W'] = self.rs_line_slope(s, ib.spx, 65)
L918
L919             df.loc[t,'DIV_STREAK'] = self.div_streak(t)
L920
L921             # --- æ¬ æãƒ¡ãƒ¢ ---
L922             fin_cols = ['REV','ROE','BETA','DIV','FCF']
L923             need_finnhub = [col for col in fin_cols if pd.isna(df.loc[t,col])]
L924             if need_finnhub:
L925                 fin_data = self.fetch_finnhub_metrics(t)
L926                 for col in need_finnhub:
L927                     val = fin_data.get(col)
L928                     if val is not None and not pd.isna(val): df.loc[t,col] = val
L929             for col in fin_cols + ['EPS','RS','TR_str','DIV_STREAK']:
L930                 if pd.isna(df.loc[t,col]):
L931                     if col=='DIV':
L932                         status = self.dividend_status(t)
L933                         if status!='none_confident': missing_logs.append({'Ticker':t,'Column':col,'Status':status})
L934                     else:
L935                         missing_logs.append({'Ticker':t,'Column':col})
L936
L937         def _trend_template_pass(row, rs_alpha_thresh=0.10):
L938             c1 = (row.get('P_OVER_150', np.nan) > 0) and (row.get('P_OVER_200', np.nan) > 0)
L939             c2 = (row.get('MA150_OVER_200', np.nan) > 0)
L940             c3 = (row.get('MA200_SLOPE_1M', np.nan) > 0)
L941             c4 = (row.get('MA50_OVER_150', np.nan) > 0) and (row.get('MA50_OVER_200', np.nan) > 0)
L942             c5 = (row.get('TR_str', np.nan) > 0)
L943             c6 = (row.get('P_OVER_LOW52', np.nan) >= 0.30)
L944             c7 = (row.get('NEAR_52W_HIGH', np.nan) >= -0.25)
L945             c8 = (row.get('RS', np.nan) >= 0.10)
L946             return bool(c1 and c2 and c3 and c4 and c5 and c6 and c7 and c8)
L947
L948         if 'trend_template' not in df.columns: df['trend_template'] = df.apply(_trend_template_pass, axis=1).fillna(False)
L949         assert 'trend_template' in df.columns
L950
L951         # === ZåŒ–ã¨åˆæˆ ===
L952         for col in ['ROE','FCF','REV','EPS']: df[f'{col}_W'] = winsorize_s(df[col], 0.02)
L953
L954         df_z = pd.DataFrame(index=df.index)
L955         for col in ['EPS','REV','ROE','FCF','RS','TR_str','BETA','DIV','DIV_STREAK']: df_z[col] = robust_z(df[col])
L956         df_z['REV'], df_z['EPS'], df_z['TR'] = robust_z(df['REV_W']), robust_z(df['EPS_W']), robust_z(df['TR'])
L957         for col in ['P_OVER_150','P_OVER_200','MA50_OVER_200','MA200_SLOPE_5M','LOW52PCT25_EXCESS','NEAR_52W_HIGH','RS_SLOPE_6W','RS_SLOPE_13W','MA200_UP_STREAK_D']: df_z[col] = robust_z(df[col])
L958
L959         # === Growthæ·±æ˜ã‚Šç³»ï¼ˆæ¬ æä¿æŒz + RAWä½µè¼‰ï¼‰ ===
L960         grw_cols = ['REV_Q_YOY','EPS_Q_YOY','REV_YOY','EPS_YOY','REV_YOY_ACC','REV_YOY_VAR','FCF_MGN','RULE40','REV_ANN_STREAK']
L961         for col in grw_cols:
L962             if col in df.columns:
L963                 raw = pd.to_numeric(df[col], errors="coerce")
L964                 df_z[col] = robust_z_keepnan(raw)
L965                 df_z[f'{col}_RAW'] = raw
L966         for k in ("TREND_SLOPE_EPS", "TREND_SLOPE_REV"):
L967             if k in df.columns and k not in df_z.columns:
L968                 raw = pd.to_numeric(df[k], errors="coerce")
L969                 df_z[k] = robust_z_keepnan(raw)
L970                 df_z[f'{k}_RAW'] = raw
L971         for col in ['DOWNSIDE_DEV','MDD_1Y','RESID_VOL','DOWN_OUTPERF','EXT_200','DIV_TTM_PS','DIV_VAR5','DIV_YOY','DIV_FCF_COVER','DEBT2EQ','CURR_RATIO','EPS_VAR_8Q','MARKET_CAP','ADV60_USD']: df_z[col] = robust_z(df[col])
L972
L973         df_z['SIZE'], df_z['LIQ'] = robust_z(np.log1p(df['MARKET_CAP'])), robust_z(np.log1p(df['ADV60_USD']))
L974         df_z['QUALITY_F'] = robust_z(0.6*df['FCF_W'] + 0.4*df['ROE_W']).clip(-3.0,3.0)
L975         df_z['YIELD_F']   = 0.3*df_z['DIV'] + 0.7*df_z['DIV_STREAK']
L976
L977         # EPSãŒèµ¤å­—ã§ã‚‚FCFãŒé»’å­—ãªã‚‰å®Ÿè³ªé»’å­—ã¨ã¿ãªã™
L978         eps_pos_mask = (df['EPS'] > 0) | (df['FCF_MGN'] > 0)
L979         df_z['EPS_POS'] = df_z['EPS'].where(eps_pos_mask, 0.0)
L980
L981         # ===== ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ãƒ­ãƒ¼ãƒ—ç®—å‡º =====
L982         def zpos(x):
L983             arr = robust_z(x)
L984             idx = getattr(x, 'index', df_z.index)
L985             return pd.Series(arr, index=idx).fillna(0.0)
L986
L987         def relu(x):
L988             ser = x if isinstance(x, pd.Series) else pd.Series(x, index=df_z.index)
L989             return ser.clip(lower=0).fillna(0.0)
L990
L991         # å£²ä¸Šãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ãƒ­ãƒ¼ãƒ—ï¼ˆå››åŠæœŸï¼‰
L992         slope_rev = 0.70*zpos(df_z['REV_Q_YOY']) + 0.30*zpos(df_z['REV_YOY_ACC'])
L993         noise_rev = relu(robust_z(df_z['REV_YOY_VAR']) - 0.8)
L994         slope_rev_combo = slope_rev - 0.25*noise_rev
L995         df_z['TREND_SLOPE_REV_RAW'] = slope_rev_combo
L996         df_z['TREND_SLOPE_REV'] = slope_rev_combo.clip(-3.0, 3.0)
L997
L998         # EPSãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ãƒ­ãƒ¼ãƒ—ï¼ˆå››åŠæœŸï¼‰
L999         slope_eps = 0.60*zpos(df_z['EPS_Q_YOY']) + 0.40*zpos(df_z['EPS_POS'])
L1000         df_z['TREND_SLOPE_EPS_RAW'] = slope_eps
L1001         df_z['TREND_SLOPE_EPS'] = slope_eps.clip(-3.0, 3.0)
L1002
L1003         # å¹´æ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆã‚µãƒ–ï¼‰
L1004         slope_rev_yr = zpos(df_z['REV_YOY'])
L1005         slope_eps_yr = zpos(df_z.get('EPS_YOY', pd.Series(0.0, index=df.index)))
L1006         streak_base = df['REV_ANN_STREAK'].clip(lower=0).fillna(0)
L1007         streak_yr = streak_base / (streak_base.abs() + 1.0)
L1008         slope_rev_yr_combo = 0.7*slope_rev_yr + 0.3*streak_yr
L1009         df_z['TREND_SLOPE_REV_YR_RAW'] = slope_rev_yr_combo
L1010         df_z['TREND_SLOPE_REV_YR'] = slope_rev_yr_combo.clip(-3.0, 3.0)
L1011         df_z['TREND_SLOPE_EPS_YR_RAW'] = slope_eps_yr
L1012         df_z['TREND_SLOPE_EPS_YR'] = slope_eps_yr.clip(-3.0, 3.0)
L1013
L1014         # ===== GRW flexible score (variable data paths) =====
L1015         grw_raw = pd.to_numeric(df.get('GRW_FLEX_SCORE'), errors="coerce")
L1016         df_z['GRW_FLEX_SCORE_RAW'] = grw_raw
L1017         df_z['GROWTH_F_RAW'] = grw_raw
L1018         df_z['GROWTH_F'] = robust_z_keepnan(grw_raw).clip(-3.0, 3.0)
L1019         df_z['GRW_FLEX_WEIGHT'] = pd.to_numeric(df.get('GRW_FLEX_WEIGHT'), errors="coerce")
L1020         df_z['GRW_FLEX_CORE_RAW'] = pd.to_numeric(df.get('GRW_FLEX_CORE'), errors="coerce")
L1021         df_z['GRW_FLEX_PRICE_RAW'] = pd.to_numeric(df.get('GRW_FLEX_PRICE'), errors="coerce")
L1022
L1023         # Debug dump for GRW composition (console OFF by default; enable only with env)
L1024         if bool(os.getenv("GRW_CONSOLE_DEBUG")):
L1025             try:
L1026                 cols = ['GROWTH_F', 'GROWTH_F_RAW', 'GRW_FLEX_WEIGHT']
L1027                 use_cols = [c for c in cols if c in df_z.columns]
L1028                 i = df_z[use_cols].copy() if use_cols else pd.DataFrame(index=df_z.index)
L1029                 i.sort_values('GROWTH_F', ascending=False, inplace=True)
L1030                 limit = max(0, min(40, len(i)))
L1031                 print("[DEBUG: GRW]")
L1032                 for t in i.index[:limit]:
L1033                     row = i.loc[t]
L1034                     parts = []
L1035                     if pd.notna(row.get('GROWTH_F')):
L1036                         parts.append(f"GROWTH_F={row.get('GROWTH_F'):.3f}")
L1037                     raw_val = row.get('GROWTH_F_RAW')
L1038                     if pd.notna(raw_val):
L1039                         parts.append(f"GROWTH_F_RAW={raw_val:.3f}")
L1040                     weight_val = row.get('GRW_FLEX_WEIGHT')
L1041                     if pd.notna(weight_val):
L1042                         parts.append(f"w={weight_val:.2f}")
L1043                     path_val = None
L1044                     try:
L1045                         path_val = info.get(t, {}).get('DEBUG_GRW_PATH')
L1046                     except Exception:
L1047                         path_val = None
L1048                     if not path_val and 'DEBUG_GRW_PATH' in df.columns:
L1049                         path_val = df.at[t, 'DEBUG_GRW_PATH']
L1050                     if path_val:
L1051                         parts.append(f"PATH={path_val}")
L1052                     parts_json = None
L1053                     try:
L1054                         parts_json = info.get(t, {}).get('DEBUG_GRW_PARTS')
L1055                     except Exception:
L1056                         parts_json = None
L1057                     if not parts_json and 'DEBUG_GRW_PARTS' in df.columns:
L1058                         parts_json = df.at[t, 'DEBUG_GRW_PARTS']
L1059                     if parts_json:
L1060                         parts.append(f"PARTS={parts_json}")
L1061                     if not parts:
L1062                         parts.append('no-data')
L1063                     print(f"Ticker: {t} | " + " ".join(parts))
L1064                 print()
L1065             except Exception as exc:
L1066                 print(f"[ERR] GRW debug dump failed: {exc}")
L1067
L1068         df_z['MOM_F'] = robust_z(0.40*df_z['RS']
L1069             + 0.15*df_z['TR_str']
L1070             + 0.15*df_z['RS_SLOPE_6W']
L1071             + 0.15*df_z['RS_SLOPE_13W']
L1072             + 0.10*df_z['MA200_SLOPE_5M']
L1073             + 0.10*df_z['MA200_UP_STREAK_D']).clip(-3.0,3.0)
L1074         df_z['VOL'] = robust_z(df['BETA'])
L1075         df_z['QAL'], df_z['YLD'], df_z['MOM'] = df_z['QUALITY_F'], df_z['YIELD_F'], df_z['MOM_F']
L1076         df_z.drop(columns=['QUALITY_F','YIELD_F','MOM_F'], inplace=True, errors='ignore')
L1077
L1078         _dump_dfz(df_z=df_z, debug_mode=getattr(cfg, "debug_mode", False))
L1079
L1080         # === begin: BIO LOSS PENALTY =====================================
L1081         try:
L1082             penalty_z = float(os.getenv("BIO_LOSS_PENALTY_Z", "0.8"))
L1083         except Exception:
L1084             penalty_z = 0.8
L1085
L1086         def _is_bio_like(t: str) -> bool:
L1087             inf = info.get(t, {}) if isinstance(info, dict) else {}
L1088             sec = str(inf.get("sector", "")).lower()
L1089             ind = str(inf.get("industry", "")).lower()
L1090             if "health" not in sec:
L1091                 return False
L1092             keys = ("biotech", "biopharma", "pharma")
L1093             return any(k in ind for k in keys)
L1094
L1095         tickers_s = pd.Index(df_z.index)
L1096         is_bio = pd.Series({t: _is_bio_like(t) for t in tickers_s})
L1097         is_loss = pd.Series({t: (pd.notna(df.loc[t,"EPS"]) and df.loc[t,"EPS"] <= 0) for t in tickers_s})
L1098         mask_bio_loss = (is_bio & is_loss).reindex(df_z.index).fillna(False)
L1099
L1100         if bool(mask_bio_loss.any()) and penalty_z > 0:
L1101             df_z.loc[mask_bio_loss, "GROWTH_F"] = df_z.loc[mask_bio_loss, "GROWTH_F"] - penalty_z
L1102             df_z["GROWTH_F"] = df_z["GROWTH_F"].clip(-3.0, 3.0)
L1103         # === end: BIO LOSS PENALTY =======================================
L1104
L1105         df_z['TRD'] = 0.0  # TRDã¯ã‚¹ã‚³ã‚¢å¯„ä¸ã‹ã‚‰å¤–ã—ã€ãƒ†ãƒ³ãƒ—ãƒ¬åˆ¤å®šã¯ãƒ•ã‚£ãƒ«ã‚¿ã§è¡Œã†ï¼ˆåˆ—ã¯è¡¨ç¤ºäº’æ›ã®ãŸã‚æ®‹ã™ï¼‰
L1106         if 'BETA' not in df_z.columns: df_z['BETA'] = robust_z(df['BETA'])
L1107
L1108         df_z['D_VOL_RAW'] = robust_z(0.40*df_z['DOWNSIDE_DEV'] + 0.22*df_z['RESID_VOL'] + 0.18*df_z['MDD_1Y'] - 0.10*df_z['DOWN_OUTPERF'] - 0.05*df_z['EXT_200'] - 0.08*df_z['SIZE'] - 0.10*df_z['LIQ'] + 0.10*df_z['BETA'])
L1109         df_z['D_QAL']     = robust_z(0.35*df_z['QAL'] + 0.20*df_z['FCF'] + 0.15*df_z['CURR_RATIO'] - 0.15*df_z['DEBT2EQ'] - 0.15*df_z['EPS_VAR_8Q'])
L1110         df_z['D_YLD']     = robust_z(0.45*df_z['DIV'] + 0.25*df_z['DIV_STREAK'] + 0.20*df_z['DIV_FCF_COVER'] - 0.10*df_z['DIV_VAR5'])
L1111         df_z['D_TRD']     = robust_z(0.40*df_z.get('MA200_SLOPE_5M',0) - 0.30*df_z.get('EXT_200',0) + 0.15*df_z.get('NEAR_52W_HIGH',0) + 0.15*df_z['TR'])
L1112
L1113         # --- é‡ã¿ã¯ cfg ã‚’å„ªå…ˆï¼ˆå¤–éƒ¨ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ï¼‰ ---
L1114         # â‘  å…¨éŠ˜æŸ„ã§ G/D ã‚¹ã‚³ã‚¢ã‚’ç®—å‡ºï¼ˆunmaskedï¼‰
L1115         g_score_all = df_z.mul(pd.Series(cfg.weights.g)).sum(axis=1)
L1116
L1117         d_comp = pd.concat({
L1118             'QAL': df_z['D_QAL'],
L1119             'YLD': df_z['D_YLD'],
L1120             'VOL': df_z['D_VOL_RAW'],
L1121             'TRD': df_z['D_TRD']
L1122         }, axis=1)
L1123         dw = pd.Series(cfg.weights.d, dtype=float).reindex(['QAL','YLD','VOL','TRD']).fillna(0.0)
L1124         globals()['D_WEIGHTS_EFF'] = dw.copy()
L1125         d_score_all = d_comp.mul(dw, axis=1).sum(axis=1)
L1126
L1127         # â‘¡ ãƒ†ãƒ³ãƒ—ãƒ¬åˆ¤å®šï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ãã®ã¾ã¾ï¼‰
L1128         mask = df['trend_template']
L1129         if not bool(mask.any()):
L1130             mask = ((df.get('P_OVER_LOW52', np.nan) >= 0.25) &
L1131                 (df.get('NEAR_52W_HIGH', np.nan) >= -0.30) &
L1132                 (df.get('RS', np.nan) >= 0.08) &
L1133                 (df.get('MA200_SLOPE_1M', np.nan) > 0) &
L1134                 (df.get('P_OVER_150', np.nan) > 0) & (df.get('P_OVER_200', np.nan) > 0) &
L1135                 (df.get('MA150_OVER_200', np.nan) > 0) &
L1136                 (df.get('MA50_OVER_150', np.nan) > 0) & (df.get('MA50_OVER_200', np.nan) > 0) &
L1137                 (df.get('TR_str', np.nan) > 0)).fillna(False)
L1138             df['trend_template'] = mask
L1139
L1140         # â‘¢ æ¡ç”¨ç”¨ã¯ maskã€è¡¨ç¤º/åˆ†æç”¨ã¯åˆ—ã§å…¨éŠ˜æŸ„ä¿å­˜
L1141         g_score = g_score_all.loc[mask]
L1142         Scorer.g_score = g_score
L1143         df_z['GSC'] = g_score_all
L1144         df_z['DSC'] = d_score_all
L1145
L1146         try:
L1147             current = (pd.read_csv("current_tickers.csv")
L1148                   .iloc[:, 0]
L1149                   .str.upper()
L1150                   .tolist())
L1151         except FileNotFoundError:
L1152             warnings.warn("current_tickers.csv not found â€” bonus skipped")
L1153             current = []
L1154
L1155         mask_bonus = g_score.index.isin(current)
L1156         if mask_bonus.any():
L1157             # 1) factor.BONUS_COEFF ã‹ã‚‰ k ã‚’æ±ºã‚ã€ç„¡ã‘ã‚Œã° 0.4
L1158             k = float(getattr(sys.modules.get("factor"), "BONUS_COEFF", 0.4))
L1159             # 2) g å´ã® Ïƒ ã‚’å–ã‚Šã€NaN ãªã‚‰ 0 ã«ä¸¸ã‚ã‚‹
L1160             sigma_g = g_score.std()
L1161             if pd.isna(sigma_g):
L1162                 sigma_g = 0.0
L1163             bonus_g = round(k * sigma_g, 3)
L1164             g_score.loc[mask_bonus] += bonus_g
L1165             Scorer.g_score = g_score
L1166             # 3) D å´ã‚‚åŒæ§˜ã« Ïƒ ã® NaN ã‚’ã‚±ã‚¢
L1167             sigma_d = d_score_all.std()
L1168             if pd.isna(sigma_d):
L1169                 sigma_d = 0.0
L1170             bonus_d = round(k * sigma_d, 3)
L1171             d_score_all.loc[d_score_all.index.isin(current)] += bonus_d
L1172
L1173         try:
L1174             df = _apply_growth_entry_flags(df, ib, self, win_breakout=5, win_pullback=5)
L1175         except Exception:
L1176             pass
L1177
L1178         df_full = df.copy()
L1179         df_full_z = df_z.copy()
L1180
L1181         from factor import FeatureBundle  # type: ignore  # å®Ÿè¡Œæ™‚importãªã—ï¼ˆå¾ªç’°å›é¿ï¼‰
L1182         return FeatureBundle(df=df,
L1183             df_z=df_z,
L1184             g_score=g_score,
L1185             d_score_all=d_score_all,
L1186             missing_logs=pd.DataFrame(missing_logs),
L1187             df_full=df_full,
L1188             df_full_z=df_full_z,
L1189             scaler=None)
L1190
L1191 def _apply_growth_entry_flags(feature_df, bundle, self_obj, win_breakout=5, win_pullback=5):
L1192     """
L1193     Gæ ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã«å¯¾ã—ã€ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®š/æŠ¼ã—ç›®åç™ºã®ã€Œç›´è¿‘Nå–¶æ¥­æ—¥å†…ã®ç™ºç«ã€ã‚’åˆ¤å®šã—ã€
L1194     æ¬¡ã®åˆ—ã‚’ feature_df ã«è¿½åŠ ã™ã‚‹ï¼ˆindex=tickerï¼‰ã€‚
L1195       - G_BREAKOUT_recent_5d : bool
L1196       - G_BREAKOUT_last_date : str "YYYY-MM-DD"
L1197       - G_PULLBACK_recent_5d : bool
L1198       - G_PULLBACK_last_date : str "YYYY-MM-DD"
L1199       - G_PIVOT_price        : float
L1200     å¤±æ•—ã—ã¦ã‚‚ä¾‹å¤–ã¯æ¡ã‚Šæ½°ã—ã€æ—¢å­˜å‡¦ç†ã‚’é˜»å®³ã—ãªã„ã€‚
L1201     """
L1202     try:
L1203         px   = bundle.px                      # çµ‚å€¤ DataFrame
L1204         hi   = bundle.data['High']
L1205         lo   = bundle.data['Low']
L1206         vol  = bundle.data['Volume']
L1207         bench= bundle.spx                     # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ Series
L1208
L1209         # Gãƒ¦ãƒ‹ãƒãƒ¼ã‚¹æ¨å®šï¼šself.g_universe å„ªå…ˆ â†’ feature_df['group']=='G' â†’ å…¨éŠ˜æŸ„
L1210         g_universe = getattr(self_obj, "g_universe", None)
L1211         if g_universe is None:
L1212             try:
L1213                 g_universe = feature_df.index[feature_df['group'].astype(str).str.upper().eq('G')].tolist()
L1214             except Exception:
L1215                 g_universe = list(feature_df.index)
L1216         if not g_universe:
L1217             return feature_df
L1218
L1219         # æŒ‡æ¨™
L1220         px = px.ffill(limit=2)
L1221         ema21 = px[g_universe].ewm(span=21, adjust=False).mean()
L1222         ma50  = px[g_universe].rolling(50).mean()
L1223         ma150 = px[g_universe].rolling(150).mean()
L1224         ma200 = px[g_universe].rolling(200).mean()
L1225         atr20 = (hi[g_universe] - lo[g_universe]).rolling(20).mean()
L1226         vol20 = vol[g_universe].rolling(20).mean()
L1227         vol50 = vol[g_universe].rolling(50).mean()
L1228
L1229         # ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆåˆæ ¼
L1230         trend_template_ok = (px[g_universe] > ma50) & (px[g_universe] > ma150) & (px[g_universe] > ma200) \
L1231                             & (ma150 > ma200) & (ma200.diff() > 0)
L1232
L1233         # æ±ç”¨ãƒ”ãƒœãƒƒãƒˆï¼šç›´è¿‘65å–¶æ¥­æ—¥ã®é«˜å€¤ï¼ˆå½“æ—¥é™¤å¤–ï¼‰
L1234         pivot_price = hi[g_universe].rolling(65).max().shift(1)
L1235
L1236         # ç›¸å¯¾åŠ›ï¼šå¹´å†…é«˜å€¤æ›´æ–°
L1237         bench_aligned = bench.reindex(px.index).ffill()
L1238         rs = px[g_universe].div(bench_aligned, axis=0)
L1239         rs_high = rs.rolling(252).max().shift(1)
L1240
L1241         # ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆã€Œç™ºç”Ÿæ—¥ã€ï¼šæ¡ä»¶ç«‹ã¡ä¸ŠãŒã‚Š
L1242         breakout_today = trend_template_ok & (px[g_universe] > pivot_price) \
L1243                          & (vol[g_universe] >= 1.5 * vol50) & (rs > rs_high)
L1244         breakout_event = breakout_today & ~breakout_today.shift(1).fillna(False)
L1245
L1246         # æŠ¼ã—ç›®åç™ºã€Œç™ºç”Ÿæ—¥ã€ï¼šEMA21å¸¯Ã—å‡ºæ¥é«˜ãƒ‰ãƒ©ã‚¤ã‚¢ãƒƒãƒ—Ã—å‰æ—¥é«˜å€¤è¶ŠãˆÃ—çµ‚å€¤EMA21ä¸Š
L1247         near_ema21_band = px[g_universe].between(ema21 - atr20, ema21 + atr20)
L1248         volume_dryup = (vol20 / vol50) <= 1.0
L1249         pullback_bounce_confirmed = (px[g_universe] > hi[g_universe].shift(1)) & (px[g_universe] > ema21)
L1250         pullback_today = trend_template_ok & near_ema21_band & volume_dryup & pullback_bounce_confirmed
L1251         pullback_event = pullback_today & ~pullback_today.shift(1).fillna(False)
L1252
L1253         # ç›´è¿‘Nå–¶æ¥­æ—¥å†…ã®ç™ºç« / æœ€çµ‚ç™ºç”Ÿæ—¥
L1254         rows = []
L1255         for t in g_universe:
L1256             def _recent_and_date(s, win):
L1257                 sw = s[t].iloc[-win:]
L1258                 if sw.any():
L1259                     d = sw[sw].index[-1]
L1260                     return True, d.strftime("%Y-%m-%d")
L1261                 return False, ""
L1262             br_recent, br_date = _recent_and_date(breakout_event, win_breakout)
L1263             pb_recent, pb_date = _recent_and_date(pullback_event, win_pullback)
L1264             rows.append((t, {
L1265                 "G_BREAKOUT_recent_5d": br_recent,
L1266                 "G_BREAKOUT_last_date": br_date,
L1267                 "G_PULLBACK_recent_5d": pb_recent,
L1268                 "G_PULLBACK_last_date": pb_date,
L1269                 "G_PIVOT_price": float(pivot_price[t].iloc[-1]) if t in pivot_price.columns else float('nan'),
L1270             }))
L1271         flags = pd.DataFrame({k: v for k, v in rows}).T
L1272
L1273         # åˆ—ã‚’ä½œæˆãƒ»ä¸Šæ›¸ã
L1274         cols = ["G_BREAKOUT_recent_5d","G_BREAKOUT_last_date","G_PULLBACK_recent_5d","G_PULLBACK_last_date","G_PIVOT_price"]
L1275         for c in cols:
L1276             if c not in feature_df.columns:
L1277                 feature_df[c] = np.nan
L1278         feature_df.loc[flags.index, flags.columns] = flags
L1279
L1280     except Exception:
L1281         pass
L1282     return feature_df
L1283
```

## <.github/workflows/weekly-report.yml>
```text
L1 name: Weekly Stock Report
L2
L3 on:
L4   push:
L5     branches: [ main ]
L6     paths-ignore:
L7       - 'CodeForChat/**'
L8   schedule:
L9     - cron: '0 0 * * 6'  # UTC 00:00 â†’ JST 09:00ï¼ˆåœŸï¼‰
L10   workflow_dispatch:
L11
L12 jobs:
L13   build-and-report:
L14     runs-on: ubuntu-latest
L15     permissions:
L16       contents: write
L17
L18     steps:
L19       - name: Debug start
L20         run: echo 'ğŸš€ DEBUGstarted'
L21               
L22       - name: Checkout repository
L23         uses: actions/checkout@v3
L24
L25       - name: Setup Python
L26         uses: actions/setup-python@v5
L27         with:
L28           python-version: '3.x'
L29           cache: 'pip'
L30           cache-dependency-path: requirements.txt
L31
L32       - name: Install dependencies
L33         run: pip install -r requirements.txt
L34
L35       - name: Prepare results directory
L36         run: mkdir -p results
L37
L38       - name: Run factor & scoring
L39         env:
L40           SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
L41           FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
L42           FIN_THREADS: "8"
L43         run: python factor.py
```

## <documents/README.md>
```text
L1 # é‹ç”¨ãƒ«ãƒ¼ãƒ«
L2
L3 ## åŸºæœ¬æ§‹æˆ
L4 - 20éŠ˜æŸ„ã‚’å‡ç­‰é…åˆ†ï¼ˆç¾é‡‘ã‚’é™¤ã1éŠ˜æŸ„ã‚ãŸã‚Š5%ï¼‰
L5 - moomooè¨¼åˆ¸ã§é‹ç”¨
L6 - **Growthæ  12éŠ˜æŸ„ / Defenseæ  8éŠ˜æŸ„**ï¼ˆNORMAL åŸºæº–ï¼‰
L7
L8 ## Barbell Growth-Defenseæ–¹é‡
L9 - Growthæ  **12éŠ˜æŸ„**ï¼šé«˜æˆé•·ã§ä¹–é›¢æºã¨ãªã‚‹æ”»ã‚ã®éŠ˜æŸ„
L10 - Defenseæ  **8éŠ˜æŸ„**ï¼šä½ãƒœãƒ©ã§å®‰å®šæˆé•·ã—é…å½“ã‚’å¢—ã‚„ã™å®ˆã‚Šã®éŠ˜æŸ„
L11 - ã€ŒçŒ›çƒˆã«ä¼¸ã³ã‚‹æ”»ã‚ Ã— ç€å®Ÿã«ç¨¼ãç›¾ã€ã®çµ„åˆã›ã§ä¹–é›¢â†’åŠæˆ»ã—ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ã‚’ç‹™ã†
L12
L13 ## ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¤å®šï¼ˆtrend_template åˆæ ¼â€œæœ¬æ•°â€ã§åˆ¤å®šï¼‰
L14 - åˆæ ¼æœ¬æ•° = current+candidate å…¨ä½“ã®ã†ã¡ã€trend_template æ¡ä»¶ã‚’æº€ãŸã—ãŸéŠ˜æŸ„ã®**æœ¬æ•°(C)**ï¼ˆåŸºæº– N_G=12ï¼‰
L15 - ã—ãã„å€¤ã¯éå»~600å–¶æ¥­æ—¥ã®åˆ†å¸ƒã‹ã‚‰**æ¯å›è‡ªå‹•æ¡ç”¨**ï¼ˆåˆ†ä½ç‚¹ã¨é‹ç”¨â€œåºŠâ€ã®maxï¼‰
L16   - ç·Šæ€¥å…¥ã‚Š: `max(q05, 12æœ¬)`ï¼ˆ= N_Gï¼‰
L17   - ç·Šæ€¥è§£é™¤: `max(q20, 18æœ¬)`ï¼ˆ= ceil(1.5Ã—12)ï¼‰
L18   - é€šå¸¸å¾©å¸°: `max(q60, 36æœ¬)`ï¼ˆ= 3Ã—N_Gï¼‰
L19 - ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹: å‰å›ãƒ¢ãƒ¼ãƒ‰ã«ä¾å­˜ï¼ˆEMERGâ†’è§£é™¤ã¯23æœ¬ä»¥ä¸Šã€CAUTIONâ†’é€šå¸¸ã¯45æœ¬ä»¥ä¸Šï¼‰
L20
L21 ## ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¥ã®ç¾é‡‘ãƒ»ãƒ‰ãƒªãƒ•ãƒˆ
L22  - **é€šå¸¸(NORMAL)** : ç¾é‡‘ **10%** / ãƒ‰ãƒªãƒ•ãƒˆé–¾å€¤ **12%**
L23  - **è­¦æˆ’(CAUTION)** : ç¾é‡‘ **12.5%** / ãƒ‰ãƒªãƒ•ãƒˆé–¾å€¤ **14%**
L24  - **ç·Šæ€¥(EMERG)** : ç¾é‡‘ **20%** / **ãƒ‰ãƒªãƒ•ãƒˆå£²è²·åœæ­¢**ï¼ˆ20Ã—5%ã«å…¨æˆ»ã—ã®ã¿ï¼‰
L25
L26 ## ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®æ¨å¥¨â€œä¿æœ‰éŠ˜æŸ„æ•°â€ï¼ˆMMFâ‰’ç¾é‡‘ï¼‰
L27 *å„æ =5%ï¼ˆ20éŠ˜æŸ„å‡ç­‰ï¼‰ã€‚ãƒ¢ãƒ¼ãƒ‰ç§»è¡Œæ™‚ã¯**Gã®æ æ•°ã®ã¿**èª¿æ•´ã—ã€å¤–ã—ãŸæ ã¯ç¾é‡‘ã¨ã—ã¦ä¿æŒã€‚*
L28
L29 - **NORMAL:** G **12** / D **8** / ç¾é‡‘åŒ–æ  **0**  
L30 - **CAUTION:** G **10** / D **8** / ç¾é‡‘åŒ–æ  **2**ï¼ˆ= 10%ï¼‰  
L31 - **EMERG:** G **8**  / D **8** / ç¾é‡‘åŒ–æ  **4**ï¼ˆ= 20%ï¼‰  
L32
L33 > å®Ÿé‹ç”¨ï¼šâ­ï¸ä½ã‚¹ã‚³ã‚¢ã®Gã‹ã‚‰é †ã«å¤–ã™ã€‚è§£é™¤æ™‚ã¯factorä¸Šä½ã‹ã‚‰è£œå……ã€‚
L34
L35 ## ãƒˆãƒ¬ãƒ¼ãƒªãƒ³ã‚°ã‚¹ãƒˆãƒƒãƒ—
L36 - **åŸºæœ¬TS (ãƒ¢ãƒ¼ãƒ‰åˆ¥):** NORMAL **15%** / CAUTION **13%** / EMERG **10%**
L37 - å«ã¿ç›ŠãŒ **+30% / +60% / +100%** åˆ°é”ã§ã€åŸºæœ¬ã‹ã‚‰ **-3pt / -6pt / -8pt** å¼•ãä¸Šã’
L38 - TSç™ºå‹•ã§æ¸›å°‘ã—ãŸéŠ˜æŸ„ã¯ç¿Œæ—¥ä»¥é™ã«è£œå……ï¼ˆâ€»ç·Šæ€¥ãƒ¢ãƒ¼ãƒ‰ä¸­ã¯è£œå……ã—ãªã„ï¼‰
L39
L40 ## åŠæˆ»ã—ï¼ˆãƒªãƒãƒ©ãƒ³ã‚¹ï¼‰æ‰‹é †
L41 ãƒ‰ãƒªãƒ•ãƒˆãƒã‚§ãƒƒã‚¯ã§**ã‚¢ãƒ©ãƒ¼ãƒˆ**ãŒå‡ºãŸå ´åˆï¼ˆåˆè¨ˆ|drift| ãŒãƒ¢ãƒ¼ãƒ‰é–¾å€¤ã‚’è¶…éã€EMERGé™¤ãï¼‰ã€ç¿Œå–¶æ¥­æ—¥ã®ç±³å›½å¯„ä»˜ãã§ä¸‹è¨˜ã‚’å®Ÿæ–½ã™ã‚‹ã€‚
L42
L43 1. **å£²å´ï¼ˆå¿…é ˆï¼‰**  
L44    Slackãƒ†ãƒ¼ãƒ–ãƒ«ã® **Î”qty ãŒãƒã‚¤ãƒŠã‚¹ã®éŠ˜æŸ„ã‚’å£²å´** ã™ã‚‹ï¼ˆå¯„ä»˜ãæˆè¡Œæ¨å¥¨ï¼‰ã€‚  
L45    ã“ã‚Œã¯ã€ŒåŠæˆ»ã—ã€è¨ˆç®—ã«åŸºã¥ãéé‡é‡ã®å‰Šæ¸›ã‚’æ„å‘³ã™ã‚‹ã€‚
L46
L47 2. **è³¼å…¥ï¼ˆä»»æ„ãƒ»åŠæˆ»ã—ç›®å®‰ï¼‰**  
L48    åŠæˆ»ã—å¾Œã®åˆè¨ˆ|drift|ã‚’**ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤ï¼ˆSlackãƒ˜ãƒƒãƒ€ã«è¡¨ç¤ºï¼‰**ã«è¿‘ã¥ã‘ã‚‹ã“ã¨ã‚’ç›®å®‰ã«ã€  
L49    **ä»»æ„ã®éŠ˜æŸ„ã‚’è²·ã„å¢—ã—**ã—ã¦ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹ï¼ˆÎ”qtyãŒãƒ—ãƒ©ã‚¹ã®éŠ˜æŸ„ã‚’å„ªå…ˆã—ã¦ã‚‚ã‚ˆã„ï¼‰ã€‚
L50
L51 3. **ãƒˆãƒ¬ãƒ¼ãƒªãƒ³ã‚°ã‚¹ãƒˆãƒƒãƒ—ã®å†è¨­å®šï¼ˆå¿…é ˆï¼‰**  
L52    ã™ã¹ã¦ã®ä¿æœ‰éŠ˜æŸ„ã«ã¤ã„ã¦ã€æœ€æ–°ã®è©•ä¾¡é¡ã«åˆã‚ã›ã¦TSã‚’**å†ç™ºæ³¨ï¼æ›´æ–°**ã™ã‚‹ã€‚  
L53    ãƒ«ãƒ¼ãƒ«ã¯ä¸‹è¨˜ï¼ˆåˆ©ç›Šåˆ°é”ã§æ®µéšçš„ã«ã‚¿ã‚¤ãƒˆåŒ–ï¼‰ï¼š  
L54    - **åŸºæœ¬TS:** -15%  
L55    - **+30% åˆ°é” â†’ TS -12%**  
L56    - **+60% åˆ°é” â†’ TS -9%**  
L57    - **+100% åˆ°é” â†’ TS -7%**  
L58    â€»ã‚¹ãƒˆãƒƒãƒ—ä¾¡æ ¼ã®å¼•ãä¸Šã’ã¯è¨±å¯ã€**å¼•ãä¸‹ã’ã¯ä¸å¯**ï¼ˆåˆ©ç›Šä¿å…¨ã®åŸå‰‡ï¼‰ã€‚
L59
L60 4. **ä¾‹å¤–ï¼ˆEMERGãƒ¢ãƒ¼ãƒ‰ï¼‰**  
L61    ç·Šæ€¥(EMERG)ã§ã¯**ãƒ‰ãƒªãƒ•ãƒˆç”±æ¥ã®å£²è²·ã¯åœæ­¢ï¼ˆâˆï¼‰**ã€‚20éŠ˜æŸ„Ã—å„5%ã¸ã®**å…¨æˆ»ã—**ã®ã¿è¨±å®¹ã€‚
L62
L63 5. **å®Ÿè¡Œã‚¿ã‚¤ãƒŸãƒ³ã‚°**
L64    - åˆ¤å®šï¼šç±³å›½å¸‚å ´çµ‚å€¤ç›´å¾Œ
L65    - åŸ·è¡Œï¼šç¿Œå–¶æ¥­æ—¥ã®ç±³å›½å¯„ä»˜ãæˆè¡Œ
L66
L67 ## ãƒ¢ãƒ¼ãƒ‰ç§»è¡Œã®å®Ÿå‹™æ‰‹é †ï¼ˆè¶…ã‚·ãƒ³ãƒ—ãƒ«ï¼‰
L68 ãƒ¢ãƒ¼ãƒ‰ãŒå¤‰ã‚ã£ãŸã‚‰ã€**MMFâ‰’ç¾é‡‘**ã¨ã—ã¦æ‰±ã„ã€**Gã®æ æ•°ã ã‘**ã‚’èª¿æ•´ã™ã‚‹ï¼š
L69 1. **Gã‚’å‰Šã‚‹**ï¼ˆCAUTION/EMERGï¼‰  
L70    - â­ï¸ä½ã‚¹ã‚³ã‚¢ã®Gã‹ã‚‰é †ã«å¤–ã™ã€‚  
L71    - **`current_tickers.csv` ã‹ã‚‰å¤–ã™GéŠ˜æŸ„ã®è¡Œã‚’å‰Šé™¤**ï¼ˆï¼ãã®æ ã¯ç¾é‡‘åŒ–ï¼‰ã€‚
L72 2. **ç¾é‡‘ã¨ã—ã¦ä¿æŒ**  
L73    - å¤–ã—ãŸæ ã¯ç¾é‡‘ï¼ˆã¾ãŸã¯MMFç›¸å½“ï¼‰ã§ãƒ—ãƒ¼ãƒ«ã€‚  
L74 3. **å¾©å¸°æ™‚ã®è£œå……**ï¼ˆNORMALã¸ï¼‰  
L75    - **`current_tickers.csv` ã«éŠ˜æŸ„ã‚’è¿½åŠ **ï¼ˆfactorä¸Šä½ã‹ã‚‰ï¼‰ã€‚  
L76    - ä»¥é™ã¯æ—¥æ¬¡ãƒ‰ãƒªãƒ•ãƒˆ/TSãƒ«ãƒ¼ãƒ«ã«å¾“ã†ã€‚
L77
L78 > driftã¯ `target_ratio = 1/éŠ˜æŸ„æ•°` ã‚’è‡ªå‹•é©ç”¨ã€‚è¡Œæ•°ã«å¿œã˜ã¦è‡ªå‹•ã§å‡ç­‰æ¯”ç‡ãŒå†è¨ˆç®—ã•ã‚Œã‚‹ã€‚
L79
L80 ## å…¥æ›¿éŠ˜æŸ„é¸å®š
L81 - Oxfordã‚­ãƒ£ãƒ”ã‚¿ãƒ«ï¼ã‚¤ãƒ³ã‚«ãƒ ã€Alpha Investorã€Motley Fool Stock Advisorã€moomooã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ç­‰ã‚’å‚è€ƒã«chatGPTã§æ¤œè¨
L82 - å¹´é–“NISAæ ã¯Growthç¾¤ã®ä¸­ã‹ã‚‰ä½ãƒœãƒ©éŠ˜æŸ„ã‚’é¸å®šã—åˆ©ç”¨ã€‚é•·æœŸä¿æŒã«ã¯ã“ã ã‚ã‚‰ãªã„ã€‚
L83
L84 ## å†ã‚¨ãƒ³ãƒˆãƒªãƒ¼ï¼ˆã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ï¼‰
L85 - TSãƒ’ãƒƒãƒˆå¾Œã®åŒéŠ˜æŸ„å†INã¯ **8å–¶æ¥­æ—¥** ã®ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ã‚’è¨­ã‘ã‚‹ï¼ˆæœŸé–“ä¸­ã¯å†INç¦æ­¢ï¼‰
L86
L87 ## å®Ÿè¡Œã‚¿ã‚¤ãƒŸãƒ³ã‚°
L88 - åˆ¤å®šï¼šç±³å›½å¸‚å ´çµ‚å€¤ç›´å¾Œ
L89 - åŸ·è¡Œï¼šç¿Œå–¶æ¥­æ—¥ã®ç±³å›½å¯„ä»˜ãæˆè¡Œ
```

## <documents/factor_design.md>
```text
L1 # factor.py è©³ç´°è¨­è¨ˆæ›¸
L2
L3 ## æ¦‚è¦
L4 - æ—¢å­˜ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®éŠ˜æŸ„ã¨æ¤œè¨ä¸­ã®éŠ˜æŸ„ç¾¤ã‚’åŒæ™‚ã«æ‰±ã†éŠ˜æŸ„é¸å®šãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€‚
L5 - ä¾¡æ ¼ãƒ»è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šè¾¼ã¿ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã¨DRRSé¸å®šã‚’è¡Œã†ã“ã¨ã§ã€ä»¥ä¸‹ã®ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆã‚’å¾—ã‚‹ã€‚
L6   - æ¡ç”¨éŠ˜æŸ„ã¨æƒœã—ãã‚‚æ¼ã‚ŒãŸéŠ˜æŸ„ã®ã‚¹ã‚³ã‚¢ä¸€è¦§
L7   - IN/OUTã®ãƒ†ã‚£ãƒƒã‚«ãƒ¼ãƒªã‚¹ãƒˆã¨OUTå´ã®ä½ã‚¹ã‚³ã‚¢éŠ˜æŸ„
L8   - æ–°æ—§ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®æ¯”è¼ƒè¡¨
L9   - æ¤œè¨ä¸­éŠ˜æŸ„ã®ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆæ•´ç†ç”¨ï¼‰
L10
L11 ## å…¨ä½“ãƒ•ãƒ­ãƒ¼
L12 1. **Input** â€“ `current_tickers.csv`ã¨`candidate_tickers.csv`ã‚’èª­ã¿è¾¼ã¿ã€yfinanceã‚„Finnhubã®APIã‹ã‚‰ä¾¡æ ¼ãƒ»è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã¦`InputBundle`ã‚’æ•´å‚™ã€‚
L13 2. **Score Calculation** â€“ ScorerãŒç‰¹å¾´é‡ã‚’è¨ˆç®—ã—å› å­ã‚¹ã‚³ã‚¢ã‚’åˆæˆã—ã¦`FeatureBundle`ã‚’ç”Ÿæˆã€‚
L14 3. **Correlation Reduction & Selection** â€“ SelectorãŒDRRSãƒ­ã‚¸ãƒƒã‚¯ã§ç›¸é–¢ã‚’æŠ‘ãˆã¤ã¤G/DéŠ˜æŸ„ã‚’é¸å®šã—`SelectionBundle`ã‚’å¾—ã‚‹ã€‚
L15 4. **Output** â€“ æ¡ç”¨çµæœã¨å‘¨è¾ºæƒ…å ±ã‚’è¡¨ãƒ»Slacké€šçŸ¥ã¨ã—ã¦å‡ºåŠ›ã€‚
L16
L17 ```mermaid
L18 flowchart LR
L19   A[Input\nAPI & å‰å‡¦ç†] --> B[Score Calculation\nç‰¹å¾´é‡ãƒ»å› å­åˆæˆ]
L20   B --> C[Correlation Reduction\nDRRSé¸å®š]
L21   C --> D[Output\nSlacké€šçŸ¥]
L22 ```
L23
L24 ## å®šæ•°ãƒ»è¨­å®š
L25 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L26 | --- | --- | --- |
L27 | `exist` / `cand` | ç¾è¡Œãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã¨æ¤œè¨ä¸­éŠ˜æŸ„ã®ãƒ†ã‚£ãƒƒã‚«ãƒ¼ãƒªã‚¹ãƒˆ | ã‚¹ã‚³ã‚¢å¯¾è±¡ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã®æ§‹æˆã€å€™è£œæ•´ç† |
L28 | `bench` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼ | ç›¸å¯¾å¼·ã•ãƒ»Î²ç®—å‡ºã€ãƒãƒ¼ãƒˆæ¯”è¼ƒ |
L29 | `CAND_PRICE_MAX` | å€™è£œéŠ˜æŸ„ã®è¨±å®¹ä¾¡æ ¼ä¸Šé™ | é«˜é¡éŠ˜æŸ„ã®äº‹å‰é™¤å¤– |
L30 | `N_G` / `N_D` | G/Dæ¡ç”¨æ ã®ä»¶æ•°ï¼ˆ**æ—¢å®š: 12 / 8**ï¼‰ | æœ€çµ‚çš„ã«é¸ã¶éŠ˜æŸ„æ•°ã®åˆ¶ç´„ |
L31 | `g_weights` / `D_weights` | å„å› å­ã®é‡ã¿dict | G/Dã‚¹ã‚³ã‚¢åˆæˆ |
L32 | `D_BETA_MAX` | Dãƒã‚±ãƒƒãƒˆã®è¨±å®¹Î²ä¸Šé™ | é«˜Î²éŠ˜æŸ„ã®é™¤å¤–ãƒ•ã‚£ãƒ«ã‚¿ |
L33 | `FILTER_SPEC` | G/Dã”ã¨ã®å‰å‡¦ç†ãƒ•ã‚£ãƒ«ã‚¿ | ãƒˆãƒ¬ãƒ³ãƒ‰ãƒã‚¹ã‚¯ã‚„Î²ä¸Šé™è¨­å®š |
L34 | `corrM` | DRRSåˆæœŸãƒ—ãƒ¼ãƒ«ã®æœ€å¤§ä»¶æ•° | ç›¸é–¢è¡Œåˆ—ã‚µã‚¤ã‚ºåˆ¶å¾¡ |
L35 | `DRRS_G` / `DRRS_D` | DRRSãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿dict | ãƒã‚±ãƒƒãƒˆåˆ¥ã®ç›¸é–¢ä½æ¸›è¨­å®š |
L36 | `DRRS_SHRINK` | æ®‹å·®ç›¸é–¢ã®å¯¾è§’ã‚·ãƒ¥ãƒªãƒ³ã‚¯ç‡ | `residual_corr`ã®å®‰å®šåŒ– |
L37 | `CROSS_MU_GD` | G-Dé–“ã‚¯ãƒ­ã‚¹ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£Î¼ | 2ãƒã‚±ãƒƒãƒˆåŒæ™‚æœ€é©åŒ–ã§ç›¸é–¢æŠ‘åˆ¶ |
L38 | `RESULTS_DIR` | é¸å®šçµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª | `_save_sel`/`_load_prev`ã®å…¥å‡ºåŠ› |
L39
L40 é¸å®šçµæœã¯`results/`é…ä¸‹ã«JSONã¨ã—ã¦ä¿å­˜ã—ã€æ¬¡å›å®Ÿè¡Œæ™‚ã«`_load_prev`ã§èª­ã¿è¾¼ã‚“ã§é¸å®šæ¡ä»¶ã«åæ˜ ã€‚
L41
L42 ## DTO/Config
L43 å„ã‚¹ãƒ†ãƒƒãƒ—é–“ã§å—ã‘æ¸¡ã™ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¨è¨­å®šå€¤ã€‚å¤‰æ•°ã®æ„å‘³åˆã„ã¨åˆ©ç”¨ç®‡æ‰€ã‚’ä»¥ä¸‹ã«ç¤ºã™ã€‚
L44
L45 ### InputBundleï¼ˆInput â†’ Scorerï¼‰
L46 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L47 | --- | --- | --- |
L48 | `cand` | å€™è£œéŠ˜æŸ„ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã®ãƒªã‚¹ãƒˆ | OUTãƒ†ãƒ¼ãƒ–ãƒ«ã‚„ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°å¯¾è±¡ã®æ¯é›†å›£ |
L49 | `tickers` | ç¾è¡Œ+å€™è£œã‚’åˆã‚ã›ãŸãƒ†ã‚£ãƒƒã‚«ãƒ¼ä¸€è¦§ | ä¾¡æ ¼ãƒ»å‡ºæ¥é«˜ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€ãƒªã‚¿ãƒ¼ãƒ³è¨ˆç®— |
L50 | `bench` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼ | ç›¸å¯¾å¼·ã•ãƒ»Î²ç®—å‡ºã€ãƒãƒ¼ãƒˆæ¯”è¼ƒ |
L51 | `data` | yfinanceã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœï¼ˆéšå±¤åˆ—ï¼‰ | `px`/`spx`/ãƒªã‚¿ãƒ¼ãƒ³ç­‰ã®åŸºç¤ãƒ‡ãƒ¼ã‚¿ |
L52 | `px` | `data['Close']`ã ã‘ã‚’æŠœãå‡ºã—ãŸä¾¡æ ¼ç³»åˆ— | æŒ‡æ¨™è¨ˆç®—ãƒ»ãƒªã‚¿ãƒ¼ãƒ³ç”Ÿæˆ |
L53 | `spx` | `data['Close'][bench]` ã®Series | `rs`ã‚„`calc_beta`ã®åŸºæº–æŒ‡æ•° |
L54 | `tickers_bulk` | `yf.Tickers`ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ | `info`ç­‰ã®ä¸€æ‹¬å–å¾— |
L55 | `info` | ãƒ†ã‚£ãƒƒã‚«ãƒ¼åˆ¥ã®yfinanceæƒ…å ±dict | ã‚»ã‚¯ã‚¿ãƒ¼åˆ¤å®šã‚„EPSè£œå®Œ |
L56 | `eps_df` | EPS TTM/ç›´è¿‘EPSç­‰ã‚’ã¾ã¨ã‚ãŸè¡¨ | æˆé•·æŒ‡æ¨™ã®ç®—å‡º |
L57 | `fcf_df` | CFOãƒ»CapExãƒ»FCF TTMã¨æƒ…å ±æºãƒ•ãƒ©ã‚° | FCF/EVã‚„é…å½“ã‚«ãƒãƒ¬ãƒƒã‚¸ |
L58 | `returns` | `px.pct_change()`ã®ãƒªã‚¿ãƒ¼ãƒ³è¡¨ | ç›¸é–¢è¡Œåˆ—ãƒ»DRRSè¨ˆç®— |
L59
L60 ### FeatureBundleï¼ˆScorer â†’ Selectorï¼‰
L61 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L62 | --- | --- | --- |
L63 | `df` | è¨ˆç®—æ¸ˆã¿æŒ‡æ¨™ã®ç”Ÿå€¤ãƒ†ãƒ¼ãƒ–ãƒ« | ãƒ‡ãƒãƒƒã‚°ãƒ»å‡ºåŠ›è¡¨ç¤º |
L64 | `df_z` | ã‚¦ã‚£ãƒ³ã‚¶ãƒ¼å¾ŒZã‚¹ã‚³ã‚¢åŒ–ã—ãŸæŒ‡æ¨™è¡¨ | å› å­ã‚¹ã‚³ã‚¢åˆæˆã€é¸å®šåŸºæº– |
L65 | `g_score` | Gãƒã‚±ãƒƒãƒˆç·åˆã‚¹ã‚³ã‚¢ | Gé¸å®šã€IN/OUTæ¯”è¼ƒ |
L66 | `d_score_all` | Dãƒã‚±ãƒƒãƒˆç·åˆã‚¹ã‚³ã‚¢ï¼ˆå…¨éŠ˜æŸ„ï¼‰ | Dé¸å®šã€ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚° |
L67 | `missing_logs` | æ¬ ææŒ‡æ¨™ã¨è£œå®ŒçŠ¶æ³ã®ãƒ­ã‚° | ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ |
L68
L69 ### SelectionBundleï¼ˆSelector â†’ Outputï¼‰
L70 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L71 | --- | --- | --- |
L72 | `resG` | Gé¸å®šçµæœã®è©³ç´°dictï¼ˆ`tickers`ã€ç›®çš„å€¤ç­‰ï¼‰ | çµæœä¿å­˜ãƒ»å¹³å‡ç›¸é–¢ãªã©ã®æŒ‡æ¨™è¡¨ç¤º |
L73 | `resD` | Dé¸å®šçµæœã®è©³ç´°dict | åŒä¸Š |
L74 | `top_G` | æœ€çµ‚æ¡ç”¨Gãƒ†ã‚£ãƒƒã‚«ãƒ¼ | æ–°ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæ§‹ç¯‰ |
L75 | `top_D` | æœ€çµ‚æ¡ç”¨Dãƒ†ã‚£ãƒƒã‚«ãƒ¼ | åŒä¸Š |
L76 | `init_G` | DRRSå‰ã®GåˆæœŸå€™è£œ | æƒœã—ãã‚‚å¤–ã‚ŒãŸéŠ˜æŸ„è¡¨ç¤º |
L77 | `init_D` | DRRSå‰ã®DåˆæœŸå€™è£œ | åŒä¸Š |
L78
L79 ### WeightsConfig
L80 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L81 | --- | --- | --- |
L82 | `g` | Gå› å­ï¼ˆGRW/MOM/VOLï¼‰ã®é‡ã¿dict | `g_score`åˆæˆ |
L83 | `d` | Då› å­ï¼ˆD_QAL/D_YLD/D_VOL_RAW/D_TRDï¼‰ã®é‡ã¿dict | `d_score_all`åˆæˆ |
L84
L85 ### DRRSParams
L86 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L87 | --- | --- | --- |
L88 | `corrM` | DRRSåˆæœŸãƒ—ãƒ¼ãƒ«ã®æœ€å¤§ä»¶æ•° | ç›¸é–¢è¡Œåˆ—ã‚µã‚¤ã‚ºåˆ¶å¾¡ |
L89 | `shrink` | æ®‹å·®ç›¸é–¢ã®ã‚·ãƒ¥ãƒªãƒ³ã‚¯ç‡ | `residual_corr`ã®å¯¾è§’å¼·èª¿ |
L90 | `G` | Gãƒã‚±ãƒƒãƒˆç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿dictï¼ˆ`lookback`ç­‰ï¼‰ | `select_bucket_drrs`è¨­å®š |
L91 | `D` | Dãƒã‚±ãƒƒãƒˆç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿dict | åŒä¸Š |
L92 | `cross_mu_gd` | G-Dã‚¯ãƒ­ã‚¹ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£ä¿‚æ•°Î¼ | `select_buckets`ã®ç›®çš„é–¢æ•° |
L93
L94 ### PipelineConfig
L95 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L96 | --- | --- | --- |
L97 | `weights` | `WeightsConfig`ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ | ã‚¹ã‚³ã‚¢åˆæˆã®é‡ã¿å‚ç…§ |
L98 | `drrs` | `DRRSParams`ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ | é¸å®šã‚¹ãƒ†ãƒƒãƒ—ã®è¨­å®šå€¤ |
L99 | `price_max` | å€™è£œéŠ˜æŸ„ã®è¨±å®¹ä¾¡æ ¼ä¸Šé™ | Inputæ®µéšã§ã®ãƒ•ã‚£ãƒ«ã‚¿ |
L100
L101 ## å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
L102 - `winsorize_s` / `robust_z` : å¤–ã‚Œå€¤å‡¦ç†ã¨Zã‚¹ã‚³ã‚¢åŒ–ã€‚
L103 - `_safe_div` / `_safe_last` : ä¾‹å¤–ã‚’æ½°ã—ãŸåˆ†å‰²ãƒ»æœ«å°¾å–å¾—ã€‚
L104 - `_load_prev` / `_save_sel` : é¸å®šçµæœã®èª­ã¿æ›¸ãã€‚
L105
L106 ## ã‚¯ãƒ©ã‚¹è¨­è¨ˆ
L107 ### Step1: Input
L108 `current_tickers.csv`ã®ç¾è¡ŒéŠ˜æŸ„ã¨`candidate_tickers.csv`ã®æ¤œè¨ä¸­éŠ˜æŸ„ã‚’èµ·ç‚¹ã«ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„ã™ã‚‹ã€‚å¤–éƒ¨I/Oã¨å‰å‡¦ç†ã‚’æ‹…å½“ã—ã€`prepare_data`ã§`InputBundle`ã‚’ç”Ÿæˆã€‚ä¾¡æ ¼ãƒ»è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã¯**yfinanceã‚’å„ªå…ˆã—ã€æ¬ æãŒã‚ã‚‹æŒ‡æ¨™ã®ã¿Finnhub APIã§è£œå®Œ**ã™ã‚‹ã€‚
L109 ä¸»ãªãƒ¡ã‚½ãƒƒãƒ‰:
L110 - `impute_eps_ttm` : å››åŠæœŸEPSÃ—4ã§TTMã‚’æ¨å®šã—æ¬ ææ™‚ã®ã¿å·®ã—æ›¿ãˆã€‚
L111 - `fetch_cfo_capex_ttm_yf` : yfinanceã®å››åŠæœŸ/å¹´æ¬¡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼ã‹ã‚‰CFOãƒ»CapExãƒ»FCF TTMã‚’ç®—å‡ºã€‚
L112 - `fetch_cfo_capex_ttm_finnhub` : yfinanceã§æ¬ ã‘ãŸéŠ˜æŸ„ã®ã¿Finnhub APIã§è£œå®Œã€‚
L113 - `compute_fcf_with_fallback` : yfinanceå€¤ã‚’åŸºæº–ã«Finnhubå€¤ã§ç©´åŸ‹ã‚ã—ã€CFO/CapEx/FCFã¨æƒ…å ±æºãƒ•ãƒ©ã‚°ã‚’è¿”ã™ã€‚
L114 - `_build_eps_df` : `info`ã‚„`quarterly_earnings`ã‹ã‚‰EPS TTMã¨ç›´è¿‘EPSã‚’è¨ˆç®—ã—ã€`impute_eps_ttm`ã§è£œå®Œã€‚
L115 - `prepare_data` :
L116     0. CSVã‹ã‚‰ç¾è¡ŒéŠ˜æŸ„ã¨å€™è£œéŠ˜æŸ„ã®ãƒ†ã‚£ãƒƒã‚«ãƒ¼ä¸€è¦§ã‚’èª­ã¿è¾¼ã‚€ã€‚
L117     1. å€™è£œéŠ˜æŸ„ã®ç¾åœ¨å€¤ã‚’å–å¾—ã—ä¾¡æ ¼ä¸Šé™ã§ãƒ•ã‚£ãƒ«ã‚¿ã€‚
L118     2. æ—¢å­˜+å€™è£œã‹ã‚‰å¯¾è±¡ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚’æ±ºå®šã—ã€ä¾¡æ ¼ãƒ»å‡ºæ¥é«˜ã‚’ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆyfinanceï¼‰ã€‚
L119     3. yfinanceå€¤ã‚’åŸºã«EPS/FCFãƒ†ãƒ¼ãƒ–ãƒ«ã‚„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç³»åˆ—ã€ãƒªã‚¿ãƒ¼ãƒ³ã‚’æ§‹ç¯‰ã—ã€æ¬ æã‚»ãƒ«ã¯Finnhubå‘¼ã³å‡ºã—ã§ç©´åŸ‹ã‚ã€‚
L120     4. ä¸Šè¨˜ã‚’`InputBundle`ã«æ ¼ç´ã—ã¦è¿”ã™ã€‚
L121
L122 ### Step2: Score Calculation (Scorer)
L123 ç‰¹å¾´é‡è¨ˆç®—ã¨ã‚¹ã‚³ã‚¢åˆæˆã‚’æ‹…å½“ã—ã€`FeatureBundle`ã‚’è¿”ã™ã€‚
L124
L125 #### è£œåŠ©é–¢æ•°
L126 - `trend(s)` : 50/150/200æ—¥ç§»å‹•å¹³å‡ã‚„52é€±ãƒ¬ãƒ³ã‚¸ã‹ã‚‰-0.5ã€œ0.5ã§æ§‹æˆã•ã‚ŒãŸãƒˆãƒ¬ãƒ³ãƒ‰æŒ‡æ¨™ã€‚
L127 - `rs(s,b)` / `tr_str(s)` / `rs_line_slope(s,b,win)` : ç›¸å¯¾å¼·ã•ã‚„çŸ­æœŸãƒˆãƒ¬ãƒ³ãƒ‰ã€RSå›å¸°å‚¾ãã‚’ç®—å‡ºã€‚
L128 - `ev_fallback` : `enterpriseValue`æ¬ ææ™‚ã«è² å‚µãƒ»ç¾é‡‘ã‹ã‚‰EVã‚’æ¨å®šã€‚
L129 - `dividend_status` / `div_streak` : é…å½“æœªè¨­å®šçŠ¶æ³ã®åˆ¤å®šã¨å¢—é…å¹´æ•°ã‚«ã‚¦ãƒ³ãƒˆã€‚
L130 - `fetch_finnhub_metrics` : Finnhub APIã‹ã‚‰EPSæˆé•·ãƒ»ROEãƒ»Î²ãªã©ä¸è¶³æŒ‡æ¨™ã‚’å–å¾—ã€‚
L131 - `calc_beta` : ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ã®å…±åˆ†æ•£ã‹ã‚‰Î²ã‚’ç®—å‡ºã€‚
L132 - `spx_to_alpha` : S&P500ã®ä½ç½®æƒ…å ±ã‹ã‚‰DRRSã§ç”¨ã„ã‚‹Î±ã‚’æ¨å®šã€‚
L133 - `soft_cap_effective_scores` / `pick_top_softcap` : ã‚»ã‚¯ã‚¿ãƒ¼ã‚½ãƒ•ãƒˆã‚­ãƒ£ãƒƒãƒ—ä»˜ãã‚¹ã‚³ã‚¢èª¿æ•´ã¨ä¸Šä½æŠ½å‡ºã€‚
L134
L135 **è£œåŠ©é–¢æ•°ã¨ç”ŸæˆæŒ‡æ¨™**
L136
L137 | è£œåŠ©é–¢æ•° | ç”ŸæˆæŒ‡æ¨™ | ç•¥ç§° |
L138 | --- | --- | --- |
L139 | `trend` | ãƒˆãƒ¬ãƒ³ãƒ‰ç·åˆå€¤ | `TR` |
L140 | `rs` | ç›¸å¯¾å¼·ã• | `RS` |
L141 | `tr_str` | ä¾¡æ ¼ã¨50æ—¥ç·šã®ä¹–é›¢ | `TR_str` |
L142 | `rs_line_slope` | RSç·šã®å›å¸°å‚¾ã | `RS_SLOPE_*` |
L143 | `calc_beta` | Î² | `BETA` |
L144 | `div_streak` | é€£ç¶šå¢—é…å¹´æ•° | `DIV_STREAK` |
L145
L146 #### `aggregate_scores` è©³ç´°
L147 1. å„éŠ˜æŸ„ã®ä¾¡æ ¼ç³»åˆ—ã‚„`info`ã‚’åŸºã«ä»¥ä¸‹ã‚’ç®—å‡ºã€‚
L148    - **ãƒˆãƒ¬ãƒ³ãƒ‰/ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ **: `TR`ã€`RS`ã€`TR_str`ã€å¤šæ§˜ãªç§»å‹•å¹³å‡æ¯”ã€`RS_SLOPE_*`ãªã©ã€‚
L149    - **ãƒªã‚¹ã‚¯**: `BETA`ã€`DOWNSIDE_DEV`ã€`MDD_1Y`ã€`RESID_VOL`ã€`DOWN_OUTPERF`ã€`EXT_200`ç­‰ã€‚
L150    - **é…å½“**: `DIV`ã€`DIV_TTM_PS`ã€`DIV_VAR5`ã€`DIV_YOY`ã€`DIV_FCF_COVER`ã€`DIV_STREAK`ã€‚
L151    - **è²¡å‹™ãƒ»æˆé•·**: `EPS`ã€`REV`ã€`ROE`ã€`FCF/EV`ã€`REV_Q_YOY`ã€`EPS_Q_YOY`ã€`REV_YOY_ACC`ã€`REV_YOY_VAR`ã€`REV_ANN_STREAK`ã€`RULE40`ã€`FCF_MGN` ç­‰ã€‚
L152    - **å®‰å®šæ€§/ã‚µã‚¤ã‚º**: `DEBT2EQ`ã€`CURR_RATIO`ã€`MARKET_CAP`ã€`ADV60_USD`ã€`EPS_VAR_8Q`ãªã©ã€‚
L153 2. æŒ‡æ¨™æ¬ æã¯Finnhub APIç­‰ã§è£œå®Œã—ã€æœªå–å¾—é …ç›®ã‚’`missing_logs`ã«è¨˜éŒ²ã€‚
L154 3. `winsorize_s`â†’`robust_z`ã§æ¨™æº–åŒ–ã—`df_z`ã¸ä¿å­˜ã€‚ã‚µã‚¤ã‚ºãƒ»æµå‹•æ€§ã¯å¯¾æ•°å¤‰æ›ã€‚
L155 4. æ­£è¦åŒ–æ¸ˆæŒ‡æ¨™ã‹ã‚‰å› å­ã‚¹ã‚³ã‚¢ã‚’åˆæˆã€‚
L156    - å„å› å­ã®æ§‹æˆã¨é‡ã¿ã¯ä»¥ä¸‹ã®é€šã‚Šã€‚
L157      - **GRW**: 0.30Ã—`REV` + 0.20Ã—`EPS_Q_YOY` + 0.15Ã—`REV_Q_YOY` + 0.15Ã—`REV_YOY_ACC` + 0.10Ã—`RULE40` + 0.10Ã—`FCF_MGN` + 0.10Ã—`REV_ANN_STREAK` âˆ’ 0.05Ã—`REV_YOY_VAR`ã€‚
L158      - **MOM**: 0.40Ã—`RS` + 0.15Ã—`TR_str` + 0.15Ã—`RS_SLOPE_6W` + 0.15Ã—`RS_SLOPE_13W` + 0.10Ã—`MA200_SLOPE_5M` + 0.10Ã—`MA200_UP_STREAK_D`ã€‚
L159      - **VOL**: `BETA`å˜ä½“ã‚’ä½¿ç”¨ã€‚
L160      - **QAL**: 0.60Ã—`FCF_W` + 0.40Ã—`ROE_W`ã§ä½œæˆã€‚
L161      - **YLD**: 0.30Ã—`DIV` + 0.70Ã—`DIV_STREAK`ã€‚
L162      - **D_QAL**: 0.35Ã—`QAL` + 0.20Ã—`FCF` + 0.15Ã—`CURR_RATIO` âˆ’ 0.15Ã—`DEBT2EQ` âˆ’ 0.15Ã—`EPS_VAR_8Q`ã€‚
L163      - **D_YLD**: 0.45Ã—`DIV` + 0.25Ã—`DIV_STREAK` + 0.20Ã—`DIV_FCF_COVER` âˆ’ 0.10Ã—`DIV_VAR5`ã€‚
L164      - **D_VOL_RAW**: 0.40Ã—`DOWNSIDE_DEV` + 0.22Ã—`RESID_VOL` + 0.18Ã—`MDD_1Y` âˆ’ 0.10Ã—`DOWN_OUTPERF` âˆ’ 0.05Ã—`EXT_200` âˆ’ 0.08Ã—`SIZE` âˆ’ 0.10Ã—`LIQ` + 0.10Ã—`BETA`ã€‚
L165      - **D_TRD**: 0.40Ã—`MA200_SLOPE_5M` âˆ’ 0.30Ã—`EXT_200` + 0.15Ã—`NEAR_52W_HIGH` + 0.15Ã—`TR`ã€‚
L166     - ä¸»ãªæŒ‡æ¨™ã®ç•¥ç§°ã¨æ„å‘³:
L167
L168       | ç•¥ç§° | è£œåŠ©é–¢æ•° | æ¦‚è¦ |
L169       | --- | --- | --- |
L170       | TR | `trend` | 50/150/200æ—¥ç§»å‹•å¹³å‡ã¨52é€±ãƒ¬ãƒ³ã‚¸ã‚’çµ„ã¿åˆã‚ã›ãŸãƒˆãƒ¬ãƒ³ãƒ‰ç·åˆå€¤ |
L171       | RS | `rs` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«å¯¾ã™ã‚‹ç›¸å¯¾å¼·ã•ï¼ˆ12M/1Mãƒªã‚¿ãƒ¼ãƒ³å·®ï¼‰ |
L172       | TR_str | `tr_str` | ä¾¡æ ¼ã¨50æ—¥ç§»å‹•å¹³å‡ã®ä¹–é›¢ |
L173       | RS_SLOPE_6W | `rs_line_slope` | ç›¸å¯¾å¼·ã•ç·šã®6é€±å›å¸°å‚¾ã |
L174       | RS_SLOPE_13W | `rs_line_slope` | ç›¸å¯¾å¼·ã•ç·šã®13é€±å›å¸°å‚¾ã |
L175       | MA200_SLOPE_5M | - | 200æ—¥ç§»å‹•å¹³å‡ã®5ã‹æœˆé¨°è½ç‡ |
L176       | MA200_UP_STREAK_D | - | 200æ—¥ç·šãŒé€£ç¶šã§ä¸Šå‘ã„ãŸæ—¥æ•° |
L177       | BETA | `calc_beta` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«å¯¾ã™ã‚‹Î² |
L178       | DOWNSIDE_DEV | - | ä¸‹æ–¹ãƒªã‚¿ãƒ¼ãƒ³ã®ã¿ã®å¹´ç‡åŒ–æ¨™æº–åå·® |
L179       | RESID_VOL | - | Î²ã§èª¿æ•´ã—ãŸæ®‹å·®ãƒªã‚¿ãƒ¼ãƒ³ã®å¹´ç‡åŒ–æ¨™æº–åå·® |
L180       | MDD_1Y | - | éå»1å¹´ã®æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³ |
L181       | DOWN_OUTPERF | - | å¸‚å ´ä¸‹è½æ—¥ã«å¯¾ã™ã‚‹å¹³å‡è¶…éãƒªã‚¿ãƒ¼ãƒ³ |
L182       | EXT_200 | - | 200æ—¥ç§»å‹•å¹³å‡ã‹ã‚‰ã®çµ¶å¯¾ä¹–é›¢ç‡ |
L183       | NEAR_52W_HIGH | - | 52é€±é«˜å€¤ã¾ã§ã®ä¸‹æ–¹è·é›¢ï¼ˆ0=é«˜å€¤ï¼‰ |
L184       | FCF_W | - | ã‚¦ã‚£ãƒ³ã‚¶ãƒ¼å‡¦ç†å¾Œã®FCF/EV |
L185       | ROE_W | - | ã‚¦ã‚£ãƒ³ã‚¶ãƒ¼å‡¦ç†å¾Œã®ROE |
L186       | FCF | - | FCF/EV |
L187       | QAL | - | FCF_Wã¨ROE_Wã‚’çµ„ã¿åˆã‚ã›ãŸå“è³ªã‚¹ã‚³ã‚¢ |
L188       | CURR_RATIO | - | æµå‹•æ¯”ç‡ |
L189       | DEBT2EQ | - | è² å‚µè³‡æœ¬å€ç‡ |
L190       | EPS_VAR_8Q | - | EPSã®8å››åŠæœŸæ¨™æº–åå·® |
L191       | DIV | - | å¹´ç‡æ›ç®—é…å½“åˆ©å›ã‚Š |
L192       | DIV_STREAK | `div_streak` | é€£ç¶šå¢—é…å¹´æ•° |
L193       | DIV_FCF_COVER | - | é…å½“ã®FCFã‚«ãƒãƒ¬ãƒƒã‚¸ |
L194       | DIV_VAR5 | - | 5å¹´é…å½“å¤‰å‹•ç‡ |
L195       | DIV_TTM_PS | - | 1æ ªå½“ãŸã‚ŠTTMé…å½“ |
L196       | DIV_YOY | - | å‰å¹´æ¯”é…å½“æˆé•·ç‡ |
L197       | REV | - | å£²ä¸Šæˆé•·ç‡TTM |
L198       | EPS_Q_YOY | - | å››åŠæœŸEPSã®å‰å¹´åŒæœŸæ¯” |
L199       | REV_Q_YOY | - | å››åŠæœŸå£²ä¸Šã®å‰å¹´åŒæœŸæ¯” |
L200       | REV_YOY_ACC | - | å£²ä¸Šæˆé•·ç‡ã®åŠ é€Ÿåˆ† |
L201       | RULE40 | - | å£²ä¸Šæˆé•·ç‡ã¨FCFãƒãƒ¼ã‚¸ãƒ³ã®åˆè¨ˆ |
L202       | FCF_MGN | - | FCFãƒãƒ¼ã‚¸ãƒ³ |
L203       | REV_ANN_STREAK | - | å¹´æ¬¡å£²ä¸Šæˆé•·ã®é€£ç¶šå¹´æ•° |
L204       | REV_YOY_VAR | - | å¹´æ¬¡å£²ä¸Šæˆé•·ç‡ã®å¤‰å‹•æ€§ |
L205       | SIZE | - | æ™‚ä¾¡ç·é¡ã®å¯¾æ•°å€¤ |
L206       | LIQ | - | 60æ—¥å¹³å‡å‡ºæ¥é«˜ãƒ‰ãƒ«ã®å¯¾æ•°å€¤ |
L207    - Gãƒã‚±ãƒƒãƒˆ: `GRW`ã€`MOM`ã€`VOL`ã‚’`cfg.weights.g`ï¼ˆ0.40/0.45/-0.15ï¼‰ã§åŠ é‡ã—`g_score`ã‚’å¾—ã‚‹ã€‚
L208    - Dãƒã‚±ãƒƒãƒˆ: `D_QAL`ã€`D_YLD`ã€`D_VOL_RAW`ã€`D_TRD`ã‚’`cfg.weights.d`ï¼ˆ0.15/0.15/-0.45/0.25ï¼‰ã§åŠ é‡ã—`d_score_all`ã‚’ç®—å‡ºã€‚
L209    - ã‚»ã‚¯ã‚¿ãƒ¼capã«ã‚ˆã‚‹`soft_cap_effective_scores`ã‚’é©ç”¨ã—ã€Gæ¡ç”¨éŠ˜æŸ„ã«ã¯ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨ã€‚
L210 5. `_apply_growth_entry_flags`ã§ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆ/æŠ¼ã—ç›®ç™ºç«çŠ¶æ³ã‚’ä»˜åŠ ã—ã€`FeatureBundle`ã‚’è¿”ã™ã€‚
L211
L212 ### Step3: Correlation Reduction & Selection (Selector)
L213 DRRSã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ç›¸é–¢ã‚’æŠ‘ãˆãŸéŠ˜æŸ„é¸å®šã‚’è¡Œã„ã€`SelectionBundle`ã‚’è¿”ã™ã€‚`results/`ã«ä¿å­˜ã•ã‚ŒãŸå‰å›é¸å®šï¼ˆ`G_selection.json` / `D_selection.json`ï¼‰ã‚’`_load_prev`ã§èª­ã¿è¾¼ã¿ã€ç›®çš„å€¤ãŒå¤§ããæ‚ªåŒ–ã—ãªã„é™ã‚Šç¶­æŒã™ã‚‹ã€‚æ–°ã—ã„æ¡ç”¨é›†åˆã¯`_save_sel`ã§JSONã«æ›¸ãå‡ºã—æ¬¡å›ä»¥é™ã®å…¥åŠ›ã«å‚™ãˆã‚‹ã€‚
L214 ä¸»ãªãƒ¡ã‚½ãƒƒãƒ‰:
L215 - `residual_corr` : åç›Šç‡è¡Œåˆ—ã‚’Zã‚¹ã‚³ã‚¢åŒ–ã—ã€ä¸Šä½ä¸»æˆåˆ†ã‚’é™¤å»ã—ãŸæ®‹å·®ã‹ã‚‰ç›¸é–¢è¡Œåˆ—ã‚’æ±‚ã‚ã€å¹³å‡ç›¸é–¢ã«å¿œã˜ã¦ã‚·ãƒ¥ãƒªãƒ³ã‚¯ã€‚
L216 - `rrqr_like_det` : ã‚¹ã‚³ã‚¢ã‚’é‡ã¿ä»˜ã‘ã—ãŸQRåˆ†è§£é¢¨ã®æ‰‹é †ã§åˆæœŸå€™è£œã‚’kä»¶æŠ½å‡ºã—ã€ã‚¹ã‚³ã‚¢ã®é«˜ã„éç›¸é–¢ãªé›†åˆã‚’å¾—ã‚‹ã€‚
L217 - `swap_local_det` / `swap_local_det_cross` : `sum(score) - Î»*within_corr - Î¼*cross_corr`ã‚’ç›®çš„é–¢æ•°ã¨ã—ã¦ã€å…¥ã‚Œæ›¿ãˆæ¢ç´¢ã§å±€æ‰€çš„ã«æœ€é©åŒ–ã€‚
L218 - `select_bucket_drrs` : ãƒ—ãƒ¼ãƒ«éŠ˜æŸ„ã¨ã‚¹ã‚³ã‚¢ã‹ã‚‰æ®‹å·®ç›¸é–¢ã‚’è¨ˆç®—ã—ã€ä¸Šè¨˜2æ®µéš(åˆæœŸé¸æŠâ†’å…¥ã‚Œæ›¿ãˆ)ã§kéŠ˜æŸ„ã‚’æ±ºå®šã€‚éå»æ¡ç”¨éŠ˜æŸ„ã¨ã®æ¯”è¼ƒã§ç›®çš„å€¤ãŒåŠ£åŒ–ã—ãªã‘ã‚Œã°ç¶­æŒã™ã‚‹ã€‚
L219 - `select_buckets` : Gãƒã‚±ãƒƒãƒˆã‚’é¸å®šå¾Œã€ãã®çµæœã‚’é™¤ã„ãŸå€™è£œã‹ã‚‰Dãƒã‚±ãƒƒãƒˆã‚’é¸ã¶ã€‚Dé¸å®šæ™‚ã¯Gã¨ã®ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£Î¼ã‚’ä»˜ä¸ã—ã€ä¸¡ãƒã‚±ãƒƒãƒˆã®åˆ†æ•£ã‚’åˆ¶å¾¡ã™ã‚‹ã€‚
L220
L221 #### ç›¸é–¢ä½æ¸›ãƒ­ã‚¸ãƒƒã‚¯è©³ç´°
L222 1. **æ®‹å·®ç›¸é–¢è¡Œåˆ—ã®æ§‹ç¯‰ (`residual_corr`)**
L223    - ãƒªã‚¿ãƒ¼ãƒ³è¡Œåˆ—`R`ã‚’Zã‚¹ã‚³ã‚¢åŒ–ã€‚
L224    - SVDã§ä¸Šä½`n_pc`ä¸»æˆåˆ†`F`ã‚’æ±‚ã‚ã€æœ€å°äºŒä¹—ã§ä¿‚æ•°`B`ã‚’ç®—å‡ºã—æ®‹å·®`E = Z - F@B`ã‚’å¾—ã‚‹ã€‚
L225    - `E`ã®ç›¸é–¢è¡Œåˆ—`C`ã‚’è¨ˆç®—ã—ã€å¹³å‡çµ¶å¯¾ç›¸é–¢ã«å¿œã˜ã¦ã‚·ãƒ¥ãƒªãƒ³ã‚¯é‡`shrink_eff`ã‚’è£œæ­£ã—ã¦å¯¾è§’ã‚’å¼·èª¿ã€‚
L226 2. **åˆæœŸå€™è£œã®æŠ½å‡º (`rrqr_like_det`)**
L227    - ã‚¹ã‚³ã‚¢ã‚’0-1æ­£è¦åŒ–ã—ãŸé‡ã¿`w`ã¨ã—ã€`Z*(1+Î³w)`ã§åˆ—ãƒãƒ«ãƒ ã‚’å¼·èª¿ã€‚
L228    - æ®‹å·®ãƒãƒ«ãƒ æœ€å¤§ã®åˆ—ã‚’é€æ¬¡é¸ã³ã€QRãƒ©ã‚¤ã‚¯ãªãƒ‡ãƒ•ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã£ã¦éç›¸é–¢ã‹ã¤é«˜ã‚¹ã‚³ã‚¢ãª`k`éŠ˜æŸ„é›†åˆ`S0`ã‚’å¾—ã‚‹ã€‚
L229 3. **å±€æ‰€æ¢ç´¢ (`swap_local_det` / `swap_local_det_cross`)**
L230    - ç›®çš„é–¢æ•°`Î£z_score âˆ’ Î»Â·within_corr âˆ’ Î¼Â·cross_corr`ã‚’æœ€å¤§åŒ–ã€‚
L231    - é¸æŠé›†åˆã®å„éŠ˜æŸ„ã‚’ä»–å€™è£œã¨å…¥ã‚Œæ›¿ãˆã€æ”¹å–„ãŒãªããªã‚‹ã¾ã§ã¾ãŸã¯`max_pass`å›ã¾ã§æ¢ç´¢ã€‚
L232    - `swap_local_det_cross`ã¯Gãƒã‚±ãƒƒãƒˆã¨ã®ã‚¯ãƒ­ã‚¹ç›¸é–¢è¡Œåˆ—`C_cross`ã‚’ä½¿ç”¨ã—ã€ãƒšãƒŠãƒ«ãƒ†ã‚£`Î¼`ã‚’ä»˜ä¸ã€‚
L233 4. **éå»æ¡ç”¨ã®ç¶­æŒã¨ã‚¯ãƒ­ã‚¹ãƒšãƒŠãƒ«ãƒ†ã‚£ (`select_bucket_drrs` / `select_buckets`)**
L234    - å±€æ‰€æ¢ç´¢çµæœ`S`ã¨éå»é›†åˆ`P`ã®ç›®çš„å€¤ã‚’æ¯”è¼ƒã—ã€`S`ãŒ`P`ã‚ˆã‚Š`Î·`æœªæº€ã®æ”¹å–„ãªã‚‰`P`ã‚’ç¶­æŒã€‚
L235    - `select_buckets`ã§ã¯Gã‚’å…ˆã«æ±ºå®šã—ã€Dé¸å®šæ™‚ã«Gã¨ã®ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£`Î¼`ã‚’åŠ ãˆã¦ã‚¯ãƒ­ã‚¹åˆ†æ•£ã‚’æŠ‘åˆ¶ã€‚
L236
L237 ### Step4: Output
L238 é¸å®šçµæœã‚’å¯è¦–åŒ–ã—å…±æœ‰ã™ã‚‹å·¥ç¨‹ã€‚ä»¥ä¸‹ã®å†…å®¹ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«åŒ–ã—ã¦æ¨™æº–å‡ºåŠ›ã¨Slackã¸é€ã‚‹ã€‚
L239 - æ¡ç”¨éŠ˜æŸ„ã¨æƒœã—ãã‚‚é¸å¤–ã¨ãªã£ãŸéŠ˜æŸ„ã®ã‚¹ã‚³ã‚¢ä¸€è¦§
L240 - IN/OUTãƒªã‚¹ãƒˆã¨OUTéŠ˜æŸ„ã®ã‚¹ã‚³ã‚¢ï¼ˆä½å¾—ç‚¹éŠ˜æŸ„ã‚’ç¢ºèªã—ã‚„ã™ãï¼‰
L241 - æ–°æ—§ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®æ¯”è¼ƒè¡¨ï¼ˆçµ„å…¥ã‚Œãƒ»é™¤å¤–ã€ã‚¹ã‚³ã‚¢å¤‰åŒ–ï¼‰
L242 - æ¤œè¨ä¸­éŠ˜æŸ„ã®ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°
L243
L244 ä¸»ãªãƒ¡ã‚½ãƒƒãƒ‰:
L245 - `display_results` : ä¸Šè¨˜ãƒ†ãƒ¼ãƒ–ãƒ«ã«åŠ ãˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã‚„åˆ†æ•£åŒ–æŒ‡æ¨™ã‚’è¡¨ç¤ºã€‚
L246 - `notify_slack` : Slack Webhookã¸åŒå†…å®¹ã‚’é€ä¿¡ã€‚
L247 - è£œåŠ©:`_avg_offdiag`ã€`_resid_avg_rho`ã€`_raw_avg_rho`ã€`_cross_block_raw_rho`ã€‚
L248
L249 ## ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
L250 1. `PipelineConfig`ã‚’æ§‹ç¯‰ã€‚
L251 2. **Step1** `Input.prepare_data`ã§`InputBundle`ã‚’ç”Ÿæˆã€‚
L252 3. **Step2** `Scorer.aggregate_scores`ã§`FeatureBundle`ã‚’å–å¾—ã€‚
L253 4. **Step3** `Selector.select_buckets`ã§`SelectionBundle`ã‚’ç®—å‡ºã€‚
L254 5. **Step4** `Output.display_results`ã¨`notify_slack`ã§çµæœã‚’å‡ºåŠ›ã€‚
```

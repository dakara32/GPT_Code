# === Chat Paste Pack ===
# Repo: dakara32/GPT_Code @ main
# Files: config.py, factor.py, scorer.py, .github/workflows/weekly-report.yml, documents/README.md, documents/factor_design.md
# ä½œæˆæ—¥æ™‚: 2025-09-19 20:47:27 (JST)
# ä½¿ã„æ–¹: ä¸‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’é †ã«è²¼ã‚Œã°ã“ã®ãƒãƒ£ãƒƒãƒˆã§å…¨ä½“æŠŠæ¡ã§ãã¾ã™ã€‚
# æ³¨è¨˜: å„ãƒ•ã‚¡ã‚¤ãƒ«ã¯å€‹åˆ¥ã« L1.. ã§è¡Œç•ªå·ä»˜ä¸ã€‚
---

## <config.py>
```text
L1 # å…±é€šè¨­å®šï¼ˆfactor / drift ã‹ã‚‰å‚ç…§ï¼‰
L2 from dataclasses import dataclass
L3
L4 TOTAL_TARGETS = 20
L5
L6 # åŸºæº–ã®ãƒã‚±ãƒƒãƒˆæ•°ï¼ˆNORMALï¼‰
L7 COUNTS_BASE = {"G": 12, "D": 8}
L8
L9 # ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®æ¨å¥¨ãƒã‚±ãƒƒãƒˆæ•°
L10 COUNTS_BY_MODE = {
L11     "NORMAL": {"G": 12, "D": 8},
L12     "CAUTION": {"G": 10, "D": 8},
L13     "EMERG": {"G": 8,  "D": 8},
L14 }
L15
L16 # ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®ãƒ‰ãƒªãƒ•ãƒˆé–¾å€¤ï¼ˆ%ï¼‰
L17 DRIFT_THRESHOLD_BY_MODE = {"NORMAL": 12, "CAUTION": 14, "EMERG": float("inf")}
L18
L19 # ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®TSï¼ˆåŸºæœ¬å¹…, å°æ•°=å‰²åˆï¼‰
L20 TS_BASE_BY_MODE = {"NORMAL": 0.15, "CAUTION": 0.13, "EMERG": 0.10}
L21 # åˆ©ç›Šåˆ°é”(+30/+60/+100%)æ™‚ã®æ®µéšã‚¿ã‚¤ãƒˆåŒ–ï¼ˆãƒã‚¤ãƒ³ãƒˆå·®ï¼‰
L22 TS_STEP_DELTAS_PT = (3, 6, 8)
L23
L24 # Breadthã®æ ¡æ­£ã¯ N_G ã«é€£å‹•ï¼ˆç·Šæ€¥è§£é™¤=ceil(1.5*N_G), é€šå¸¸å¾©å¸°=3*N_Gï¼‰
L25 N_G = COUNTS_BASE["G"]
L26 N_D = COUNTS_BASE["D"]
L27
```

## <factor.py>
```text
L1 '''ROLE: Orchestration ONLYï¼ˆå¤–éƒ¨I/Oãƒ»SSOTãƒ»Slackå‡ºåŠ›ï¼‰, è¨ˆç®—ã¯ scorer.py'''
L2 # === NOTE: æ©Ÿèƒ½ãƒ»å…¥å‡ºåŠ›ãƒ»ãƒ­ã‚°æ–‡è¨€ãƒ»ä¾‹å¤–æŒ™å‹•ã¯ä¸å¤‰ã€‚å®‰å…¨ãªçŸ­ç¸®ï¼ˆimportçµ±åˆ/è¤‡æ•°ä»£å…¥/å†…åŒ…è¡¨è¨˜/ãƒ¡ã‚½ãƒƒãƒ‰ãƒã‚§ãƒ¼ãƒ³/ä¸€è¡ŒåŒ–/ç©ºè¡Œåœ§ç¸®ãªã©ï¼‰ã®ã¿é©ç”¨ ===
L3 BONUS_COEFF = 0.55  # æ¨å¥¨: æ”»ã‚=0.45 / ä¸­åº¸=0.55 / å®ˆã‚Š=0.65
L4 SWAP_DELTA_Z = 0.15   # åƒ…å·®åˆ¤å®š: Ïƒã®15%ã€‚(ç·©ã‚=0.10 / æ¨™æº–=0.15 / å›ºã‚=0.20)
L5 SWAP_KEEP_BUFFER = 3  # n_target+ã“ã®é †ä½ä»¥å†…ã®ç¾è¡Œã¯ä¿æŒã€‚(ç²˜ã‚Šå¼±=2 / æ¨™æº–=3 / ç²˜ã‚Šå¼·=4ã€œ5)
L6 import os, time, requests
L7 import logging
L8 from time import perf_counter
L9 from dataclasses import dataclass
L10 from typing import Any, Dict, List, Tuple
L11 from concurrent.futures import ThreadPoolExecutor
L12 import numpy as np
L13 import pandas as pd
L14 import yfinance as yf
L15 from scipy.stats import zscore  # used via scorer
L16 from scorer import Scorer, ttm_div_yield_portfolio, _log
L17 import config
L18
L19 # ãã®ä»–
L20 debug_mode, FINNHUB_API_KEY = True, os.environ.get("FINNHUB_API_KEY")
L21
L22 logger = logging.getLogger(__name__)
L23 if debug_mode:
L24     logging.basicConfig(level=logging.INFO, force=True)
L25 else:
L26     logging.basicConfig(level=logging.WARNING, force=True)
L27
L28 class T:
L29     t = perf_counter()
L30     log = staticmethod(lambda tag: (lambda now=perf_counter(): (print(f"[T] {tag}: {now - T.t:.2f}s"), setattr(T, "t", now))[-1])())
L31
L32 T.log("start")
L33
L34 # === ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã¨å®šæ•°ï¼ˆå†’é ­ã«å›ºå®šï¼‰ ===
L35 exist, cand = [pd.read_csv(f, header=None)[0].tolist() for f in ("current_tickers.csv","candidate_tickers.csv")]
L36 T.log(f"csv loaded: exist={len(exist)} cand={len(cand)}")
L37 CAND_PRICE_MAX, bench = 450, '^GSPC'  # ä¾¡æ ¼ä¸Šé™ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
L38 N_G, N_D = config.N_G, config.N_D  # G/Dæ ã‚µã‚¤ã‚ºï¼ˆNORMALåŸºæº–: G12/D8ï¼‰
L39 g_weights = {'GROWTH_F':0.35,'MOM':0.55,'VOL':-0.10}
L40 D_BETA_MAX = float(os.environ.get("D_BETA_MAX", "0.8"))
L41 FILTER_SPEC = {"G":{"pre_mask":["trend_template"]},"D":{"pre_filter":{"beta_max":D_BETA_MAX}}}
L42 D_weights = {'QAL':0.1,'YLD':0.3,'VOL':-0.5,'TRD':0.1}
L43 _fmt_w = lambda w: " ".join(f"{k}{int(v*100)}" for k, v in w.items())
L44
L45 # DRRS åˆæœŸãƒ—ãƒ¼ãƒ«ãƒ»å„ç¨®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
L46 corrM = 45
L47 DRRS_G, DRRS_D = dict(lookback=252,n_pc=3,gamma=1.2,lam=0.68,eta=0.8), dict(lookback=504,n_pc=4,gamma=0.8,lam=0.85,eta=0.5)
L48 DRRS_SHRINK = 0.10  # æ®‹å·®ç›¸é–¢ã®å¯¾è§’ã‚·ãƒ¥ãƒªãƒ³ã‚¯ï¼ˆåŸºç¤ï¼‰
L49
L50 # ã‚¯ãƒ­ã‚¹ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆæœªå®šç¾©ãªã‚‰è¨­å®šï¼‰
L51 try: CROSS_MU_GD
L52 except NameError: CROSS_MU_GD = 0.40  # æ¨å¥¨ 0.35â€“0.45ï¼ˆlam=0.85æƒ³å®šï¼‰
L53
L54 # å‡ºåŠ›é–¢é€£
L55 RESULTS_DIR = "results"
L56 os.makedirs(RESULTS_DIR, exist_ok=True)
L57
L58 # === å…±æœ‰DTOï¼ˆã‚¯ãƒ©ã‚¹é–“I/Oå¥‘ç´„ï¼‰ï¼‹ Config ===
L59 @dataclass(frozen=True)
L60 class InputBundle:
L61     # Input â†’ Scorer ã§å—ã‘æ¸¡ã™ç´ æï¼ˆI/Oç¦æ­¢ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ï¼‰
L62     cand: List[str]
L63     tickers: List[str]
L64     bench: str
L65     data: pd.DataFrame              # yfinance downloadçµæœï¼ˆ'Close','Volume'ç­‰ã®éšå±¤åˆ—ï¼‰
L66     px: pd.DataFrame                # data['Close']
L67     spx: pd.Series                  # data['Close'][bench]
L68     tickers_bulk: object            # yfinance.Tickers
L69     info: Dict[str, dict]           # yfinance info per ticker
L70     eps_df: pd.DataFrame            # ['eps_ttm','eps_q_recent',...]
L71     fcf_df: pd.DataFrame            # ['fcf_ttm', ...]
L72     returns: pd.DataFrame           # px[tickers].pct_change()
L73
L74 @dataclass(frozen=True)
L75 class FeatureBundle:
L76     df: pd.DataFrame
L77     df_z: pd.DataFrame
L78     g_score: pd.Series
L79     d_score_all: pd.Series
L80     missing_logs: pd.DataFrame
L81     df_full: pd.DataFrame | None = None
L82     df_full_z: pd.DataFrame | None = None
L83     scaler: Any | None = None
L84
L85 @dataclass(frozen=True)
L86 class SelectionBundle:
L87     resG: dict
L88     resD: dict
L89     top_G: List[str]
L90     top_D: List[str]
L91     init_G: List[str]
L92     init_D: List[str]
L93
L94 @dataclass(frozen=True)
L95 class WeightsConfig:
L96     g: Dict[str,float]
L97     d: Dict[str,float]
L98
L99 @dataclass(frozen=True)
L100 class DRRSParams:
L101     corrM: int
L102     shrink: float
L103     G: Dict[str,float]   # lookback, n_pc, gamma, lam, eta
L104     D: Dict[str,float]
L105     cross_mu_gd: float
L106
L107 @dataclass(frozen=True)
L108 class PipelineConfig:
L109     weights: WeightsConfig
L110     drrs: DRRSParams
L111     price_max: float
L112     debug_mode: bool = False
L113
L114 # === å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆè¤‡æ•°ã‚¯ãƒ©ã‚¹ã§ä½¿ç”¨ï¼‰ ===
L115 # (unused local utils removed â€“ use scorer.py versions if needed)
L116
L117 _env_true = lambda name, default=False: (os.getenv(name) or str(default)).strip().lower() == "true"
L118
L119 def _post_slack(payload: dict):
L120     url = os.getenv("SLACK_WEBHOOK_URL")
L121     if not url: print("âš ï¸ SLACK_WEBHOOK_URL æœªè¨­å®š"); return
L122     try:
L123         requests.post(url, json=payload).raise_for_status()
L124     except Exception as e:
L125         print(f"âš ï¸ Slacké€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
L126
L127 def _slack_send_text_chunks(url: str, text: str, chunk: int = 2800) -> None:
L128     """Slackã¸ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²é€ä¿¡ï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å½¢å¼ï¼‰ã€‚"""
L129
L130     def _post_text(payload: str) -> None:
L131         try:
L132             resp = requests.post(url, json={"text": payload})
L133             print(f"[DBG] debug_post status={getattr(resp, 'status_code', None)} size={len(payload)}")
L134             if resp is not None:
L135                 resp.raise_for_status()
L136         except Exception as e:
L137             print(f"[ERR] debug_post_failed: {e}")
L138
L139     body = str(text or "").strip()
L140     if not body:
L141         print("[DBG] skip debug send: empty body")
L142         return
L143
L144     lines = body.splitlines()
L145     block: list[str] = []
L146     block_len = 0
L147
L148     def _flush() -> None:
L149         nonlocal block, block_len
L150         if not block:
L151             return
L152         payload = "```" + "\n".join(block) + "```"
L153         _post_text(payload)
L154         block, block_len = [], 0
L155
L156     for raw in lines:
L157         line = raw or ""
L158         while len(line) > chunk:
L159             head, line = line[:chunk], line[chunk:]
L160             _flush()
L161             _post_text("```" + head + "```")
L162         add_len = len(line) if not block else len(line) + 1
L163         if block and block_len + add_len > chunk:
L164             _flush()
L165             add_len = len(line)
L166         block.append(line)
L167         block_len += add_len
L168     _flush()
L169
L170 def _disjoint_keepG(top_G, top_D, poolD):
L171     """Gé‡è¤‡ã‚’Dã‹ã‚‰é™¤å»ã—ã€poolDã§é †æ¬¡è£œå……ï¼ˆæ¯æ¸‡æ™‚ã¯å…ƒéŠ˜æŸ„ç¶­æŒï¼‰ã€‚"""
L172     used, D, i = set(top_G), list(top_D), 0
L173     for j, t in enumerate(D):
L174         if t in used:
L175             while i < len(poolD) and (poolD[i] in used or poolD[i] in D):
L176                 i += 1
L177             if i < len(poolD):
L178                 D[j] = poolD[i]; used.add(D[j]); i += 1
L179     return top_G, D
L180
L181
L182 def _sticky_keep_current(agg: pd.Series, pick: list[str], incumbents: list[str],
L183                          n_target: int, delta_z: float, keep_buffer: int) -> list[str]:
L184     import pandas as pd, numpy as np
L185     sel = list(pick)
L186     if not sel: return sel
L187     ranked_sel = agg.reindex(sel).sort_values(ascending=False)
L188     kth = ranked_sel.iloc[min(len(sel), n_target)-1]
L189     sigma = float(agg.std()) if pd.notna(agg.std()) else 0.0
L190     thresh = kth - delta_z * sigma
L191     ranked_all = agg.sort_values(ascending=False)
L192     cand = [t for t in incumbents if (t not in sel) and (t in agg.index)]
L193     for t in cand:
L194         within_score = (pd.notna(agg[t]) and agg[t] >= thresh)
L195         within_rank  = (t in ranked_all.index) and (ranked_all.index.get_loc(t) < n_target + keep_buffer)
L196         if within_score or within_rank:
L197             non_inc = [x for x in sel if x not in incumbents]
L198             if not non_inc: break
L199             weakest = min(non_inc, key=lambda x: agg.get(x, -np.inf))
L200             if weakest in sel and agg.get(t, -np.inf) >= agg.get(weakest, -np.inf):
L201                 sel.remove(weakest); sel.append(t)
L202     if len(sel) > n_target:
L203         sel = sorted(sel, key=lambda x: agg.get(x, -1e9), reverse=True)[:n_target]
L204     return sel
L205
L206
L207 # === Inputï¼šå¤–éƒ¨I/Oã¨å‰å‡¦ç†ï¼ˆCSV/APIãƒ»æ¬ æè£œå®Œï¼‰ ===
L208 class Input:
L209     def __init__(self, cand, exist, bench, price_max, finnhub_api_key=None):
L210         self.cand, self.exist, self.bench, self.price_max = cand, exist, bench, price_max
L211         self.api_key = finnhub_api_key or os.environ.get("FINNHUB_API_KEY")
L212
L213     # ---- ï¼ˆInputå°‚ç”¨ï¼‰EPSè£œå®Œãƒ»FCFç®—å‡ºç³» ----
L214     @staticmethod
L215     def _sec_headers():
L216         mail = (os.getenv("SEC_CONTACT_EMAIL") or "yasonba55@gmail.com").strip()
L217         app = (os.getenv("SEC_APP_NAME") or "FactorBot/1.0").strip()
L218         return {
L219             "User-Agent": f"{app} ({mail})",
L220             "From": mail,
L221             "Accept": "application/json",
L222         }
L223
L224     @staticmethod
L225     def _sec_get(url: str, retries: int = 3, backoff: float = 0.5):
L226         for i in range(retries):
L227             r = requests.get(url, headers=Input._sec_headers(), timeout=20)
L228             if r.status_code in (429, 503, 403):
L229                 time.sleep(min(2 ** i * backoff, 8.0))
L230                 continue
L231             r.raise_for_status()
L232             return r.json()
L233         r.raise_for_status()
L234
L235     @staticmethod
L236     def _sec_ticker_map():
L237         j = Input._sec_get("https://data.sec.gov/api/xbrl/company_tickers.json")
L238         mp = {}
L239         for _, v in (j or {}).items():
L240             try:
L241                 mp[str(v["ticker"]).upper()] = f"{int(v['cik_str']):010d}"
L242             except Exception:
L243                 pass
L244         return mp
L245
L246     # --- è¿½åŠ : ADR/OTCå‘ã‘ã®ç°¡æ˜“æ­£è¦åŒ–ï¼ˆæœ«å°¾Y/F, ãƒ‰ãƒƒãƒˆç­‰ï¼‰ ---
L247     @staticmethod
L248     def _normalize_ticker(sym: str) -> list[str]:
L249         s = (sym or "").upper().strip()
L250         # è¿½åŠ : å…ˆé ­ã®$ã‚„å…¨è§’ã®è¨˜å·ã‚’é™¤å»
L251         s = s.lstrip("$").replace("ï¼„", "").replace("ï¼", ".").replace("ï¼", "-")
L252         cand: list[str] = []
L253
L254         def add(x: str) -> None:
L255             if x and x not in cand:
L256                 cand.append(x)
L257
L258         # 1) åŸæ–‡ã‚’æœ€å„ªå…ˆï¼ˆSECã¯ BRK.B, BF.B ãªã© . ã‚’æ­£å¼æ¡ç”¨ï¼‰
L259         add(s)
L260         # 2) Yahooç³»ãƒãƒªã‚¢ãƒ³ãƒˆï¼ˆ. ã¨ - ã®æºã‚Œã‚’ç›¸äº’ã«ï¼‰
L261         if "." in s:
L262             add(s.replace(".", "-"))
L263             add(s.replace(".", ""))
L264         if "-" in s:
L265             add(s.replace("-", "."))
L266             add(s.replace("-", ""))
L267         # 3) ãƒ‰ãƒƒãƒˆãƒ»ãƒã‚¤ãƒ•ãƒ³ãƒ»ãƒ”ãƒªã‚ªãƒ‰ç„¡ã—ç‰ˆï¼ˆæœ€å¾Œã®ä¿é™ºï¼‰
L268         add(s.replace("-", "").replace(".", ""))
L269         # 4) ADRç°¡æ˜“ï¼šæœ«å°¾Y/Fã®é™¤å»ï¼ˆSECãƒãƒƒãƒ—ã¯æœ¬ä½“ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚’æŒã¤ã“ã¨ãŒã‚ã‚‹ï¼‰
L270         if len(s) >= 2 and s[-1] in {"Y", "F"}:
L271             add(s[:-1])
L272         return cand
L273
L274     @staticmethod
L275     def _sec_companyfacts(cik: str):
L276         return Input._sec_get(f"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json")
L277
L278     @staticmethod
L279     def _units_for_tags(facts: dict, namespaces: list[str], tags: list[str]) -> list[dict]:
L280         """facts ã‹ã‚‰ namespace/tag ã‚’æ¨ªæ–­ã—ã¦ units é…åˆ—ã‚’åé›†ï¼ˆå­˜åœ¨é †ã«é€£çµï¼‰ã€‚"""
L281         out: list[dict] = []
L282         facts = facts or {}
L283         for ns in namespaces:
L284             try:
L285                 node = facts.get("facts", {}).get(ns, {})
L286             except Exception:
L287                 node = {}
L288             for tg in tags:
L289                 try:
L290                     units = node[tg]["units"]
L291                 except Exception:
L292                     continue
L293                 picks: list[dict] = []
L294                 if "USD/shares" in units:
L295                     picks.extend(list(units["USD/shares"]))
L296                 if "USD" in units:
L297                     picks.extend(list(units["USD"]))
L298                 if not picks:
L299                     for arr in units.values():
L300                         picks.extend(list(arr))
L301                 out.extend(picks)
L302         return out
L303
L304     @staticmethod
L305     def _only_quarterly(arr: list[dict]) -> list[dict]:
L306         """companyfactsã®æ··åœ¨é…åˆ—ã‹ã‚‰ã€å››åŠæœŸã€ã ã‘ã‚’æŠ½å‡ºã€‚
L307
L308         - frame ã« "Q" ã‚’å«ã‚€ï¼ˆä¾‹: CY2024Q2Iï¼‰
L309         - fp ãŒ Q1/Q2/Q3/Q4
L310         - form ãŒ 10-Q/10-Q/A/6-K
L311         """
L312         if not arr:
L313             return []
L314         q_forms = {"10-Q", "10-Q/A", "6-K"}
L315
L316         def is_q(x: dict) -> bool:
L317             frame = (x.get("frame") or "").upper()
L318             fp = (x.get("fp") or "").upper()
L319             form = (x.get("form") or "").upper()
L320             return ("Q" in frame) or (fp in {"Q1", "Q2", "Q3", "Q4"}) or (form in q_forms)
L321
L322         out = [x for x in arr if is_q(x)]
L323         out.sort(key=lambda x: (x.get("end") or ""), reverse=True)
L324         return out
L325
L326     @staticmethod
L327     def _series_from_facts_with_dates(arr, key_val="val", key_dt="end", normalize=float):
L328         """companyfactsã‚¢ã‚¤ãƒ†ãƒ é…åˆ—ã‹ã‚‰ (date,value) ã‚’è¿”ã™ã€‚dateã¯YYYY-MM-DDã‚’æƒ³å®šã€‚"""
L329         out: List[Tuple[str, float]] = []
L330         for x in (arr or []):
L331             try:
L332                 v = x.get(key_val)
L333                 d = x.get(key_dt)
L334                 if d is None:
L335                     continue
L336                 out.append((str(d), normalize(v) if v is not None else float("nan")))
L337             except Exception:
L338                 continue
L339         # end(=æ—¥ä»˜)ã®é™é †ã«ã‚½ãƒ¼ãƒˆï¼ˆæœ€æ–°â†’å¤ã„ï¼‰
L340         out.sort(key=lambda t: t[0], reverse=True)
L341         return out
L342
L343     def fetch_eps_rev_from_sec(self, tickers: list[str]) -> dict:
L344         out = {}
L345         t2cik = self._sec_ticker_map()
L346         n_map = n_rev = n_eps = 0
L347         miss_map: list[str] = []
L348         miss_facts: list[str] = []
L349         for t in tickers:
L350             candidates: list[str] = []
L351
L352             def add(key: str) -> None:
L353                 if key and key not in candidates:
L354                     candidates.append(key)
L355
L356             add((t or "").upper())
L357             for key in self._normalize_ticker(t):
L358                 add(key)
L359
L360             cik = None
L361             for key in candidates:
L362                 cik = t2cik.get(key)
L363                 if cik:
L364                     break
L365             if not cik:
L366                 out[t] = {}
L367                 miss_map.append(t)
L368                 continue
L369             try:
L370                 j = self._sec_companyfacts(cik)
L371                 facts = j or {}
L372                 rev_tags = [
L373                     "Revenues",
L374                     "RevenueFromContractWithCustomerExcludingAssessedTax",
L375                     "SalesRevenueNet",
L376                     "SalesRevenueGoodsNet",
L377                     "SalesRevenueServicesNet",
L378                     "Revenue",
L379                 ]
L380                 eps_tags = [
L381                     "EarningsPerShareDiluted",
L382                     "EarningsPerShareBasicAndDiluted",
L383                     "EarningsPerShare",
L384                     "EarningsPerShareBasic",
L385                 ]
L386                 rev_arr = self._units_for_tags(facts, ["us-gaap", "ifrs-full"], rev_tags)
L387                 eps_arr = self._units_for_tags(facts, ["us-gaap", "ifrs-full"], eps_tags)
L388                 rev_q_items = self._only_quarterly(rev_arr)
L389                 eps_q_items = self._only_quarterly(eps_arr)
L390                 # (date,value) ã§å–å¾—
L391                 rev_pairs = self._series_from_facts_with_dates(rev_q_items)
L392                 eps_pairs = self._series_from_facts_with_dates(eps_q_items)
L393                 rev_vals = [v for (_d, v) in rev_pairs]
L394                 eps_vals = [v for (_d, v) in eps_pairs]
L395                 rev_q = float(rev_vals[0]) if rev_vals else float("nan")
L396                 eps_q = float(eps_vals[0]) if eps_vals else float("nan")
L397                 rev_ttm = float(sum([v for v in rev_vals[:4] if v == v])) if rev_vals else float("nan")
L398                 eps_ttm = float(sum([v for v in eps_vals[:4] if v == v])) if eps_vals else float("nan")
L399                 out[t] = {
L400                     "eps_q_recent": eps_q,
L401                     "eps_ttm": eps_ttm,
L402                     "rev_q_recent": rev_q,
L403                     "rev_ttm": rev_ttm,
L404                     # å¾Œæ®µã§DatetimeIndexåŒ–ã§ãã‚‹ã‚ˆã† (date,value) ã‚’ä¿æŒã€‚å€¤ã ã‘ã®äº’æ›ã‚­ãƒ¼ã‚‚æ®‹ã™ã€‚
L405                     "eps_q_series_pairs": eps_pairs[:16],
L406                     "rev_q_series_pairs": rev_pairs[:16],
L407                     "eps_q_series": eps_vals[:16],
L408                     "rev_q_series": rev_vals[:16],
L409                 }
L410                 n_map += 1
L411                 if rev_vals:
L412                     n_rev += 1
L413                 if eps_vals:
L414                     n_eps += 1
L415             except Exception:
L416                 out[t] = {}
L417                 miss_facts.append(t)
L418             time.sleep(0.30)
L419         # å–å¾—ã‚µãƒãƒªã‚’ãƒ­ã‚°ï¼ˆActionsã§ç¢ºèªã—ã‚„ã™ã„ã‚ˆã† printï¼‰
L420         try:
L421             total = len(tickers)
L422             print(f"[SEC] map={n_map}/{total}  rev_q_hit={n_rev}  eps_q_hit={n_eps}")
L423             # ãƒ‡ãƒãƒƒã‚°: å–å¾—æœ¬æ•°ã®åˆ†å¸ƒï¼ˆå…ˆé ­ã®ã¿ï¼‰
L424             try:
L425                 lens = [len((out.get(t, {}) or {}).get("rev_q_series", [])) for t in tickers]
L426                 print(f"[SEC] rev_q_series length: min={min(lens) if lens else 0} "
L427                       f"p25={np.percentile(lens,25) if lens else 0} median={np.median(lens) if lens else 0} "
L428                       f"p75={np.percentile(lens,75) if lens else 0} max={max(lens) if lens else 0}")
L429             except Exception:
L430                 pass
L431             if miss_map:
L432                 print(f"[SEC] no CIK map: {len(miss_map)} (ã‚µãƒ³ãƒ—ãƒ«ä¾‹) {miss_map[:20]}")
L433             if miss_facts:
L434                 print(f"[SEC] CIKã‚ã‚Š ã ãŒå¯¾è±¡factãªã—: {len(miss_facts)} (ã‚µãƒ³ãƒ—ãƒ«ä¾‹) {miss_facts[:20]}")
L435         except Exception:
L436             pass
L437         return out
L438
L439     def sec_dryrun_sample(self, tickers: list[str] | None = None) -> None:
L440         if not _env_true("SEC_DRYRUN_SAMPLE", False):
L441             return
L442         sample = tickers or ["BRK.B", "BF.B", "GOOGL", "META", "UBER", "PBR.A", "TSM", "NARI", "EVBN", "SWAV"]
L443         print(f"[SEC-DRYRUN] sample tickers: {sample}")
L444         try:
L445             t2cik = self._sec_ticker_map()
L446             hits = 0
L447             for sym in sample:
L448                 candidates: list[str] = []
L449
L450                 def add(key: str) -> None:
L451                     if key and key not in candidates:
L452                         candidates.append(key)
L453
L454                 add((sym or "").upper())
L455                 for alt in self._normalize_ticker(sym):
L456                     add(alt)
L457                 if any(t2cik.get(key) for key in candidates):
L458                     hits += 1
L459             sec_data = self.fetch_eps_rev_from_sec(sample)
L460             rev_hits = sum(1 for v in sec_data.values() if v.get("rev_q_series"))
L461             eps_hits = sum(1 for v in sec_data.values() if v.get("eps_q_series"))
L462             total = len(sample)
L463             print(f"[SEC-DRYRUN] CIK map hit: {hits}/{total}  rev_q_series hits: {rev_hits}  eps_q_series hits: {eps_hits}")
L464         except Exception as e:
L465             print(f"[SEC-DRYRUN] error: {e}")
L466     @staticmethod
L467     def impute_eps_ttm(df: pd.DataFrame, ttm_col: str="eps_ttm", q_col: str="eps_q_recent", out_col: str|None=None) -> pd.DataFrame:
L468         out_col = out_col or ttm_col; df = df.copy(); df["eps_imputed"] = False
L469         cand = df[q_col]*4; ok = df[ttm_col].isna() & cand.replace([np.inf,-np.inf], np.nan).notna()
L470         df.loc[ok, out_col], df.loc[ok,"eps_imputed"] = cand[ok], True; return df
L471
L472     _CF_ALIASES = {"cfo":["Operating Cash Flow","Total Cash From Operating Activities"], "capex":["Capital Expenditure","Capital Expenditures"]}
L473
L474     @staticmethod
L475     def _pick_row(df: pd.DataFrame, names: list[str]) -> pd.Series|None:
L476         if df is None or df.empty: return None
L477         idx_lower={str(i).lower():i for i in df.index}
L478         for n in names:
L479             k=n.lower()
L480             if k in idx_lower: return df.loc[idx_lower[k]]
L481         return None
L482
L483     @staticmethod
L484     def _sum_last_n(s: pd.Series|None, n: int) -> float|None:
L485         if s is None or s.empty: return None
L486         v=s.dropna().astype(float); return None if v.empty else v.iloc[:n].sum()
L487
L488     @staticmethod
L489     def _latest(s: pd.Series|None) -> float|None:
L490         if s is None or s.empty: return None
L491         v=s.dropna().astype(float); return v.iloc[0] if not v.empty else None
L492
L493     def fetch_cfo_capex_ttm_yf(self, tickers: list[str]) -> pd.DataFrame:
L494         from concurrent.futures import ThreadPoolExecutor, as_completed
L495         pick, sumn, latest, aliases = self._pick_row, self._sum_last_n, self._latest, self._CF_ALIASES
L496
L497         def one(t: str):
L498             try:
L499                 tk = yf.Ticker(t)  # â˜… ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¯æ¸¡ã•ãªã„ï¼ˆYFãŒcurl_cffiã§ç®¡ç†ï¼‰
L500                 qcf = tk.quarterly_cashflow
L501                 cfo_q, capex_q = pick(qcf, aliases["cfo"]), pick(qcf, aliases["capex"])
L502                 fcf_q = pick(qcf, ["Free Cash Flow","FreeCashFlow","Free cash flow"])
L503                 cfo, capex, fcf = sumn(cfo_q,4), sumn(capex_q,4), sumn(fcf_q,4)
L504                 if any(v is None for v in (cfo, capex, fcf)):
L505                     acf = tk.cashflow
L506                     if cfo   is None: cfo   = latest(pick(acf, aliases["cfo"]))
L507                     if capex is None: capex = latest(pick(acf, aliases["capex"]))
L508                     if fcf   is None: fcf   = latest(pick(acf, ["Free Cash Flow","FreeCashFlow","Free cash flow"]))
L509             except Exception as e:
L510                 print(f"[warn] yf financials error: {t}: {e}"); cfo=capex=fcf=None
L511             n=np.nan
L512             return {"ticker":t,
L513                     "cfo_ttm_yf":   n if cfo   is None else cfo,
L514                     "capex_ttm_yf": n if capex is None else capex,
L515                     "fcf_ttm_yf_direct": n if fcf is None else fcf}
L516
L517         rows, mw = [], int(os.getenv("FIN_THREADS","8"))
L518         with ThreadPoolExecutor(max_workers=mw) as ex:
L519             rows=[f.result() for f in as_completed(ex.submit(one,t) for t in tickers)]
L520         return pd.DataFrame(rows).set_index("ticker")
L521
L522     _FINN_CFO_KEYS = ["netCashProvidedByOperatingActivities","netCashFromOperatingActivities","cashFlowFromOperatingActivities","operatingCashFlow"]
L523     _FINN_CAPEX_KEYS = ["capitalExpenditure","capitalExpenditures","purchaseOfPPE","investmentsInPropertyPlantAndEquipment"]
L524
L525     @staticmethod
L526     def _first_key(d: dict, keys: list[str]):
L527         for k in keys:
L528             if k in d and d[k] is not None: return d[k]
L529         return None
L530
L531     @staticmethod
L532     def _finn_get(session: requests.Session, url: str, params: dict, retries: int=3, sleep_s: float=0.5):
L533         for i in range(retries):
L534             r = session.get(url, params=params, timeout=15)
L535             if r.status_code==429: time.sleep(min(2**i*sleep_s,4.0)); continue
L536             r.raise_for_status(); return r.json()
L537         r.raise_for_status()
L538
L539     def fetch_cfo_capex_ttm_finnhub(self, tickers: list[str], api_key: str|None=None) -> pd.DataFrame:
L540         api_key = api_key or os.getenv("FINNHUB_API_KEY")
L541         if not api_key: raise ValueError("Finnhub API key not provided. Set FINNHUB_API_KEY or pass api_key=")
L542         base, s, rows = "https://finnhub.io/api/v1", requests.Session(), []
L543         for sym in tickers:
L544             cfo_ttm = capex_ttm = None
L545             try:
L546                 j = self._finn_get(s, f"{base}/stock/cash-flow", {"symbol":sym,"frequency":"quarterly","limit":8,"token":api_key})
L547                 arr = j.get("cashFlow") or []; cfo_vals, capex_vals = [], []
L548                 for item in arr[:4]:
L549                     cfo_vals.append(self._first_key(item,self._FINN_CFO_KEYS)); capex_vals.append(self._first_key(item,self._FINN_CAPEX_KEYS))
L550                 if any(v is not None for v in cfo_vals): cfo_ttm = float(np.nansum([np.nan if v is None else float(v) for v in cfo_vals]))
L551                 if any(v is not None for v in capex_vals): capex_ttm = float(np.nansum([np.nan if v is None else float(v) for v in capex_vals]))
L552             except Exception: pass
L553             if cfo_ttm is None or capex_ttm is None:
L554                 try:
L555                     j = self._finn_get(s, f"{base}/stock/cash-flow", {"symbol":sym,"frequency":"annual","limit":1,"token":api_key})
L556                     arr = j.get("cashFlow") or []
L557                     if arr:
L558                         item0 = arr[0]
L559                         if cfo_ttm is None:
L560                             v = self._first_key(item0,self._FINN_CFO_KEYS)
L561                             if v is not None: cfo_ttm = float(v)
L562                         if capex_ttm is None:
L563                             v = self._first_key(item0,self._FINN_CAPEX_KEYS)
L564                             if v is not None: capex_ttm = float(v)
L565                 except Exception: pass
L566             rows.append({"ticker":sym,"cfo_ttm_fh":np.nan if cfo_ttm is None else cfo_ttm,"capex_ttm_fh":np.nan if capex_ttm is None else capex_ttm})
L567         return pd.DataFrame(rows).set_index("ticker")
L568
L569     def compute_fcf_with_fallback(self, tickers: list[str], finnhub_api_key: str|None=None) -> pd.DataFrame:
L570         yf_df = self.fetch_cfo_capex_ttm_yf(tickers)
L571         T.log("financials (yf) done")
L572         miss_mask = yf_df[["cfo_ttm_yf","capex_ttm_yf","fcf_ttm_yf_direct"]].isna().any(axis=1)
L573         need = yf_df.index[miss_mask].tolist(); print(f"[T] yf financials missing: {len(need)} {need[:10]}{'...' if len(need)>10 else ''}")
L574         if need:
L575             fh_df = self.fetch_cfo_capex_ttm_finnhub(need, api_key=finnhub_api_key)
L576             df = yf_df.join(fh_df, how="left")
L577             for col_yf, col_fh in [("cfo_ttm_yf","cfo_ttm_fh"),("capex_ttm_yf","capex_ttm_fh")]:
L578                 df[col_yf] = df[col_yf].fillna(df[col_fh])
L579             print("[T] financials (finnhub) done (fallback only)")
L580         else:
L581             df = yf_df.assign(cfo_ttm_fh=np.nan, capex_ttm_fh=np.nan)
L582             print("[T] financials (finnhub) skipped (no missing)")
L583         df["cfo_ttm"]  = df["cfo_ttm_yf"].where(df["cfo_ttm_yf"].notna(), df["cfo_ttm_fh"])
L584         df["capex_ttm"] = df["capex_ttm_yf"].where(df["capex_ttm_yf"].notna(), df["capex_ttm_fh"])
L585         cfo, capex = pd.to_numeric(df["cfo_ttm"], errors="coerce"), pd.to_numeric(df["capex_ttm"], errors="coerce").abs()
L586         fcf_calc = cfo - capex
L587         fcf_direct = pd.to_numeric(df.get("fcf_ttm_yf_direct"), errors="coerce")
L588         df["fcf_ttm"] = fcf_calc.where(fcf_calc.notna(), fcf_direct)
L589         df["cfo_source"]  = np.where(df["cfo_ttm_yf"].notna(),"yfinance",np.where(df["cfo_ttm_fh"].notna(),"finnhub",""))
L590         df["capex_source"] = np.where(df["capex_ttm_yf"].notna(),"yfinance",np.where(df["capex_ttm_fh"].notna(),"finnhub",""))
L591         df["fcf_imputed"] = df[["cfo_ttm","capex_ttm"]].isna().any(axis=1) & df["fcf_ttm"].notna()
L592         cols = ["cfo_ttm_yf","capex_ttm_yf","cfo_ttm_fh","capex_ttm_fh","cfo_ttm","capex_ttm","fcf_ttm","fcf_ttm_yf_direct","cfo_source","capex_source","fcf_imputed"]
L593         return df[cols].sort_index()
L594
L595     def _build_eps_df(self, tickers, tickers_bulk, info, sec_map: dict | None = None):
L596         eps_rows=[]
L597         for t in tickers:
L598             info_t = info[t]
L599             sec_t = (sec_map or {}).get(t, {})
L600             eps_ttm = sec_t.get("eps_ttm", info_t.get("trailingEps", np.nan))
L601             eps_q = sec_t.get("eps_q_recent", np.nan)
L602             try:
L603                 qearn, so = tickers_bulk.tickers[t].quarterly_earnings, info_t.get("sharesOutstanding")
L604                 if so and qearn is not None and not qearn.empty and "Earnings" in qearn.columns:
L605                     eps_ttm_q = qearn["Earnings"].head(4).sum()/so
L606                     if pd.notna(eps_ttm_q) and (pd.isna(eps_ttm) or (abs(eps_ttm)>0 and abs(eps_ttm/eps_ttm_q)>3)): eps_ttm = eps_ttm_q
L607                     if pd.isna(eps_q):
L608                         eps_q = qearn["Earnings"].iloc[-1]/so
L609             except Exception: pass
L610             rev_ttm = sec_t.get("rev_ttm", np.nan)
L611             rev_q = sec_t.get("rev_q_recent", np.nan)
L612             if (not sec_t) or pd.isna(rev_ttm):
L613                 try:
L614                     tk = tickers_bulk.tickers[t]
L615                     qfin = getattr(tk, "quarterly_financials", None)
L616                     if qfin is not None and not qfin.empty:
L617                         idx_lower = {str(i).lower(): i for i in qfin.index}
L618                         rev_idx = None
L619                         for name in ("Total Revenue", "TotalRevenue"):
L620                             key = name.lower()
L621                             if key in idx_lower:
L622                                 rev_idx = idx_lower[key]
L623                                 break
L624                         if rev_idx is not None:
L625                             rev_series = pd.to_numeric(qfin.loc[rev_idx], errors="coerce").dropna()
L626                             if not rev_series.empty:
L627                                 rev_ttm_yf = float(rev_series.head(4).sum())
L628                                 if pd.isna(rev_ttm):
L629                                     rev_ttm = rev_ttm_yf
L630                                 if pd.isna(rev_q):
L631                                     rev_q = float(rev_series.iloc[0])
L632                 except Exception:
L633                     pass
L634             eps_rows.append({"ticker":t,"eps_ttm":eps_ttm,"eps_q_recent":eps_q,"rev_ttm":rev_ttm,"rev_q_recent":rev_q})
L635         return self.impute_eps_ttm(pd.DataFrame(eps_rows).set_index("ticker"))
L636
L637     def prepare_data(self):
L638         """Fetch price and fundamental data for all tickers."""
L639         self.sec_dryrun_sample()
L640         cand_info = yf.Tickers(" ".join(self.cand)); cand_prices = {}
L641         for t in self.cand:
L642             try: cand_prices[t] = cand_info.tickers[t].fast_info.get("lastPrice", np.inf)
L643             except Exception as e: print(f"{t}: price fetch failed ({e})"); cand_prices[t] = np.inf
L644         cand_f = [t for t,p in cand_prices.items() if p<=self.price_max]
L645         T.log("price cap filter done (CAND_PRICE_MAX)")
L646         # å…¥åŠ›ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã®é‡è¤‡ã‚’é™¤å»ã—ã€ç¾è¡Œâ†’å€™è£œã®é †åºã‚’ç¶­æŒ
L647         tickers = list(dict.fromkeys(self.exist + cand_f))
L648         T.log(f"universe prepared: unique={len(tickers)} bench={self.bench}")
L649         data = yf.download(tickers + [self.bench], period="600d",
L650                            auto_adjust=True, progress=False, threads=False)
L651         T.log("yf.download done")
L652         px = data["Close"].dropna(how="all", axis=1).ffill(limit=2)
L653         spx = data["Close"][self.bench].reindex(px.index).ffill()
L654         clip_days = int(os.getenv("PRICE_CLIP_DAYS", "0"))   # 0ãªã‚‰ç„¡åŠ¹ï¼ˆæ—¢å®šï¼‰
L655         if clip_days > 0:
L656             px  = px.tail(clip_days + 1)
L657             spx = spx.tail(clip_days + 1)
L658             logger.info("[T] price window clipped by env: %d rows (PRICE_CLIP_DAYS=%d)", len(px), clip_days)
L659         else:
L660             logger.info("[T] price window clip skipped; rows=%d", len(px))
L661         tickers_bulk, info = yf.Tickers(" ".join(tickers)), {}
L662         for t in tickers:
L663             try:
L664                 info[t] = tickers_bulk.tickers[t].info
L665             except Exception as e:
L666                 logger.info("[warn] %s: info fetch failed (%s)", t, e)
L667                 info[t] = {}
L668         try:
L669             sec_map = self.fetch_eps_rev_from_sec(tickers)
L670         except Exception as e:
L671             logger.warning("[SEC] fetch_eps_rev_from_sec failed: %s", e)
L672             sec_map = {}
L673
L674         def _brief_len(s):
L675             try:
L676                 if isinstance(s, pd.Series):
L677                     return int(s.dropna().size)
L678                 if isinstance(s, (list, tuple)):
L679                     return len([v for v in s if pd.notna(v)])
L680                 if isinstance(s, np.ndarray):
L681                     return int(np.count_nonzero(~pd.isna(s)))
L682                 return int(bool(s))
L683             except Exception:
L684                 return 0
L685
L686         def _has_entries(val) -> bool:
L687             try:
L688                 if isinstance(val, pd.Series):
L689                     return not val.dropna().empty
L690                 if isinstance(val, (list, tuple)):
L691                     return any(pd.notna(v) for v in val)
L692                 return bool(val)
L693             except Exception:
L694                 return False
L695
L696         have_rev = 0
L697         have_eps = 0
L698         rev_lens: list[int] = []
L699         eps_lens: list[int] = []
L700         samples: list[tuple[str, int, str, float | None, int, str, float | None]] = []
L701
L702         for t in tickers:
L703             entry = info.get(t, {})
L704             m = (sec_map or {}).get(t) or {}
L705             if entry is None or not isinstance(entry, dict):
L706                 entry = {}
L707                 info[t] = entry
L708
L709             if m:
L710                 pairs_r = m.get("rev_q_series_pairs") or []
L711                 pairs_e = m.get("eps_q_series_pairs") or []
L712                 if pairs_r:
L713                     idx = pd.to_datetime([d for (d, _v) in pairs_r], errors="coerce")
L714                     val = pd.to_numeric([v for (_d, v) in pairs_r], errors="coerce")
L715                     s = pd.Series(val, index=idx).sort_index()
L716                     entry["SEC_REV_Q_SERIES"] = s
L717                 else:
L718                     entry["SEC_REV_Q_SERIES"] = m.get("rev_q_series") or []
L719                 if pairs_e:
L720                     idx = pd.to_datetime([d for (d, _v) in pairs_e], errors="coerce")
L721                     val = pd.to_numeric([v for (_d, v) in pairs_e], errors="coerce")
L722                     s = pd.Series(val, index=idx).sort_index()
L723                     entry["SEC_EPS_Q_SERIES"] = s
L724                 else:
L725                     entry["SEC_EPS_Q_SERIES"] = m.get("eps_q_series") or []
L726
L727             r = entry.get("SEC_REV_Q_SERIES")
L728             e = entry.get("SEC_EPS_Q_SERIES")
L729             if _has_entries(r):
L730                 have_rev += 1
L731             if _has_entries(e):
L732                 have_eps += 1
L733             lr = _brief_len(r)
L734             le = _brief_len(e)
L735             rev_lens.append(lr)
L736             eps_lens.append(le)
L737             if len(samples) < 8:
L738                 try:
L739                     rd = getattr(r, "index", [])[-1] if lr > 0 else None
L740                     rv = float(r.iloc[-1]) if lr > 0 else None
L741                     ed = getattr(e, "index", [])[-1] if le > 0 else None
L742                     ev = float(e.iloc[-1]) if le > 0 else None
L743                     samples.append((t, lr, str(rd) if rd is not None else "-", rv, le, str(ed) if ed is not None else "-", ev))
L744                 except Exception:
L745                     samples.append((t, lr, "-", None, le, "-", None))
L746
L747         logger.info("[SEC] series attach: rev_q=%d/%d, eps_q=%d/%d", have_rev, len(tickers), have_eps, len(tickers))
L748
L749         if rev_lens:
L750             rev_lens_sorted = sorted(rev_lens)
L751             eps_lens_sorted = sorted(eps_lens)
L752             _log(
L753                 "SEC_SERIES",
L754                 f"rev_len min/med/max={rev_lens_sorted[0]}/{rev_lens_sorted[len(rev_lens)//2]}/{rev_lens_sorted[-1]} "
L755                 f"eps_len min/med/max={eps_lens_sorted[0]}/{eps_lens_sorted[len(eps_lens)//2]}/{eps_lens_sorted[-1]}",
L756             )
L757         for (t, lr, rd, rv, le, ed, ev) in samples:
L758             _log("SEC_SERIES_SMP", f"{t}  rev_len={lr} last=({rd},{rv})  eps_len={le} last=({ed},{ev})")
L759         eps_df = self._build_eps_df(tickers, tickers_bulk, info, sec_map=sec_map)
L760         # index é‡è¤‡ãŒã‚ã‚‹ã¨ .loc[t, col] ãŒ Series ã«ãªã‚Šä»£å…¥æ™‚ã« ValueError ã‚’èª˜ç™ºã™ã‚‹
L761         if not eps_df.index.is_unique:
L762             eps_df = eps_df[~eps_df.index.duplicated(keep="last")]
L763         eps_df = eps_df.assign(
L764             EPS_TTM=eps_df["eps_ttm"],
L765             EPS_Q_LastQ=eps_df["eps_q_recent"],
L766             REV_TTM=eps_df["rev_ttm"],
L767             REV_Q_LastQ=eps_df["rev_q_recent"],
L768         )
L769         # ã“ã“ã§éNaNä»¶æ•°ã‚’ã‚µãƒãƒªè¡¨ç¤ºï¼ˆæ¬ æçŠ¶æ³ã®å³æ™‚æŠŠæ¡ç”¨ï¼‰
L770         try:
L771             n = len(eps_df)
L772             c_eps = int(eps_df["EPS_TTM"].notna().sum())
L773             c_rev = int(eps_df["REV_TTM"].notna().sum())
L774             print(f"[SEC] eps_ttm non-NaN: {c_eps}/{n}  rev_ttm non-NaN: {c_rev}/{n}")
L775         except Exception:
L776             pass
L777         fcf_df = self.compute_fcf_with_fallback(tickers, finnhub_api_key=self.api_key)
L778         T.log("eps/fcf prep done")
L779         returns = px[tickers].pct_change()
L780         T.log("price prep/returns done")
L781         return dict(cand=cand_f, tickers=tickers, data=data, px=px, spx=spx, tickers_bulk=tickers_bulk, info=info, eps_df=eps_df, fcf_df=fcf_df, returns=returns)
L782
L783 # === Selectorï¼šç›¸é–¢ä½æ¸›ãƒ»é¸å®šï¼ˆã‚¹ã‚³ã‚¢ï¼†ãƒªã‚¿ãƒ¼ãƒ³ã ã‘èª­ã‚€ï¼‰ ===
L784 class Selector:
L785     # ---- DRRS helpersï¼ˆSelectorå°‚ç”¨ï¼‰ ----
L786     @staticmethod
L787     def _z_np(X: np.ndarray) -> np.ndarray:
L788         X = np.asarray(X, dtype=np.float32); m = np.nanmean(X, axis=0, keepdims=True); s = np.nanstd(X, axis=0, keepdims=True)+1e-9
L789         return (np.nan_to_num(X)-m)/s
L790
L791     @classmethod
L792     def residual_corr(cls, R: np.ndarray, n_pc: int=3, shrink: float=0.1) -> np.ndarray:
L793         Z = cls._z_np(R); U,S,_ = np.linalg.svd(Z, full_matrices=False); F = U[:,:n_pc]*S[:n_pc]; B = np.linalg.lstsq(F, Z, rcond=None)[0]
L794         E = Z - F@B; C = np.corrcoef(E, rowvar=False)
L795         off = C - np.diag(np.diag(C)); iu = np.triu_indices_from(off,1); avg_abs = np.nanmean(np.abs(off[iu])) if iu[0].size else 0.0
L796         shrink_eff = float(np.clip(shrink + 0.5*avg_abs, 0.1, 0.6)); N = C.shape[0]
L797         return (1.0 - shrink_eff)*C + shrink_eff*np.eye(N, dtype=C.dtype)
L798
L799     @classmethod
L800     def rrqr_like_det(cls, R: np.ndarray, score: np.ndarray, k: int, gamma: float=1.0):
L801         Z, w = cls._z_np(R), (score-score.min())/(np.ptp(score)+1e-12); X = Z*(1.0 + gamma*w)
L802         N, k = X.shape[1], max(0, min(k, X.shape[1]))
L803         if k==0: return []
L804         S, selected, Rres = [], np.zeros(N, dtype=bool), X.copy()
L805         for _ in range(k):
L806             norms = (Rres*Rres).sum(axis=0); cand = np.where(~selected)[0]
L807             j = sorted(cand, key=lambda c:(-norms[c], -w[c], c))[0]
L808             S.append(j); selected[j]=True; u = X[:,j:j+1]; u/=(np.linalg.norm(u)+1e-12); Rres = Rres - u @ (u.T @ Rres)
L809         return sorted(S)
L810
L811     @staticmethod
L812     def _obj(corrM: np.ndarray, score: np.ndarray, idx, lam: float) -> float:
L813         idx = list(idx); P = corrM[np.ix_(idx, idx)]; s = (score-score.mean())/(score.std()+1e-9)
L814         return float(s[idx].sum() - lam*((P.sum()-np.trace(P))/2.0))
L815
L816     @classmethod
L817     def swap_local_det(cls, corrM: np.ndarray, score: np.ndarray, idx, lam: float=0.6, max_pass: int=15):
L818         S, best, improved, passes = sorted(idx), cls._obj(corrM, score, idx, lam), True, 0
L819         while improved and passes<max_pass:
L820             improved, passes = False, passes+1
L821             for i,out in enumerate(list(S)):
L822                 for inn in range(len(score)):
L823                     if inn in S: continue
L824                     cand = sorted(S[:i]+[inn]+S[i+1:]); v = cls._obj(corrM, score, cand, lam)
L825                     if v>best+1e-10: S, best, improved = cand, v, True; break
L826                 if improved: break
L827         return S, best
L828
L829     @staticmethod
L830     def _obj_with_cross(C_within: np.ndarray, C_cross: np.ndarray|None, score: np.ndarray, idx, lam: float, mu: float) -> float:
L831         idx = list(idx); P = C_within[np.ix_(idx, idx)]; s = (score-score.mean())/(score.std()+1e-9)
L832         within = (P.sum()-np.trace(P))/2.0; cross = 0.0
L833         if C_cross is not None and C_cross.size>0: cross = C_cross[idx,:].sum()
L834         return float(s[idx].sum() - lam*within - mu*cross)
L835
L836     @classmethod
L837     def swap_local_det_cross(cls, C_within: np.ndarray, C_cross: np.ndarray|None, score: np.ndarray, idx, lam: float=0.6, mu: float=0.3, max_pass: int=15):
L838         S, best, improved, passes, N = sorted(idx), cls._obj_with_cross(C_within,C_cross,score,idx,lam,mu), True, 0, len(score)
L839         while improved and passes<max_pass:
L840             improved, passes = False, passes+1
L841             for i,out in enumerate(list(S)):
L842                 for inn in range(N):
L843                     if inn in S: continue
L844                     cand = sorted(S[:i]+[inn]+S[i+1:]); v = cls._obj_with_cross(C_within,C_cross,score,cand,lam,mu)
L845                     if v>best+1e-10: S, best, improved = cand, v, True; break
L846                 if improved: break
L847         return S, best
L848
L849     @staticmethod
L850     def avg_corr(C: np.ndarray, idx) -> float:
L851         k = len(idx); P = C[np.ix_(idx, idx)]
L852         return float((P.sum()-np.trace(P))/(k*(k-1)+1e-12))
L853
L854     @classmethod
L855     def select_bucket_drrs(cls, returns_df: pd.DataFrame, score_ser: pd.Series, pool_tickers: list[str], k: int, *, n_pc: int, gamma: float, lam: float, lookback: int, shrink: float=0.10, g_fixed_tickers: list[str]|None=None, mu: float=0.0):
L856         g_fixed = [t for t in (g_fixed_tickers or []) if t in returns_df.columns]
L857         union = [t for t in pool_tickers if t in returns_df.columns]
L858         for t in g_fixed:
L859             if t not in union: union.append(t)
L860         Rdf_all = returns_df[union]; Rdf_all = Rdf_all.iloc[-lookback:] if len(Rdf_all)>=lookback else Rdf_all; Rdf_all = Rdf_all.dropna()
L861         pool_eff, g_eff = [t for t in pool_tickers if t in Rdf_all.columns], [t for t in g_fixed if t in Rdf_all.columns]
L862         if len(pool_eff)==0: return dict(idx=[], tickers=[], avg_res_corr=np.nan, sum_score=0.0, objective=-np.inf)
L863         score = score_ser.reindex(pool_eff).to_numpy(dtype=np.float32)
L864         C_all = cls.residual_corr(Rdf_all.to_numpy(), n_pc=n_pc, shrink=shrink)
L865         col_pos = {c:i for i,c in enumerate(Rdf_all.columns)}; pool_pos = [col_pos[t] for t in pool_eff]
L866         C_within, C_cross = C_all[np.ix_(pool_pos,pool_pos)], None
L867         if len(g_eff)>0 and mu>0.0:
L868             g_pos = [col_pos[t] for t in g_eff]; C_cross = C_all[np.ix_(pool_pos,g_pos)]
L869         R_pool = Rdf_all[pool_eff].to_numpy(); S0 = cls.rrqr_like_det(R_pool, score, k, gamma=gamma)
L870         S, Jn = (cls.swap_local_det_cross(C_within, C_cross, score, S0, lam=lam, mu=mu, max_pass=15) if C_cross is not None else cls.swap_local_det(C_within, score, S0, lam=lam, max_pass=15))
L871         selected_tickers = [pool_eff[i] for i in S]
L872         return dict(idx=S, tickers=selected_tickers, avg_res_corr=cls.avg_corr(C_within,S), sum_score=float(score[S].sum()), objective=float(Jn))
L873
L874     # ---- é¸å®šï¼ˆã‚¹ã‚³ã‚¢ Series / returns ã ã‘ã‚’å—ã‘ã‚‹ï¼‰----
L875 # === Outputï¼šå‡ºåŠ›æ•´å½¢ã¨é€ä¿¡ï¼ˆè¡¨ç¤ºãƒ»Slackï¼‰ ===
L876 class Output:
L877
L878     def __init__(self, debug=None):
L879         # self.debug ã¯ä½¿ã‚ãªã„ï¼ˆäº’æ›ã®ãŸã‚å¼•æ•°ã¯å—ã‘ã‚‹ãŒç„¡è¦–ï¼‰
L880         self.miss_df = self.g_table = self.d_table = self.io_table = self.df_metrics_fmt = self.debug_table = None
L881         self.g_title = self.d_title = ""
L882         self.g_formatters = self.d_formatters = {}
L883         # ä½ã‚¹ã‚³ã‚¢ï¼ˆGSC+DSCï¼‰Top10 è¡¨ç¤º/é€ä¿¡ç”¨
L884         self.low10_table = None
L885         self.debug_text = ""   # ãƒ‡ãƒãƒƒã‚°ç”¨æœ¬æ–‡ã¯ã“ã“ã«ä¸€æœ¬åŒ–
L886         self._debug_logged = False
L887
L888     # --- è¡¨ç¤ºï¼ˆå…ƒ display_results ã®ãƒ­ã‚¸ãƒƒã‚¯ãã®ã¾ã¾ï¼‰ ---
L889     def display_results(self, *, exist, bench, df_z, g_score, d_score_all,
L890                         init_G, init_D, top_G, top_D, **kwargs):
L891         logger.info("ğŸ“Œ reached display_results")
L892         pd.set_option('display.float_format','{:.3f}'.format)
L893         print("ğŸ“ˆ ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼åˆ†æ•£æœ€é©åŒ–ã®çµæœ")
L894         if self.miss_df is not None and not self.miss_df.empty:
L895             print("Missing Data:")
L896             print(self.miss_df.to_string(index=False))
L897
L898         # ---- è¡¨ç¤ºç”¨ï¼šChanges/Near-Miss ã®ã‚¹ã‚³ã‚¢æºã‚’â€œæœ€çµ‚é›†è¨ˆâ€ã«çµ±ä¸€ã™ã‚‹ãƒ—ãƒ­ã‚­ã‚· ----
L899         try:
L900             sc = getattr(self, "_sc", None)
L901             agg_G = getattr(sc, "_agg_G", None)
L902             agg_D = getattr(sc, "_agg_D", None)
L903         except Exception:
L904             sc = agg_G = agg_D = None
L905         class _SeriesProxy:
L906             __slots__ = ("primary", "fallback")
L907             def __init__(self, primary, fallback): self.primary, self.fallback = primary, fallback
L908             def get(self, key, default=None):
L909                 try:
L910                     v = self.primary.get(key) if hasattr(self.primary, "get") else None
L911                     if v is not None and not (isinstance(v, float) and v != v):
L912                         return v
L913                 except Exception:
L914                     pass
L915                 try:
L916                     return self.fallback.get(key) if hasattr(self.fallback, "get") else default
L917                 except Exception:
L918                     return default
L919         g_score = _SeriesProxy(agg_G, g_score)
L920         d_score_all = _SeriesProxy(agg_D, d_score_all)
L921         near_G = getattr(sc, "_near_G", []) if sc else []
L922         near_D = getattr(sc, "_near_D", []) if sc else []
L923
L924         extra_G = [t for t in init_G if t not in top_G][:5]; G_UNI = top_G + extra_G
L925         gsc_series = pd.Series({t: g_score.get(t) for t in G_UNI}, name='GSC')
L926         self.g_table = pd.concat([df_z.loc[G_UNI,['GROWTH_F','MOM','TRD','VOL']], gsc_series], axis=1)
L927         self.g_table.index = [t + ("â­ï¸" if t in top_G else "") for t in G_UNI]
L928         self.g_formatters = {col:"{:.2f}".format for col in ['GROWTH_F','MOM','TRD','VOL']}; self.g_formatters['GSC'] = "{:.3f}".format
L929         self.g_title = (f"[Gæ  / {N_G} / {_fmt_w(g_weights)} / corrM={corrM} / "
L930                         f"LB={DRRS_G['lookback']} nPC={DRRS_G['n_pc']} Î³={DRRS_G['gamma']} Î»={DRRS_G['lam']} Î·={DRRS_G['eta']} shrink={DRRS_SHRINK}]")
L931         if near_G:
L932             add = [t for t in near_G if t not in set(G_UNI)][:10]
L933             if len(add) < 10:
L934                 try:
L935                     aggG = getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False)
L936                     out_now = sorted(set(exist) - set(top_G + top_D))  # ä»Šå› OUT
L937                     used = set(G_UNI + add)
L938                     def _push(lst):
L939                         nonlocal add, used
L940                         for t in lst:
L941                             if len(add) == 10: break
L942                             if t in aggG.index and t not in used:
L943                                 add.append(t); used.add(t)
L944                     _push(out_now)           # â‘  ä»Šå› OUT ã‚’å„ªå…ˆ
L945                     _push(list(aggG.index))  # â‘¡ ã¾ã è¶³ã‚Šãªã‘ã‚Œã°ä¸Šä½ã§å……å¡«
L946                 except Exception:
L947                     pass
L948             if add:
L949                 near_tbl = pd.concat([df_z.loc[add,['GROWTH_F','MOM','TRD','VOL']], pd.Series({t: g_score.get(t) for t in add}, name='GSC')], axis=1)
L950                 self.g_table = pd.concat([self.g_table, near_tbl], axis=0)
L951         print(self.g_title); print(self.g_table.to_string(formatters=self.g_formatters))
L952
L953         extra_D = [t for t in init_D if t not in top_D][:5]; D_UNI = top_D + extra_D
L954         cols_D = ['QAL','YLD','VOL','TRD']; d_disp = pd.DataFrame(index=D_UNI)
L955         d_disp['QAL'], d_disp['YLD'], d_disp['VOL'], d_disp['TRD'] = df_z.loc[D_UNI,'D_QAL'], df_z.loc[D_UNI,'D_YLD'], df_z.loc[D_UNI,'D_VOL_RAW'], df_z.loc[D_UNI,'D_TRD']
L956         dsc_series = pd.Series({t: d_score_all.get(t) for t in D_UNI}, name='DSC')
L957         self.d_table = pd.concat([d_disp, dsc_series], axis=1); self.d_table.index = [t + ("â­ï¸" if t in top_D else "") for t in D_UNI]
L958         self.d_formatters = {col:"{:.2f}".format for col in cols_D}; self.d_formatters['DSC']="{:.3f}".format
L959         import scorer
L960         dw_eff = scorer.D_WEIGHTS_EFF
L961         self.d_title = (f"[Dæ  / {N_D} / {_fmt_w(dw_eff)} / corrM={corrM} / "
L962                         f"LB={DRRS_D['lookback']} nPC={DRRS_D['n_pc']} Î³={DRRS_D['gamma']} Î»={DRRS_D['lam']} Î¼={CROSS_MU_GD} Î·={DRRS_D['eta']} shrink={DRRS_SHRINK}]")
L963         if near_D:
L964             add = [t for t in near_D if t not in set(D_UNI)][:10]
L965             if add:
L966                 d_disp2 = pd.DataFrame(index=add)
L967                 d_disp2['QAL'], d_disp2['YLD'], d_disp2['VOL'], d_disp2['TRD'] = df_z.loc[add,'D_QAL'], df_z.loc[add,'D_YLD'], df_z.loc[add,'D_VOL_RAW'], df_z.loc[add,'D_TRD']
L968                 near_tbl = pd.concat([d_disp2, pd.Series({t: d_score_all.get(t) for t in add}, name='DSC')], axis=1)
L969                 self.d_table = pd.concat([self.d_table, near_tbl], axis=0)
L970         print(self.d_title); print(self.d_table.to_string(formatters=self.d_formatters))
L971
L972         # === Changesï¼ˆIN ã® GSC/DSC ã‚’è¡¨ç¤ºã€‚OUT ã¯éŠ˜æŸ„åã®ã¿ï¼‰ ===
L973         in_list = sorted(set(list(top_G)+list(top_D)) - set(exist))
L974         out_list = sorted(set(exist) - set(list(top_G)+list(top_D)))
L975
L976         self.io_table = pd.DataFrame({
L977             'IN': pd.Series(in_list),
L978             '/ OUT': pd.Series(out_list)
L979         })
L980         g_list = [f"{g_score.get(t):.3f}" if pd.notna(g_score.get(t)) else 'â€”' for t in out_list]
L981         d_list = [f"{d_score_all.get(t):.3f}" if pd.notna(d_score_all.get(t)) else 'â€”' for t in out_list]
L982         self.io_table['GSC'] = pd.Series(g_list)
L983         self.io_table['DSC'] = pd.Series(d_list)
L984
L985         print("Changes:")
L986         print(self.io_table.to_string(index=False))
L987
L988         all_tickers = list(set(exist + list(top_G) + list(top_D) + [bench])); prices = yf.download(all_tickers, period='1y', auto_adjust=True, progress=False, threads=False)['Close'].ffill(limit=2)
L989         ret = prices.pct_change(); portfolios = {'CUR':exist,'NEW':list(top_G)+list(top_D)}; metrics={}
L990         for name,ticks in portfolios.items():
L991             pr = ret[ticks].mean(axis=1, skipna=True).dropna(); cum = (1+pr).cumprod()-1; n = len(pr)
L992             if n>=252: ann_ret, ann_vol = (1+cum.iloc[-1])**(252/n)-1, pr.std()*np.sqrt(252)
L993             else: ann_ret, ann_vol = cum.iloc[-1], pr.std()*np.sqrt(n)
L994             sharpe, drawdown = ann_ret/ann_vol, (cum - cum.cummax()).min()
L995             if len(ticks)>=2:
L996                 C_raw = ret[ticks].corr(); RAW_rho = C_raw.mask(np.eye(len(ticks), dtype=bool)).stack().mean()
L997                 R = ret[ticks].dropna().to_numpy(); C_resid = Selector.residual_corr(R, n_pc=3, shrink=DRRS_SHRINK)
L998                 RESID_rho = float((C_resid.sum()-np.trace(C_resid))/(C_resid.shape[0]*(C_resid.shape[0]-1)))
L999             else: RAW_rho = RESID_rho = np.nan
L1000             divy = ttm_div_yield_portfolio(ticks); metrics[name] = {'RET':ann_ret,'VOL':ann_vol,'SHP':sharpe,'MDD':drawdown,'RAWÏ':RAW_rho,'RESIDÏ':RESID_rho,'DIVY':divy}
L1001         df_metrics = pd.DataFrame(metrics).T; df_metrics_pct = df_metrics.copy(); self.df_metrics = df_metrics
L1002         for col in ['RET','VOL','MDD','DIVY']: df_metrics_pct[col] = df_metrics_pct[col]*100
L1003         cols_order = ['RET','VOL','SHP','MDD','RAWÏ','RESIDÏ','DIVY']; df_metrics_pct = df_metrics_pct.reindex(columns=cols_order)
L1004         def _fmt_row(s):
L1005             return pd.Series({'RET':f"{s['RET']:.1f}%",'VOL':f"{s['VOL']:.1f}%",'SHP':f"{s['SHP']:.1f}",'MDD':f"{s['MDD']:.1f}%",'RAWÏ':(f"{s['RAWÏ']:.2f}" if pd.notna(s['RAWÏ']) else "NaN"),'RESIDÏ':(f"{s['RESIDÏ']:.2f}" if pd.notna(s['RESIDÏ']) else "NaN"),'DIVY':f"{s['DIVY']:.1f}%"})
L1006         self.df_metrics_fmt = df_metrics_pct.apply(_fmt_row, axis=1); print("Performance Comparison:"); print(self.df_metrics_fmt.to_string())
L1007         # === è¿½åŠ : GSC+DSC ãŒä½ã„é † TOP10 ===
L1008         try:
L1009             all_scores = pd.DataFrame({'GSC': df_z['GSC'], 'DSC': df_z['DSC']}).copy()
L1010             all_scores['G_plus_D'] = all_scores['GSC'] + all_scores['DSC']
L1011             all_scores = all_scores.dropna(subset=['G_plus_D'])
L1012             self.low10_table = all_scores.sort_values('G_plus_D', ascending=True).head(10).round(3)
L1013             print("Low Score Candidates (GSC+DSC bottom 10):")
L1014             print(self.low10_table.to_string())
L1015         except Exception as e:
L1016             print(f"[warn] low-score ranking failed: {e}")
L1017             self.low10_table = None
L1018         self.debug_text = ""
L1019         if debug_mode:
L1020             logger.info("debug_mode=True: df_z dump handled in scorer; skipping factor-side debug output")
L1021         else:
L1022             logger.debug(
L1023                 "skip debug log: debug_mode=%s debug_text_empty=%s",
L1024                 debug_mode, True
L1025             )
L1026         self._debug_logged = True
L1027
L1028     # --- Slacké€ä¿¡ï¼ˆå…ƒ notify_slack ã®ãƒ­ã‚¸ãƒƒã‚¯ãã®ã¾ã¾ï¼‰ ---
L1029     def notify_slack(self):
L1030         SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")
L1031
L1032         if not SLACK_WEBHOOK_URL:
L1033             print("âš ï¸ SLACK_WEBHOOK_URL not set (main report skipped)")
L1034             return
L1035
L1036         def _filter_suffix_from(spec: dict, group: str) -> str:
L1037             g = spec.get(group, {})
L1038             parts = [str(m) for m in g.get("pre_mask", [])]
L1039             for k, v in (g.get("pre_filter", {}) or {}).items():
L1040                 base, op = (k[:-4], "<") if k.endswith("_max") else ((k[:-4], ">") if k.endswith("_min") else (k, "="))
L1041                 name = {"beta": "Î²"}.get(base, base)
L1042                 try:
L1043                     val = f"{float(v):g}"
L1044                 except Exception:
L1045                     val = str(v)
L1046                 parts.append(f"{name}{op}{val}")
L1047             return "" if not parts else " / filter:" + " & ".join(parts)
L1048
L1049         def _inject_filter_suffix(title: str, group: str) -> str:
L1050             suf = _filter_suffix_from(FILTER_SPEC, group)
L1051             return f"{title[:-1]}{suf}]" if suf and title.endswith("]") else (title + suf)
L1052
L1053         def _blk(title, tbl, fmt=None, drop=()):
L1054             if tbl is None or getattr(tbl, 'empty', False):
L1055                 return f"{title}\n(é¸å®šãªã—)\n"
L1056             if drop and hasattr(tbl, 'columns'):
L1057                 keep = [c for c in tbl.columns if c not in drop]
L1058                 tbl, fmt = tbl[keep], {k: v for k, v in (fmt or {}).items() if k in keep}
L1059             return f"{title}\n```{tbl.to_string(formatters=fmt)}```\n"
L1060
L1061         message = "ğŸ“ˆ ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼åˆ†æ•£æœ€é©åŒ–ã®çµæœ\n"
L1062         if self.miss_df is not None and not self.miss_df.empty:
L1063             message += "Missing Data\n```" + self.miss_df.to_string(index=False) + "```\n"
L1064         message += _blk(_inject_filter_suffix(self.g_title, "G"), self.g_table, self.g_formatters, drop=("TRD",))
L1065         message += _blk(_inject_filter_suffix(self.d_title, "D"), self.d_table, self.d_formatters)
L1066         message += "Changes\n" + ("(å¤‰æ›´ãªã—)\n" if self.io_table is None or getattr(self.io_table, 'empty', False) else f"```{self.io_table.to_string(index=False)}```\n")
L1067         message += "Performance Comparison:\n```" + self.df_metrics_fmt.to_string() + "```"
L1068
L1069         try:
L1070             r = requests.post(SLACK_WEBHOOK_URL, json={"text": message})
L1071             print(f"[DBG] main_post status={getattr(r, 'status_code', None)} size={len(message)}")
L1072             if r is not None:
L1073                 r.raise_for_status()
L1074         except Exception as e:
L1075             print(f"[ERR] main_post_failed: {e}")
L1076
L1077 def _infer_g_universe(feature_df, selected12=None, near5=None):
L1078     try:
L1079         out = feature_df.index[feature_df['group'].astype(str).str.upper().eq('G')].tolist()
L1080         if out: return out
L1081     except Exception:
L1082         pass
L1083     base = set()
L1084     for lst in (selected12 or []), (near5 or []):
L1085         for x in (lst or []): base.add(x)
L1086     return list(base) if base else list(feature_df.index)
L1087
L1088 def _fmt_with_fire_mark(tickers, feature_df):
L1089     out = []
L1090     for t in tickers or []:
L1091         try:
L1092             br = bool(feature_df.at[t, "G_BREAKOUT_recent_5d"])
L1093             pb = bool(feature_df.at[t, "G_PULLBACK_recent_5d"])
L1094             out.append(f"{t}{' ğŸ”¥' if (br or pb) else ''}")
L1095         except Exception:
L1096             out.append(t)
L1097     return out
L1098
L1099 def _label_recent_event(t, feature_df):
L1100     try:
L1101         br = bool(feature_df.at[t, "G_BREAKOUT_recent_5d"]); dbr = str(feature_df.at[t, "G_BREAKOUT_last_date"]) if br else ""
L1102         pb = bool(feature_df.at[t, "G_PULLBACK_recent_5d"]); dpb = str(feature_df.at[t, "G_PULLBACK_last_date"]) if pb else ""
L1103         if   br and not pb: return f"{t}ï¼ˆãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®š {dbr}ï¼‰"
L1104         elif pb and not br: return f"{t}ï¼ˆæŠ¼ã—ç›®åç™º {dpb}ï¼‰"
L1105         elif br and pb:     return f"{t}ï¼ˆãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®š {dbr}ï¼æŠ¼ã—ç›®åç™º {dpb}ï¼‰"
L1106     except Exception:
L1107         pass
L1108     return t
L1109
L1110 # === ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å¯è¦–åŒ–ï¼šG/Då…±é€šãƒ•ãƒ­ãƒ¼ï¼ˆå‡ºåŠ›ã¯ä¸å¤‰ï¼‰ ===
L1111
L1112 def io_build_input_bundle() -> InputBundle:
L1113     """
L1114     æ—¢å­˜ã®ã€ãƒ‡ãƒ¼ã‚¿å–å¾—â†’å‰å‡¦ç†ã€ã‚’å®Ÿè¡Œã—ã€InputBundle ã‚’è¿”ã™ã€‚
L1115     å‡¦ç†å†…å®¹ãƒ»åˆ—åãƒ»ä¸¸ã‚ãƒ»ä¾‹å¤–ãƒ»ãƒ­ã‚°æ–‡è¨€ã¯ç¾è¡Œã©ãŠã‚Šï¼ˆå¤‰æ›´ç¦æ­¢ï¼‰ã€‚
L1116     """
L1117     state = Input(cand=cand, exist=exist, bench=bench, price_max=CAND_PRICE_MAX, finnhub_api_key=FINNHUB_API_KEY).prepare_data()
L1118     return InputBundle(cand=state["cand"], tickers=state["tickers"], bench=bench, data=state["data"], px=state["px"], spx=state["spx"], tickers_bulk=state["tickers_bulk"], info=state["info"], eps_df=state["eps_df"], fcf_df=state["fcf_df"], returns=state["returns"])
L1119
L1120 def run_group(sc: Scorer, group: str, inb: InputBundle, cfg: PipelineConfig,
L1121               n_target: int) -> tuple[list, float, float, float]:
L1122     """
L1123     G/Dã‚’åŒä¸€æ‰‹é †ã§å‡¦ç†ï¼šæ¡ç‚¹â†’ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼â†’é¸å®šï¼ˆç›¸é–¢ä½æ¸›è¾¼ã¿ï¼‰ã€‚
L1124     æˆ»ã‚Šå€¤ï¼š(pick, avg_res_corr, sum_score, objective)
L1125     JSONä¿å­˜ã¯æ—¢å­˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆã‚­ãƒ¼åãƒ»ä¸¸ã‚æ¡ãƒ»é †åºï¼‰ã‚’è¸è¥²ã€‚
L1126     """
L1127     sc.cfg = cfg
L1128
L1129     if hasattr(sc, "score_build_features"):
L1130         feat = sc.score_build_features(inb)
L1131         if not hasattr(sc, "_feat_logged"):
L1132             T.log("features built (scorer)")
L1133             sc._feat_logged = True
L1134         agg = sc.score_aggregate(feat, group, cfg) if hasattr(sc, "score_aggregate") else feat
L1135     else:
L1136         fb = sc.aggregate_scores(inb, cfg)
L1137         if not hasattr(sc, "_feat_logged"):
L1138             T.log("features built (scorer)")
L1139             sc._feat_logged = True
L1140         sc._feat = fb
L1141         agg = fb.g_score if group == "G" else fb.d_score_all
L1142         if group == "D" and hasattr(fb, "df"):
L1143             agg = agg[fb.df['BETA'] < D_BETA_MAX]
L1144
L1145     if hasattr(sc, "filter_candidates"):
L1146         agg = agg[sc.filter_candidates(inb, agg, group, cfg)]
L1147
L1148     selector = Selector()
L1149     if hasattr(sc, "select_diversified"):
L1150         pick, avg_r, sum_sc, obj = sc.select_diversified(agg, group, cfg, n_target,
L1151             selector=selector, prev_tickers=None,
L1152             corrM=cfg.drrs.corrM, shrink=cfg.drrs.shrink,
L1153             cross_mu=cfg.drrs.cross_mu_gd)
L1154     else:
L1155         if group == "G":
L1156             init = agg.nlargest(min(cfg.drrs.corrM, len(agg))).index.tolist()
L1157             res = selector.select_bucket_drrs(returns_df=inb.returns, score_ser=agg, pool_tickers=init, k=n_target,
L1158                 n_pc=cfg.drrs.G.get("n_pc", 3), gamma=cfg.drrs.G.get("gamma", 1.2),
L1159                 lam=cfg.drrs.G.get("lam", 0.68),
L1160                 lookback=cfg.drrs.G.get("lookback", 252),
L1161                 shrink=cfg.drrs.shrink, g_fixed_tickers=None, mu=0.0)
L1162         else:
L1163             init = agg.nlargest(min(cfg.drrs.corrM, len(agg))).index.tolist()
L1164             g_fixed = getattr(sc, "_top_G", None)
L1165             res = selector.select_bucket_drrs(returns_df=inb.returns, score_ser=agg, pool_tickers=init, k=n_target,
L1166                 n_pc=cfg.drrs.D.get("n_pc", 4), gamma=cfg.drrs.D.get("gamma", 0.8),
L1167                 lam=cfg.drrs.D.get("lam", 0.85),
L1168                 lookback=cfg.drrs.D.get("lookback", 504),
L1169                 shrink=cfg.drrs.shrink, g_fixed_tickers=g_fixed,
L1170                 mu=cfg.drrs.cross_mu_gd)
L1171         pick = res["tickers"]; avg_r = res["avg_res_corr"]
L1172         sum_sc = res["sum_score"]; obj = res["objective"]
L1173         if group == "D":
L1174             _, pick = _disjoint_keepG(getattr(sc, "_top_G", []), pick, init)
L1175             T.log("selection finalized (G/D)")
L1176     try:
L1177         inc = [t for t in exist if t in agg.index]
L1178         pick = _sticky_keep_current(
L1179             agg=agg, pick=pick, incumbents=inc, n_target=n_target,
L1180             delta_z=SWAP_DELTA_Z, keep_buffer=SWAP_KEEP_BUFFER
L1181         )
L1182     except Exception as _e:
L1183         print(f"[warn] sticky_keep_current skipped: {str(_e)}")
L1184     # --- Near-Miss: æƒœã—ãã‚‚é¸ã°ã‚Œãªã‹ã£ãŸä¸Šä½10ã‚’ä¿æŒï¼ˆSlackè¡¨ç¤ºç”¨ï¼‰ ---
L1185     # 5) Near-Miss ã¨æœ€çµ‚é›†è¨ˆSeriesã‚’ä¿æŒï¼ˆè¡¨ç¤ºå°‚ç”¨ã€‚è¨ˆç®—ã¸å½±éŸ¿ãªã—ï¼‰
L1186     try:
L1187         pool = agg.drop(index=[t for t in pick if t in agg.index], errors="ignore")
L1188         near10 = list(pool.sort_values(ascending=False).head(10).index)
L1189         setattr(sc, f"_near_{group}", near10)
L1190         setattr(sc, f"_agg_{group}", agg)
L1191     except Exception:
L1192         pass
L1193
L1194     if group == "D":
L1195         T.log("save done")
L1196     if group == "G":
L1197         sc._top_G = pick
L1198     return pick, avg_r, sum_sc, obj
L1199
L1200 def run_pipeline() -> SelectionBundle:
L1201     """
L1202     G/Då…±é€šãƒ•ãƒ­ãƒ¼ã®å…¥å£ã€‚I/Oã¯ã“ã“ã ã‘ã§å®Ÿæ–½ã—ã€è¨ˆç®—ã¯Scorerã«å§”è­²ã€‚
L1203     Slackæ–‡è¨€ãƒ»ä¸¸ã‚ãƒ»é †åºã¯æ—¢å­˜ã® Output ã‚’ç”¨ã„ã¦å¤‰æ›´ã—ãªã„ã€‚
L1204     """
L1205     inb = io_build_input_bundle()
L1206     cfg = PipelineConfig(
L1207         weights=WeightsConfig(g=g_weights, d=D_weights),
L1208         drrs=DRRSParams(
L1209             corrM=corrM, shrink=DRRS_SHRINK,
L1210             G=DRRS_G, D=DRRS_D, cross_mu_gd=CROSS_MU_GD
L1211         ),
L1212         price_max=CAND_PRICE_MAX,
L1213         debug_mode=debug_mode
L1214     )
L1215     sc = Scorer()
L1216     top_G, avgG, sumG, objG = run_group(sc, "G", inb, cfg, N_G)
L1217     poolG = list(getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False).index)
L1218     alpha = Scorer.spx_to_alpha(inb.spx)
L1219     sectors = {t:(inb.info.get(t,{}).get("sector") or "U") for t in poolG}; scores = {t:Scorer.g_score.get(t,0.0) for t in poolG}
L1220     top_G = Scorer.pick_top_softcap(scores, sectors, N=N_G, cap=2, alpha=alpha, hard=5)
L1221     sc._top_G = top_G
L1222     try:
L1223         aggG = getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False)
L1224         sc._near_G = [t for t in aggG.index if t not in set(top_G)][:10]
L1225     except Exception:
L1226         pass
L1227     base = sum(Scorer.g_score.get(t,0.0) for t in poolG[:N_G])
L1228     effs = sum(Scorer.g_score.get(t,0.0) for t in top_G)
L1229     print(f"[soft_cap2] score_cost={(base-effs)/max(1e-9,abs(base)):.2%}, alpha={alpha:.3f}")
L1230     top_D, avgD, sumD, objD = run_group(sc, "D", inb, cfg, N_D)
L1231     fb = getattr(sc, "_feat", None)
L1232     near_G = getattr(sc, "_near_G", [])
L1233     selected12 = list(top_G)
L1234     df = fb.df if fb is not None else pd.DataFrame()
L1235     guni = _infer_g_universe(df, selected12, near_G)
L1236     try:
L1237         fire_recent = [t for t in guni
L1238                        if (str(df.at[t, "G_BREAKOUT_recent_5d"]) == "True") or
L1239                           (str(df.at[t, "G_PULLBACK_recent_5d"]) == "True")]
L1240     except Exception: fire_recent = []
L1241
L1242     lines = [
L1243         "ã€Gæ ãƒ¬ãƒãƒ¼ãƒˆï½œé€±æ¬¡ãƒ¢ãƒ‹ã‚¿ï¼ˆç›´è¿‘5å–¶æ¥­æ—¥ï¼‰ã€‘",
L1244         "ã€å‡¡ä¾‹ã€‘ğŸ”¥=ç›´è¿‘5å–¶æ¥­æ—¥å†…ã«ã€Œãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®šã€ã¾ãŸã¯ã€ŒæŠ¼ã—ç›®åç™ºã€ã‚’æ¤œçŸ¥",
L1245         f"é¸å®š{N_G}: {', '.join(_fmt_with_fire_mark(selected12, df))}" if selected12 else f"é¸å®š{N_G}: ãªã—",
L1246         f"æ¬¡ç‚¹10: {', '.join(_fmt_with_fire_mark(near_G, df))}" if near_G else "æ¬¡ç‚¹10: ãªã—",]
L1247
L1248     if fire_recent:
L1249         fire_list = ", ".join([_label_recent_event(t, df) for t in fire_recent])
L1250         lines.append(f"éå»5å–¶æ¥­æ—¥ã®æ¤œçŸ¥: {fire_list}")
L1251     else:
L1252         lines.append("éå»5å–¶æ¥­æ—¥ã®æ¤œçŸ¥: ãªã—")
L1253
L1254     try:
L1255         webhook = os.environ.get("SLACK_WEBHOOK_URL", "")
L1256         if webhook:
L1257             requests.post(webhook, json={"text": "\n".join([s for s in lines if s != ""])}, timeout=10)
L1258     except Exception:
L1259         pass
L1260
L1261     out = Output()
L1262     # è¡¨ç¤ºå´ã‹ã‚‰é¸å®šæ™‚ã®é›†è¨ˆã¸ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†ã«ä¿æŒï¼ˆè¡¨ç¤ºå°‚ç”¨ãƒ»å‰¯ä½œç”¨ãªã—ï¼‰
L1263     try: out._sc = sc
L1264     except Exception: pass
L1265     if hasattr(sc, "_feat"):
L1266         try:
L1267             fb = sc._feat
L1268             out.miss_df = fb.missing_logs
L1269             out.display_results(
L1270                 exist=exist,
L1271                 bench=bench,
L1272                 df_z=fb.df_z,
L1273                 g_score=fb.g_score,
L1274                 d_score_all=fb.d_score_all,
L1275                 init_G=top_G,
L1276                 init_D=top_D,
L1277                 top_G=top_G,
L1278                 top_D=top_D,
L1279                 df_full_z=getattr(fb, "df_full_z", None),
L1280                 prev_G=getattr(sc, "_prev_G", exist),
L1281                 prev_D=getattr(sc, "_prev_D", exist),
L1282             )
L1283         except Exception:
L1284             pass
L1285     out.notify_slack()
L1286     sb = SelectionBundle(resG={"tickers": top_G, "avg_res_corr": avgG,
L1287               "sum_score": sumG, "objective": objG},
L1288         resD={"tickers": top_D, "avg_res_corr": avgD,
L1289               "sum_score": sumD, "objective": objD},
L1290         top_G=top_G, top_D=top_D, init_G=top_G, init_D=top_D)
L1291
L1292     # --- Low Score Candidates (GSC+DSC bottom 10) : send before debug dump ---
L1293     try:
L1294         _low_df = (pd.DataFrame({"GSC": fb.g_score, "DSC": fb.d_score_all})
L1295               .assign(G_plus_D=lambda x: x["GSC"] + x["DSC"])
L1296               .sort_values("G_plus_D")
L1297               .head(10)
L1298               .round(3))
L1299         low_msg = "Low Score Candidates (GSC+DSC bottom 10)\n" + _low_df.to_string(index=True, index_names=False)
L1300         _post_slack({"text": f"```{low_msg}```"})
L1301     except Exception as _e:
L1302         _post_slack({"text": f"```Low Score Candidates: ä½œæˆå¤±æ•—: {_e}```"})
L1303
L1304     return sb
L1305
L1306 if __name__ == "__main__":
L1307     run_pipeline()
```

## <scorer.py>
```text
L1 # scorer.py
L2 # kawatest
L3 # =============================================================================
L4 # Scorer: ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼/æŒ‡æ¨™ã®ç”Ÿæˆã¨åˆæˆã‚¹ã‚³ã‚¢ç®—å‡ºã‚’æ‹…ã†ç´”ç²‹å±¤
L5 #
L6 # ã€ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã ã‘èª­ã‚ã°åˆ†ã‹ã‚‹ãƒã‚¤ãƒ³ãƒˆã€‘
L7 # - å…¥åŠ›(InputBundle)ã¯ã€Œä¾¡æ ¼/å‡ºæ¥é«˜/ãƒ™ãƒ³ãƒ/åŸºæœ¬æƒ…å ±/EPS/FCF/ãƒªã‚¿ãƒ¼ãƒ³ã€ã‚’å«ã‚€DTO
L8 # - å‡ºåŠ›(FeatureBundle)ã¯ã€Œrawç‰¹å¾´é‡ dfã€ã€Œæ¨™æº–åŒ– df_zã€ã€ŒG/D ã‚¹ã‚³ã‚¢ã€ã€Œæ¬ æãƒ­ã‚°ã€
L9 # - é‡ã¿ç­‰ã®ã‚³ãƒ³ãƒ•ã‚£ã‚°(PipelineConfig)ã¯ factor ã‹ã‚‰æ¸¡ã™ï¼ˆcfg å¿…é ˆï¼‰
L10 # - æ—§ã‚«ãƒ©ãƒ åã¯ Scorer å†…ã§è‡ªå‹•ãƒªãƒãƒ¼ãƒ ã—ã¦å—ã‘å…¥ã‚Œï¼ˆå¾Œæ–¹äº’æ›ï¼‰
L11 #   ä¾‹) eps_ttm -> EPS_TTM, eps_q_recent -> EPS_Q_LastQ, fcf_ttm -> FCF_TTM
L12 #
L13 # ã€I/Oå¥‘ç´„ï¼ˆScorerãŒå‚ç…§ã™ã‚‹InputBundleãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰ã€‘
L14 #   - cand: List[str]    â€¦ å€™è£œéŠ˜æŸ„ï¼ˆå˜ä½“å®Ÿè¡Œã§ã¯æœªä½¿ç”¨ï¼‰
L15 #   - tickers: List[str] â€¦ å¯¾è±¡éŠ˜æŸ„ãƒªã‚¹ãƒˆ
L16 #   - bench: str         â€¦ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼ï¼ˆä¾‹ '^GSPC'ï¼‰
L17 #   - data: pd.DataFrame â€¦ yfinance downloadçµæœ ('Close','Volume' ç­‰ã®éšå±¤åˆ—)
L18 #   - px: pd.DataFrame   â€¦ data['Close'] ç›¸å½“ï¼ˆçµ‚å€¤ï¼‰
L19 #   - spx: pd.Series     â€¦ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®çµ‚å€¤
L20 #   - tickers_bulk: object         â€¦ yfinance.Tickers
L21 #   - info: Dict[str, dict]        â€¦ yfinance info per ticker
L22 #   - eps_df: pd.DataFrame         â€¦ å¿…é ˆåˆ—: EPS_TTM, EPS_Q_LastQï¼ˆæ—§åã‚‚å¯ï¼‰
L23 #   - fcf_df: pd.DataFrame         â€¦ å¿…é ˆåˆ—: FCF_TTMï¼ˆæ—§åã‚‚å¯ï¼‰
L24 #   - returns: pd.DataFrame        â€¦ px[tickers].pct_change() ç›¸å½“
L25 #
L26 # â€»å…¥å‡ºåŠ›ã®å½¢å¼ãƒ»ä¾‹å¤–æ–‡è¨€ã¯æ—¢å­˜å®Ÿè£…ã‚’å¤‰ãˆã¾ã›ã‚“ï¼ˆå®‰å…¨ãªçŸ­ç¸®ã®ã¿ï¼‰
L27 # =============================================================================
L28
L29 import logging
L30 import os, sys, warnings
L31 import json
L32 import requests
L33 import numpy as np
L34 import pandas as pd
L35 import yfinance as yf
L36 from typing import Any, TYPE_CHECKING
L37 from scipy.stats import zscore
L38 from datetime import datetime as _dt
L39
L40 DEBUG_SCOPE_STRICT = (
L41     os.getenv("DEBUG_SCOPE_STRICT", "false").strip().lower() == "true"
L42 )
L43
L44 FACTOR_COLUMNS = {
L45     "GRW": [
L46         "GROWTH_F",
L47         "GRW_FLEX_SCORE",
L48         "GRW_REV_YOY_Q",
L49         "GRW_REV_ACC_Q",
L50         "GRW_REV_QOQ",
L51         "GRW_REV_TTM2",
L52         "GRW_REV_YOY_Y",
L53         "GRW_PRICE_PROXY",
L54         "GRW_PATH",
L55     ],
L56     "MOM": ["MOM", "MOM_RAW", "MOM_P12M", "MOM_P6M", "MOM_P1M"],
L57     "VOL": ["VOL", "VOL_SD", "VOL_BETA"],
L58     "QUAL": ["QUAL", "ROE", "ROA", "FCF_MGN"],
L59     "VAL": ["VAL", "PE", "PB", "PS", "EVEBITDA"],
L60 }
L61
L62 if TYPE_CHECKING:
L63     from factor import PipelineConfig  # type: ignore  # å®Ÿè¡Œæ™‚importãªã—ï¼ˆå¾ªç’°å›é¿ï¼‰
L64
L65 logger = logging.getLogger(__name__)
L66
L67
L68 def _log(stage, msg):
L69     try:
L70         print(f"[DBG][{_dt.utcnow().isoformat(timespec='seconds')}Z][{stage}] {msg}")
L71     except Exception:
L72         print(f"[DBG][{stage}] {msg}")
L73
L74
L75 def _detect_used_cols(df, df_z):
L76     used = set()
L77
L78     try:
L79         import factor as _f
L80
L81         for k in getattr(_f, "g_weights", {}).keys():
L82             if k in df_z.columns:
L83                 used.add(k)
L84     except Exception:
L85         pass
L86
L87     try:
L88         import scorer as _sc
L89
L90         for k in getattr(_sc, "D_WEIGHTS_EFF", {}).keys():
L91             if k in df_z.columns:
L92                 used.add(k)
L93     except Exception:
L94         pass
L95
L96     for k in [
L97         "GROWTH_F",
L98         "MOM",
L99         "TRD",
L100         "VOL",
L101         "D_QAL",
L102         "D_YLD",
L103         "D_VOL_RAW",
L104         "D_TRD",
L105     ]:
L106         if k in df_z.columns:
L107             used.add(k)
L108
L109     grw_cols = [
L110         "GRW_PATH",
L111         "GRW_FLEX_SCORE",
L112         "GROWTH_F",
L113         "GRW_REV_YOY_Q",
L114         "GRW_REV_ACC_Q",
L115         "GRW_REV_QOQ",
L116         "GRW_REV_TTM2",
L117         "GRW_REV_YOY_Y",
L118         "GRW_PRICE_PROXY",
L119     ]
L120     for k in grw_cols:
L121         if (k in df.columns) or (k in df_z.columns):
L122             used.add(k)
L123
L124     for c in df_z.columns:
L125         if isinstance(c, str) and c.startswith("D_"):
L126             used.add(c)
L127
L128     num = df_z.select_dtypes(include=["number"])
L129     if not num.empty:
L130         var_top = num.var().sort_values(ascending=False).head(20).index.tolist()
L131         used.update(var_top)
L132
L133     return sorted(used)
L134
L135
L136 def _reorder_for_debug(df, df_z, factor_cols=FACTOR_COLUMNS):
L137     cols: list[str] = []
L138     for fac in ["GRW", "MOM", "VOL", "QUAL", "VAL"]:
L139         for c in factor_cols.get(fac, []):
L140             if c in getattr(df_z, "columns", []) or c in getattr(df, "columns", []):
L141                 cols.append(c)
L142     seen: set[str] = set()
L143     ordered: list[str] = []
L144     for c in cols:
L145         if c not in seen:
L146             ordered.append(c)
L147             seen.add(c)
L148     return ordered
L149
L150
L151 def dump_dfz_scoped(df, df_z, *, topk=20, logger=None):
L152     import numpy as np, pandas as pd, logging
L153
L154     lg = logger or logging.getLogger(__name__)
L155
L156     if not DEBUG_SCOPE_STRICT:
L157         dfz = df_z.copy()
L158         lg.info("DEBUG scope: disabled (showing ALL %d columns).", dfz.shape[1])
L159     else:
L160         rel = _detect_used_cols(df, df_z)
L161         dfz = df_z[[c for c in rel if c in df_z.columns]].copy()
L162         if dfz.shape[1] < 15:
L163             num = df_z.select_dtypes(include=["number"])
L164             add = []
L165             if not num.empty:
L166                 add = [
L167                     c
L168                     for c in num.var().sort_values(ascending=False).head(30).index
L169                     if c not in dfz.columns
L170                 ]
L171                 if dfz.shape[1] < 15:
L172                     add = add[: 15 - dfz.shape[1]]
L173                 else:
L174                     add = []
L175             dfz = pd.concat([dfz, df_z[add]], axis=1)
L176             lg.info("DEBUG scope too small â†’ fallback add %d cols", len(add))
L177         excluded = [c for c in df_z.columns if c not in dfz.columns]
L178         lg.info(
L179             "DEBUG scope: %d relevant cols kept, %d excluded.",
L180             dfz.shape[1],
L181             len(excluded),
L182         )
L183
L184     nan_top = dfz.isna().sum().sort_values(ascending=False).head(topk)
L185     lg.info("scorer:NaN columns (top%d):", topk)
L186     for c, n in nan_top.items():
L187         lg.info("%s\t%d", c, int(n))
L188
L189     num_dfz = dfz.select_dtypes(include=["number"])
L190     if not num_dfz.empty:
L191         ztop = (num_dfz == 0).mean().sort_values(ascending=False).head(topk)
L192         lg.info("scorer:Zero-dominated columns (top%d):", topk)
L193         for c, r in ztop.items():
L194             lg.info("%s\t%.2f%%", c, 100.0 * float(r))
L195
L196     return dfz
L197
L198
L199 def save_factor_debug_csv(df, df_z, path="out/factor_debug_latest.csv"):
L200     import os, pandas as pd, logging
L201
L202     lg = logging.getLogger(__name__)
L203     try:
L204         cols = _reorder_for_debug(df, df_z)
L205         dump = pd.DataFrame(index=df.index)
L206         for c in cols:
L207             if c in getattr(df, "columns", []):
L208                 dump[c] = df[c]
L209             if c in getattr(df_z, "columns", []):
L210                 dump[c] = df_z[c]
L211         dump.reset_index(names=["symbol"], inplace=True)
L212         if path:
L213             dirpath = os.path.dirname(path) or "."
L214             os.makedirs(dirpath, exist_ok=True)
L215             dump.to_csv(path, index=False)
L216         lg.info(
L217             "factor debug CSV saved: %s (cols=%d rows=%d)",
L218             path,
L219             dump.shape[1],
L220             dump.shape[0],
L221         )
L222     except Exception as e:
L223         lg.warning("factor debug CSV failed: %s", e)
L224
L225
L226 def log_grw_stats(df, df_z, logger):
L227     import numpy as np, pandas as pd
L228
L229     try:
L230         s = pd.to_numeric(df.get("GRW_FLEX_SCORE", pd.Series(dtype=float)), errors="coerce")
L231         z = pd.to_numeric(df_z.get("GROWTH_F", pd.Series(dtype=float)), errors="coerce")
L232         if s.size:
L233             logger.info(
L234                 "GRW raw stats: n=%d, median=%.3f, mad=%.3f, std=%.3f",
L235                 s.count(),
L236                 np.nanmedian(s),
L237                 np.nanmedian(np.abs(s - np.nanmedian(s))),
L238                 np.nanstd(s),
L239             )
L240         if z.size and not z.dropna().empty:
L241             clip_hi = float((z >= 2.95).mean() * 100.0)
L242             clip_lo = float((z <= -2.95).mean() * 100.0)
L243             logger.info(
L244                 "GRW z stats: min=%.2f, p25=%.2f, med=%.2f, p75=%.2f, max=%.2f, clipped_hi=%.1f%%, clipped_lo=%.1f%%",
L245                 np.nanmin(z),
L246                 np.nanpercentile(z.dropna(), 25),
L247                 np.nanmedian(z),
L248                 np.nanpercentile(z.dropna(), 75),
L249                 np.nanmax(z),
L250                 clip_hi,
L251                 clip_lo,
L252             )
L253         if "GRW_PATH" in getattr(df, "columns", []):
L254             logger.info(
L255                 "GRW path breakdown: %s",
L256                 df["GRW_PATH"].value_counts(dropna=False).to_dict(),
L257             )
L258     except Exception as e:
L259         logger.warning("GRW stats log failed: %s", e)
L260
L261
L262 def _grw_record_to_df(t: str, info_t: dict, df):
L263     if not isinstance(df, pd.DataFrame):
L264         return
L265     raw_parts = info_t.get("DEBUG_GRW_PARTS") if isinstance(info_t, dict) else None
L266     parts: dict[str, Any] = {}
L267     if isinstance(raw_parts, str):
L268         try:
L269             parts = json.loads(raw_parts)
L270         except Exception:
L271             parts = {}
L272     elif isinstance(raw_parts, dict):
L273         parts = raw_parts
L274     path = info_t.get("DEBUG_GRW_PATH") if isinstance(info_t, dict) else None
L275     score = info_t.get("GRW_SCORE") if isinstance(info_t, dict) else None
L276
L277     def _part_value(key: str):
L278         value = parts.get(key) if isinstance(parts, dict) else None
L279         if value is None:
L280             return np.nan
L281         try:
L282             return float(value)
L283         except Exception:
L284             return np.nan
L285
L286     df.loc[t, "GRW_PATH"] = path
L287     df.loc[t, "GRW_FLEX_SCORE"] = np.nan if score is None else float(score)
L288     df.loc[t, "GRW_REV_YOY_Q"] = _part_value("rev_yoy_q")
L289     df.loc[t, "GRW_REV_ACC_Q"] = _part_value("rev_acc_q")
L290     df.loc[t, "GRW_REV_QOQ"] = _part_value("rev_qoq")
L291     df.loc[t, "GRW_REV_TTM2"] = _part_value("rev_ttm2")
L292     df.loc[t, "GRW_REV_YOY_Y"] = _part_value("rev_yoy_y")
L293     df.loc[t, "GRW_PRICE_PROXY"] = _part_value("price_proxy")
L294
L295 # ---- Dividend Helpers -------------------------------------------------------
L296 def _last_close(t, price_map=None):
L297     if price_map and (c := price_map.get(t)) is not None: return float(c)
L298     try:
L299         h = yf.Ticker(t).history(period="5d")["Close"].dropna()
L300         return float(h.iloc[-1]) if len(h) else np.nan
L301     except Exception:
L302         return np.nan
L303
L304 def _ttm_div_sum(t, lookback_days=400):
L305     try:
L306         div = yf.Ticker(t).dividends
L307         if div is None or len(div) == 0: return 0.0
L308         cutoff = pd.Timestamp.utcnow().tz_localize(None) - pd.Timedelta(days=lookback_days)
L309         ttm = float(div[div.index.tz_localize(None) >= cutoff].sum())
L310         return ttm if ttm > 0 else float(div.tail(4).sum())
L311     except Exception:
L312         return 0.0
L313
L314 def ttm_div_yield_portfolio(tickers, price_map=None):
L315     ys = [(lambda c, s: (s/c) if (np.isfinite(c) and c>0 and s>0) else 0.0)(_last_close(t, price_map), _ttm_div_sum(t)) for t in tickers]
L316     return float(np.mean(ys)) if ys else 0.0
L317
L318 # ---- ç°¡æ˜“ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆå®‰å…¨ãªçŸ­ç¸®ã®ã¿ï¼‰ -----------------------------------
L319 def winsorize_s(s: pd.Series, p=0.02):
L320     if s is None or s.dropna().empty: return s
L321     lo, hi = np.nanpercentile(s.astype(float), [100*p, 100*(1-p)]); return s.clip(lo, hi)
L322
L323 def robust_z(s: pd.Series, p=0.02):
L324     s2 = winsorize_s(s,p); return np.nan_to_num(zscore(s2.fillna(s2.mean())))
L325
L326 def robust_z_keepnan(s: pd.Series) -> pd.Series:
L327     """robust_z variant that preserves NaNs and falls back to rank-z when needed."""
L328     if s is None:
L329         return pd.Series(dtype=float)
L330     v = pd.to_numeric(s, errors="coerce")
L331     m = np.nanmedian(v)
L332     mad = np.nanmedian(np.abs(v - m))
L333     z = (v - m) / (1.4826 * mad + 1e-9)
L334     if np.nanstd(z) < 1e-9:
L335         r = v.rank(method="average", na_option="keep")
L336         z = (r - np.nanmean(r)) / (np.nanstd(r) + 1e-9)
L337     return pd.Series(z, index=v.index, dtype=float)
L338
L339
L340 def _dump_dfz(
L341     df: pd.DataFrame,
L342     df_z: pd.DataFrame,
L343     debug_mode: bool,
L344     max_rows: int = 400,
L345     ndigits: int = 3,
L346 ) -> None:
L347     """df_z ã‚’ System log(INFO) ã¸ãƒ€ãƒ³ãƒ—ã™ã‚‹ç°¡æ½”ãªãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£."""
L348
L349     if not debug_mode:
L350         return
L351     try:
L352         dfz_scoped = dump_dfz_scoped(df, df_z, topk=20, logger=logger)
L353         ordered = _reorder_for_debug(df, df_z)
L354         rel_set = set(dfz_scoped.columns)
L355         view_cols = [c for c in ordered if c in rel_set]
L356         if not view_cols:
L357             view_cols = list(dfz_scoped.columns)
L358         view = dfz_scoped[view_cols].copy()
L359         view = view.apply(
L360             lambda s: s.round(ndigits)
L361             if getattr(getattr(s, "dtype", None), "kind", "") in ("f", "i")
L362             else s
L363         )
L364         if len(view) > max_rows:
L365             view = view.iloc[:max_rows]
L366
L367         logger.info("===== DF_Z DUMP START =====")
L368         logger.info("\n%s", view.to_string(max_rows=None, max_cols=None))
L369         logger.info("===== DF_Z DUMP END =====")
L370     except Exception as exc:
L371         logger.warning("df_z dump failed: %s", exc)
L372
L373 def _safe_div(a, b):
L374     try: return np.nan if (b is None or float(b)==0 or pd.isna(b)) else float(a)/float(b)
L375     except Exception: return np.nan
L376
L377 def _safe_last(series: pd.Series, default=np.nan):
L378     try: return float(series.iloc[-1])
L379     except Exception: return default
L380
L381
L382 def _ensure_series(x):
L383     if x is None:
L384         return pd.Series(dtype=float)
L385     if isinstance(x, pd.Series):
L386         return x.dropna()
L387     if isinstance(x, (list, tuple)):
L388         if len(x) and isinstance(x[0], (tuple, list)) and len(x[0]) == 2:
L389             dt = pd.to_datetime([d for d, _ in x], errors="coerce")
L390             v = pd.to_numeric([_v for _, _v in x], errors="coerce")
L391             return pd.Series(v, index=dt).dropna()
L392         return pd.Series(pd.to_numeric(list(x), errors="coerce")).dropna()
L393     try:
L394         return pd.Series(x).dropna()
L395     except Exception:
L396         return pd.Series(dtype=float)
L397
L398
L399 def _to_quarterly(s: pd.Series) -> pd.Series:
L400     if s.empty or not isinstance(s.index, pd.DatetimeIndex):
L401         return s
L402     return s.resample("Q").last().dropna()
L403
L404
L405 def _ttm_yoy_from_quarterly(qs: pd.Series) -> pd.Series:
L406     if qs is None or qs.empty:
L407         return pd.Series(dtype=float)
L408     ttm = qs.rolling(4, min_periods=2).sum()
L409     yoy = ttm.pct_change(4)
L410     return yoy
L411
L412
L413 def _nz(x) -> float:
L414     if x is None:
L415         return 0.0
L416     try:
L417         value = float(x)
L418     except Exception:
L419         return 0.0
L420     if not np.isfinite(value):
L421         return 0.0
L422     return value
L423
L424
L425 def _winsor(x, lo=-2.0, hi=2.0) -> float:
L426     v = _nz(x)
L427     if v < lo:
L428         return float(lo)
L429     if v > hi:
L430         return float(hi)
L431     return float(v)
L432
L433
L434 def _round_debug(x, ndigits: int = 4):
L435     try:
L436         value = float(x)
L437     except Exception:
L438         return None
L439     if not np.isfinite(value):
L440         return None
L441     return round(value, ndigits)
L442
L443
L444 def _calc_grw_flexible(
L445     ticker: str,
L446     info_entry: dict | None,
L447     close_series: pd.Series | None,
L448     volume_series: pd.Series | None,
L449 ):
L450     info_entry = info_entry if isinstance(info_entry, dict) else {}
L451
L452     s_rev_q = _ensure_series(info_entry.get("SEC_REV_Q_SERIES"))
L453     s_eps_q = _ensure_series(info_entry.get("SEC_EPS_Q_SERIES"))
L454     s_rev_y = _ensure_series(info_entry.get("SEC_REV_Y_SERIES"))
L455
L456     nQ = int(getattr(s_rev_q, "size", 0))
L457     nY = int(getattr(s_rev_y, "size", 0))
L458
L459     parts: dict[str, Any] = {"nQ": nQ, "nY": nY}
L460     path = "NONE"
L461     w = 0.0
L462
L463     def _valid_ratio(a, b):
L464         try:
L465             na, nb = float(a), float(b)
L466         except Exception:
L467             return None
L468         if not np.isfinite(na) or not np.isfinite(nb) or nb == 0:
L469             return None
L470         return na, nb
L471
L472     def yoy_q(series: pd.Series) -> float | None:
L473         s = _ensure_series(series)
L474         if s.empty:
L475             return None
L476         s = s.sort_index()
L477         if isinstance(s.index, pd.DatetimeIndex):
L478             last_idx = s.index[-1]
L479             window_start = last_idx - pd.DateOffset(months=15)
L480             window_end = last_idx - pd.DateOffset(months=9)
L481             candidates = s.loc[(s.index >= window_start) & (s.index <= window_end)]
L482             if candidates.empty:
L483                 candidates = s.loc[s.index <= window_end]
L484             if candidates.empty:
L485                 return None
L486             v1 = candidates.iloc[-1]
L487             v0 = s.iloc[-1]
L488         else:
L489             if s.size < 5:
L490                 return None
L491             v0 = s.iloc[-1]
L492             v1 = s.iloc[-5]
L493         pair = _valid_ratio(v0, v1)
L494         if pair is None:
L495             return None
L496         a, b = pair
L497         return float(a / b - 1.0)
L498
L499     def qoq(series: pd.Series) -> float | None:
L500         s = _ensure_series(series)
L501         if s.size < 2:
L502             return None
L503         s = s.sort_index()
L504         v0, v1 = s.iloc[-1], s.iloc[-2]
L505         pair = _valid_ratio(v0, v1)
L506         if pair is None:
L507             return None
L508         a, b = pair
L509         return float(a / b - 1.0)
L510
L511     def ttm_delta(series: pd.Series) -> float | None:
L512         s = _ensure_series(series)
L513         if s.size < 2:
L514             return None
L515         s = s.sort_index()
L516         k = int(min(4, s.size))
L517         cur_slice = s.iloc[-k:]
L518         prev_slice = s.iloc[:-k]
L519         if prev_slice.empty:
L520             return None
L521         prev_k = int(min(k, prev_slice.size))
L522         cur_sum = float(cur_slice.sum())
L523         prev_sum = float(prev_slice.iloc[-prev_k:].sum())
L524         pair = _valid_ratio(cur_sum, prev_sum)
L525         if pair is None:
L526             return None
L527         a, b = pair
L528         return float(a / b - 1.0)
L529
L530     def yoy_y(series: pd.Series) -> float | None:
L531         s = _ensure_series(series)
L532         if s.size < 2:
L533             return None
L534         s = s.sort_index()
L535         v0, v1 = s.iloc[-1], s.iloc[-2]
L536         pair = _valid_ratio(v0, v1)
L537         if pair is None:
L538             return None
L539         a, b = pair
L540         return float(a / b - 1.0)
L541
L542     def price_proxy_growth() -> float | None:
L543         if not isinstance(close_series, pd.Series):
L544             return None
L545         close = close_series.sort_index().dropna()
L546         if close.empty:
L547             return None
L548         hh_window = int(min(126, len(close)))
L549         if hh_window < 20:
L550             return None
L551         hh = close.rolling(hh_window).max().iloc[-1]
L552         prox = None
L553         if np.isfinite(hh) and hh > 0:
L554             prox = float(close.iloc[-1] / hh)
L555         rs6 = None
L556         if len(close) >= 63:
L557             rs6 = float(close.pct_change(63).iloc[-1])
L558         rs12 = None
L559         if len(close) >= 126:
L560             rs12 = float(close.pct_change(126).iloc[-1])
L561         vexp = None
L562         if isinstance(volume_series, pd.Series):
L563             vol = volume_series.reindex(close.index).dropna()
L564             if len(vol) >= 50:
L565                 v20 = vol.rolling(20).mean().iloc[-1]
L566                 v50 = vol.rolling(50).mean().iloc[-1]
L567                 if np.isfinite(v20) and np.isfinite(v50) and v50 > 0:
L568                     vexp = float(v20 / v50 - 1.0)
L569         prox = 0.0 if prox is None or not np.isfinite(prox) else prox
L570         rs6 = 0.0 if rs6 is None or not np.isfinite(rs6) else rs6
L571         rs12 = 0.0 if rs12 is None or not np.isfinite(rs12) else rs12
L572         vexp = 0.0 if vexp is None or not np.isfinite(vexp) else vexp
L573         return 0.5 * prox + 0.3 * rs6 + 0.2 * rs12 + 0.2 * vexp
L574
L575     price_alt = price_proxy_growth() or 0.0
L576     core = 0.0
L577     core_raw = 0.0
L578     price_raw = price_alt
L579
L580     if nQ >= 5:
L581         path = "P5"
L582         yq = yoy_q(s_rev_q)
L583         parts["rev_yoy_q"] = yq
L584         tmp_prev = s_rev_q.iloc[:-1] if s_rev_q.size > 1 else s_rev_q
L585         acc = None
L586         if tmp_prev.size >= 5 and yq is not None:
L587             yq_prev = yoy_q(tmp_prev)
L588             if yq_prev is not None:
L589                 acc = float(yq - yq_prev)
L590         parts["rev_acc_q"] = acc
L591         eps_yoy = yoy_q(s_eps_q) if s_eps_q.size >= 5 else None
L592         parts["eps_yoy_q"] = eps_yoy
L593         eps_acc = None
L594         if eps_yoy is not None and s_eps_q.size > 5:
L595             eps_prev = s_eps_q.iloc[:-1]
L596             if eps_prev.size >= 5:
L597                 eps_prev_yoy = yoy_q(eps_prev)
L598                 if eps_prev_yoy is not None:
L599                     eps_acc = float(eps_yoy - eps_prev_yoy)
L600         parts["eps_acc_q"] = eps_acc
L601         w = 1.0
L602         core_raw = (
L603             0.60 * _nz(yq)
L604             + 0.20 * _nz(acc)
L605             + 0.15 * _nz(eps_yoy)
L606             + 0.05 * _nz(eps_acc)
L607         )
L608         price_alt = 0.0
L609     elif 2 <= nQ <= 4:
L610         path = "P24"
L611         rev_qoq = qoq(s_rev_q)
L612         rev_ttm2 = ttm_delta(s_rev_q)
L613         parts["rev_qoq"] = rev_qoq
L614         parts["rev_ttm2"] = rev_ttm2
L615         eps_qoq = qoq(s_eps_q) if s_eps_q.size >= 2 else None
L616         parts["eps_qoq"] = eps_qoq
L617         w = min(1.0, nQ / 5.0)
L618         core_raw = 0.6 * _nz(rev_qoq) + 0.3 * _nz(rev_ttm2) + 0.1 * _nz(eps_qoq)
L619     else:
L620         path = "P1Y"
L621         rev_yoy_y = yoy_y(s_rev_y) if nY >= 2 else None
L622         parts["rev_yoy_y"] = rev_yoy_y
L623         w = 0.6 * min(1.0, nY / 3.0) if nY >= 2 else 0.4
L624         core_raw = _nz(rev_yoy_y)
L625         if nQ <= 1 and nY < 2 and price_alt == 0.0:
L626             price_alt = price_proxy_growth() or 0.0
L627
L628     core = _winsor(core_raw, lo=-1.5, hi=1.5)
L629     price_alt = _winsor(price_alt, lo=-1.5, hi=1.5)
L630     grw = _winsor(w * core + (1.0 - w) * (0.5 * _nz(price_alt)), lo=-2.0, hi=2.0)
L631
L632     parts.update(
L633         {
L634             "core_raw": core_raw,
L635             "core": core,
L636             "price_proxy_raw": price_raw,
L637             "price_proxy": price_alt,
L638             "weight": w,
L639             "score": grw,
L640         }
L641     )
L642
L643     parts_out: dict[str, Any] = {
L644         "nQ": nQ,
L645         "nY": nY,
L646     }
L647     for key, value in parts.items():
L648         if key in ("nQ", "nY"):
L649             continue
L650         rounded = _round_debug(value)
L651         parts_out[key] = rounded
L652
L653     info_entry["DEBUG_GRW_PATH"] = path
L654     info_entry["DEBUG_GRW_PARTS"] = json.dumps(parts_out, ensure_ascii=False, sort_keys=True)
L655     info_entry["GRW_SCORE"] = grw
L656     info_entry["GRW_WEIGHT"] = w
L657     info_entry["GRW_CORE"] = core
L658     info_entry["GRW_PRICE_PROXY"] = price_alt
L659
L660     return {
L661         "score": grw,
L662         "path": path,
L663         "parts": info_entry["DEBUG_GRW_PARTS"],
L664         "weight": w,
L665         "core": core,
L666         "price_proxy": price_alt,
L667     }
L668
L669
L670 D_WEIGHTS_EFF = None  # å‡ºåŠ›è¡¨ç¤ºäº’æ›ã®ãŸã‚
L671
L672
L673 def _scalar(v):
L674     """å˜ä¸€ã‚»ãƒ«ä»£å…¥ç”¨ã«å€¤ã‚’ã‚¹ã‚«ãƒ©ãƒ¼ã¸æ­£è¦åŒ–ã™ã‚‹ã€‚
L675
L676     - pandas Series -> .iloc[-1]ï¼ˆæœ€å¾Œã‚’æ¡ç”¨ï¼‰
L677     - list/tuple/ndarray -> æœ€å¾Œã®è¦ç´ 
L678     - ãã‚Œä»¥å¤–          -> ãã®ã¾ã¾
L679     å–å¾—å¤±æ•—æ™‚ã¯ np.nan ã‚’è¿”ã™ã€‚
L680     """
L681     import numpy as _np
L682     import pandas as _pd
L683     try:
L684         if isinstance(v, _pd.Series):
L685             return v.iloc[-1] if len(v) else _np.nan
L686         if isinstance(v, (list, tuple, _np.ndarray)):
L687             return v[-1] if len(v) else _np.nan
L688         return v
L689     except Exception:
L690         return _np.nan
L691
L692
L693 # ---- Scorer æœ¬ä½“ -------------------------------------------------------------
L694 class Scorer:
L695     """
L696     - factor.py ã‹ã‚‰ã¯ `aggregate_scores(ib, cfg)` ã‚’å‘¼ã¶ã ã‘ã§OKã€‚
L697     - cfg ã¯å¿…é ˆï¼ˆfactor.PipelineConfig ã‚’æ¸¡ã™ï¼‰ã€‚
L698     - æ—§ã‚«ãƒ©ãƒ åã‚’è‡ªå‹•ãƒªãƒãƒ¼ãƒ ã—ã¦æ–°ã‚¹ã‚­ãƒ¼ãƒã«å¸åã—ã¾ã™ã€‚
L699     """
L700
L701     # === å…ˆé ­ã§æ—§â†’æ–°ã‚«ãƒ©ãƒ åãƒãƒƒãƒ—ï¼ˆç§»è¡Œç”¨ï¼‰ ===
L702     EPS_RENAME = {"eps_ttm":"EPS_TTM", "eps_q_recent":"EPS_Q_LastQ"}
L703     FCF_RENAME = {"fcf_ttm":"FCF_TTM"}
L704
L705     # === ã‚¹ã‚­ãƒ¼ãƒç°¡æ˜“ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€ä½é™ï¼‰ ===
L706     @staticmethod
L707     def _validate_ib_for_scorer(ib: Any):
L708         miss = [a for a in ["tickers","bench","data","px","spx","tickers_bulk","info","eps_df","fcf_df","returns"] if not hasattr(ib,a) or getattr(ib,a) is None]
L709         if miss: raise ValueError(f"InputBundle is missing required attributes for Scorer: {miss}")
L710         if any(c in ib.eps_df.columns for c in Scorer.EPS_RENAME): ib.eps_df.rename(columns=Scorer.EPS_RENAME, inplace=True)
L711         if any(c in ib.fcf_df.columns for c in Scorer.FCF_RENAME): ib.fcf_df.rename(columns=Scorer.FCF_RENAME, inplace=True)
L712         need_eps, need_fcf = {"EPS_TTM","EPS_Q_LastQ"},{"FCF_TTM"}
L713         if not need_eps.issubset(ib.eps_df.columns): raise ValueError(f"eps_df must contain columns {need_eps} (accepts old names via auto-rename). Got: {list(ib.eps_df.columns)}")
L714         if not need_fcf.issubset(ib.fcf_df.columns): raise ValueError(f"fcf_df must contain columns {need_fcf} (accepts old names via auto-rename). Got: {list(ib.fcf_df.columns)}")
L715
L716     # ----ï¼ˆScorerå°‚ç”¨ï¼‰ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ»æŒ‡æ¨™ç³» ----
L717     @staticmethod
L718     def trend(s: pd.Series):
L719         if len(s)<200: return np.nan
L720         sma50, sma150, sma200 = s.rolling(50).mean().iloc[-1], s.rolling(150).mean().iloc[-1], s.rolling(200).mean().iloc[-1]
L721         prev200, p = s.rolling(200).mean().iloc[-21], s.iloc[-1]
L722         lo_52 = s[-252:].min() if len(s)>=252 else s.min(); hi_52 = s[-252:].max() if len(s)>=252 else s.max()
L723         rng = (hi_52 - lo_52) if hi_52>lo_52 else np.nan
L724         clip = lambda x,lo,hi: (np.nan if pd.isna(x) else max(lo,min(hi,x)))
L725         a = clip(p/(s.rolling(50).mean().iloc[-1]) - 1, -0.5, 0.5)
L726         b = clip(sma50/sma150 - 1, -0.5, 0.5)
L727         c = clip(sma150/sma200 - 1, -0.5, 0.5)
L728         d = clip(sma200/prev200 - 1, -0.2, 0.2)
L729         e = clip((p - lo_52) / (rng if rng and rng>0 else np.nan) - 0.5, -0.5, 0.5)
L730         parts = [0.0 if pd.isna(x) else x for x in (a,b,c,d,e)]
L731         return 0.30*parts[0] + 0.20*parts[1] + 0.15*parts[2] + 0.15*parts[3] + 0.20*parts[4]
L732
L733     @staticmethod
L734     def rs(s, b):
L735         n, nb = len(s), len(b)
L736         if n<60 or nb<60: return np.nan
L737         L12 = 252 if n>=252 and nb>=252 else min(n,nb)-1; L1 = 22 if n>=22 and nb>=22 else max(5, min(n,nb)//3)
L738         r12, r1, br12, br1 = s.iloc[-1]/s.iloc[-L12]-1, s.iloc[-1]/s.iloc[-L1]-1, b.iloc[-1]/b.iloc[-L12]-1, b.iloc[-1]/b.iloc[-L1]-1
L739         return (r12 - br12)*0.7 + (r1 - br1)*0.3
L740
L741     @staticmethod
L742     def tr_str(s):
L743         if s is None:
L744             return np.nan
L745         s = s.ffill(limit=2).dropna()
L746         if len(s) < 50:
L747             return np.nan
L748         ma50 = s.rolling(50, min_periods=50).mean()
L749         last_ma = ma50.iloc[-1]
L750         last_px = s.iloc[-1]
L751         return float(last_px/last_ma - 1.0) if pd.notna(last_ma) and pd.notna(last_px) else np.nan
L752
L753     @staticmethod
L754     def rs_line_slope(s: pd.Series, b: pd.Series, win: int) -> float:
L755         r = (s/b).dropna()
L756         if len(r) < win: return np.nan
L757         y, x = np.log(r.iloc[-win:]), np.arange(win, dtype=float)
L758         try: return float(np.polyfit(x, y, 1)[0])
L759         except Exception: return np.nan
L760
L761     @staticmethod
L762     def ev_fallback(info_t: dict, tk: yf.Ticker) -> float:
L763         ev = info_t.get('enterpriseValue', np.nan)
L764         if pd.notna(ev) and ev>0: return float(ev)
L765         mc, debt, cash = info_t.get('marketCap', np.nan), np.nan, np.nan
L766         try:
L767             bs = tk.quarterly_balance_sheet
L768             if bs is not None and not bs.empty:
L769                 c = bs.columns[0]
L770                 for k in ("Total Debt","Long Term Debt","Short Long Term Debt"):
L771                     if k in bs.index: debt = float(bs.loc[k,c]); break
L772                 for k in ("Cash And Cash Equivalents","Cash And Cash Equivalents And Short Term Investments","Cash"):
L773                     if k in bs.index: cash = float(bs.loc[k,c]); break
L774         except Exception: pass
L775         if pd.notna(mc): return float(mc + (0 if pd.isna(debt) else debt) - (0 if pd.isna(cash) else cash))
L776         return np.nan
L777
L778     @staticmethod
L779     def dividend_status(ticker: str) -> str:
L780         t = yf.Ticker(ticker)
L781         try:
L782             if not t.dividends.empty: return "has"
L783         except Exception: return "unknown"
L784         try:
L785             a = t.actions
L786             if (a is not None and not a.empty and "Stock Splits" in a.columns and a["Stock Splits"].abs().sum()>0): return "none_confident"
L787         except Exception: pass
L788         try:
L789             fi = t.fast_info
L790             if any(getattr(fi,k,None) for k in ("last_dividend_date","dividend_rate","dividend_yield")): return "maybe_missing"
L791         except Exception: pass
L792         return "unknown"
L793
L794     @staticmethod
L795     def div_streak(t):
L796         try:
L797             divs = yf.Ticker(t).dividends.dropna(); ann = divs.groupby(divs.index.year).sum(); ann = ann[ann.index<pd.Timestamp.today().year]
L798             years, streak = sorted(ann.index), 0
L799             for i in range(len(years)-1,0,-1):
L800                 if ann[years[i]] > ann[years[i-1]]: streak += 1
L801                 else: break
L802             return streak
L803         except Exception: return 0
L804
L805     @staticmethod
L806     def fetch_finnhub_metrics(symbol):
L807         api_key = os.environ.get("FINNHUB_API_KEY")
L808         if not api_key: return {}
L809         url, params = "https://finnhub.io/api/v1/stock/metric", {"symbol":symbol,"metric":"all","token":api_key}
L810         try:
L811             r = requests.get(url, params=params, timeout=10); r.raise_for_status(); m = r.json().get("metric",{})
L812             return {'EPS':m.get('epsGrowthTTMYoy'),'REV':m.get('revenueGrowthTTMYoy'),'ROE':m.get('roeTTM'),'BETA':m.get('beta'),'DIV':m.get('dividendYieldIndicatedAnnual'),'FCF':(m.get('freeCashFlowTTM')/m.get('enterpriseValue')) if m.get('freeCashFlowTTM') and m.get('enterpriseValue') else None}
L813         except Exception: return {}
L814
L815     @staticmethod
L816     def calc_beta(series: pd.Series, market: pd.Series, lookback=252):
L817         r, m = series.pct_change().dropna(), market.pct_change().dropna()
L818         n = min(len(r), len(m), lookback)
L819         if n<60: return np.nan
L820         r, m = r.iloc[-n:], m.iloc[-n:]; cov, var = np.cov(r, m)[0,1], np.var(m)
L821         return np.nan if var==0 else cov/var
L822
L823     @staticmethod
L824     def spx_to_alpha(spx: pd.Series, bands=(0.03,0.10), w=(0.6,0.4),
L825                      span=5, q=(0.20,0.40), alphas=(0.05,0.08,0.10)) -> float:
L826         """
L827         S&P500æŒ‡æ•°ã®ã¿ã‹ã‚‰æ“¬ä¼¼breadthã‚’ä½œã‚Šã€å±¥æ­´åˆ†ä½ã§Î±ã‚’æ®µéšæ±ºå®šã€‚
L828         bands=(Â±3%, Â±10%), w=(50DMA,200DMA), åˆ†ä½q=(20%,40%), alphas=(ä½,ä¸­,é«˜)
L829         """
L830         ma50, ma200 = spx.rolling(50).mean(), spx.rolling(200).mean()
L831         b50, b200 = ((spx/ma50 - 1)+bands[0])/(2*bands[0]), ((spx/ma200 - 1)+bands[1])/(2*bands[1])
L832         hist = (w[0]*b50 + w[1]*b200).clip(0,1).ewm(span=span).mean()
L833         b, (lo, mid) = float(hist.iloc[-1]), (float(hist.quantile(q[0])), float(hist.quantile(q[1])))
L834         return alphas[0] if b < lo else alphas[1] if b < mid else alphas[2]
L835
L836     @staticmethod
L837     def soft_cap_effective_scores(scores: pd.Series|dict, sectors: dict, cap=2, alpha=0.08) -> pd.Series:
L838         """
L839         åŒä¸€ã‚»ã‚¯ã‚¿ãƒ¼capè¶…éï¼ˆ3æœ¬ç›®ä»¥é™ï¼‰ã« Î±Ã—æ®µéšæ¸›ç‚¹ã‚’èª²ã—ãŸâ€œæœ‰åŠ¹ã‚¹ã‚³ã‚¢â€Seriesã‚’è¿”ã™ã€‚
L840         æˆ»ã‚Šå€¤ã¯é™é †ã‚½ãƒ¼ãƒˆæ¸ˆã¿ã€‚
L841         """
L842         s = pd.Series(scores, dtype=float); order = s.sort_values(ascending=False).index
L843         cnt, pen = {}, {}
L844         for t in order:
L845             sec = sectors.get(t, "U"); cnt[sec] = cnt.get(sec,0) + 1; pen[t] = alpha*max(0, cnt[sec]-cap)
L846         return (s - pd.Series(pen)).sort_values(ascending=False)
L847
L848     @staticmethod
L849     def pick_top_softcap(scores: pd.Series|dict, sectors: dict, N: int, cap=2, alpha=0.08, hard: int|None=5) -> list[str]:
L850         """
L851         soft-capé©ç”¨å¾Œã®ä¸Šä½Nãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚’è¿”ã™ã€‚hard>0ãªã‚‰éå¸¸ç”¨ãƒãƒ¼ãƒ‰ä¸Šé™ã§åŒä¸€ã‚»ã‚¯ã‚¿ãƒ¼è¶…éã‚’é–“å¼•ãï¼ˆæ—¢å®š=5ï¼‰ã€‚
L852         """
L853         eff = Scorer.soft_cap_effective_scores(scores, sectors, cap, alpha)
L854         if not hard:
L855             return list(eff.head(N).index)
L856         pick, used = [], {}
L857         for t in eff.index:
L858             s = sectors.get(t, "U")
L859             if used.get(s,0) < hard:
L860                 pick.append(t); used[s] = used.get(s,0) + 1
L861             if len(pick) == N: break
L862         return pick
L863
L864     @staticmethod
L865     def trend_template_breadth_series(px: pd.DataFrame, spx: pd.Series, win_days: int | None = None) -> pd.Series:
L866         """
L867         å„å–¶æ¥­æ—¥ã® trend_template åˆæ ¼æœ¬æ•°ï¼ˆåˆæ ¼â€œæœ¬æ•°â€=Cï¼‰ã‚’è¿”ã™ã€‚
L868         - px: åˆ—=tickerï¼ˆãƒ™ãƒ³ãƒã¯å«ã‚ãªã„ï¼‰
L869         - spx: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ Seriesï¼ˆpx.index ã«æ•´åˆ—ï¼‰
L870         - win_days: æœ«å°¾ã®è¨ˆç®—å¯¾è±¡å–¶æ¥­æ—¥æ•°ï¼ˆNoneâ†’å…¨ä½“ã€æ—¢å®š600ã¯å‘¼ã³å‡ºã—å´æŒ‡å®šï¼‰
L871         ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼†rollingã®ã¿ã§è»½é‡ã€‚æ¬ æã¯ False æ‰±ã„ã€‚
L872         """
L873         import numpy as np, pandas as pd
L874         if px is None or px.empty:
L875             return pd.Series(dtype=int)
L876         px = px.dropna(how="all", axis=1)
L877         if win_days and win_days > 0:
L878             px = px.tail(win_days)
L879         if px.empty:
L880             return pd.Series(dtype=int)
L881         spx = spx.reindex(px.index).ffill()
L882
L883         ma50  = px.rolling(50).mean()
L884         ma150 = px.rolling(150).mean()
L885         ma200 = px.rolling(200).mean()
L886
L887         tt = (px > ma150)
L888         tt &= (px > ma200)
L889         tt &= (ma150 > ma200)
L890         tt &= (ma200 - ma200.shift(21) > 0)
L891         tt &= (ma50  > ma150)
L892         tt &= (ma50  > ma200)
L893         tt &= (px    > ma50)
L894
L895         lo252 = px.rolling(252).min()
L896         hi252 = px.rolling(252).max()
L897         tt &= (px.divide(lo252).sub(1.0) >= 0.30)   # P_OVER_LOW52 >= 0.30
L898         tt &= (px >= (0.75 * hi252))                # NEAR_52W_HIGH >= -0.25
L899
L900         r12  = px.divide(px.shift(252)).sub(1.0)
L901         br12 = spx.divide(spx.shift(252)).sub(1.0)
L902         r1   = px.divide(px.shift(22)).sub(1.0)
L903         br1  = spx.divide(spx.shift(22)).sub(1.0)
L904         rs   = 0.7*(r12.sub(br12, axis=0)) + 0.3*(r1.sub(br1, axis=0))
L905         tt &= (rs >= 0.10)
L906
L907         return tt.fillna(False).sum(axis=1).astype(int)
L908
L909     # ---- ã‚¹ã‚³ã‚¢é›†è¨ˆï¼ˆDTO/Configã‚’å—ã‘å–ã‚Šã€FeatureBundleã‚’è¿”ã™ï¼‰ ----
L910     def aggregate_scores(self, ib: Any, cfg):
L911         if cfg is None:
L912             raise ValueError("cfg is required; pass factor.PipelineConfig")
L913         self._validate_ib_for_scorer(ib)
L914
L915         px, spx, tickers = ib.px, ib.spx, ib.tickers
L916         tickers_bulk, info, eps_df, fcf_df = ib.tickers_bulk, ib.info, ib.eps_df, ib.fcf_df
L917
L918         df, missing_logs = pd.DataFrame(index=tickers), []
L919         for t in tickers:
L920             d, s = info[t], px[t]; ev = self.ev_fallback(d, tickers_bulk.tickers[t])
L921             try:
L922                 volume_series_full = ib.data['Volume'][t]
L923             except Exception:
L924                 volume_series_full = None
L925
L926             grw_result = _calc_grw_flexible(t, d, s, volume_series_full)
L927             _grw_record_to_df(t, d, df)
L928             df.loc[t,'GRW_FLEX_SCORE'] = grw_result.get('score')
L929             df.loc[t,'GRW_FLEX_WEIGHT'] = grw_result.get('weight')
L930             df.loc[t,'GRW_FLEX_CORE'] = grw_result.get('core')
L931             df.loc[t,'GRW_FLEX_PRICE'] = grw_result.get('price_proxy')
L932             df.loc[t,'DEBUG_GRW_PATH'] = grw_result.get('path')
L933             df.loc[t,'DEBUG_GRW_PARTS'] = grw_result.get('parts')
L934
L935             # --- åŸºæœ¬ç‰¹å¾´ ---
L936             df.loc[t,'TR']   = self.trend(s)
L937             df.loc[t,'EPS']  = _scalar(eps_df.loc[t,'EPS_TTM']) if t in eps_df.index else np.nan
L938             df.loc[t,'EPS_Q'] = _scalar(eps_df.loc[t,'EPS_Q_LastQ']) if t in eps_df.index else np.nan
L939             df.loc[t,'REV_TTM'] = _scalar(eps_df.loc[t,'REV_TTM']) if t in eps_df.index else np.nan
L940             df.loc[t,'REV_Q']   = _scalar(eps_df.loc[t,'REV_Q_LastQ']) if t in eps_df.index else np.nan
L941             df.loc[t,'REV']  = d.get('revenueGrowth',np.nan)
L942             df.loc[t,'ROE']  = d.get('returnOnEquity',np.nan)
L943             df.loc[t,'BETA'] = self.calc_beta(s, spx, lookback=252)
L944
L945             # --- é…å½“ï¼ˆæ¬ æè£œå®Œå«ã‚€ï¼‰ ---
L946             div = d.get('dividendYield') if d.get('dividendYield') is not None else d.get('trailingAnnualDividendYield')
L947             if div is None or pd.isna(div):
L948                 try:
L949                     divs = yf.Ticker(t).dividends
L950                     if divs is not None and not divs.empty:
L951                         last_close = s.iloc[-1]; div_1y = divs[divs.index >= (divs.index.max() - pd.Timedelta(days=365))].sum()
L952                         if last_close and last_close>0: div = float(div_1y/last_close)
L953                 except Exception: pass
L954             df.loc[t,'DIV'] = 0.0 if (div is None or pd.isna(div)) else float(div)
L955
L956             # --- FCF/EV ---
L957             fcf_val = fcf_df.loc[t,'FCF_TTM'] if t in fcf_df.index else np.nan
L958             df.loc[t,'FCF'] = (fcf_val/ev) if (pd.notna(fcf_val) and pd.notna(ev) and ev>0) else np.nan
L959
L960             # --- ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ãƒ»ãƒœãƒ©é–¢é€£ ---
L961             df.loc[t,'RS'], df.loc[t,'TR_str'] = self.rs(s, spx), self.tr_str(s)
L962             r, rm = s.pct_change().dropna(), spx.pct_change().dropna()
L963             n = int(min(len(r), len(rm)))
L964
L965             DOWNSIDE_DEV = np.nan
L966             if n>=60:
L967                 r6 = r.iloc[-min(len(r),126):]; neg = r6[r6<0]
L968                 if len(neg)>=10: DOWNSIDE_DEV = float(neg.std(ddof=0)*np.sqrt(252))
L969             df.loc[t,'DOWNSIDE_DEV'] = DOWNSIDE_DEV
L970
L971             MDD_1Y = np.nan
L972             try:
L973                 w = s.iloc[-min(len(s),252):].dropna()
L974                 if len(w)>=30:
L975                     roll_max = w.cummax(); MDD_1Y = float((w/roll_max - 1.0).min())
L976             except Exception: pass
L977             df.loc[t,'MDD_1Y'] = MDD_1Y
L978
L979             RESID_VOL = np.nan
L980             if n>=120:
L981                 rr, rrm = r.iloc[-n:].align(rm.iloc[-n:], join='inner')
L982                 if len(rr)==len(rrm) and len(rr)>=120 and rrm.var()>0:
L983                     beta = float(np.cov(rr, rrm)[0,1]/np.var(rrm)); resid = rr - beta*rrm
L984                     RESID_VOL = float(resid.std(ddof=0)*np.sqrt(252))
L985             df.loc[t,'RESID_VOL'] = RESID_VOL
L986
L987             DOWN_OUTPERF = np.nan
L988             if n>=60:
L989                 m, x = rm.iloc[-n:], r.iloc[-n:]; mask = m<0
L990                 if mask.sum()>=10:
L991                     mr, sr = float(m[mask].mean()), float(x[mask].mean())
L992                     DOWN_OUTPERF = (sr - mr)/abs(mr) if mr!=0 else np.nan
L993             df.loc[t,'DOWN_OUTPERF'] = DOWN_OUTPERF
L994
L995             # --- é•·æœŸç§»å‹•å¹³å‡/ä½ç½® ---
L996             sma200 = s.rolling(200).mean(); df.loc[t,'EXT_200'] = np.nan
L997             if pd.notna(sma200.iloc[-1]) and sma200.iloc[-1]!=0: df.loc[t,'EXT_200'] = abs(float(s.iloc[-1]/sma200.iloc[-1]-1.0))
L998
L999             # --- é…å½“ã®è©³ç´°ç³» ---
L1000             DIV_TTM_PS=DIV_VAR5=DIV_YOY=DIV_FCF_COVER=np.nan
L1001             try:
L1002                 divs = yf.Ticker(t).dividends.dropna()
L1003                 if not divs.empty:
L1004                     last_close = s.iloc[-1]; div_1y = float(divs[divs.index >= (divs.index.max()-pd.Timedelta(days=365))].sum())
L1005                     DIV_TTM_PS = div_1y if div_1y>0 else np.nan
L1006                     ann = divs.groupby(divs.index.year).sum()
L1007                     if len(ann)>=2 and ann.iloc[-2]!=0: DIV_YOY = float(ann.iloc[-1]/ann.iloc[-2]-1.0)
L1008                     tail = ann.iloc[-5:] if len(ann)>=5 else ann
L1009                     if len(tail)>=3 and tail.mean()!=0: DIV_VAR5 = float(tail.std(ddof=1)/abs(tail.mean()))
L1010                 so = d.get('sharesOutstanding',None)
L1011                 if so and pd.notna(DIV_TTM_PS) and pd.notna(fcf_val) and fcf_val!=0:
L1012                     DIV_FCF_COVER = float((fcf_val)/(DIV_TTM_PS*float(so)))
L1013             except Exception: pass
L1014             df.loc[t,'DIV_TTM_PS'], df.loc[t,'DIV_VAR5'], df.loc[t,'DIV_YOY'], df.loc[t,'DIV_FCF_COVER'] = DIV_TTM_PS, DIV_VAR5, DIV_YOY, DIV_FCF_COVER
L1015
L1016             # --- è²¡å‹™å®‰å®šæ€§ ---
L1017             df.loc[t,'DEBT2EQ'], df.loc[t,'CURR_RATIO'] = d.get('debtToEquity',np.nan), d.get('currentRatio',np.nan)
L1018
L1019             # --- EPS å¤‰å‹• ---
L1020             EPS_VAR_8Q = np.nan
L1021             try:
L1022                 qe, so = tickers_bulk.tickers[t].quarterly_earnings, d.get('sharesOutstanding',None)
L1023                 if qe is not None and not qe.empty and so:
L1024                     eps_q = (qe['Earnings'].dropna().astype(float)/float(so)).replace([np.inf,-np.inf],np.nan)
L1025                     if len(eps_q)>=4: EPS_VAR_8Q = float(eps_q.iloc[-min(8,len(eps_q)):].std(ddof=1))
L1026             except Exception: pass
L1027             df.loc[t,'EPS_VAR_8Q'] = EPS_VAR_8Q
L1028
L1029             # --- ã‚µã‚¤ã‚º/æµå‹•æ€§ ---
L1030             df.loc[t,'MARKET_CAP'] = d.get('marketCap',np.nan); adv60 = np.nan
L1031             try:
L1032                 if isinstance(volume_series_full, pd.Series):
L1033                     vol_series = volume_series_full.reindex(s.index).dropna()
L1034                     if len(vol_series) >= 5:
L1035                         aligned_px = s.reindex(vol_series.index).dropna()
L1036                         if len(aligned_px) == len(vol_series):
L1037                             dv = (vol_series*aligned_px).rolling(60).mean()
L1038                             if not dv.dropna().empty:
L1039                                 adv60 = float(dv.dropna().iloc[-1])
L1040             except Exception:
L1041                 pass
L1042             df.loc[t,'ADV60_USD'] = adv60
L1043
L1044             # --- Rule of 40 ã‚„å‘¨è¾º ---
L1045             total_rev_ttm = d.get('totalRevenue',np.nan)
L1046             FCF_MGN = _safe_div(fcf_val, total_rev_ttm)
L1047             df.loc[t,'FCF_MGN'] = FCF_MGN
L1048             rule40 = np.nan
L1049             try:
L1050                 r = df.loc[t,'REV']; rule40 = (r if pd.notna(r) else np.nan) + (FCF_MGN if pd.notna(FCF_MGN) else np.nan)
L1051             except Exception: pass
L1052             df.loc[t,'RULE40'] = rule40
L1053
L1054             # --- ãƒˆãƒ¬ãƒ³ãƒ‰è£œåŠ© ---
L1055             sma50  = s.rolling(50).mean()
L1056             sma150 = s.rolling(150).mean()
L1057             sma200 = s.rolling(200).mean()
L1058             p = _safe_last(s)
L1059
L1060             df.loc[t,'MA50_OVER_150'] = (_safe_last(sma50)/_safe_last(sma150) - 1
L1061                 if pd.notna(_safe_last(sma50)) and pd.notna(_safe_last(sma150)) and _safe_last(sma150)!=0 else np.nan)
L1062             df.loc[t,'MA150_OVER_200'] = (_safe_last(sma150)/_safe_last(sma200) - 1
L1063                 if pd.notna(_safe_last(sma150)) and pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan)
L1064
L1065             lo52 = s[-252:].min() if len(s)>=252 else s.min()
L1066             df.loc[t,'P_OVER_LOW52'] = (p/lo52 - 1) if (lo52 and lo52>0 and pd.notna(p)) else np.nan
L1067
L1068             df.loc[t,'MA200_SLOPE_1M'] = np.nan
L1069             if len(sma200.dropna()) >= 21:
L1070                 cur200 = _safe_last(sma200)
L1071                 old2001 = float(sma200.iloc[-21])
L1072                 if old2001:
L1073                     df.loc[t,'MA200_SLOPE_1M'] = cur200/old2001 - 1
L1074
L1075             df.loc[t,'P_OVER_150'] = p/_safe_last(sma150)-1 if pd.notna(_safe_last(sma150)) and _safe_last(sma150)!=0 else np.nan
L1076             df.loc[t,'P_OVER_200'] = p/_safe_last(sma200)-1 if pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L1077             df.loc[t,'MA50_OVER_200'] = _safe_last(sma50)/_safe_last(sma200)-1 if pd.notna(_safe_last(sma50)) and pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L1078             df.loc[t,'MA200_SLOPE_5M'] = np.nan
L1079             if len(sma200.dropna())>=105:
L1080                 cur200, old200 = _safe_last(sma200), float(sma200.iloc[-105])
L1081                 if old200 and old200!=0: df.loc[t,'MA200_SLOPE_5M'] = cur200/old200 - 1
L1082             # NEW: 200æ—¥ç·šãŒé€£ç¶šã§ä¸Šå‘ãã®ã€Œæ—¥æ•°ã€
L1083             df.loc[t,'MA200_UP_STREAK_D'] = np.nan
L1084             try:
L1085                 s200 = sma200.dropna()
L1086                 if len(s200) >= 2:
L1087                     diff200 = s200.diff()
L1088                     up = 0
L1089                     for v in diff200.iloc[::-1]:
L1090                         if pd.isna(v) or v <= 0:
L1091                             break
L1092                         up += 1
L1093                     df.loc[t,'MA200_UP_STREAK_D'] = float(up)
L1094             except Exception:
L1095                 pass
L1096             df.loc[t,'LOW52PCT25_EXCESS'] = np.nan if (lo52 is None or lo52<=0 or pd.isna(p)) else (p/(lo52*1.25)-1)
L1097             hi52 = s[-252:].max() if len(s)>=252 else s.max(); df.loc[t,'NEAR_52W_HIGH'] = np.nan
L1098             if hi52 and hi52>0 and pd.notna(p):
L1099                 d_hi = (p/hi52)-1.0; df.loc[t,'NEAR_52W_HIGH'] = -abs(min(0.0, d_hi))
L1100             df.loc[t,'RS_SLOPE_6W'] = self.rs_line_slope(s, ib.spx, 30)
L1101             df.loc[t,'RS_SLOPE_13W'] = self.rs_line_slope(s, ib.spx, 65)
L1102
L1103             df.loc[t,'DIV_STREAK'] = self.div_streak(t)
L1104
L1105             # --- æ¬ æãƒ¡ãƒ¢ ---
L1106             fin_cols = ['REV','ROE','BETA','DIV','FCF']
L1107             need_finnhub = [col for col in fin_cols if pd.isna(df.loc[t,col])]
L1108             if need_finnhub:
L1109                 fin_data = self.fetch_finnhub_metrics(t)
L1110                 for col in need_finnhub:
L1111                     val = fin_data.get(col)
L1112                     if val is not None and not pd.isna(val): df.loc[t,col] = val
L1113             for col in fin_cols + ['EPS','RS','TR_str','DIV_STREAK']:
L1114                 if pd.isna(df.loc[t,col]):
L1115                     if col=='DIV':
L1116                         status = self.dividend_status(t)
L1117                         if status!='none_confident': missing_logs.append({'Ticker':t,'Column':col,'Status':status})
L1118                     else:
L1119                         missing_logs.append({'Ticker':t,'Column':col})
L1120
L1121         def _pick_series(entry: dict, keys: list[str]):
L1122             for k in keys:
L1123                 val = entry.get(k) if isinstance(entry, dict) else None
L1124                 if val is None:
L1125                     continue
L1126                 try:
L1127                     if hasattr(val, "empty") and getattr(val, "empty"):
L1128                         continue
L1129                 except Exception:
L1130                     pass
L1131                 if isinstance(val, (list, tuple)) and len(val) == 0:
L1132                     continue
L1133                 return val
L1134             return None
L1135
L1136         def _has_sec_series(val) -> bool:
L1137             try:
L1138                 if isinstance(val, pd.Series):
L1139                     return not val.dropna().empty
L1140                 if isinstance(val, (list, tuple)):
L1141                     return any(pd.notna(v) for v in val)
L1142                 return bool(val)
L1143             except Exception:
L1144                 return False
L1145
L1146         def _series_len(val) -> int:
L1147             try:
L1148                 if isinstance(val, pd.Series):
L1149                     return int(val.dropna().size)
L1150                 if isinstance(val, (list, tuple)):
L1151                     return len(val)
L1152                 return int(bool(val))
L1153             except Exception:
L1154                 return 0
L1155
L1156         cnt_rev_series = sum(1 for _t, d in info.items() if _has_sec_series(d.get("SEC_REV_Q_SERIES")))
L1157         cnt_eps_series = sum(1 for _t, d in info.items() if _has_sec_series(d.get("SEC_EPS_Q_SERIES")))
L1158         logger.info(
L1159             "[DERIV] SEC series presence: REV_Q=%d, EPS_Q=%d (universe=%d)",
L1160             cnt_rev_series,
L1161             cnt_eps_series,
L1162             len(info),
L1163         )
L1164
L1165         rev_q_ge5 = 0
L1166         ttm_yoy_avail = 0
L1167         wrote_growth = 0
L1168
L1169         for t in tickers:
L1170             try:
L1171                 d = info.get(t, {}) or {}
L1172                 rev_series = d.get("SEC_REV_Q_SERIES")
L1173                 eps_series = d.get("SEC_EPS_Q_SERIES")
L1174                 fallback_qearn = False
L1175                 try:
L1176                     qe = tickers_bulk.tickers[t].quarterly_earnings
L1177                     fallback_qearn = bool(qe is not None and not getattr(qe, "empty", True))
L1178                 except Exception:
L1179                     qe = None
L1180                 logger.debug(
L1181                     "[DERIV] %s: rev_q_len=%s eps_q_len=%s fallback_qearn=%s",
L1182                     t,
L1183                     _series_len(rev_series),
L1184                     _series_len(eps_series),
L1185                     fallback_qearn,
L1186                 )
L1187
L1188                 r_src = _pick_series(d, ["SEC_REV_Q_SERIES", "rev_q_series_pairs", "rev_q_series"])
L1189                 e_src = _pick_series(d, ["SEC_EPS_Q_SERIES", "eps_q_series_pairs", "eps_q_series"])
L1190                 r_raw = _ensure_series(r_src)
L1191                 e_raw = _ensure_series(e_src)
L1192                 _log("DERIV_SRC", f"{t} rev_raw_len={r_raw.size} eps_raw_len={e_raw.size}")
L1193
L1194                 r_q = _to_quarterly(r_raw)
L1195                 e_q = _to_quarterly(e_raw)
L1196                 _log("DERIV_Q", f"{t} rev_q_len={r_q.size} eps_q_len={e_q.size}")
L1197                 if r_q.size >= 5:
L1198                     rev_q_ge5 += 1
L1199
L1200                 r_yoy_ttm = _ttm_yoy_from_quarterly(r_q)
L1201                 e_yoy_ttm = _ttm_yoy_from_quarterly(e_q)
L1202                 has_ttm = int(not r_yoy_ttm.dropna().empty)
L1203                 ttm_yoy_avail += has_ttm
L1204                 _log("DERIV_TTM", f"{t} rev_ttm_yoy_len={r_yoy_ttm.dropna().size} eps_ttm_yoy_len={e_yoy_ttm.dropna().size}")
L1205
L1206                 def _q_yoy(qs):
L1207                     return np.nan if qs is None or len(qs) < 5 else float(qs.iloc[-1] / qs.iloc[-5] - 1.0)
L1208
L1209                 rev_q_yoy = _q_yoy(r_q)
L1210                 eps_q_yoy = _q_yoy(e_q)
L1211
L1212                 def _annual_from(qs: pd.Series, yoy_ttm: pd.Series):
L1213                     if isinstance(qs.index, pd.DatetimeIndex) and len(qs) >= 8:
L1214                         ann = qs.groupby(qs.index.year).last().pct_change()
L1215                         ann_dn = ann.dropna()
L1216                         if not ann_dn.empty:
L1217                             y = float(ann_dn.iloc[-1])
L1218                             acc = float(ann_dn.tail(3).mean()) if ann_dn.size >= 3 else np.nan
L1219                             var = float(ann_dn.tail(4).var()) if ann_dn.size >= 4 else np.nan
L1220                             return y, acc, var
L1221                     yoy_dn = yoy_ttm.dropna()
L1222                     if yoy_dn.empty:
L1223                         return np.nan, np.nan, np.nan
L1224                     return (
L1225                         float(yoy_dn.iloc[-1]),
L1226                         float(yoy_dn.tail(3).mean() if yoy_dn.size >= 3 else np.nan),
L1227                         float(yoy_dn.tail(4).var() if yoy_dn.size >= 4 else np.nan),
L1228                     )
L1229
L1230                 rev_yoy, rev_acc, rev_var = _annual_from(r_q, r_yoy_ttm)
L1231                 eps_yoy, _, _ = _annual_from(e_q, e_yoy_ttm)
L1232
L1233                 def _pos_streak(s: pd.Series):
L1234                     s = s.dropna()
L1235                     if s.empty:
L1236                         return np.nan
L1237                     b = (s > 0).astype(int).to_numpy()[::-1]
L1238                     k = 0
L1239                     for v in b:
L1240                         if v == 1:
L1241                             k += 1
L1242                         else:
L1243                             break
L1244                     return float(k)
L1245
L1246                 rev_ann_streak = _pos_streak(r_yoy_ttm)
L1247
L1248                 df.loc[t, "REV_Q_YOY"] = rev_q_yoy
L1249                 df.loc[t, "EPS_Q_YOY"] = eps_q_yoy
L1250                 df.loc[t, "REV_YOY"] = rev_yoy
L1251                 df.loc[t, "EPS_YOY"] = eps_yoy
L1252                 df.loc[t, "REV_YOY_ACC"] = rev_acc
L1253                 df.loc[t, "REV_YOY_VAR"] = rev_var
L1254                 df.loc[t, "REV_ANN_STREAK"] = rev_ann_streak
L1255
L1256                 wrote_growth += 1
L1257                 _log(
L1258                     "DERIV_WRITE",
L1259                     f"{t} wrote: Q_YOY(rev={rev_q_yoy}, eps={eps_q_yoy}) ANN(rev_yoy={rev_yoy}, acc={rev_acc}, var={rev_var}) streak={rev_ann_streak}",
L1260                 )
L1261
L1262             except Exception as e:
L1263                 logger.warning("[DERIV_WARN] %s growth-derivatives failed: %s", t, e)
L1264                 _log("DERIV_WARN", f"{t} {type(e).__name__}: {e}")
L1265
L1266         _log("DERIV_SUMMARY", f"rev_q_len>=5: {rev_q_ge5}/{len(tickers)}  ttm_yoy_available: {ttm_yoy_avail}  wrote_growth_for: {wrote_growth}")
L1267
L1268         try:
L1269             cols = [
L1270                 "REV_Q_YOY",
L1271                 "EPS_Q_YOY",
L1272                 "REV_YOY",
L1273                 "EPS_YOY",
L1274                 "REV_YOY_ACC",
L1275                 "REV_YOY_VAR",
L1276                 "REV_ANN_STREAK",
L1277             ]
L1278             cnt = {c: int(df[c].count()) for c in cols if c in df.columns}
L1279             _log("DERIV_NONNAN_COUNTS", str(cnt))
L1280         except Exception as e:
L1281             _log("DERIV_NONNAN_COUNTS", f"error: {e}")
L1282
L1283         def _trend_template_pass(row, rs_alpha_thresh=0.10):
L1284             c1 = (row.get('P_OVER_150', np.nan) > 0) and (row.get('P_OVER_200', np.nan) > 0)
L1285             c2 = (row.get('MA150_OVER_200', np.nan) > 0)
L1286             c3 = (row.get('MA200_SLOPE_1M', np.nan) > 0)
L1287             c4 = (row.get('MA50_OVER_150', np.nan) > 0) and (row.get('MA50_OVER_200', np.nan) > 0)
L1288             c5 = (row.get('TR_str', np.nan) > 0)
L1289             c6 = (row.get('P_OVER_LOW52', np.nan) >= 0.30)
L1290             c7 = (row.get('NEAR_52W_HIGH', np.nan) >= -0.25)
L1291             c8 = (row.get('RS', np.nan) >= 0.10)
L1292             return bool(c1 and c2 and c3 and c4 and c5 and c6 and c7 and c8)
L1293
L1294         if 'trend_template' not in df.columns: df['trend_template'] = df.apply(_trend_template_pass, axis=1).fillna(False)
L1295         assert 'trend_template' in df.columns
L1296
L1297         # === ZåŒ–ã¨åˆæˆ ===
L1298         for col in ['ROE','FCF','REV','EPS']: df[f'{col}_W'] = winsorize_s(df[col], 0.02)
L1299
L1300         df_z = pd.DataFrame(index=df.index)
L1301         for col in ['EPS','REV','ROE','FCF','RS','TR_str','BETA','DIV','DIV_STREAK']: df_z[col] = robust_z(df[col])
L1302         df_z['REV'], df_z['EPS'], df_z['TR'] = robust_z(df['REV_W']), robust_z(df['EPS_W']), robust_z(df['TR'])
L1303         for col in ['P_OVER_150','P_OVER_200','MA50_OVER_200','MA200_SLOPE_5M','LOW52PCT25_EXCESS','NEAR_52W_HIGH','RS_SLOPE_6W','RS_SLOPE_13W','MA200_UP_STREAK_D']: df_z[col] = robust_z(df[col])
L1304
L1305         # === Growthæ·±æ˜ã‚Šç³»ï¼ˆæ¬ æä¿æŒz + RAWä½µè¼‰ï¼‰ ===
L1306         grw_cols = ['REV_Q_YOY','EPS_Q_YOY','REV_YOY','EPS_YOY','REV_YOY_ACC','REV_YOY_VAR','FCF_MGN','RULE40','REV_ANN_STREAK']
L1307         for col in grw_cols:
L1308             if col in df.columns:
L1309                 raw = pd.to_numeric(df[col], errors="coerce")
L1310                 df_z[col] = robust_z_keepnan(raw)
L1311                 df_z[f'{col}_RAW'] = raw
L1312         for k in ("TREND_SLOPE_EPS", "TREND_SLOPE_REV"):
L1313             if k in df.columns and k not in df_z.columns:
L1314                 raw = pd.to_numeric(df[k], errors="coerce")
L1315                 df_z[k] = robust_z_keepnan(raw)
L1316                 df_z[f'{k}_RAW'] = raw
L1317         for col in ['DOWNSIDE_DEV','MDD_1Y','RESID_VOL','DOWN_OUTPERF','EXT_200','DIV_TTM_PS','DIV_VAR5','DIV_YOY','DIV_FCF_COVER','DEBT2EQ','CURR_RATIO','EPS_VAR_8Q','MARKET_CAP','ADV60_USD']: df_z[col] = robust_z(df[col])
L1318
L1319         df_z['SIZE'], df_z['LIQ'] = robust_z(np.log1p(df['MARKET_CAP'])), robust_z(np.log1p(df['ADV60_USD']))
L1320         df_z['QUALITY_F'] = robust_z(0.6*df['FCF_W'] + 0.4*df['ROE_W']).clip(-3.0,3.0)
L1321         df_z['YIELD_F']   = 0.3*df_z['DIV'] + 0.7*df_z['DIV_STREAK']
L1322
L1323         # EPSãŒèµ¤å­—ã§ã‚‚FCFãŒé»’å­—ãªã‚‰å®Ÿè³ªé»’å­—ã¨ã¿ãªã™
L1324         eps_pos_mask = (df['EPS'] > 0) | (df['FCF_MGN'] > 0)
L1325         df_z['EPS_POS'] = df_z['EPS'].where(eps_pos_mask, 0.0)
L1326
L1327         # ===== ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ãƒ­ãƒ¼ãƒ—ç®—å‡º =====
L1328         def zpos(x):
L1329             arr = robust_z(x)
L1330             idx = getattr(x, 'index', df_z.index)
L1331             return pd.Series(arr, index=idx).fillna(0.0)
L1332
L1333         def relu(x):
L1334             ser = x if isinstance(x, pd.Series) else pd.Series(x, index=df_z.index)
L1335             return ser.clip(lower=0).fillna(0.0)
L1336
L1337         # å£²ä¸Šãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ãƒ­ãƒ¼ãƒ—ï¼ˆå››åŠæœŸï¼‰
L1338         slope_rev = 0.70*zpos(df_z['REV_Q_YOY']) + 0.30*zpos(df_z['REV_YOY_ACC'])
L1339         noise_rev = relu(robust_z(df_z['REV_YOY_VAR']) - 0.8)
L1340         slope_rev_combo = slope_rev - 0.25*noise_rev
L1341         df_z['TREND_SLOPE_REV_RAW'] = slope_rev_combo
L1342         df_z['TREND_SLOPE_REV'] = slope_rev_combo.clip(-3.0, 3.0)
L1343
L1344         # EPSãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ãƒ­ãƒ¼ãƒ—ï¼ˆå››åŠæœŸï¼‰
L1345         slope_eps = 0.60*zpos(df_z['EPS_Q_YOY']) + 0.40*zpos(df_z['EPS_POS'])
L1346         df_z['TREND_SLOPE_EPS_RAW'] = slope_eps
L1347         df_z['TREND_SLOPE_EPS'] = slope_eps.clip(-3.0, 3.0)
L1348
L1349         # å¹´æ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆã‚µãƒ–ï¼‰
L1350         slope_rev_yr = zpos(df_z['REV_YOY'])
L1351         slope_eps_yr = zpos(df_z.get('EPS_YOY', pd.Series(0.0, index=df.index)))
L1352         streak_base = df['REV_ANN_STREAK'].clip(lower=0).fillna(0)
L1353         streak_yr = streak_base / (streak_base.abs() + 1.0)
L1354         slope_rev_yr_combo = 0.7*slope_rev_yr + 0.3*streak_yr
L1355         df_z['TREND_SLOPE_REV_YR_RAW'] = slope_rev_yr_combo
L1356         df_z['TREND_SLOPE_REV_YR'] = slope_rev_yr_combo.clip(-3.0, 3.0)
L1357         df_z['TREND_SLOPE_EPS_YR_RAW'] = slope_eps_yr
L1358         df_z['TREND_SLOPE_EPS_YR'] = slope_eps_yr.clip(-3.0, 3.0)
L1359
L1360         # ===== GRW flexible score (variable data paths) =====
L1361         grw_raw = pd.to_numeric(df.get('GRW_FLEX_SCORE'), errors="coerce")
L1362         df_z['GRW_FLEX_SCORE_RAW'] = grw_raw
L1363         df_z['GROWTH_F_RAW'] = grw_raw
L1364         df_z['GROWTH_F'] = robust_z_keepnan(grw_raw).clip(-3.0, 3.0)
L1365         df_z['GRW_FLEX_WEIGHT'] = pd.to_numeric(df.get('GRW_FLEX_WEIGHT'), errors="coerce")
L1366         df_z['GRW_FLEX_CORE_RAW'] = pd.to_numeric(df.get('GRW_FLEX_CORE'), errors="coerce")
L1367         df_z['GRW_FLEX_PRICE_RAW'] = pd.to_numeric(df.get('GRW_FLEX_PRICE'), errors="coerce")
L1368
L1369         # Debug dump for GRW composition (console OFF by default; enable only with env)
L1370         if bool(os.getenv("GRW_CONSOLE_DEBUG")):
L1371             try:
L1372                 cols = ['GROWTH_F', 'GROWTH_F_RAW', 'GRW_FLEX_WEIGHT']
L1373                 use_cols = [c for c in cols if c in df_z.columns]
L1374                 i = df_z[use_cols].copy() if use_cols else pd.DataFrame(index=df_z.index)
L1375                 i.sort_values('GROWTH_F', ascending=False, inplace=True)
L1376                 limit = max(0, min(40, len(i)))
L1377                 print("[DEBUG: GRW]")
L1378                 for t in i.index[:limit]:
L1379                     row = i.loc[t]
L1380                     parts = []
L1381                     if pd.notna(row.get('GROWTH_F')):
L1382                         parts.append(f"GROWTH_F={row.get('GROWTH_F'):.3f}")
L1383                     raw_val = row.get('GROWTH_F_RAW')
L1384                     if pd.notna(raw_val):
L1385                         parts.append(f"GROWTH_F_RAW={raw_val:.3f}")
L1386                     weight_val = row.get('GRW_FLEX_WEIGHT')
L1387                     if pd.notna(weight_val):
L1388                         parts.append(f"w={weight_val:.2f}")
L1389                     path_val = None
L1390                     try:
L1391                         path_val = info.get(t, {}).get('DEBUG_GRW_PATH')
L1392                     except Exception:
L1393                         path_val = None
L1394                     if not path_val and 'DEBUG_GRW_PATH' in df.columns:
L1395                         path_val = df.at[t, 'DEBUG_GRW_PATH']
L1396                     if path_val:
L1397                         parts.append(f"PATH={path_val}")
L1398                     parts_json = None
L1399                     try:
L1400                         parts_json = info.get(t, {}).get('DEBUG_GRW_PARTS')
L1401                     except Exception:
L1402                         parts_json = None
L1403                     if not parts_json and 'DEBUG_GRW_PARTS' in df.columns:
L1404                         parts_json = df.at[t, 'DEBUG_GRW_PARTS']
L1405                     if parts_json:
L1406                         parts.append(f"PARTS={parts_json}")
L1407                     if not parts:
L1408                         parts.append('no-data')
L1409                     print(f"Ticker: {t} | " + " ".join(parts))
L1410                 print()
L1411             except Exception as exc:
L1412                 print(f"[ERR] GRW debug dump failed: {exc}")
L1413
L1414         df_z['MOM_F'] = robust_z(0.40*df_z['RS']
L1415             + 0.15*df_z['TR_str']
L1416             + 0.15*df_z['RS_SLOPE_6W']
L1417             + 0.15*df_z['RS_SLOPE_13W']
L1418             + 0.10*df_z['MA200_SLOPE_5M']
L1419             + 0.10*df_z['MA200_UP_STREAK_D']).clip(-3.0,3.0)
L1420         df_z['VOL'] = robust_z(df['BETA'])
L1421         df_z['QAL'], df_z['YLD'], df_z['MOM'] = df_z['QUALITY_F'], df_z['YIELD_F'], df_z['MOM_F']
L1422         df_z.drop(columns=['QUALITY_F','YIELD_F','MOM_F'], inplace=True, errors='ignore')
L1423
L1424         _dump_dfz(
L1425             df=df,
L1426             df_z=df_z,
L1427             debug_mode=getattr(cfg, "debug_mode", False),
L1428         )
L1429         if getattr(cfg, "debug_mode", False):
L1430             log_grw_stats(df, df_z, logger)
L1431         save_factor_debug_csv(df, df_z)
L1432
L1433         # === begin: BIO LOSS PENALTY =====================================
L1434         try:
L1435             penalty_z = float(os.getenv("BIO_LOSS_PENALTY_Z", "0.8"))
L1436         except Exception:
L1437             penalty_z = 0.8
L1438
L1439         def _is_bio_like(t: str) -> bool:
L1440             inf = info.get(t, {}) if isinstance(info, dict) else {}
L1441             sec = str(inf.get("sector", "")).lower()
L1442             ind = str(inf.get("industry", "")).lower()
L1443             if "health" not in sec:
L1444                 return False
L1445             keys = ("biotech", "biopharma", "pharma")
L1446             return any(k in ind for k in keys)
L1447
L1448         tickers_s = pd.Index(df_z.index)
L1449         is_bio = pd.Series({t: _is_bio_like(t) for t in tickers_s})
L1450         is_loss = pd.Series({t: (pd.notna(df.loc[t,"EPS"]) and df.loc[t,"EPS"] <= 0) for t in tickers_s})
L1451         mask_bio_loss = (is_bio & is_loss).reindex(df_z.index).fillna(False)
L1452
L1453         if bool(mask_bio_loss.any()) and penalty_z > 0:
L1454             df_z.loc[mask_bio_loss, "GROWTH_F"] = df_z.loc[mask_bio_loss, "GROWTH_F"] - penalty_z
L1455             df_z["GROWTH_F"] = df_z["GROWTH_F"].clip(-3.0, 3.0)
L1456         # === end: BIO LOSS PENALTY =======================================
L1457
L1458         df_z['TRD'] = 0.0  # TRDã¯ã‚¹ã‚³ã‚¢å¯„ä¸ã‹ã‚‰å¤–ã—ã€ãƒ†ãƒ³ãƒ—ãƒ¬åˆ¤å®šã¯ãƒ•ã‚£ãƒ«ã‚¿ã§è¡Œã†ï¼ˆåˆ—ã¯è¡¨ç¤ºäº’æ›ã®ãŸã‚æ®‹ã™ï¼‰
L1459         if 'BETA' not in df_z.columns: df_z['BETA'] = robust_z(df['BETA'])
L1460
L1461         df_z['D_VOL_RAW'] = robust_z(0.40*df_z['DOWNSIDE_DEV'] + 0.22*df_z['RESID_VOL'] + 0.18*df_z['MDD_1Y'] - 0.10*df_z['DOWN_OUTPERF'] - 0.05*df_z['EXT_200'] - 0.08*df_z['SIZE'] - 0.10*df_z['LIQ'] + 0.10*df_z['BETA'])
L1462         df_z['D_QAL']     = robust_z(0.35*df_z['QAL'] + 0.20*df_z['FCF'] + 0.15*df_z['CURR_RATIO'] - 0.15*df_z['DEBT2EQ'] - 0.15*df_z['EPS_VAR_8Q'])
L1463         df_z['D_YLD']     = robust_z(0.45*df_z['DIV'] + 0.25*df_z['DIV_STREAK'] + 0.20*df_z['DIV_FCF_COVER'] - 0.10*df_z['DIV_VAR5'])
L1464         df_z['D_TRD']     = robust_z(0.40*df_z.get('MA200_SLOPE_5M',0) - 0.30*df_z.get('EXT_200',0) + 0.15*df_z.get('NEAR_52W_HIGH',0) + 0.15*df_z['TR'])
L1465
L1466         # --- é‡ã¿ã¯ cfg ã‚’å„ªå…ˆï¼ˆå¤–éƒ¨ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ï¼‰ ---
L1467         # â‘  å…¨éŠ˜æŸ„ã§ G/D ã‚¹ã‚³ã‚¢ã‚’ç®—å‡ºï¼ˆunmaskedï¼‰
L1468         g_score_all = df_z.mul(pd.Series(cfg.weights.g)).sum(axis=1)
L1469
L1470         d_comp = pd.concat({
L1471             'QAL': df_z['D_QAL'],
L1472             'YLD': df_z['D_YLD'],
L1473             'VOL': df_z['D_VOL_RAW'],
L1474             'TRD': df_z['D_TRD']
L1475         }, axis=1)
L1476         dw = pd.Series(cfg.weights.d, dtype=float).reindex(['QAL','YLD','VOL','TRD']).fillna(0.0)
L1477         globals()['D_WEIGHTS_EFF'] = dw.copy()
L1478         d_score_all = d_comp.mul(dw, axis=1).sum(axis=1)
L1479
L1480         # â‘¡ ãƒ†ãƒ³ãƒ—ãƒ¬åˆ¤å®šï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ãã®ã¾ã¾ï¼‰
L1481         mask = df['trend_template']
L1482         if not bool(mask.any()):
L1483             mask = ((df.get('P_OVER_LOW52', np.nan) >= 0.25) &
L1484                 (df.get('NEAR_52W_HIGH', np.nan) >= -0.30) &
L1485                 (df.get('RS', np.nan) >= 0.08) &
L1486                 (df.get('MA200_SLOPE_1M', np.nan) > 0) &
L1487                 (df.get('P_OVER_150', np.nan) > 0) & (df.get('P_OVER_200', np.nan) > 0) &
L1488                 (df.get('MA150_OVER_200', np.nan) > 0) &
L1489                 (df.get('MA50_OVER_150', np.nan) > 0) & (df.get('MA50_OVER_200', np.nan) > 0) &
L1490                 (df.get('TR_str', np.nan) > 0)).fillna(False)
L1491             df['trend_template'] = mask
L1492
L1493         # â‘¢ æ¡ç”¨ç”¨ã¯ maskã€è¡¨ç¤º/åˆ†æç”¨ã¯åˆ—ã§å…¨éŠ˜æŸ„ä¿å­˜
L1494         g_score = g_score_all.loc[mask]
L1495         Scorer.g_score = g_score
L1496         df_z['GSC'] = g_score_all
L1497         df_z['DSC'] = d_score_all
L1498
L1499         try:
L1500             current = (pd.read_csv("current_tickers.csv")
L1501                   .iloc[:, 0]
L1502                   .str.upper()
L1503                   .tolist())
L1504         except FileNotFoundError:
L1505             warnings.warn("current_tickers.csv not found â€” bonus skipped")
L1506             current = []
L1507
L1508         mask_bonus = g_score.index.isin(current)
L1509         if mask_bonus.any():
L1510             # 1) factor.BONUS_COEFF ã‹ã‚‰ k ã‚’æ±ºã‚ã€ç„¡ã‘ã‚Œã° 0.4
L1511             k = float(getattr(sys.modules.get("factor"), "BONUS_COEFF", 0.4))
L1512             # 2) g å´ã® Ïƒ ã‚’å–ã‚Šã€NaN ãªã‚‰ 0 ã«ä¸¸ã‚ã‚‹
L1513             sigma_g = g_score.std()
L1514             if pd.isna(sigma_g):
L1515                 sigma_g = 0.0
L1516             bonus_g = round(k * sigma_g, 3)
L1517             g_score.loc[mask_bonus] += bonus_g
L1518             Scorer.g_score = g_score
L1519             # 3) D å´ã‚‚åŒæ§˜ã« Ïƒ ã® NaN ã‚’ã‚±ã‚¢
L1520             sigma_d = d_score_all.std()
L1521             if pd.isna(sigma_d):
L1522                 sigma_d = 0.0
L1523             bonus_d = round(k * sigma_d, 3)
L1524             d_score_all.loc[d_score_all.index.isin(current)] += bonus_d
L1525
L1526         try:
L1527             df = _apply_growth_entry_flags(df, ib, self, win_breakout=5, win_pullback=5)
L1528         except Exception:
L1529             pass
L1530
L1531         df_full = df.copy()
L1532         df_full_z = df_z.copy()
L1533
L1534         from factor import FeatureBundle  # type: ignore  # å®Ÿè¡Œæ™‚importãªã—ï¼ˆå¾ªç’°å›é¿ï¼‰
L1535         return FeatureBundle(df=df,
L1536             df_z=df_z,
L1537             g_score=g_score,
L1538             d_score_all=d_score_all,
L1539             missing_logs=pd.DataFrame(missing_logs),
L1540             df_full=df_full,
L1541             df_full_z=df_full_z,
L1542             scaler=None)
L1543
L1544 def _apply_growth_entry_flags(feature_df, bundle, self_obj, win_breakout=5, win_pullback=5):
L1545     """
L1546     Gæ ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã«å¯¾ã—ã€ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®š/æŠ¼ã—ç›®åç™ºã®ã€Œç›´è¿‘Nå–¶æ¥­æ—¥å†…ã®ç™ºç«ã€ã‚’åˆ¤å®šã—ã€
L1547     æ¬¡ã®åˆ—ã‚’ feature_df ã«è¿½åŠ ã™ã‚‹ï¼ˆindex=tickerï¼‰ã€‚
L1548       - G_BREAKOUT_recent_5d : bool
L1549       - G_BREAKOUT_last_date : str "YYYY-MM-DD"
L1550       - G_PULLBACK_recent_5d : bool
L1551       - G_PULLBACK_last_date : str "YYYY-MM-DD"
L1552       - G_PIVOT_price        : float
L1553     å¤±æ•—ã—ã¦ã‚‚ä¾‹å¤–ã¯æ¡ã‚Šæ½°ã—ã€æ—¢å­˜å‡¦ç†ã‚’é˜»å®³ã—ãªã„ã€‚
L1554     """
L1555     try:
L1556         px   = bundle.px                      # çµ‚å€¤ DataFrame
L1557         hi   = bundle.data['High']
L1558         lo   = bundle.data['Low']
L1559         vol  = bundle.data['Volume']
L1560         bench= bundle.spx                     # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ Series
L1561
L1562         # Gãƒ¦ãƒ‹ãƒãƒ¼ã‚¹æ¨å®šï¼šself.g_universe å„ªå…ˆ â†’ feature_df['group']=='G' â†’ å…¨éŠ˜æŸ„
L1563         g_universe = getattr(self_obj, "g_universe", None)
L1564         if g_universe is None:
L1565             try:
L1566                 g_universe = feature_df.index[feature_df['group'].astype(str).str.upper().eq('G')].tolist()
L1567             except Exception:
L1568                 g_universe = list(feature_df.index)
L1569         if not g_universe:
L1570             return feature_df
L1571
L1572         # æŒ‡æ¨™
L1573         px = px.ffill(limit=2)
L1574         ema21 = px[g_universe].ewm(span=21, adjust=False).mean()
L1575         ma50  = px[g_universe].rolling(50).mean()
L1576         ma150 = px[g_universe].rolling(150).mean()
L1577         ma200 = px[g_universe].rolling(200).mean()
L1578         atr20 = (hi[g_universe] - lo[g_universe]).rolling(20).mean()
L1579         vol20 = vol[g_universe].rolling(20).mean()
L1580         vol50 = vol[g_universe].rolling(50).mean()
L1581
L1582         # ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆåˆæ ¼
L1583         trend_template_ok = (px[g_universe] > ma50) & (px[g_universe] > ma150) & (px[g_universe] > ma200) \
L1584                             & (ma150 > ma200) & (ma200.diff() > 0)
L1585
L1586         # æ±ç”¨ãƒ”ãƒœãƒƒãƒˆï¼šç›´è¿‘65å–¶æ¥­æ—¥ã®é«˜å€¤ï¼ˆå½“æ—¥é™¤å¤–ï¼‰
L1587         pivot_price = hi[g_universe].rolling(65).max().shift(1)
L1588
L1589         # ç›¸å¯¾åŠ›ï¼šå¹´å†…é«˜å€¤æ›´æ–°
L1590         bench_aligned = bench.reindex(px.index).ffill()
L1591         rs = px[g_universe].div(bench_aligned, axis=0)
L1592         rs_high = rs.rolling(252).max().shift(1)
L1593
L1594         # ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆã€Œç™ºç”Ÿæ—¥ã€ï¼šæ¡ä»¶ç«‹ã¡ä¸ŠãŒã‚Š
L1595         breakout_today = trend_template_ok & (px[g_universe] > pivot_price) \
L1596                          & (vol[g_universe] >= 1.5 * vol50) & (rs > rs_high)
L1597         breakout_event = breakout_today & ~breakout_today.shift(1).fillna(False)
L1598
L1599         # æŠ¼ã—ç›®åç™ºã€Œç™ºç”Ÿæ—¥ã€ï¼šEMA21å¸¯Ã—å‡ºæ¥é«˜ãƒ‰ãƒ©ã‚¤ã‚¢ãƒƒãƒ—Ã—å‰æ—¥é«˜å€¤è¶ŠãˆÃ—çµ‚å€¤EMA21ä¸Š
L1600         near_ema21_band = px[g_universe].between(ema21 - atr20, ema21 + atr20)
L1601         volume_dryup = (vol20 / vol50) <= 1.0
L1602         pullback_bounce_confirmed = (px[g_universe] > hi[g_universe].shift(1)) & (px[g_universe] > ema21)
L1603         pullback_today = trend_template_ok & near_ema21_band & volume_dryup & pullback_bounce_confirmed
L1604         pullback_event = pullback_today & ~pullback_today.shift(1).fillna(False)
L1605
L1606         # ç›´è¿‘Nå–¶æ¥­æ—¥å†…ã®ç™ºç« / æœ€çµ‚ç™ºç”Ÿæ—¥
L1607         rows = []
L1608         for t in g_universe:
L1609             def _recent_and_date(s, win):
L1610                 sw = s[t].iloc[-win:]
L1611                 if sw.any():
L1612                     d = sw[sw].index[-1]
L1613                     return True, d.strftime("%Y-%m-%d")
L1614                 return False, ""
L1615             br_recent, br_date = _recent_and_date(breakout_event, win_breakout)
L1616             pb_recent, pb_date = _recent_and_date(pullback_event, win_pullback)
L1617             rows.append((t, {
L1618                 "G_BREAKOUT_recent_5d": br_recent,
L1619                 "G_BREAKOUT_last_date": br_date,
L1620                 "G_PULLBACK_recent_5d": pb_recent,
L1621                 "G_PULLBACK_last_date": pb_date,
L1622                 "G_PIVOT_price": float(pivot_price[t].iloc[-1]) if t in pivot_price.columns else float('nan'),
L1623             }))
L1624         flags = pd.DataFrame({k: v for k, v in rows}).T
L1625
L1626         # åˆ—ã‚’ä½œæˆãƒ»ä¸Šæ›¸ã
L1627         cols = ["G_BREAKOUT_recent_5d","G_BREAKOUT_last_date","G_PULLBACK_recent_5d","G_PULLBACK_last_date","G_PIVOT_price"]
L1628         for c in cols:
L1629             if c not in feature_df.columns:
L1630                 feature_df[c] = np.nan
L1631         feature_df.loc[flags.index, flags.columns] = flags
L1632
L1633     except Exception:
L1634         pass
L1635     return feature_df
L1636
```

## <.github/workflows/weekly-report.yml>
```text
L1 name: Weekly Stock Report
L2
L3 on:
L4   push:
L5     branches: [ main ]
L6     paths-ignore:
L7       - 'CodeForChat/**'
L8   schedule:
L9     - cron: '0 0 * * 6'  # UTC 00:00 â†’ JST 09:00ï¼ˆåœŸï¼‰
L10   workflow_dispatch:
L11
L12 jobs:
L13   build-and-report:
L14     runs-on: ubuntu-latest
L15     permissions:
L16       contents: write
L17
L18     steps:
L19       - name: Debug start
L20         run: echo 'ğŸš€ DEBUGstarted'
L21               
L22       - name: Checkout repository
L23         uses: actions/checkout@v3
L24
L25       - name: Setup Python
L26         uses: actions/setup-python@v5
L27         with:
L28           python-version: '3.x'
L29           cache: 'pip'
L30           cache-dependency-path: requirements.txt
L31
L32       - name: Install dependencies
L33         run: pip install -r requirements.txt
L34
L35       - name: Prepare results directory
L36         run: mkdir -p results
L37
L38       - name: Run factor & scoring
L39         env:
L40           SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
L41           FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
L42           FIN_THREADS: "8"
L43         run: python factor.py
```

## <documents/README.md>
```text
L1 # é‹ç”¨ãƒ«ãƒ¼ãƒ«
L2
L3 ## åŸºæœ¬æ§‹æˆ
L4 - 20éŠ˜æŸ„ã‚’å‡ç­‰é…åˆ†ï¼ˆç¾é‡‘ã‚’é™¤ã1éŠ˜æŸ„ã‚ãŸã‚Š5%ï¼‰
L5 - moomooè¨¼åˆ¸ã§é‹ç”¨
L6 - **Growthæ  12éŠ˜æŸ„ / Defenseæ  8éŠ˜æŸ„**ï¼ˆNORMAL åŸºæº–ï¼‰
L7
L8 ## Barbell Growth-Defenseæ–¹é‡
L9 - Growthæ  **12éŠ˜æŸ„**ï¼šé«˜æˆé•·ã§ä¹–é›¢æºã¨ãªã‚‹æ”»ã‚ã®éŠ˜æŸ„
L10 - Defenseæ  **8éŠ˜æŸ„**ï¼šä½ãƒœãƒ©ã§å®‰å®šæˆé•·ã—é…å½“ã‚’å¢—ã‚„ã™å®ˆã‚Šã®éŠ˜æŸ„
L11 - ã€ŒçŒ›çƒˆã«ä¼¸ã³ã‚‹æ”»ã‚ Ã— ç€å®Ÿã«ç¨¼ãç›¾ã€ã®çµ„åˆã›ã§ä¹–é›¢â†’åŠæˆ»ã—ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ã‚’ç‹™ã†
L12
L13 ## ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¤å®šï¼ˆtrend_template åˆæ ¼â€œæœ¬æ•°â€ã§åˆ¤å®šï¼‰
L14 - åˆæ ¼æœ¬æ•° = current+candidate å…¨ä½“ã®ã†ã¡ã€trend_template æ¡ä»¶ã‚’æº€ãŸã—ãŸéŠ˜æŸ„ã®**æœ¬æ•°(C)**ï¼ˆåŸºæº– N_G=12ï¼‰
L15 - ã—ãã„å€¤ã¯éå»~600å–¶æ¥­æ—¥ã®åˆ†å¸ƒã‹ã‚‰**æ¯å›è‡ªå‹•æ¡ç”¨**ï¼ˆåˆ†ä½ç‚¹ã¨é‹ç”¨â€œåºŠâ€ã®maxï¼‰
L16   - ç·Šæ€¥å…¥ã‚Š: `max(q05, 12æœ¬)`ï¼ˆ= N_Gï¼‰
L17   - ç·Šæ€¥è§£é™¤: `max(q20, 18æœ¬)`ï¼ˆ= ceil(1.5Ã—12)ï¼‰
L18   - é€šå¸¸å¾©å¸°: `max(q60, 36æœ¬)`ï¼ˆ= 3Ã—N_Gï¼‰
L19 - ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹: å‰å›ãƒ¢ãƒ¼ãƒ‰ã«ä¾å­˜ï¼ˆEMERGâ†’è§£é™¤ã¯23æœ¬ä»¥ä¸Šã€CAUTIONâ†’é€šå¸¸ã¯45æœ¬ä»¥ä¸Šï¼‰
L20
L21 ## ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¥ã®ç¾é‡‘ãƒ»ãƒ‰ãƒªãƒ•ãƒˆ
L22  - **é€šå¸¸(NORMAL)** : ç¾é‡‘ **10%** / ãƒ‰ãƒªãƒ•ãƒˆé–¾å€¤ **12%**
L23  - **è­¦æˆ’(CAUTION)** : ç¾é‡‘ **12.5%** / ãƒ‰ãƒªãƒ•ãƒˆé–¾å€¤ **14%**
L24  - **ç·Šæ€¥(EMERG)** : ç¾é‡‘ **20%** / **ãƒ‰ãƒªãƒ•ãƒˆå£²è²·åœæ­¢**ï¼ˆ20Ã—5%ã«å…¨æˆ»ã—ã®ã¿ï¼‰
L25
L26 ## ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®æ¨å¥¨â€œä¿æœ‰éŠ˜æŸ„æ•°â€ï¼ˆMMFâ‰’ç¾é‡‘ï¼‰
L27 *å„æ =5%ï¼ˆ20éŠ˜æŸ„å‡ç­‰ï¼‰ã€‚ãƒ¢ãƒ¼ãƒ‰ç§»è¡Œæ™‚ã¯**Gã®æ æ•°ã®ã¿**èª¿æ•´ã—ã€å¤–ã—ãŸæ ã¯ç¾é‡‘ã¨ã—ã¦ä¿æŒã€‚*
L28
L29 - **NORMAL:** G **12** / D **8** / ç¾é‡‘åŒ–æ  **0**  
L30 - **CAUTION:** G **10** / D **8** / ç¾é‡‘åŒ–æ  **2**ï¼ˆ= 10%ï¼‰  
L31 - **EMERG:** G **8**  / D **8** / ç¾é‡‘åŒ–æ  **4**ï¼ˆ= 20%ï¼‰  
L32
L33 > å®Ÿé‹ç”¨ï¼šâ­ï¸ä½ã‚¹ã‚³ã‚¢ã®Gã‹ã‚‰é †ã«å¤–ã™ã€‚è§£é™¤æ™‚ã¯factorä¸Šä½ã‹ã‚‰è£œå……ã€‚
L34
L35 ## ãƒˆãƒ¬ãƒ¼ãƒªãƒ³ã‚°ã‚¹ãƒˆãƒƒãƒ—
L36 - **åŸºæœ¬TS (ãƒ¢ãƒ¼ãƒ‰åˆ¥):** NORMAL **15%** / CAUTION **13%** / EMERG **10%**
L37 - å«ã¿ç›ŠãŒ **+30% / +60% / +100%** åˆ°é”ã§ã€åŸºæœ¬ã‹ã‚‰ **-3pt / -6pt / -8pt** å¼•ãä¸Šã’
L38 - TSç™ºå‹•ã§æ¸›å°‘ã—ãŸéŠ˜æŸ„ã¯ç¿Œæ—¥ä»¥é™ã«è£œå……ï¼ˆâ€»ç·Šæ€¥ãƒ¢ãƒ¼ãƒ‰ä¸­ã¯è£œå……ã—ãªã„ï¼‰
L39
L40 ## åŠæˆ»ã—ï¼ˆãƒªãƒãƒ©ãƒ³ã‚¹ï¼‰æ‰‹é †
L41 ãƒ‰ãƒªãƒ•ãƒˆãƒã‚§ãƒƒã‚¯ã§**ã‚¢ãƒ©ãƒ¼ãƒˆ**ãŒå‡ºãŸå ´åˆï¼ˆåˆè¨ˆ|drift| ãŒãƒ¢ãƒ¼ãƒ‰é–¾å€¤ã‚’è¶…éã€EMERGé™¤ãï¼‰ã€ç¿Œå–¶æ¥­æ—¥ã®ç±³å›½å¯„ä»˜ãã§ä¸‹è¨˜ã‚’å®Ÿæ–½ã™ã‚‹ã€‚
L42
L43 1. **å£²å´ï¼ˆå¿…é ˆï¼‰**  
L44    Slackãƒ†ãƒ¼ãƒ–ãƒ«ã® **Î”qty ãŒãƒã‚¤ãƒŠã‚¹ã®éŠ˜æŸ„ã‚’å£²å´** ã™ã‚‹ï¼ˆå¯„ä»˜ãæˆè¡Œæ¨å¥¨ï¼‰ã€‚  
L45    ã“ã‚Œã¯ã€ŒåŠæˆ»ã—ã€è¨ˆç®—ã«åŸºã¥ãéé‡é‡ã®å‰Šæ¸›ã‚’æ„å‘³ã™ã‚‹ã€‚
L46
L47 2. **è³¼å…¥ï¼ˆä»»æ„ãƒ»åŠæˆ»ã—ç›®å®‰ï¼‰**  
L48    åŠæˆ»ã—å¾Œã®åˆè¨ˆ|drift|ã‚’**ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤ï¼ˆSlackãƒ˜ãƒƒãƒ€ã«è¡¨ç¤ºï¼‰**ã«è¿‘ã¥ã‘ã‚‹ã“ã¨ã‚’ç›®å®‰ã«ã€  
L49    **ä»»æ„ã®éŠ˜æŸ„ã‚’è²·ã„å¢—ã—**ã—ã¦ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹ï¼ˆÎ”qtyãŒãƒ—ãƒ©ã‚¹ã®éŠ˜æŸ„ã‚’å„ªå…ˆã—ã¦ã‚‚ã‚ˆã„ï¼‰ã€‚
L50
L51 3. **ãƒˆãƒ¬ãƒ¼ãƒªãƒ³ã‚°ã‚¹ãƒˆãƒƒãƒ—ã®å†è¨­å®šï¼ˆå¿…é ˆï¼‰**  
L52    ã™ã¹ã¦ã®ä¿æœ‰éŠ˜æŸ„ã«ã¤ã„ã¦ã€æœ€æ–°ã®è©•ä¾¡é¡ã«åˆã‚ã›ã¦TSã‚’**å†ç™ºæ³¨ï¼æ›´æ–°**ã™ã‚‹ã€‚  
L53    ãƒ«ãƒ¼ãƒ«ã¯ä¸‹è¨˜ï¼ˆåˆ©ç›Šåˆ°é”ã§æ®µéšçš„ã«ã‚¿ã‚¤ãƒˆåŒ–ï¼‰ï¼š  
L54    - **åŸºæœ¬TS:** -15%  
L55    - **+30% åˆ°é” â†’ TS -12%**  
L56    - **+60% åˆ°é” â†’ TS -9%**  
L57    - **+100% åˆ°é” â†’ TS -7%**  
L58    â€»ã‚¹ãƒˆãƒƒãƒ—ä¾¡æ ¼ã®å¼•ãä¸Šã’ã¯è¨±å¯ã€**å¼•ãä¸‹ã’ã¯ä¸å¯**ï¼ˆåˆ©ç›Šä¿å…¨ã®åŸå‰‡ï¼‰ã€‚
L59
L60 4. **ä¾‹å¤–ï¼ˆEMERGãƒ¢ãƒ¼ãƒ‰ï¼‰**  
L61    ç·Šæ€¥(EMERG)ã§ã¯**ãƒ‰ãƒªãƒ•ãƒˆç”±æ¥ã®å£²è²·ã¯åœæ­¢ï¼ˆâˆï¼‰**ã€‚20éŠ˜æŸ„Ã—å„5%ã¸ã®**å…¨æˆ»ã—**ã®ã¿è¨±å®¹ã€‚
L62
L63 5. **å®Ÿè¡Œã‚¿ã‚¤ãƒŸãƒ³ã‚°**
L64    - åˆ¤å®šï¼šç±³å›½å¸‚å ´çµ‚å€¤ç›´å¾Œ
L65    - åŸ·è¡Œï¼šç¿Œå–¶æ¥­æ—¥ã®ç±³å›½å¯„ä»˜ãæˆè¡Œ
L66
L67 ## ãƒ¢ãƒ¼ãƒ‰ç§»è¡Œã®å®Ÿå‹™æ‰‹é †ï¼ˆè¶…ã‚·ãƒ³ãƒ—ãƒ«ï¼‰
L68 ãƒ¢ãƒ¼ãƒ‰ãŒå¤‰ã‚ã£ãŸã‚‰ã€**MMFâ‰’ç¾é‡‘**ã¨ã—ã¦æ‰±ã„ã€**Gã®æ æ•°ã ã‘**ã‚’èª¿æ•´ã™ã‚‹ï¼š
L69 1. **Gã‚’å‰Šã‚‹**ï¼ˆCAUTION/EMERGï¼‰  
L70    - â­ï¸ä½ã‚¹ã‚³ã‚¢ã®Gã‹ã‚‰é †ã«å¤–ã™ã€‚  
L71    - **`current_tickers.csv` ã‹ã‚‰å¤–ã™GéŠ˜æŸ„ã®è¡Œã‚’å‰Šé™¤**ï¼ˆï¼ãã®æ ã¯ç¾é‡‘åŒ–ï¼‰ã€‚
L72 2. **ç¾é‡‘ã¨ã—ã¦ä¿æŒ**  
L73    - å¤–ã—ãŸæ ã¯ç¾é‡‘ï¼ˆã¾ãŸã¯MMFç›¸å½“ï¼‰ã§ãƒ—ãƒ¼ãƒ«ã€‚  
L74 3. **å¾©å¸°æ™‚ã®è£œå……**ï¼ˆNORMALã¸ï¼‰  
L75    - **`current_tickers.csv` ã«éŠ˜æŸ„ã‚’è¿½åŠ **ï¼ˆfactorä¸Šä½ã‹ã‚‰ï¼‰ã€‚  
L76    - ä»¥é™ã¯æ—¥æ¬¡ãƒ‰ãƒªãƒ•ãƒˆ/TSãƒ«ãƒ¼ãƒ«ã«å¾“ã†ã€‚
L77
L78 > driftã¯ `target_ratio = 1/éŠ˜æŸ„æ•°` ã‚’è‡ªå‹•é©ç”¨ã€‚è¡Œæ•°ã«å¿œã˜ã¦è‡ªå‹•ã§å‡ç­‰æ¯”ç‡ãŒå†è¨ˆç®—ã•ã‚Œã‚‹ã€‚
L79
L80 ## å…¥æ›¿éŠ˜æŸ„é¸å®š
L81 - Oxfordã‚­ãƒ£ãƒ”ã‚¿ãƒ«ï¼ã‚¤ãƒ³ã‚«ãƒ ã€Alpha Investorã€Motley Fool Stock Advisorã€moomooã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ç­‰ã‚’å‚è€ƒã«chatGPTã§æ¤œè¨
L82 - å¹´é–“NISAæ ã¯Growthç¾¤ã®ä¸­ã‹ã‚‰ä½ãƒœãƒ©éŠ˜æŸ„ã‚’é¸å®šã—åˆ©ç”¨ã€‚é•·æœŸä¿æŒã«ã¯ã“ã ã‚ã‚‰ãªã„ã€‚
L83
L84 ## å†ã‚¨ãƒ³ãƒˆãƒªãƒ¼ï¼ˆã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ï¼‰
L85 - TSãƒ’ãƒƒãƒˆå¾Œã®åŒéŠ˜æŸ„å†INã¯ **8å–¶æ¥­æ—¥** ã®ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ã‚’è¨­ã‘ã‚‹ï¼ˆæœŸé–“ä¸­ã¯å†INç¦æ­¢ï¼‰
L86
L87 ## å®Ÿè¡Œã‚¿ã‚¤ãƒŸãƒ³ã‚°
L88 - åˆ¤å®šï¼šç±³å›½å¸‚å ´çµ‚å€¤ç›´å¾Œ
L89 - åŸ·è¡Œï¼šç¿Œå–¶æ¥­æ—¥ã®ç±³å›½å¯„ä»˜ãæˆè¡Œ
```

## <documents/factor_design.md>
```text
L1 # factor.py è©³ç´°è¨­è¨ˆæ›¸
L2
L3 ## æ¦‚è¦
L4 - æ—¢å­˜ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®éŠ˜æŸ„ã¨æ¤œè¨ä¸­ã®éŠ˜æŸ„ç¾¤ã‚’åŒæ™‚ã«æ‰±ã†éŠ˜æŸ„é¸å®šãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€‚
L5 - ä¾¡æ ¼ãƒ»è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šè¾¼ã¿ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã¨DRRSé¸å®šã‚’è¡Œã†ã“ã¨ã§ã€ä»¥ä¸‹ã®ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆã‚’å¾—ã‚‹ã€‚
L6   - æ¡ç”¨éŠ˜æŸ„ã¨æƒœã—ãã‚‚æ¼ã‚ŒãŸéŠ˜æŸ„ã®ã‚¹ã‚³ã‚¢ä¸€è¦§
L7   - IN/OUTã®ãƒ†ã‚£ãƒƒã‚«ãƒ¼ãƒªã‚¹ãƒˆã¨OUTå´ã®ä½ã‚¹ã‚³ã‚¢éŠ˜æŸ„
L8   - æ–°æ—§ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®æ¯”è¼ƒè¡¨
L9   - æ¤œè¨ä¸­éŠ˜æŸ„ã®ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆæ•´ç†ç”¨ï¼‰
L10
L11 ## å…¨ä½“ãƒ•ãƒ­ãƒ¼
L12 1. **Input** â€“ `current_tickers.csv`ã¨`candidate_tickers.csv`ã‚’èª­ã¿è¾¼ã¿ã€yfinanceã‚„Finnhubã®APIã‹ã‚‰ä¾¡æ ¼ãƒ»è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã¦`InputBundle`ã‚’æ•´å‚™ã€‚
L13 2. **Score Calculation** â€“ ScorerãŒç‰¹å¾´é‡ã‚’è¨ˆç®—ã—å› å­ã‚¹ã‚³ã‚¢ã‚’åˆæˆã—ã¦`FeatureBundle`ã‚’ç”Ÿæˆã€‚
L14 3. **Correlation Reduction & Selection** â€“ SelectorãŒDRRSãƒ­ã‚¸ãƒƒã‚¯ã§ç›¸é–¢ã‚’æŠ‘ãˆã¤ã¤G/DéŠ˜æŸ„ã‚’é¸å®šã—`SelectionBundle`ã‚’å¾—ã‚‹ã€‚
L15 4. **Output** â€“ æ¡ç”¨çµæœã¨å‘¨è¾ºæƒ…å ±ã‚’è¡¨ãƒ»Slacké€šçŸ¥ã¨ã—ã¦å‡ºåŠ›ã€‚
L16
L17 ```mermaid
L18 flowchart LR
L19   A[Input\nAPI & å‰å‡¦ç†] --> B[Score Calculation\nç‰¹å¾´é‡ãƒ»å› å­åˆæˆ]
L20   B --> C[Correlation Reduction\nDRRSé¸å®š]
L21   C --> D[Output\nSlacké€šçŸ¥]
L22 ```
L23
L24 ## å®šæ•°ãƒ»è¨­å®š
L25 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L26 | --- | --- | --- |
L27 | `exist` / `cand` | ç¾è¡Œãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã¨æ¤œè¨ä¸­éŠ˜æŸ„ã®ãƒ†ã‚£ãƒƒã‚«ãƒ¼ãƒªã‚¹ãƒˆ | ã‚¹ã‚³ã‚¢å¯¾è±¡ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã®æ§‹æˆã€å€™è£œæ•´ç† |
L28 | `bench` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼ | ç›¸å¯¾å¼·ã•ãƒ»Î²ç®—å‡ºã€ãƒãƒ¼ãƒˆæ¯”è¼ƒ |
L29 | `CAND_PRICE_MAX` | å€™è£œéŠ˜æŸ„ã®è¨±å®¹ä¾¡æ ¼ä¸Šé™ | é«˜é¡éŠ˜æŸ„ã®äº‹å‰é™¤å¤– |
L30 | `N_G` / `N_D` | G/Dæ¡ç”¨æ ã®ä»¶æ•°ï¼ˆ**æ—¢å®š: 12 / 8**ï¼‰ | æœ€çµ‚çš„ã«é¸ã¶éŠ˜æŸ„æ•°ã®åˆ¶ç´„ |
L31 | `g_weights` / `D_weights` | å„å› å­ã®é‡ã¿dict | G/Dã‚¹ã‚³ã‚¢åˆæˆ |
L32 | `D_BETA_MAX` | Dãƒã‚±ãƒƒãƒˆã®è¨±å®¹Î²ä¸Šé™ | é«˜Î²éŠ˜æŸ„ã®é™¤å¤–ãƒ•ã‚£ãƒ«ã‚¿ |
L33 | `FILTER_SPEC` | G/Dã”ã¨ã®å‰å‡¦ç†ãƒ•ã‚£ãƒ«ã‚¿ | ãƒˆãƒ¬ãƒ³ãƒ‰ãƒã‚¹ã‚¯ã‚„Î²ä¸Šé™è¨­å®š |
L34 | `corrM` | DRRSåˆæœŸãƒ—ãƒ¼ãƒ«ã®æœ€å¤§ä»¶æ•° | ç›¸é–¢è¡Œåˆ—ã‚µã‚¤ã‚ºåˆ¶å¾¡ |
L35 | `DRRS_G` / `DRRS_D` | DRRSãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿dict | ãƒã‚±ãƒƒãƒˆåˆ¥ã®ç›¸é–¢ä½æ¸›è¨­å®š |
L36 | `DRRS_SHRINK` | æ®‹å·®ç›¸é–¢ã®å¯¾è§’ã‚·ãƒ¥ãƒªãƒ³ã‚¯ç‡ | `residual_corr`ã®å®‰å®šåŒ– |
L37 | `CROSS_MU_GD` | G-Dé–“ã‚¯ãƒ­ã‚¹ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£Î¼ | 2ãƒã‚±ãƒƒãƒˆåŒæ™‚æœ€é©åŒ–ã§ç›¸é–¢æŠ‘åˆ¶ |
L38 | `RESULTS_DIR` | é¸å®šçµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª | `_save_sel`/`_load_prev`ã®å…¥å‡ºåŠ› |
L39
L40 é¸å®šçµæœã¯`results/`é…ä¸‹ã«JSONã¨ã—ã¦ä¿å­˜ã—ã€æ¬¡å›å®Ÿè¡Œæ™‚ã«`_load_prev`ã§èª­ã¿è¾¼ã‚“ã§é¸å®šæ¡ä»¶ã«åæ˜ ã€‚
L41
L42 ## DTO/Config
L43 å„ã‚¹ãƒ†ãƒƒãƒ—é–“ã§å—ã‘æ¸¡ã™ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¨è¨­å®šå€¤ã€‚å¤‰æ•°ã®æ„å‘³åˆã„ã¨åˆ©ç”¨ç®‡æ‰€ã‚’ä»¥ä¸‹ã«ç¤ºã™ã€‚
L44
L45 ### InputBundleï¼ˆInput â†’ Scorerï¼‰
L46 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L47 | --- | --- | --- |
L48 | `cand` | å€™è£œéŠ˜æŸ„ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã®ãƒªã‚¹ãƒˆ | OUTãƒ†ãƒ¼ãƒ–ãƒ«ã‚„ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°å¯¾è±¡ã®æ¯é›†å›£ |
L49 | `tickers` | ç¾è¡Œ+å€™è£œã‚’åˆã‚ã›ãŸãƒ†ã‚£ãƒƒã‚«ãƒ¼ä¸€è¦§ | ä¾¡æ ¼ãƒ»å‡ºæ¥é«˜ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€ãƒªã‚¿ãƒ¼ãƒ³è¨ˆç®— |
L50 | `bench` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼ | ç›¸å¯¾å¼·ã•ãƒ»Î²ç®—å‡ºã€ãƒãƒ¼ãƒˆæ¯”è¼ƒ |
L51 | `data` | yfinanceã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœï¼ˆéšå±¤åˆ—ï¼‰ | `px`/`spx`/ãƒªã‚¿ãƒ¼ãƒ³ç­‰ã®åŸºç¤ãƒ‡ãƒ¼ã‚¿ |
L52 | `px` | `data['Close']`ã ã‘ã‚’æŠœãå‡ºã—ãŸä¾¡æ ¼ç³»åˆ— | æŒ‡æ¨™è¨ˆç®—ãƒ»ãƒªã‚¿ãƒ¼ãƒ³ç”Ÿæˆ |
L53 | `spx` | `data['Close'][bench]` ã®Series | `rs`ã‚„`calc_beta`ã®åŸºæº–æŒ‡æ•° |
L54 | `tickers_bulk` | `yf.Tickers`ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ | `info`ç­‰ã®ä¸€æ‹¬å–å¾— |
L55 | `info` | ãƒ†ã‚£ãƒƒã‚«ãƒ¼åˆ¥ã®yfinanceæƒ…å ±dict | ã‚»ã‚¯ã‚¿ãƒ¼åˆ¤å®šã‚„EPSè£œå®Œ |
L56 | `eps_df` | EPS TTM/ç›´è¿‘EPSç­‰ã‚’ã¾ã¨ã‚ãŸè¡¨ | æˆé•·æŒ‡æ¨™ã®ç®—å‡º |
L57 | `fcf_df` | CFOãƒ»CapExãƒ»FCF TTMã¨æƒ…å ±æºãƒ•ãƒ©ã‚° | FCF/EVã‚„é…å½“ã‚«ãƒãƒ¬ãƒƒã‚¸ |
L58 | `returns` | `px.pct_change()`ã®ãƒªã‚¿ãƒ¼ãƒ³è¡¨ | ç›¸é–¢è¡Œåˆ—ãƒ»DRRSè¨ˆç®— |
L59
L60 ### FeatureBundleï¼ˆScorer â†’ Selectorï¼‰
L61 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L62 | --- | --- | --- |
L63 | `df` | è¨ˆç®—æ¸ˆã¿æŒ‡æ¨™ã®ç”Ÿå€¤ãƒ†ãƒ¼ãƒ–ãƒ« | ãƒ‡ãƒãƒƒã‚°ãƒ»å‡ºåŠ›è¡¨ç¤º |
L64 | `df_z` | ã‚¦ã‚£ãƒ³ã‚¶ãƒ¼å¾ŒZã‚¹ã‚³ã‚¢åŒ–ã—ãŸæŒ‡æ¨™è¡¨ | å› å­ã‚¹ã‚³ã‚¢åˆæˆã€é¸å®šåŸºæº– |
L65 | `g_score` | Gãƒã‚±ãƒƒãƒˆç·åˆã‚¹ã‚³ã‚¢ | Gé¸å®šã€IN/OUTæ¯”è¼ƒ |
L66 | `d_score_all` | Dãƒã‚±ãƒƒãƒˆç·åˆã‚¹ã‚³ã‚¢ï¼ˆå…¨éŠ˜æŸ„ï¼‰ | Dé¸å®šã€ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚° |
L67 | `missing_logs` | æ¬ ææŒ‡æ¨™ã¨è£œå®ŒçŠ¶æ³ã®ãƒ­ã‚° | ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ |
L68
L69 ### SelectionBundleï¼ˆSelector â†’ Outputï¼‰
L70 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L71 | --- | --- | --- |
L72 | `resG` | Gé¸å®šçµæœã®è©³ç´°dictï¼ˆ`tickers`ã€ç›®çš„å€¤ç­‰ï¼‰ | çµæœä¿å­˜ãƒ»å¹³å‡ç›¸é–¢ãªã©ã®æŒ‡æ¨™è¡¨ç¤º |
L73 | `resD` | Dé¸å®šçµæœã®è©³ç´°dict | åŒä¸Š |
L74 | `top_G` | æœ€çµ‚æ¡ç”¨Gãƒ†ã‚£ãƒƒã‚«ãƒ¼ | æ–°ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæ§‹ç¯‰ |
L75 | `top_D` | æœ€çµ‚æ¡ç”¨Dãƒ†ã‚£ãƒƒã‚«ãƒ¼ | åŒä¸Š |
L76 | `init_G` | DRRSå‰ã®GåˆæœŸå€™è£œ | æƒœã—ãã‚‚å¤–ã‚ŒãŸéŠ˜æŸ„è¡¨ç¤º |
L77 | `init_D` | DRRSå‰ã®DåˆæœŸå€™è£œ | åŒä¸Š |
L78
L79 ### WeightsConfig
L80 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L81 | --- | --- | --- |
L82 | `g` | Gå› å­ï¼ˆGRW/MOM/VOLï¼‰ã®é‡ã¿dict | `g_score`åˆæˆ |
L83 | `d` | Då› å­ï¼ˆD_QAL/D_YLD/D_VOL_RAW/D_TRDï¼‰ã®é‡ã¿dict | `d_score_all`åˆæˆ |
L84
L85 ### DRRSParams
L86 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L87 | --- | --- | --- |
L88 | `corrM` | DRRSåˆæœŸãƒ—ãƒ¼ãƒ«ã®æœ€å¤§ä»¶æ•° | ç›¸é–¢è¡Œåˆ—ã‚µã‚¤ã‚ºåˆ¶å¾¡ |
L89 | `shrink` | æ®‹å·®ç›¸é–¢ã®ã‚·ãƒ¥ãƒªãƒ³ã‚¯ç‡ | `residual_corr`ã®å¯¾è§’å¼·èª¿ |
L90 | `G` | Gãƒã‚±ãƒƒãƒˆç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿dictï¼ˆ`lookback`ç­‰ï¼‰ | `select_bucket_drrs`è¨­å®š |
L91 | `D` | Dãƒã‚±ãƒƒãƒˆç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿dict | åŒä¸Š |
L92 | `cross_mu_gd` | G-Dã‚¯ãƒ­ã‚¹ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£ä¿‚æ•°Î¼ | `select_buckets`ã®ç›®çš„é–¢æ•° |
L93
L94 ### PipelineConfig
L95 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L96 | --- | --- | --- |
L97 | `weights` | `WeightsConfig`ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ | ã‚¹ã‚³ã‚¢åˆæˆã®é‡ã¿å‚ç…§ |
L98 | `drrs` | `DRRSParams`ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ | é¸å®šã‚¹ãƒ†ãƒƒãƒ—ã®è¨­å®šå€¤ |
L99 | `price_max` | å€™è£œéŠ˜æŸ„ã®è¨±å®¹ä¾¡æ ¼ä¸Šé™ | Inputæ®µéšã§ã®ãƒ•ã‚£ãƒ«ã‚¿ |
L100
L101 ## å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
L102 - `winsorize_s` / `robust_z` : å¤–ã‚Œå€¤å‡¦ç†ã¨Zã‚¹ã‚³ã‚¢åŒ–ã€‚
L103 - `_safe_div` / `_safe_last` : ä¾‹å¤–ã‚’æ½°ã—ãŸåˆ†å‰²ãƒ»æœ«å°¾å–å¾—ã€‚
L104 - `_load_prev` / `_save_sel` : é¸å®šçµæœã®èª­ã¿æ›¸ãã€‚
L105
L106 ## ã‚¯ãƒ©ã‚¹è¨­è¨ˆ
L107 ### Step1: Input
L108 `current_tickers.csv`ã®ç¾è¡ŒéŠ˜æŸ„ã¨`candidate_tickers.csv`ã®æ¤œè¨ä¸­éŠ˜æŸ„ã‚’èµ·ç‚¹ã«ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„ã™ã‚‹ã€‚å¤–éƒ¨I/Oã¨å‰å‡¦ç†ã‚’æ‹…å½“ã—ã€`prepare_data`ã§`InputBundle`ã‚’ç”Ÿæˆã€‚ä¾¡æ ¼ãƒ»è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã¯**yfinanceã‚’å„ªå…ˆã—ã€æ¬ æãŒã‚ã‚‹æŒ‡æ¨™ã®ã¿Finnhub APIã§è£œå®Œ**ã™ã‚‹ã€‚
L109 ä¸»ãªãƒ¡ã‚½ãƒƒãƒ‰:
L110 - `impute_eps_ttm` : å››åŠæœŸEPSÃ—4ã§TTMã‚’æ¨å®šã—æ¬ ææ™‚ã®ã¿å·®ã—æ›¿ãˆã€‚
L111 - `fetch_cfo_capex_ttm_yf` : yfinanceã®å››åŠæœŸ/å¹´æ¬¡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼ã‹ã‚‰CFOãƒ»CapExãƒ»FCF TTMã‚’ç®—å‡ºã€‚
L112 - `fetch_cfo_capex_ttm_finnhub` : yfinanceã§æ¬ ã‘ãŸéŠ˜æŸ„ã®ã¿Finnhub APIã§è£œå®Œã€‚
L113 - `compute_fcf_with_fallback` : yfinanceå€¤ã‚’åŸºæº–ã«Finnhubå€¤ã§ç©´åŸ‹ã‚ã—ã€CFO/CapEx/FCFã¨æƒ…å ±æºãƒ•ãƒ©ã‚°ã‚’è¿”ã™ã€‚
L114 - `_build_eps_df` : `info`ã‚„`quarterly_earnings`ã‹ã‚‰EPS TTMã¨ç›´è¿‘EPSã‚’è¨ˆç®—ã—ã€`impute_eps_ttm`ã§è£œå®Œã€‚
L115 - `prepare_data` :
L116     0. CSVã‹ã‚‰ç¾è¡ŒéŠ˜æŸ„ã¨å€™è£œéŠ˜æŸ„ã®ãƒ†ã‚£ãƒƒã‚«ãƒ¼ä¸€è¦§ã‚’èª­ã¿è¾¼ã‚€ã€‚
L117     1. å€™è£œéŠ˜æŸ„ã®ç¾åœ¨å€¤ã‚’å–å¾—ã—ä¾¡æ ¼ä¸Šé™ã§ãƒ•ã‚£ãƒ«ã‚¿ã€‚
L118     2. æ—¢å­˜+å€™è£œã‹ã‚‰å¯¾è±¡ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚’æ±ºå®šã—ã€ä¾¡æ ¼ãƒ»å‡ºæ¥é«˜ã‚’ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆyfinanceï¼‰ã€‚
L119     3. yfinanceå€¤ã‚’åŸºã«EPS/FCFãƒ†ãƒ¼ãƒ–ãƒ«ã‚„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç³»åˆ—ã€ãƒªã‚¿ãƒ¼ãƒ³ã‚’æ§‹ç¯‰ã—ã€æ¬ æã‚»ãƒ«ã¯Finnhubå‘¼ã³å‡ºã—ã§ç©´åŸ‹ã‚ã€‚
L120     4. ä¸Šè¨˜ã‚’`InputBundle`ã«æ ¼ç´ã—ã¦è¿”ã™ã€‚
L121
L122 ### Step2: Score Calculation (Scorer)
L123 ç‰¹å¾´é‡è¨ˆç®—ã¨ã‚¹ã‚³ã‚¢åˆæˆã‚’æ‹…å½“ã—ã€`FeatureBundle`ã‚’è¿”ã™ã€‚
L124
L125 #### è£œåŠ©é–¢æ•°
L126 - `trend(s)` : 50/150/200æ—¥ç§»å‹•å¹³å‡ã‚„52é€±ãƒ¬ãƒ³ã‚¸ã‹ã‚‰-0.5ã€œ0.5ã§æ§‹æˆã•ã‚ŒãŸãƒˆãƒ¬ãƒ³ãƒ‰æŒ‡æ¨™ã€‚
L127 - `rs(s,b)` / `tr_str(s)` / `rs_line_slope(s,b,win)` : ç›¸å¯¾å¼·ã•ã‚„çŸ­æœŸãƒˆãƒ¬ãƒ³ãƒ‰ã€RSå›å¸°å‚¾ãã‚’ç®—å‡ºã€‚
L128 - `ev_fallback` : `enterpriseValue`æ¬ ææ™‚ã«è² å‚µãƒ»ç¾é‡‘ã‹ã‚‰EVã‚’æ¨å®šã€‚
L129 - `dividend_status` / `div_streak` : é…å½“æœªè¨­å®šçŠ¶æ³ã®åˆ¤å®šã¨å¢—é…å¹´æ•°ã‚«ã‚¦ãƒ³ãƒˆã€‚
L130 - `fetch_finnhub_metrics` : Finnhub APIã‹ã‚‰EPSæˆé•·ãƒ»ROEãƒ»Î²ãªã©ä¸è¶³æŒ‡æ¨™ã‚’å–å¾—ã€‚
L131 - `calc_beta` : ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ã®å…±åˆ†æ•£ã‹ã‚‰Î²ã‚’ç®—å‡ºã€‚
L132 - `spx_to_alpha` : S&P500ã®ä½ç½®æƒ…å ±ã‹ã‚‰DRRSã§ç”¨ã„ã‚‹Î±ã‚’æ¨å®šã€‚
L133 - `soft_cap_effective_scores` / `pick_top_softcap` : ã‚»ã‚¯ã‚¿ãƒ¼ã‚½ãƒ•ãƒˆã‚­ãƒ£ãƒƒãƒ—ä»˜ãã‚¹ã‚³ã‚¢èª¿æ•´ã¨ä¸Šä½æŠ½å‡ºã€‚
L134
L135 **è£œåŠ©é–¢æ•°ã¨ç”ŸæˆæŒ‡æ¨™**
L136
L137 | è£œåŠ©é–¢æ•° | ç”ŸæˆæŒ‡æ¨™ | ç•¥ç§° |
L138 | --- | --- | --- |
L139 | `trend` | ãƒˆãƒ¬ãƒ³ãƒ‰ç·åˆå€¤ | `TR` |
L140 | `rs` | ç›¸å¯¾å¼·ã• | `RS` |
L141 | `tr_str` | ä¾¡æ ¼ã¨50æ—¥ç·šã®ä¹–é›¢ | `TR_str` |
L142 | `rs_line_slope` | RSç·šã®å›å¸°å‚¾ã | `RS_SLOPE_*` |
L143 | `calc_beta` | Î² | `BETA` |
L144 | `div_streak` | é€£ç¶šå¢—é…å¹´æ•° | `DIV_STREAK` |
L145
L146 #### `aggregate_scores` è©³ç´°
L147 1. å„éŠ˜æŸ„ã®ä¾¡æ ¼ç³»åˆ—ã‚„`info`ã‚’åŸºã«ä»¥ä¸‹ã‚’ç®—å‡ºã€‚
L148    - **ãƒˆãƒ¬ãƒ³ãƒ‰/ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ **: `TR`ã€`RS`ã€`TR_str`ã€å¤šæ§˜ãªç§»å‹•å¹³å‡æ¯”ã€`RS_SLOPE_*`ãªã©ã€‚
L149    - **ãƒªã‚¹ã‚¯**: `BETA`ã€`DOWNSIDE_DEV`ã€`MDD_1Y`ã€`RESID_VOL`ã€`DOWN_OUTPERF`ã€`EXT_200`ç­‰ã€‚
L150    - **é…å½“**: `DIV`ã€`DIV_TTM_PS`ã€`DIV_VAR5`ã€`DIV_YOY`ã€`DIV_FCF_COVER`ã€`DIV_STREAK`ã€‚
L151    - **è²¡å‹™ãƒ»æˆé•·**: `EPS`ã€`REV`ã€`ROE`ã€`FCF/EV`ã€`REV_Q_YOY`ã€`EPS_Q_YOY`ã€`REV_YOY_ACC`ã€`REV_YOY_VAR`ã€`REV_ANN_STREAK`ã€`RULE40`ã€`FCF_MGN` ç­‰ã€‚
L152    - **å®‰å®šæ€§/ã‚µã‚¤ã‚º**: `DEBT2EQ`ã€`CURR_RATIO`ã€`MARKET_CAP`ã€`ADV60_USD`ã€`EPS_VAR_8Q`ãªã©ã€‚
L153 2. æŒ‡æ¨™æ¬ æã¯Finnhub APIç­‰ã§è£œå®Œã—ã€æœªå–å¾—é …ç›®ã‚’`missing_logs`ã«è¨˜éŒ²ã€‚
L154 3. `winsorize_s`â†’`robust_z`ã§æ¨™æº–åŒ–ã—`df_z`ã¸ä¿å­˜ã€‚ã‚µã‚¤ã‚ºãƒ»æµå‹•æ€§ã¯å¯¾æ•°å¤‰æ›ã€‚
L155 4. æ­£è¦åŒ–æ¸ˆæŒ‡æ¨™ã‹ã‚‰å› å­ã‚¹ã‚³ã‚¢ã‚’åˆæˆã€‚
L156    - å„å› å­ã®æ§‹æˆã¨é‡ã¿ã¯ä»¥ä¸‹ã®é€šã‚Šã€‚
L157      - **GRW**: 0.30Ã—`REV` + 0.20Ã—`EPS_Q_YOY` + 0.15Ã—`REV_Q_YOY` + 0.15Ã—`REV_YOY_ACC` + 0.10Ã—`RULE40` + 0.10Ã—`FCF_MGN` + 0.10Ã—`REV_ANN_STREAK` âˆ’ 0.05Ã—`REV_YOY_VAR`ã€‚
L158      - **MOM**: 0.40Ã—`RS` + 0.15Ã—`TR_str` + 0.15Ã—`RS_SLOPE_6W` + 0.15Ã—`RS_SLOPE_13W` + 0.10Ã—`MA200_SLOPE_5M` + 0.10Ã—`MA200_UP_STREAK_D`ã€‚
L159      - **VOL**: `BETA`å˜ä½“ã‚’ä½¿ç”¨ã€‚
L160      - **QAL**: 0.60Ã—`FCF_W` + 0.40Ã—`ROE_W`ã§ä½œæˆã€‚
L161      - **YLD**: 0.30Ã—`DIV` + 0.70Ã—`DIV_STREAK`ã€‚
L162      - **D_QAL**: 0.35Ã—`QAL` + 0.20Ã—`FCF` + 0.15Ã—`CURR_RATIO` âˆ’ 0.15Ã—`DEBT2EQ` âˆ’ 0.15Ã—`EPS_VAR_8Q`ã€‚
L163      - **D_YLD**: 0.45Ã—`DIV` + 0.25Ã—`DIV_STREAK` + 0.20Ã—`DIV_FCF_COVER` âˆ’ 0.10Ã—`DIV_VAR5`ã€‚
L164      - **D_VOL_RAW**: 0.40Ã—`DOWNSIDE_DEV` + 0.22Ã—`RESID_VOL` + 0.18Ã—`MDD_1Y` âˆ’ 0.10Ã—`DOWN_OUTPERF` âˆ’ 0.05Ã—`EXT_200` âˆ’ 0.08Ã—`SIZE` âˆ’ 0.10Ã—`LIQ` + 0.10Ã—`BETA`ã€‚
L165      - **D_TRD**: 0.40Ã—`MA200_SLOPE_5M` âˆ’ 0.30Ã—`EXT_200` + 0.15Ã—`NEAR_52W_HIGH` + 0.15Ã—`TR`ã€‚
L166     - ä¸»ãªæŒ‡æ¨™ã®ç•¥ç§°ã¨æ„å‘³:
L167
L168       | ç•¥ç§° | è£œåŠ©é–¢æ•° | æ¦‚è¦ |
L169       | --- | --- | --- |
L170       | TR | `trend` | 50/150/200æ—¥ç§»å‹•å¹³å‡ã¨52é€±ãƒ¬ãƒ³ã‚¸ã‚’çµ„ã¿åˆã‚ã›ãŸãƒˆãƒ¬ãƒ³ãƒ‰ç·åˆå€¤ |
L171       | RS | `rs` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«å¯¾ã™ã‚‹ç›¸å¯¾å¼·ã•ï¼ˆ12M/1Mãƒªã‚¿ãƒ¼ãƒ³å·®ï¼‰ |
L172       | TR_str | `tr_str` | ä¾¡æ ¼ã¨50æ—¥ç§»å‹•å¹³å‡ã®ä¹–é›¢ |
L173       | RS_SLOPE_6W | `rs_line_slope` | ç›¸å¯¾å¼·ã•ç·šã®6é€±å›å¸°å‚¾ã |
L174       | RS_SLOPE_13W | `rs_line_slope` | ç›¸å¯¾å¼·ã•ç·šã®13é€±å›å¸°å‚¾ã |
L175       | MA200_SLOPE_5M | - | 200æ—¥ç§»å‹•å¹³å‡ã®5ã‹æœˆé¨°è½ç‡ |
L176       | MA200_UP_STREAK_D | - | 200æ—¥ç·šãŒé€£ç¶šã§ä¸Šå‘ã„ãŸæ—¥æ•° |
L177       | BETA | `calc_beta` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«å¯¾ã™ã‚‹Î² |
L178       | DOWNSIDE_DEV | - | ä¸‹æ–¹ãƒªã‚¿ãƒ¼ãƒ³ã®ã¿ã®å¹´ç‡åŒ–æ¨™æº–åå·® |
L179       | RESID_VOL | - | Î²ã§èª¿æ•´ã—ãŸæ®‹å·®ãƒªã‚¿ãƒ¼ãƒ³ã®å¹´ç‡åŒ–æ¨™æº–åå·® |
L180       | MDD_1Y | - | éå»1å¹´ã®æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³ |
L181       | DOWN_OUTPERF | - | å¸‚å ´ä¸‹è½æ—¥ã«å¯¾ã™ã‚‹å¹³å‡è¶…éãƒªã‚¿ãƒ¼ãƒ³ |
L182       | EXT_200 | - | 200æ—¥ç§»å‹•å¹³å‡ã‹ã‚‰ã®çµ¶å¯¾ä¹–é›¢ç‡ |
L183       | NEAR_52W_HIGH | - | 52é€±é«˜å€¤ã¾ã§ã®ä¸‹æ–¹è·é›¢ï¼ˆ0=é«˜å€¤ï¼‰ |
L184       | FCF_W | - | ã‚¦ã‚£ãƒ³ã‚¶ãƒ¼å‡¦ç†å¾Œã®FCF/EV |
L185       | ROE_W | - | ã‚¦ã‚£ãƒ³ã‚¶ãƒ¼å‡¦ç†å¾Œã®ROE |
L186       | FCF | - | FCF/EV |
L187       | QAL | - | FCF_Wã¨ROE_Wã‚’çµ„ã¿åˆã‚ã›ãŸå“è³ªã‚¹ã‚³ã‚¢ |
L188       | CURR_RATIO | - | æµå‹•æ¯”ç‡ |
L189       | DEBT2EQ | - | è² å‚µè³‡æœ¬å€ç‡ |
L190       | EPS_VAR_8Q | - | EPSã®8å››åŠæœŸæ¨™æº–åå·® |
L191       | DIV | - | å¹´ç‡æ›ç®—é…å½“åˆ©å›ã‚Š |
L192       | DIV_STREAK | `div_streak` | é€£ç¶šå¢—é…å¹´æ•° |
L193       | DIV_FCF_COVER | - | é…å½“ã®FCFã‚«ãƒãƒ¬ãƒƒã‚¸ |
L194       | DIV_VAR5 | - | 5å¹´é…å½“å¤‰å‹•ç‡ |
L195       | DIV_TTM_PS | - | 1æ ªå½“ãŸã‚ŠTTMé…å½“ |
L196       | DIV_YOY | - | å‰å¹´æ¯”é…å½“æˆé•·ç‡ |
L197       | REV | - | å£²ä¸Šæˆé•·ç‡TTM |
L198       | EPS_Q_YOY | - | å››åŠæœŸEPSã®å‰å¹´åŒæœŸæ¯” |
L199       | REV_Q_YOY | - | å››åŠæœŸå£²ä¸Šã®å‰å¹´åŒæœŸæ¯” |
L200       | REV_YOY_ACC | - | å£²ä¸Šæˆé•·ç‡ã®åŠ é€Ÿåˆ† |
L201       | RULE40 | - | å£²ä¸Šæˆé•·ç‡ã¨FCFãƒãƒ¼ã‚¸ãƒ³ã®åˆè¨ˆ |
L202       | FCF_MGN | - | FCFãƒãƒ¼ã‚¸ãƒ³ |
L203       | REV_ANN_STREAK | - | å¹´æ¬¡å£²ä¸Šæˆé•·ã®é€£ç¶šå¹´æ•° |
L204       | REV_YOY_VAR | - | å¹´æ¬¡å£²ä¸Šæˆé•·ç‡ã®å¤‰å‹•æ€§ |
L205       | SIZE | - | æ™‚ä¾¡ç·é¡ã®å¯¾æ•°å€¤ |
L206       | LIQ | - | 60æ—¥å¹³å‡å‡ºæ¥é«˜ãƒ‰ãƒ«ã®å¯¾æ•°å€¤ |
L207    - Gãƒã‚±ãƒƒãƒˆ: `GRW`ã€`MOM`ã€`VOL`ã‚’`cfg.weights.g`ï¼ˆ0.40/0.45/-0.15ï¼‰ã§åŠ é‡ã—`g_score`ã‚’å¾—ã‚‹ã€‚
L208    - Dãƒã‚±ãƒƒãƒˆ: `D_QAL`ã€`D_YLD`ã€`D_VOL_RAW`ã€`D_TRD`ã‚’`cfg.weights.d`ï¼ˆ0.15/0.15/-0.45/0.25ï¼‰ã§åŠ é‡ã—`d_score_all`ã‚’ç®—å‡ºã€‚
L209    - ã‚»ã‚¯ã‚¿ãƒ¼capã«ã‚ˆã‚‹`soft_cap_effective_scores`ã‚’é©ç”¨ã—ã€Gæ¡ç”¨éŠ˜æŸ„ã«ã¯ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨ã€‚
L210 5. `_apply_growth_entry_flags`ã§ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆ/æŠ¼ã—ç›®ç™ºç«çŠ¶æ³ã‚’ä»˜åŠ ã—ã€`FeatureBundle`ã‚’è¿”ã™ã€‚
L211
L212 ### Step3: Correlation Reduction & Selection (Selector)
L213 DRRSã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ç›¸é–¢ã‚’æŠ‘ãˆãŸéŠ˜æŸ„é¸å®šã‚’è¡Œã„ã€`SelectionBundle`ã‚’è¿”ã™ã€‚`results/`ã«ä¿å­˜ã•ã‚ŒãŸå‰å›é¸å®šï¼ˆ`G_selection.json` / `D_selection.json`ï¼‰ã‚’`_load_prev`ã§èª­ã¿è¾¼ã¿ã€ç›®çš„å€¤ãŒå¤§ããæ‚ªåŒ–ã—ãªã„é™ã‚Šç¶­æŒã™ã‚‹ã€‚æ–°ã—ã„æ¡ç”¨é›†åˆã¯`_save_sel`ã§JSONã«æ›¸ãå‡ºã—æ¬¡å›ä»¥é™ã®å…¥åŠ›ã«å‚™ãˆã‚‹ã€‚
L214 ä¸»ãªãƒ¡ã‚½ãƒƒãƒ‰:
L215 - `residual_corr` : åç›Šç‡è¡Œåˆ—ã‚’Zã‚¹ã‚³ã‚¢åŒ–ã—ã€ä¸Šä½ä¸»æˆåˆ†ã‚’é™¤å»ã—ãŸæ®‹å·®ã‹ã‚‰ç›¸é–¢è¡Œåˆ—ã‚’æ±‚ã‚ã€å¹³å‡ç›¸é–¢ã«å¿œã˜ã¦ã‚·ãƒ¥ãƒªãƒ³ã‚¯ã€‚
L216 - `rrqr_like_det` : ã‚¹ã‚³ã‚¢ã‚’é‡ã¿ä»˜ã‘ã—ãŸQRåˆ†è§£é¢¨ã®æ‰‹é †ã§åˆæœŸå€™è£œã‚’kä»¶æŠ½å‡ºã—ã€ã‚¹ã‚³ã‚¢ã®é«˜ã„éç›¸é–¢ãªé›†åˆã‚’å¾—ã‚‹ã€‚
L217 - `swap_local_det` / `swap_local_det_cross` : `sum(score) - Î»*within_corr - Î¼*cross_corr`ã‚’ç›®çš„é–¢æ•°ã¨ã—ã¦ã€å…¥ã‚Œæ›¿ãˆæ¢ç´¢ã§å±€æ‰€çš„ã«æœ€é©åŒ–ã€‚
L218 - `select_bucket_drrs` : ãƒ—ãƒ¼ãƒ«éŠ˜æŸ„ã¨ã‚¹ã‚³ã‚¢ã‹ã‚‰æ®‹å·®ç›¸é–¢ã‚’è¨ˆç®—ã—ã€ä¸Šè¨˜2æ®µéš(åˆæœŸé¸æŠâ†’å…¥ã‚Œæ›¿ãˆ)ã§kéŠ˜æŸ„ã‚’æ±ºå®šã€‚éå»æ¡ç”¨éŠ˜æŸ„ã¨ã®æ¯”è¼ƒã§ç›®çš„å€¤ãŒåŠ£åŒ–ã—ãªã‘ã‚Œã°ç¶­æŒã™ã‚‹ã€‚
L219 - `select_buckets` : Gãƒã‚±ãƒƒãƒˆã‚’é¸å®šå¾Œã€ãã®çµæœã‚’é™¤ã„ãŸå€™è£œã‹ã‚‰Dãƒã‚±ãƒƒãƒˆã‚’é¸ã¶ã€‚Dé¸å®šæ™‚ã¯Gã¨ã®ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£Î¼ã‚’ä»˜ä¸ã—ã€ä¸¡ãƒã‚±ãƒƒãƒˆã®åˆ†æ•£ã‚’åˆ¶å¾¡ã™ã‚‹ã€‚
L220
L221 #### ç›¸é–¢ä½æ¸›ãƒ­ã‚¸ãƒƒã‚¯è©³ç´°
L222 1. **æ®‹å·®ç›¸é–¢è¡Œåˆ—ã®æ§‹ç¯‰ (`residual_corr`)**
L223    - ãƒªã‚¿ãƒ¼ãƒ³è¡Œåˆ—`R`ã‚’Zã‚¹ã‚³ã‚¢åŒ–ã€‚
L224    - SVDã§ä¸Šä½`n_pc`ä¸»æˆåˆ†`F`ã‚’æ±‚ã‚ã€æœ€å°äºŒä¹—ã§ä¿‚æ•°`B`ã‚’ç®—å‡ºã—æ®‹å·®`E = Z - F@B`ã‚’å¾—ã‚‹ã€‚
L225    - `E`ã®ç›¸é–¢è¡Œåˆ—`C`ã‚’è¨ˆç®—ã—ã€å¹³å‡çµ¶å¯¾ç›¸é–¢ã«å¿œã˜ã¦ã‚·ãƒ¥ãƒªãƒ³ã‚¯é‡`shrink_eff`ã‚’è£œæ­£ã—ã¦å¯¾è§’ã‚’å¼·èª¿ã€‚
L226 2. **åˆæœŸå€™è£œã®æŠ½å‡º (`rrqr_like_det`)**
L227    - ã‚¹ã‚³ã‚¢ã‚’0-1æ­£è¦åŒ–ã—ãŸé‡ã¿`w`ã¨ã—ã€`Z*(1+Î³w)`ã§åˆ—ãƒãƒ«ãƒ ã‚’å¼·èª¿ã€‚
L228    - æ®‹å·®ãƒãƒ«ãƒ æœ€å¤§ã®åˆ—ã‚’é€æ¬¡é¸ã³ã€QRãƒ©ã‚¤ã‚¯ãªãƒ‡ãƒ•ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã£ã¦éç›¸é–¢ã‹ã¤é«˜ã‚¹ã‚³ã‚¢ãª`k`éŠ˜æŸ„é›†åˆ`S0`ã‚’å¾—ã‚‹ã€‚
L229 3. **å±€æ‰€æ¢ç´¢ (`swap_local_det` / `swap_local_det_cross`)**
L230    - ç›®çš„é–¢æ•°`Î£z_score âˆ’ Î»Â·within_corr âˆ’ Î¼Â·cross_corr`ã‚’æœ€å¤§åŒ–ã€‚
L231    - é¸æŠé›†åˆã®å„éŠ˜æŸ„ã‚’ä»–å€™è£œã¨å…¥ã‚Œæ›¿ãˆã€æ”¹å–„ãŒãªããªã‚‹ã¾ã§ã¾ãŸã¯`max_pass`å›ã¾ã§æ¢ç´¢ã€‚
L232    - `swap_local_det_cross`ã¯Gãƒã‚±ãƒƒãƒˆã¨ã®ã‚¯ãƒ­ã‚¹ç›¸é–¢è¡Œåˆ—`C_cross`ã‚’ä½¿ç”¨ã—ã€ãƒšãƒŠãƒ«ãƒ†ã‚£`Î¼`ã‚’ä»˜ä¸ã€‚
L233 4. **éå»æ¡ç”¨ã®ç¶­æŒã¨ã‚¯ãƒ­ã‚¹ãƒšãƒŠãƒ«ãƒ†ã‚£ (`select_bucket_drrs` / `select_buckets`)**
L234    - å±€æ‰€æ¢ç´¢çµæœ`S`ã¨éå»é›†åˆ`P`ã®ç›®çš„å€¤ã‚’æ¯”è¼ƒã—ã€`S`ãŒ`P`ã‚ˆã‚Š`Î·`æœªæº€ã®æ”¹å–„ãªã‚‰`P`ã‚’ç¶­æŒã€‚
L235    - `select_buckets`ã§ã¯Gã‚’å…ˆã«æ±ºå®šã—ã€Dé¸å®šæ™‚ã«Gã¨ã®ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£`Î¼`ã‚’åŠ ãˆã¦ã‚¯ãƒ­ã‚¹åˆ†æ•£ã‚’æŠ‘åˆ¶ã€‚
L236
L237 ### Step4: Output
L238 é¸å®šçµæœã‚’å¯è¦–åŒ–ã—å…±æœ‰ã™ã‚‹å·¥ç¨‹ã€‚ä»¥ä¸‹ã®å†…å®¹ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«åŒ–ã—ã¦æ¨™æº–å‡ºåŠ›ã¨Slackã¸é€ã‚‹ã€‚
L239 - æ¡ç”¨éŠ˜æŸ„ã¨æƒœã—ãã‚‚é¸å¤–ã¨ãªã£ãŸéŠ˜æŸ„ã®ã‚¹ã‚³ã‚¢ä¸€è¦§
L240 - IN/OUTãƒªã‚¹ãƒˆã¨OUTéŠ˜æŸ„ã®ã‚¹ã‚³ã‚¢ï¼ˆä½å¾—ç‚¹éŠ˜æŸ„ã‚’ç¢ºèªã—ã‚„ã™ãï¼‰
L241 - æ–°æ—§ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®æ¯”è¼ƒè¡¨ï¼ˆçµ„å…¥ã‚Œãƒ»é™¤å¤–ã€ã‚¹ã‚³ã‚¢å¤‰åŒ–ï¼‰
L242 - æ¤œè¨ä¸­éŠ˜æŸ„ã®ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°
L243
L244 ä¸»ãªãƒ¡ã‚½ãƒƒãƒ‰:
L245 - `display_results` : ä¸Šè¨˜ãƒ†ãƒ¼ãƒ–ãƒ«ã«åŠ ãˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã‚„åˆ†æ•£åŒ–æŒ‡æ¨™ã‚’è¡¨ç¤ºã€‚
L246 - `notify_slack` : Slack Webhookã¸åŒå†…å®¹ã‚’é€ä¿¡ã€‚
L247 - è£œåŠ©:`_avg_offdiag`ã€`_resid_avg_rho`ã€`_raw_avg_rho`ã€`_cross_block_raw_rho`ã€‚
L248
L249 ## ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
L250 1. `PipelineConfig`ã‚’æ§‹ç¯‰ã€‚
L251 2. **Step1** `Input.prepare_data`ã§`InputBundle`ã‚’ç”Ÿæˆã€‚
L252 3. **Step2** `Scorer.aggregate_scores`ã§`FeatureBundle`ã‚’å–å¾—ã€‚
L253 4. **Step3** `Selector.select_buckets`ã§`SelectionBundle`ã‚’ç®—å‡ºã€‚
L254 5. **Step4** `Output.display_results`ã¨`notify_slack`ã§çµæœã‚’å‡ºåŠ›ã€‚
```

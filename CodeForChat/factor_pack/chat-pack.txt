# === Chat Paste Pack ===
# Repo: dakara32/GPT_Code @ main
# Files: config.py, factor.py, scorer.py, .github/workflows/weekly-report.yml, documents/README.md, documents/factor_design.md
# ä½œæˆæ—¥æ™‚: 2025-09-19 20:27:28 (JST)
# ä½¿ã„æ–¹: ä¸‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’é †ã«è²¼ã‚Œã°ã“ã®ãƒãƒ£ãƒƒãƒˆã§å…¨ä½“æŠŠæ¡ã§ãã¾ã™ã€‚
# æ³¨è¨˜: å„ãƒ•ã‚¡ã‚¤ãƒ«ã¯å€‹åˆ¥ã« L1.. ã§è¡Œç•ªå·ä»˜ä¸ã€‚
---

## <config.py>
```text
L1 # å…±é€šè¨­å®šï¼ˆfactor / drift ã‹ã‚‰å‚ç…§ï¼‰
L2 from dataclasses import dataclass
L3
L4 TOTAL_TARGETS = 20
L5
L6 # åŸºæº–ã®ãƒã‚±ãƒƒãƒˆæ•°ï¼ˆNORMALï¼‰
L7 COUNTS_BASE = {"G": 12, "D": 8}
L8
L9 # ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®æ¨å¥¨ãƒã‚±ãƒƒãƒˆæ•°
L10 COUNTS_BY_MODE = {
L11     "NORMAL": {"G": 12, "D": 8},
L12     "CAUTION": {"G": 10, "D": 8},
L13     "EMERG": {"G": 8,  "D": 8},
L14 }
L15
L16 # ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®ãƒ‰ãƒªãƒ•ãƒˆé–¾å€¤ï¼ˆ%ï¼‰
L17 DRIFT_THRESHOLD_BY_MODE = {"NORMAL": 12, "CAUTION": 14, "EMERG": float("inf")}
L18
L19 # ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®TSï¼ˆåŸºæœ¬å¹…, å°æ•°=å‰²åˆï¼‰
L20 TS_BASE_BY_MODE = {"NORMAL": 0.15, "CAUTION": 0.13, "EMERG": 0.10}
L21 # åˆ©ç›Šåˆ°é”(+30/+60/+100%)æ™‚ã®æ®µéšã‚¿ã‚¤ãƒˆåŒ–ï¼ˆãƒã‚¤ãƒ³ãƒˆå·®ï¼‰
L22 TS_STEP_DELTAS_PT = (3, 6, 8)
L23
L24 # Breadthã®æ ¡æ­£ã¯ N_G ã«é€£å‹•ï¼ˆç·Šæ€¥è§£é™¤=ceil(1.5*N_G), é€šå¸¸å¾©å¸°=3*N_Gï¼‰
L25 N_G = COUNTS_BASE["G"]
L26 N_D = COUNTS_BASE["D"]
L27
```

## <factor.py>
```text
L1 '''ROLE: Orchestration ONLYï¼ˆå¤–éƒ¨I/Oãƒ»SSOTãƒ»Slackå‡ºåŠ›ï¼‰, è¨ˆç®—ã¯ scorer.py'''
L2 # === NOTE: æ©Ÿèƒ½ãƒ»å…¥å‡ºåŠ›ãƒ»ãƒ­ã‚°æ–‡è¨€ãƒ»ä¾‹å¤–æŒ™å‹•ã¯ä¸å¤‰ã€‚å®‰å…¨ãªçŸ­ç¸®ï¼ˆimportçµ±åˆ/è¤‡æ•°ä»£å…¥/å†…åŒ…è¡¨è¨˜/ãƒ¡ã‚½ãƒƒãƒ‰ãƒã‚§ãƒ¼ãƒ³/ä¸€è¡ŒåŒ–/ç©ºè¡Œåœ§ç¸®ãªã©ï¼‰ã®ã¿é©ç”¨ ===
L3 BONUS_COEFF = 0.55  # æ¨å¥¨: æ”»ã‚=0.45 / ä¸­åº¸=0.55 / å®ˆã‚Š=0.65
L4 SWAP_DELTA_Z = 0.15   # åƒ…å·®åˆ¤å®š: Ïƒã®15%ã€‚(ç·©ã‚=0.10 / æ¨™æº–=0.15 / å›ºã‚=0.20)
L5 SWAP_KEEP_BUFFER = 3  # n_target+ã“ã®é †ä½ä»¥å†…ã®ç¾è¡Œã¯ä¿æŒã€‚(ç²˜ã‚Šå¼±=2 / æ¨™æº–=3 / ç²˜ã‚Šå¼·=4ã€œ5)
L6 import os, time, requests
L7 import logging
L8 from time import perf_counter
L9 from dataclasses import dataclass
L10 from typing import Any, Dict, List, Tuple
L11 from concurrent.futures import ThreadPoolExecutor
L12 import numpy as np
L13 import pandas as pd
L14 import yfinance as yf
L15 from scipy.stats import zscore  # used via scorer
L16 from scorer import Scorer, ttm_div_yield_portfolio, _log
L17 import config
L18
L19 # ãã®ä»–
L20 debug_mode, FINNHUB_API_KEY = True, os.environ.get("FINNHUB_API_KEY")
L21
L22 logger = logging.getLogger(__name__)
L23 if debug_mode:
L24     logging.basicConfig(level=logging.INFO, force=True)
L25 else:
L26     logging.basicConfig(level=logging.WARNING, force=True)
L27
L28 class T:
L29     t = perf_counter()
L30     log = staticmethod(lambda tag: (lambda now=perf_counter(): (print(f"[T] {tag}: {now - T.t:.2f}s"), setattr(T, "t", now))[-1])())
L31
L32 T.log("start")
L33
L34 # === ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã¨å®šæ•°ï¼ˆå†’é ­ã«å›ºå®šï¼‰ ===
L35 exist, cand = [pd.read_csv(f, header=None)[0].tolist() for f in ("current_tickers.csv","candidate_tickers.csv")]
L36 T.log(f"csv loaded: exist={len(exist)} cand={len(cand)}")
L37 CAND_PRICE_MAX, bench = 450, '^GSPC'  # ä¾¡æ ¼ä¸Šé™ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
L38 N_G, N_D = config.N_G, config.N_D  # G/Dæ ã‚µã‚¤ã‚ºï¼ˆNORMALåŸºæº–: G12/D8ï¼‰
L39 g_weights = {'GROWTH_F':0.35,'MOM':0.55,'VOL':-0.10}
L40 D_BETA_MAX = float(os.environ.get("D_BETA_MAX", "0.8"))
L41 FILTER_SPEC = {"G":{"pre_mask":["trend_template"]},"D":{"pre_filter":{"beta_max":D_BETA_MAX}}}
L42 D_weights = {'QAL':0.1,'YLD':0.3,'VOL':-0.5,'TRD':0.1}
L43 _fmt_w = lambda w: " ".join(f"{k}{int(v*100)}" for k, v in w.items())
L44
L45 # DRRS åˆæœŸãƒ—ãƒ¼ãƒ«ãƒ»å„ç¨®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
L46 corrM = 45
L47 DRRS_G, DRRS_D = dict(lookback=252,n_pc=3,gamma=1.2,lam=0.68,eta=0.8), dict(lookback=504,n_pc=4,gamma=0.8,lam=0.85,eta=0.5)
L48 DRRS_SHRINK = 0.10  # æ®‹å·®ç›¸é–¢ã®å¯¾è§’ã‚·ãƒ¥ãƒªãƒ³ã‚¯ï¼ˆåŸºç¤ï¼‰
L49
L50 # ã‚¯ãƒ­ã‚¹ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆæœªå®šç¾©ãªã‚‰è¨­å®šï¼‰
L51 try: CROSS_MU_GD
L52 except NameError: CROSS_MU_GD = 0.40  # æ¨å¥¨ 0.35â€“0.45ï¼ˆlam=0.85æƒ³å®šï¼‰
L53
L54 # å‡ºåŠ›é–¢é€£
L55 RESULTS_DIR = "results"
L56 os.makedirs(RESULTS_DIR, exist_ok=True)
L57
L58 # === å…±æœ‰DTOï¼ˆã‚¯ãƒ©ã‚¹é–“I/Oå¥‘ç´„ï¼‰ï¼‹ Config ===
L59 @dataclass(frozen=True)
L60 class InputBundle:
L61     # Input â†’ Scorer ã§å—ã‘æ¸¡ã™ç´ æï¼ˆI/Oç¦æ­¢ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ï¼‰
L62     cand: List[str]
L63     tickers: List[str]
L64     bench: str
L65     data: pd.DataFrame              # yfinance downloadçµæœï¼ˆ'Close','Volume'ç­‰ã®éšå±¤åˆ—ï¼‰
L66     px: pd.DataFrame                # data['Close']
L67     spx: pd.Series                  # data['Close'][bench]
L68     tickers_bulk: object            # yfinance.Tickers
L69     info: Dict[str, dict]           # yfinance info per ticker
L70     eps_df: pd.DataFrame            # ['eps_ttm','eps_q_recent',...]
L71     fcf_df: pd.DataFrame            # ['fcf_ttm', ...]
L72     returns: pd.DataFrame           # px[tickers].pct_change()
L73
L74 @dataclass(frozen=True)
L75 class FeatureBundle:
L76     df: pd.DataFrame
L77     df_z: pd.DataFrame
L78     g_score: pd.Series
L79     d_score_all: pd.Series
L80     missing_logs: pd.DataFrame
L81     df_full: pd.DataFrame | None = None
L82     df_full_z: pd.DataFrame | None = None
L83     scaler: Any | None = None
L84
L85 @dataclass(frozen=True)
L86 class SelectionBundle:
L87     resG: dict
L88     resD: dict
L89     top_G: List[str]
L90     top_D: List[str]
L91     init_G: List[str]
L92     init_D: List[str]
L93
L94 @dataclass(frozen=True)
L95 class WeightsConfig:
L96     g: Dict[str,float]
L97     d: Dict[str,float]
L98
L99 @dataclass(frozen=True)
L100 class DRRSParams:
L101     corrM: int
L102     shrink: float
L103     G: Dict[str,float]   # lookback, n_pc, gamma, lam, eta
L104     D: Dict[str,float]
L105     cross_mu_gd: float
L106
L107 @dataclass(frozen=True)
L108 class PipelineConfig:
L109     weights: WeightsConfig
L110     drrs: DRRSParams
L111     price_max: float
L112     debug_mode: bool = False
L113
L114 # === å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆè¤‡æ•°ã‚¯ãƒ©ã‚¹ã§ä½¿ç”¨ï¼‰ ===
L115 # (unused local utils removed â€“ use scorer.py versions if needed)
L116
L117 _env_true = lambda name, default=False: (os.getenv(name) or str(default)).strip().lower() == "true"
L118
L119 def _post_slack(payload: dict):
L120     url = os.getenv("SLACK_WEBHOOK_URL")
L121     if not url: print("âš ï¸ SLACK_WEBHOOK_URL æœªè¨­å®š"); return
L122     try:
L123         requests.post(url, json=payload).raise_for_status()
L124     except Exception as e:
L125         print(f"âš ï¸ Slacké€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
L126
L127 def _slack_send_text_chunks(url: str, text: str, chunk: int = 2800) -> None:
L128     """Slackã¸ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²é€ä¿¡ï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å½¢å¼ï¼‰ã€‚"""
L129
L130     def _post_text(payload: str) -> None:
L131         try:
L132             resp = requests.post(url, json={"text": payload})
L133             print(f"[DBG] debug_post status={getattr(resp, 'status_code', None)} size={len(payload)}")
L134             if resp is not None:
L135                 resp.raise_for_status()
L136         except Exception as e:
L137             print(f"[ERR] debug_post_failed: {e}")
L138
L139     body = str(text or "").strip()
L140     if not body:
L141         print("[DBG] skip debug send: empty body")
L142         return
L143
L144     lines = body.splitlines()
L145     block: list[str] = []
L146     block_len = 0
L147
L148     def _flush() -> None:
L149         nonlocal block, block_len
L150         if not block:
L151             return
L152         payload = "```" + "\n".join(block) + "```"
L153         _post_text(payload)
L154         block, block_len = [], 0
L155
L156     for raw in lines:
L157         line = raw or ""
L158         while len(line) > chunk:
L159             head, line = line[:chunk], line[chunk:]
L160             _flush()
L161             _post_text("```" + head + "```")
L162         add_len = len(line) if not block else len(line) + 1
L163         if block and block_len + add_len > chunk:
L164             _flush()
L165             add_len = len(line)
L166         block.append(line)
L167         block_len += add_len
L168     _flush()
L169
L170 def _disjoint_keepG(top_G, top_D, poolD):
L171     """Gé‡è¤‡ã‚’Dã‹ã‚‰é™¤å»ã—ã€poolDã§é †æ¬¡è£œå……ï¼ˆæ¯æ¸‡æ™‚ã¯å…ƒéŠ˜æŸ„ç¶­æŒï¼‰ã€‚"""
L172     used, D, i = set(top_G), list(top_D), 0
L173     for j, t in enumerate(D):
L174         if t in used:
L175             while i < len(poolD) and (poolD[i] in used or poolD[i] in D):
L176                 i += 1
L177             if i < len(poolD):
L178                 D[j] = poolD[i]; used.add(D[j]); i += 1
L179     return top_G, D
L180
L181
L182 def _sticky_keep_current(agg: pd.Series, pick: list[str], incumbents: list[str],
L183                          n_target: int, delta_z: float, keep_buffer: int) -> list[str]:
L184     import pandas as pd, numpy as np
L185     sel = list(pick)
L186     if not sel: return sel
L187     ranked_sel = agg.reindex(sel).sort_values(ascending=False)
L188     kth = ranked_sel.iloc[min(len(sel), n_target)-1]
L189     sigma = float(agg.std()) if pd.notna(agg.std()) else 0.0
L190     thresh = kth - delta_z * sigma
L191     ranked_all = agg.sort_values(ascending=False)
L192     cand = [t for t in incumbents if (t not in sel) and (t in agg.index)]
L193     for t in cand:
L194         within_score = (pd.notna(agg[t]) and agg[t] >= thresh)
L195         within_rank  = (t in ranked_all.index) and (ranked_all.index.get_loc(t) < n_target + keep_buffer)
L196         if within_score or within_rank:
L197             non_inc = [x for x in sel if x not in incumbents]
L198             if not non_inc: break
L199             weakest = min(non_inc, key=lambda x: agg.get(x, -np.inf))
L200             if weakest in sel and agg.get(t, -np.inf) >= agg.get(weakest, -np.inf):
L201                 sel.remove(weakest); sel.append(t)
L202     if len(sel) > n_target:
L203         sel = sorted(sel, key=lambda x: agg.get(x, -1e9), reverse=True)[:n_target]
L204     return sel
L205
L206
L207 # === Inputï¼šå¤–éƒ¨I/Oã¨å‰å‡¦ç†ï¼ˆCSV/APIãƒ»æ¬ æè£œå®Œï¼‰ ===
L208 class Input:
L209     def __init__(self, cand, exist, bench, price_max, finnhub_api_key=None):
L210         self.cand, self.exist, self.bench, self.price_max = cand, exist, bench, price_max
L211         self.api_key = finnhub_api_key or os.environ.get("FINNHUB_API_KEY")
L212
L213     # ---- ï¼ˆInputå°‚ç”¨ï¼‰EPSè£œå®Œãƒ»FCFç®—å‡ºç³» ----
L214     @staticmethod
L215     def _sec_headers():
L216         mail = (os.getenv("SEC_CONTACT_EMAIL") or "yasonba55@gmail.com").strip()
L217         app = (os.getenv("SEC_APP_NAME") or "FactorBot/1.0").strip()
L218         return {
L219             "User-Agent": f"{app} ({mail})",
L220             "From": mail,
L221             "Accept": "application/json",
L222         }
L223
L224     @staticmethod
L225     def _sec_get(url: str, retries: int = 3, backoff: float = 0.5):
L226         for i in range(retries):
L227             r = requests.get(url, headers=Input._sec_headers(), timeout=20)
L228             if r.status_code in (429, 503, 403):
L229                 time.sleep(min(2 ** i * backoff, 8.0))
L230                 continue
L231             r.raise_for_status()
L232             return r.json()
L233         r.raise_for_status()
L234
L235     @staticmethod
L236     def _sec_ticker_map():
L237         j = Input._sec_get("https://data.sec.gov/api/xbrl/company_tickers.json")
L238         mp = {}
L239         for _, v in (j or {}).items():
L240             try:
L241                 mp[str(v["ticker"]).upper()] = f"{int(v['cik_str']):010d}"
L242             except Exception:
L243                 pass
L244         return mp
L245
L246     # --- è¿½åŠ : ADR/OTCå‘ã‘ã®ç°¡æ˜“æ­£è¦åŒ–ï¼ˆæœ«å°¾Y/F, ãƒ‰ãƒƒãƒˆç­‰ï¼‰ ---
L247     @staticmethod
L248     def _normalize_ticker(sym: str) -> list[str]:
L249         s = (sym or "").upper().strip()
L250         # è¿½åŠ : å…ˆé ­ã®$ã‚„å…¨è§’ã®è¨˜å·ã‚’é™¤å»
L251         s = s.lstrip("$").replace("ï¼„", "").replace("ï¼", ".").replace("ï¼", "-")
L252         cand: list[str] = []
L253
L254         def add(x: str) -> None:
L255             if x and x not in cand:
L256                 cand.append(x)
L257
L258         # 1) åŸæ–‡ã‚’æœ€å„ªå…ˆï¼ˆSECã¯ BRK.B, BF.B ãªã© . ã‚’æ­£å¼æ¡ç”¨ï¼‰
L259         add(s)
L260         # 2) Yahooç³»ãƒãƒªã‚¢ãƒ³ãƒˆï¼ˆ. ã¨ - ã®æºã‚Œã‚’ç›¸äº’ã«ï¼‰
L261         if "." in s:
L262             add(s.replace(".", "-"))
L263             add(s.replace(".", ""))
L264         if "-" in s:
L265             add(s.replace("-", "."))
L266             add(s.replace("-", ""))
L267         # 3) ãƒ‰ãƒƒãƒˆãƒ»ãƒã‚¤ãƒ•ãƒ³ãƒ»ãƒ”ãƒªã‚ªãƒ‰ç„¡ã—ç‰ˆï¼ˆæœ€å¾Œã®ä¿é™ºï¼‰
L268         add(s.replace("-", "").replace(".", ""))
L269         # 4) ADRç°¡æ˜“ï¼šæœ«å°¾Y/Fã®é™¤å»ï¼ˆSECãƒãƒƒãƒ—ã¯æœ¬ä½“ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚’æŒã¤ã“ã¨ãŒã‚ã‚‹ï¼‰
L270         if len(s) >= 2 and s[-1] in {"Y", "F"}:
L271             add(s[:-1])
L272         return cand
L273
L274     @staticmethod
L275     def _sec_companyfacts(cik: str):
L276         return Input._sec_get(f"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json")
L277
L278     @staticmethod
L279     def _units_for_tags(facts: dict, namespaces: list[str], tags: list[str]) -> list[dict]:
L280         """facts ã‹ã‚‰ namespace/tag ã‚’æ¨ªæ–­ã—ã¦ units é…åˆ—ã‚’åé›†ï¼ˆå­˜åœ¨é †ã«é€£çµï¼‰ã€‚"""
L281         out: list[dict] = []
L282         facts = facts or {}
L283         for ns in namespaces:
L284             try:
L285                 node = facts.get("facts", {}).get(ns, {})
L286             except Exception:
L287                 node = {}
L288             for tg in tags:
L289                 try:
L290                     units = node[tg]["units"]
L291                 except Exception:
L292                     continue
L293                 picks: list[dict] = []
L294                 if "USD/shares" in units:
L295                     picks.extend(list(units["USD/shares"]))
L296                 if "USD" in units:
L297                     picks.extend(list(units["USD"]))
L298                 if not picks:
L299                     for arr in units.values():
L300                         picks.extend(list(arr))
L301                 out.extend(picks)
L302         return out
L303
L304     @staticmethod
L305     def _only_quarterly(arr: list[dict]) -> list[dict]:
L306         """companyfactsã®æ··åœ¨é…åˆ—ã‹ã‚‰ã€å››åŠæœŸã€ã ã‘ã‚’æŠ½å‡ºã€‚
L307
L308         - frame ã« "Q" ã‚’å«ã‚€ï¼ˆä¾‹: CY2024Q2Iï¼‰
L309         - fp ãŒ Q1/Q2/Q3/Q4
L310         - form ãŒ 10-Q/10-Q/A/6-K
L311         """
L312         if not arr:
L313             return []
L314         q_forms = {"10-Q", "10-Q/A", "6-K"}
L315
L316         def is_q(x: dict) -> bool:
L317             frame = (x.get("frame") or "").upper()
L318             fp = (x.get("fp") or "").upper()
L319             form = (x.get("form") or "").upper()
L320             return ("Q" in frame) or (fp in {"Q1", "Q2", "Q3", "Q4"}) or (form in q_forms)
L321
L322         out = [x for x in arr if is_q(x)]
L323         out.sort(key=lambda x: (x.get("end") or ""), reverse=True)
L324         return out
L325
L326     @staticmethod
L327     def _series_from_facts_with_dates(arr, key_val="val", key_dt="end", normalize=float):
L328         """companyfactsã‚¢ã‚¤ãƒ†ãƒ é…åˆ—ã‹ã‚‰ (date,value) ã‚’è¿”ã™ã€‚dateã¯YYYY-MM-DDã‚’æƒ³å®šã€‚"""
L329         out: List[Tuple[str, float]] = []
L330         for x in (arr or []):
L331             try:
L332                 v = x.get(key_val)
L333                 d = x.get(key_dt)
L334                 if d is None:
L335                     continue
L336                 out.append((str(d), normalize(v) if v is not None else float("nan")))
L337             except Exception:
L338                 continue
L339         # end(=æ—¥ä»˜)ã®é™é †ã«ã‚½ãƒ¼ãƒˆï¼ˆæœ€æ–°â†’å¤ã„ï¼‰
L340         out.sort(key=lambda t: t[0], reverse=True)
L341         return out
L342
L343     def fetch_eps_rev_from_sec(self, tickers: list[str]) -> dict:
L344         out = {}
L345         t2cik = self._sec_ticker_map()
L346         n_map = n_rev = n_eps = 0
L347         miss_map: list[str] = []
L348         miss_facts: list[str] = []
L349         for t in tickers:
L350             candidates: list[str] = []
L351
L352             def add(key: str) -> None:
L353                 if key and key not in candidates:
L354                     candidates.append(key)
L355
L356             add((t or "").upper())
L357             for key in self._normalize_ticker(t):
L358                 add(key)
L359
L360             cik = None
L361             for key in candidates:
L362                 cik = t2cik.get(key)
L363                 if cik:
L364                     break
L365             if not cik:
L366                 out[t] = {}
L367                 miss_map.append(t)
L368                 continue
L369             try:
L370                 j = self._sec_companyfacts(cik)
L371                 facts = j or {}
L372                 rev_tags = [
L373                     "Revenues",
L374                     "RevenueFromContractWithCustomerExcludingAssessedTax",
L375                     "SalesRevenueNet",
L376                     "SalesRevenueGoodsNet",
L377                     "SalesRevenueServicesNet",
L378                     "Revenue",
L379                 ]
L380                 eps_tags = [
L381                     "EarningsPerShareDiluted",
L382                     "EarningsPerShareBasicAndDiluted",
L383                     "EarningsPerShare",
L384                     "EarningsPerShareBasic",
L385                 ]
L386                 rev_arr = self._units_for_tags(facts, ["us-gaap", "ifrs-full"], rev_tags)
L387                 eps_arr = self._units_for_tags(facts, ["us-gaap", "ifrs-full"], eps_tags)
L388                 rev_q_items = self._only_quarterly(rev_arr)
L389                 eps_q_items = self._only_quarterly(eps_arr)
L390                 # (date,value) ã§å–å¾—
L391                 rev_pairs = self._series_from_facts_with_dates(rev_q_items)
L392                 eps_pairs = self._series_from_facts_with_dates(eps_q_items)
L393                 rev_vals = [v for (_d, v) in rev_pairs]
L394                 eps_vals = [v for (_d, v) in eps_pairs]
L395                 rev_q = float(rev_vals[0]) if rev_vals else float("nan")
L396                 eps_q = float(eps_vals[0]) if eps_vals else float("nan")
L397                 rev_ttm = float(sum([v for v in rev_vals[:4] if v == v])) if rev_vals else float("nan")
L398                 eps_ttm = float(sum([v for v in eps_vals[:4] if v == v])) if eps_vals else float("nan")
L399                 out[t] = {
L400                     "eps_q_recent": eps_q,
L401                     "eps_ttm": eps_ttm,
L402                     "rev_q_recent": rev_q,
L403                     "rev_ttm": rev_ttm,
L404                     # å¾Œæ®µã§DatetimeIndexåŒ–ã§ãã‚‹ã‚ˆã† (date,value) ã‚’ä¿æŒã€‚å€¤ã ã‘ã®äº’æ›ã‚­ãƒ¼ã‚‚æ®‹ã™ã€‚
L405                     "eps_q_series_pairs": eps_pairs[:16],
L406                     "rev_q_series_pairs": rev_pairs[:16],
L407                     "eps_q_series": eps_vals[:16],
L408                     "rev_q_series": rev_vals[:16],
L409                 }
L410                 n_map += 1
L411                 if rev_vals:
L412                     n_rev += 1
L413                 if eps_vals:
L414                     n_eps += 1
L415             except Exception:
L416                 out[t] = {}
L417                 miss_facts.append(t)
L418             time.sleep(0.30)
L419         # å–å¾—ã‚µãƒãƒªã‚’ãƒ­ã‚°ï¼ˆActionsã§ç¢ºèªã—ã‚„ã™ã„ã‚ˆã† printï¼‰
L420         try:
L421             total = len(tickers)
L422             print(f"[SEC] map={n_map}/{total}  rev_q_hit={n_rev}  eps_q_hit={n_eps}")
L423             # ãƒ‡ãƒãƒƒã‚°: å–å¾—æœ¬æ•°ã®åˆ†å¸ƒï¼ˆå…ˆé ­ã®ã¿ï¼‰
L424             try:
L425                 lens = [len((out.get(t, {}) or {}).get("rev_q_series", [])) for t in tickers]
L426                 print(f"[SEC] rev_q_series length: min={min(lens) if lens else 0} "
L427                       f"p25={np.percentile(lens,25) if lens else 0} median={np.median(lens) if lens else 0} "
L428                       f"p75={np.percentile(lens,75) if lens else 0} max={max(lens) if lens else 0}")
L429             except Exception:
L430                 pass
L431             if miss_map:
L432                 print(f"[SEC] no CIK map: {len(miss_map)} (ã‚µãƒ³ãƒ—ãƒ«ä¾‹) {miss_map[:20]}")
L433             if miss_facts:
L434                 print(f"[SEC] CIKã‚ã‚Š ã ãŒå¯¾è±¡factãªã—: {len(miss_facts)} (ã‚µãƒ³ãƒ—ãƒ«ä¾‹) {miss_facts[:20]}")
L435         except Exception:
L436             pass
L437         return out
L438
L439     def sec_dryrun_sample(self, tickers: list[str] | None = None) -> None:
L440         if not _env_true("SEC_DRYRUN_SAMPLE", False):
L441             return
L442         sample = tickers or ["BRK.B", "BF.B", "GOOGL", "META", "UBER", "PBR.A", "TSM", "NARI", "EVBN", "SWAV"]
L443         print(f"[SEC-DRYRUN] sample tickers: {sample}")
L444         try:
L445             t2cik = self._sec_ticker_map()
L446             hits = 0
L447             for sym in sample:
L448                 candidates: list[str] = []
L449
L450                 def add(key: str) -> None:
L451                     if key and key not in candidates:
L452                         candidates.append(key)
L453
L454                 add((sym or "").upper())
L455                 for alt in self._normalize_ticker(sym):
L456                     add(alt)
L457                 if any(t2cik.get(key) for key in candidates):
L458                     hits += 1
L459             sec_data = self.fetch_eps_rev_from_sec(sample)
L460             rev_hits = sum(1 for v in sec_data.values() if v.get("rev_q_series"))
L461             eps_hits = sum(1 for v in sec_data.values() if v.get("eps_q_series"))
L462             total = len(sample)
L463             print(f"[SEC-DRYRUN] CIK map hit: {hits}/{total}  rev_q_series hits: {rev_hits}  eps_q_series hits: {eps_hits}")
L464         except Exception as e:
L465             print(f"[SEC-DRYRUN] error: {e}")
L466     @staticmethod
L467     def impute_eps_ttm(df: pd.DataFrame, ttm_col: str="eps_ttm", q_col: str="eps_q_recent", out_col: str|None=None) -> pd.DataFrame:
L468         out_col = out_col or ttm_col; df = df.copy(); df["eps_imputed"] = False
L469         cand = df[q_col]*4; ok = df[ttm_col].isna() & cand.replace([np.inf,-np.inf], np.nan).notna()
L470         df.loc[ok, out_col], df.loc[ok,"eps_imputed"] = cand[ok], True; return df
L471
L472     _CF_ALIASES = {"cfo":["Operating Cash Flow","Total Cash From Operating Activities"], "capex":["Capital Expenditure","Capital Expenditures"]}
L473
L474     @staticmethod
L475     def _pick_row(df: pd.DataFrame, names: list[str]) -> pd.Series|None:
L476         if df is None or df.empty: return None
L477         idx_lower={str(i).lower():i for i in df.index}
L478         for n in names:
L479             k=n.lower()
L480             if k in idx_lower: return df.loc[idx_lower[k]]
L481         return None
L482
L483     @staticmethod
L484     def _sum_last_n(s: pd.Series|None, n: int) -> float|None:
L485         if s is None or s.empty: return None
L486         v=s.dropna().astype(float); return None if v.empty else v.iloc[:n].sum()
L487
L488     @staticmethod
L489     def _latest(s: pd.Series|None) -> float|None:
L490         if s is None or s.empty: return None
L491         v=s.dropna().astype(float); return v.iloc[0] if not v.empty else None
L492
L493     def fetch_cfo_capex_ttm_yf(self, tickers: list[str]) -> pd.DataFrame:
L494         from concurrent.futures import ThreadPoolExecutor, as_completed
L495         pick, sumn, latest, aliases = self._pick_row, self._sum_last_n, self._latest, self._CF_ALIASES
L496
L497         def one(t: str):
L498             try:
L499                 tk = yf.Ticker(t)  # â˜… ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¯æ¸¡ã•ãªã„ï¼ˆYFãŒcurl_cffiã§ç®¡ç†ï¼‰
L500                 qcf = tk.quarterly_cashflow
L501                 cfo_q, capex_q = pick(qcf, aliases["cfo"]), pick(qcf, aliases["capex"])
L502                 fcf_q = pick(qcf, ["Free Cash Flow","FreeCashFlow","Free cash flow"])
L503                 cfo, capex, fcf = sumn(cfo_q,4), sumn(capex_q,4), sumn(fcf_q,4)
L504                 if any(v is None for v in (cfo, capex, fcf)):
L505                     acf = tk.cashflow
L506                     if cfo   is None: cfo   = latest(pick(acf, aliases["cfo"]))
L507                     if capex is None: capex = latest(pick(acf, aliases["capex"]))
L508                     if fcf   is None: fcf   = latest(pick(acf, ["Free Cash Flow","FreeCashFlow","Free cash flow"]))
L509             except Exception as e:
L510                 print(f"[warn] yf financials error: {t}: {e}"); cfo=capex=fcf=None
L511             n=np.nan
L512             return {"ticker":t,
L513                     "cfo_ttm_yf":   n if cfo   is None else cfo,
L514                     "capex_ttm_yf": n if capex is None else capex,
L515                     "fcf_ttm_yf_direct": n if fcf is None else fcf}
L516
L517         rows, mw = [], int(os.getenv("FIN_THREADS","8"))
L518         with ThreadPoolExecutor(max_workers=mw) as ex:
L519             rows=[f.result() for f in as_completed(ex.submit(one,t) for t in tickers)]
L520         return pd.DataFrame(rows).set_index("ticker")
L521
L522     _FINN_CFO_KEYS = ["netCashProvidedByOperatingActivities","netCashFromOperatingActivities","cashFlowFromOperatingActivities","operatingCashFlow"]
L523     _FINN_CAPEX_KEYS = ["capitalExpenditure","capitalExpenditures","purchaseOfPPE","investmentsInPropertyPlantAndEquipment"]
L524
L525     @staticmethod
L526     def _first_key(d: dict, keys: list[str]):
L527         for k in keys:
L528             if k in d and d[k] is not None: return d[k]
L529         return None
L530
L531     @staticmethod
L532     def _finn_get(session: requests.Session, url: str, params: dict, retries: int=3, sleep_s: float=0.5):
L533         for i in range(retries):
L534             r = session.get(url, params=params, timeout=15)
L535             if r.status_code==429: time.sleep(min(2**i*sleep_s,4.0)); continue
L536             r.raise_for_status(); return r.json()
L537         r.raise_for_status()
L538
L539     def fetch_cfo_capex_ttm_finnhub(self, tickers: list[str], api_key: str|None=None) -> pd.DataFrame:
L540         api_key = api_key or os.getenv("FINNHUB_API_KEY")
L541         if not api_key: raise ValueError("Finnhub API key not provided. Set FINNHUB_API_KEY or pass api_key=")
L542         base, s, rows = "https://finnhub.io/api/v1", requests.Session(), []
L543         for sym in tickers:
L544             cfo_ttm = capex_ttm = None
L545             try:
L546                 j = self._finn_get(s, f"{base}/stock/cash-flow", {"symbol":sym,"frequency":"quarterly","limit":8,"token":api_key})
L547                 arr = j.get("cashFlow") or []; cfo_vals, capex_vals = [], []
L548                 for item in arr[:4]:
L549                     cfo_vals.append(self._first_key(item,self._FINN_CFO_KEYS)); capex_vals.append(self._first_key(item,self._FINN_CAPEX_KEYS))
L550                 if any(v is not None for v in cfo_vals): cfo_ttm = float(np.nansum([np.nan if v is None else float(v) for v in cfo_vals]))
L551                 if any(v is not None for v in capex_vals): capex_ttm = float(np.nansum([np.nan if v is None else float(v) for v in capex_vals]))
L552             except Exception: pass
L553             if cfo_ttm is None or capex_ttm is None:
L554                 try:
L555                     j = self._finn_get(s, f"{base}/stock/cash-flow", {"symbol":sym,"frequency":"annual","limit":1,"token":api_key})
L556                     arr = j.get("cashFlow") or []
L557                     if arr:
L558                         item0 = arr[0]
L559                         if cfo_ttm is None:
L560                             v = self._first_key(item0,self._FINN_CFO_KEYS)
L561                             if v is not None: cfo_ttm = float(v)
L562                         if capex_ttm is None:
L563                             v = self._first_key(item0,self._FINN_CAPEX_KEYS)
L564                             if v is not None: capex_ttm = float(v)
L565                 except Exception: pass
L566             rows.append({"ticker":sym,"cfo_ttm_fh":np.nan if cfo_ttm is None else cfo_ttm,"capex_ttm_fh":np.nan if capex_ttm is None else capex_ttm})
L567         return pd.DataFrame(rows).set_index("ticker")
L568
L569     def compute_fcf_with_fallback(self, tickers: list[str], finnhub_api_key: str|None=None) -> pd.DataFrame:
L570         yf_df = self.fetch_cfo_capex_ttm_yf(tickers)
L571         T.log("financials (yf) done")
L572         miss_mask = yf_df[["cfo_ttm_yf","capex_ttm_yf","fcf_ttm_yf_direct"]].isna().any(axis=1)
L573         need = yf_df.index[miss_mask].tolist(); print(f"[T] yf financials missing: {len(need)} {need[:10]}{'...' if len(need)>10 else ''}")
L574         if need:
L575             fh_df = self.fetch_cfo_capex_ttm_finnhub(need, api_key=finnhub_api_key)
L576             df = yf_df.join(fh_df, how="left")
L577             for col_yf, col_fh in [("cfo_ttm_yf","cfo_ttm_fh"),("capex_ttm_yf","capex_ttm_fh")]:
L578                 df[col_yf] = df[col_yf].fillna(df[col_fh])
L579             print("[T] financials (finnhub) done (fallback only)")
L580         else:
L581             df = yf_df.assign(cfo_ttm_fh=np.nan, capex_ttm_fh=np.nan)
L582             print("[T] financials (finnhub) skipped (no missing)")
L583         df["cfo_ttm"]  = df["cfo_ttm_yf"].where(df["cfo_ttm_yf"].notna(), df["cfo_ttm_fh"])
L584         df["capex_ttm"] = df["capex_ttm_yf"].where(df["capex_ttm_yf"].notna(), df["capex_ttm_fh"])
L585         cfo, capex = pd.to_numeric(df["cfo_ttm"], errors="coerce"), pd.to_numeric(df["capex_ttm"], errors="coerce").abs()
L586         fcf_calc = cfo - capex
L587         fcf_direct = pd.to_numeric(df.get("fcf_ttm_yf_direct"), errors="coerce")
L588         df["fcf_ttm"] = fcf_calc.where(fcf_calc.notna(), fcf_direct)
L589         df["cfo_source"]  = np.where(df["cfo_ttm_yf"].notna(),"yfinance",np.where(df["cfo_ttm_fh"].notna(),"finnhub",""))
L590         df["capex_source"] = np.where(df["capex_ttm_yf"].notna(),"yfinance",np.where(df["capex_ttm_fh"].notna(),"finnhub",""))
L591         df["fcf_imputed"] = df[["cfo_ttm","capex_ttm"]].isna().any(axis=1) & df["fcf_ttm"].notna()
L592         cols = ["cfo_ttm_yf","capex_ttm_yf","cfo_ttm_fh","capex_ttm_fh","cfo_ttm","capex_ttm","fcf_ttm","fcf_ttm_yf_direct","cfo_source","capex_source","fcf_imputed"]
L593         return df[cols].sort_index()
L594
L595     def _build_eps_df(self, tickers, tickers_bulk, info, sec_map: dict | None = None):
L596         eps_rows=[]
L597         for t in tickers:
L598             info_t = info[t]
L599             sec_t = (sec_map or {}).get(t, {})
L600             eps_ttm = sec_t.get("eps_ttm", info_t.get("trailingEps", np.nan))
L601             eps_q = sec_t.get("eps_q_recent", np.nan)
L602             try:
L603                 qearn, so = tickers_bulk.tickers[t].quarterly_earnings, info_t.get("sharesOutstanding")
L604                 if so and qearn is not None and not qearn.empty and "Earnings" in qearn.columns:
L605                     eps_ttm_q = qearn["Earnings"].head(4).sum()/so
L606                     if pd.notna(eps_ttm_q) and (pd.isna(eps_ttm) or (abs(eps_ttm)>0 and abs(eps_ttm/eps_ttm_q)>3)): eps_ttm = eps_ttm_q
L607                     if pd.isna(eps_q):
L608                         eps_q = qearn["Earnings"].iloc[-1]/so
L609             except Exception: pass
L610             rev_ttm = sec_t.get("rev_ttm", np.nan)
L611             rev_q = sec_t.get("rev_q_recent", np.nan)
L612             if (not sec_t) or pd.isna(rev_ttm):
L613                 try:
L614                     tk = tickers_bulk.tickers[t]
L615                     qfin = getattr(tk, "quarterly_financials", None)
L616                     if qfin is not None and not qfin.empty:
L617                         idx_lower = {str(i).lower(): i for i in qfin.index}
L618                         rev_idx = None
L619                         for name in ("Total Revenue", "TotalRevenue"):
L620                             key = name.lower()
L621                             if key in idx_lower:
L622                                 rev_idx = idx_lower[key]
L623                                 break
L624                         if rev_idx is not None:
L625                             rev_series = pd.to_numeric(qfin.loc[rev_idx], errors="coerce").dropna()
L626                             if not rev_series.empty:
L627                                 rev_ttm_yf = float(rev_series.head(4).sum())
L628                                 if pd.isna(rev_ttm):
L629                                     rev_ttm = rev_ttm_yf
L630                                 if pd.isna(rev_q):
L631                                     rev_q = float(rev_series.iloc[0])
L632                 except Exception:
L633                     pass
L634             eps_rows.append({"ticker":t,"eps_ttm":eps_ttm,"eps_q_recent":eps_q,"rev_ttm":rev_ttm,"rev_q_recent":rev_q})
L635         return self.impute_eps_ttm(pd.DataFrame(eps_rows).set_index("ticker"))
L636
L637     def prepare_data(self):
L638         """Fetch price and fundamental data for all tickers."""
L639         self.sec_dryrun_sample()
L640         cand_info = yf.Tickers(" ".join(self.cand)); cand_prices = {}
L641         for t in self.cand:
L642             try: cand_prices[t] = cand_info.tickers[t].fast_info.get("lastPrice", np.inf)
L643             except Exception as e: print(f"{t}: price fetch failed ({e})"); cand_prices[t] = np.inf
L644         cand_f = [t for t,p in cand_prices.items() if p<=self.price_max]
L645         T.log("price cap filter done (CAND_PRICE_MAX)")
L646         # å…¥åŠ›ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã®é‡è¤‡ã‚’é™¤å»ã—ã€ç¾è¡Œâ†’å€™è£œã®é †åºã‚’ç¶­æŒ
L647         tickers = list(dict.fromkeys(self.exist + cand_f))
L648         T.log(f"universe prepared: unique={len(tickers)} bench={self.bench}")
L649         data = yf.download(tickers + [self.bench], period="600d",
L650                            auto_adjust=True, progress=False, threads=False)
L651         T.log("yf.download done")
L652         px = data["Close"].dropna(how="all", axis=1).ffill(limit=2)
L653         spx = data["Close"][self.bench].reindex(px.index).ffill()
L654         clip_days = int(os.getenv("PRICE_CLIP_DAYS", "0"))   # 0ãªã‚‰ç„¡åŠ¹ï¼ˆæ—¢å®šï¼‰
L655         if clip_days > 0:
L656             px  = px.tail(clip_days + 1)
L657             spx = spx.tail(clip_days + 1)
L658             logger.info("[T] price window clipped by env: %d rows (PRICE_CLIP_DAYS=%d)", len(px), clip_days)
L659         else:
L660             logger.info("[T] price window clip skipped; rows=%d", len(px))
L661         tickers_bulk, info = yf.Tickers(" ".join(tickers)), {}
L662         for t in tickers:
L663             try:
L664                 info[t] = tickers_bulk.tickers[t].info
L665             except Exception as e:
L666                 logger.info("[warn] %s: info fetch failed (%s)", t, e)
L667                 info[t] = {}
L668         try:
L669             sec_map = self.fetch_eps_rev_from_sec(tickers)
L670         except Exception as e:
L671             logger.warning("[SEC] fetch_eps_rev_from_sec failed: %s", e)
L672             sec_map = {}
L673
L674         def _brief_len(s):
L675             try:
L676                 if isinstance(s, pd.Series):
L677                     return int(s.dropna().size)
L678                 if isinstance(s, (list, tuple)):
L679                     return len([v for v in s if pd.notna(v)])
L680                 if isinstance(s, np.ndarray):
L681                     return int(np.count_nonzero(~pd.isna(s)))
L682                 return int(bool(s))
L683             except Exception:
L684                 return 0
L685
L686         def _has_entries(val) -> bool:
L687             try:
L688                 if isinstance(val, pd.Series):
L689                     return not val.dropna().empty
L690                 if isinstance(val, (list, tuple)):
L691                     return any(pd.notna(v) for v in val)
L692                 return bool(val)
L693             except Exception:
L694                 return False
L695
L696         have_rev = 0
L697         have_eps = 0
L698         rev_lens: list[int] = []
L699         eps_lens: list[int] = []
L700         samples: list[tuple[str, int, str, float | None, int, str, float | None]] = []
L701
L702         for t in tickers:
L703             entry = info.get(t, {})
L704             m = (sec_map or {}).get(t) or {}
L705             if entry is None or not isinstance(entry, dict):
L706                 entry = {}
L707                 info[t] = entry
L708
L709             if m:
L710                 pairs_r = m.get("rev_q_series_pairs") or []
L711                 pairs_e = m.get("eps_q_series_pairs") or []
L712                 if pairs_r:
L713                     idx = pd.to_datetime([d for (d, _v) in pairs_r], errors="coerce")
L714                     val = pd.to_numeric([v for (_d, v) in pairs_r], errors="coerce")
L715                     s = pd.Series(val, index=idx).sort_index()
L716                     entry["SEC_REV_Q_SERIES"] = s
L717                 else:
L718                     entry["SEC_REV_Q_SERIES"] = m.get("rev_q_series") or []
L719                 if pairs_e:
L720                     idx = pd.to_datetime([d for (d, _v) in pairs_e], errors="coerce")
L721                     val = pd.to_numeric([v for (_d, v) in pairs_e], errors="coerce")
L722                     s = pd.Series(val, index=idx).sort_index()
L723                     entry["SEC_EPS_Q_SERIES"] = s
L724                 else:
L725                     entry["SEC_EPS_Q_SERIES"] = m.get("eps_q_series") or []
L726
L727             r = entry.get("SEC_REV_Q_SERIES")
L728             e = entry.get("SEC_EPS_Q_SERIES")
L729             if _has_entries(r):
L730                 have_rev += 1
L731             if _has_entries(e):
L732                 have_eps += 1
L733             lr = _brief_len(r)
L734             le = _brief_len(e)
L735             rev_lens.append(lr)
L736             eps_lens.append(le)
L737             if len(samples) < 8:
L738                 try:
L739                     rd = getattr(r, "index", [])[-1] if lr > 0 else None
L740                     rv = float(r.iloc[-1]) if lr > 0 else None
L741                     ed = getattr(e, "index", [])[-1] if le > 0 else None
L742                     ev = float(e.iloc[-1]) if le > 0 else None
L743                     samples.append((t, lr, str(rd) if rd is not None else "-", rv, le, str(ed) if ed is not None else "-", ev))
L744                 except Exception:
L745                     samples.append((t, lr, "-", None, le, "-", None))
L746
L747         logger.info("[SEC] series attach: rev_q=%d/%d, eps_q=%d/%d", have_rev, len(tickers), have_eps, len(tickers))
L748
L749         if rev_lens:
L750             rev_lens_sorted = sorted(rev_lens)
L751             eps_lens_sorted = sorted(eps_lens)
L752             _log(
L753                 "SEC_SERIES",
L754                 f"rev_len min/med/max={rev_lens_sorted[0]}/{rev_lens_sorted[len(rev_lens)//2]}/{rev_lens_sorted[-1]} "
L755                 f"eps_len min/med/max={eps_lens_sorted[0]}/{eps_lens_sorted[len(eps_lens)//2]}/{eps_lens_sorted[-1]}",
L756             )
L757         for (t, lr, rd, rv, le, ed, ev) in samples:
L758             _log("SEC_SERIES_SMP", f"{t}  rev_len={lr} last=({rd},{rv})  eps_len={le} last=({ed},{ev})")
L759         eps_df = self._build_eps_df(tickers, tickers_bulk, info, sec_map=sec_map)
L760         # index é‡è¤‡ãŒã‚ã‚‹ã¨ .loc[t, col] ãŒ Series ã«ãªã‚Šä»£å…¥æ™‚ã« ValueError ã‚’èª˜ç™ºã™ã‚‹
L761         if not eps_df.index.is_unique:
L762             eps_df = eps_df[~eps_df.index.duplicated(keep="last")]
L763         eps_df = eps_df.assign(
L764             EPS_TTM=eps_df["eps_ttm"],
L765             EPS_Q_LastQ=eps_df["eps_q_recent"],
L766             REV_TTM=eps_df["rev_ttm"],
L767             REV_Q_LastQ=eps_df["rev_q_recent"],
L768         )
L769         # ã“ã“ã§éNaNä»¶æ•°ã‚’ã‚µãƒãƒªè¡¨ç¤ºï¼ˆæ¬ æçŠ¶æ³ã®å³æ™‚æŠŠæ¡ç”¨ï¼‰
L770         try:
L771             n = len(eps_df)
L772             c_eps = int(eps_df["EPS_TTM"].notna().sum())
L773             c_rev = int(eps_df["REV_TTM"].notna().sum())
L774             print(f"[SEC] eps_ttm non-NaN: {c_eps}/{n}  rev_ttm non-NaN: {c_rev}/{n}")
L775         except Exception:
L776             pass
L777         fcf_df = self.compute_fcf_with_fallback(tickers, finnhub_api_key=self.api_key)
L778         T.log("eps/fcf prep done")
L779         returns = px[tickers].pct_change()
L780         T.log("price prep/returns done")
L781         return dict(cand=cand_f, tickers=tickers, data=data, px=px, spx=spx, tickers_bulk=tickers_bulk, info=info, eps_df=eps_df, fcf_df=fcf_df, returns=returns)
L782
L783 # === Selectorï¼šç›¸é–¢ä½æ¸›ãƒ»é¸å®šï¼ˆã‚¹ã‚³ã‚¢ï¼†ãƒªã‚¿ãƒ¼ãƒ³ã ã‘èª­ã‚€ï¼‰ ===
L784 class Selector:
L785     # ---- DRRS helpersï¼ˆSelectorå°‚ç”¨ï¼‰ ----
L786     @staticmethod
L787     def _z_np(X: np.ndarray) -> np.ndarray:
L788         X = np.asarray(X, dtype=np.float32); m = np.nanmean(X, axis=0, keepdims=True); s = np.nanstd(X, axis=0, keepdims=True)+1e-9
L789         return (np.nan_to_num(X)-m)/s
L790
L791     @classmethod
L792     def residual_corr(cls, R: np.ndarray, n_pc: int=3, shrink: float=0.1) -> np.ndarray:
L793         Z = cls._z_np(R); U,S,_ = np.linalg.svd(Z, full_matrices=False); F = U[:,:n_pc]*S[:n_pc]; B = np.linalg.lstsq(F, Z, rcond=None)[0]
L794         E = Z - F@B; C = np.corrcoef(E, rowvar=False)
L795         off = C - np.diag(np.diag(C)); iu = np.triu_indices_from(off,1); avg_abs = np.nanmean(np.abs(off[iu])) if iu[0].size else 0.0
L796         shrink_eff = float(np.clip(shrink + 0.5*avg_abs, 0.1, 0.6)); N = C.shape[0]
L797         return (1.0 - shrink_eff)*C + shrink_eff*np.eye(N, dtype=C.dtype)
L798
L799     @classmethod
L800     def rrqr_like_det(cls, R: np.ndarray, score: np.ndarray, k: int, gamma: float=1.0):
L801         Z, w = cls._z_np(R), (score-score.min())/(np.ptp(score)+1e-12); X = Z*(1.0 + gamma*w)
L802         N, k = X.shape[1], max(0, min(k, X.shape[1]))
L803         if k==0: return []
L804         S, selected, Rres = [], np.zeros(N, dtype=bool), X.copy()
L805         for _ in range(k):
L806             norms = (Rres*Rres).sum(axis=0); cand = np.where(~selected)[0]
L807             j = sorted(cand, key=lambda c:(-norms[c], -w[c], c))[0]
L808             S.append(j); selected[j]=True; u = X[:,j:j+1]; u/=(np.linalg.norm(u)+1e-12); Rres = Rres - u @ (u.T @ Rres)
L809         return sorted(S)
L810
L811     @staticmethod
L812     def _obj(corrM: np.ndarray, score: np.ndarray, idx, lam: float) -> float:
L813         idx = list(idx); P = corrM[np.ix_(idx, idx)]; s = (score-score.mean())/(score.std()+1e-9)
L814         return float(s[idx].sum() - lam*((P.sum()-np.trace(P))/2.0))
L815
L816     @classmethod
L817     def swap_local_det(cls, corrM: np.ndarray, score: np.ndarray, idx, lam: float=0.6, max_pass: int=15):
L818         S, best, improved, passes = sorted(idx), cls._obj(corrM, score, idx, lam), True, 0
L819         while improved and passes<max_pass:
L820             improved, passes = False, passes+1
L821             for i,out in enumerate(list(S)):
L822                 for inn in range(len(score)):
L823                     if inn in S: continue
L824                     cand = sorted(S[:i]+[inn]+S[i+1:]); v = cls._obj(corrM, score, cand, lam)
L825                     if v>best+1e-10: S, best, improved = cand, v, True; break
L826                 if improved: break
L827         return S, best
L828
L829     @staticmethod
L830     def _obj_with_cross(C_within: np.ndarray, C_cross: np.ndarray|None, score: np.ndarray, idx, lam: float, mu: float) -> float:
L831         idx = list(idx); P = C_within[np.ix_(idx, idx)]; s = (score-score.mean())/(score.std()+1e-9)
L832         within = (P.sum()-np.trace(P))/2.0; cross = 0.0
L833         if C_cross is not None and C_cross.size>0: cross = C_cross[idx,:].sum()
L834         return float(s[idx].sum() - lam*within - mu*cross)
L835
L836     @classmethod
L837     def swap_local_det_cross(cls, C_within: np.ndarray, C_cross: np.ndarray|None, score: np.ndarray, idx, lam: float=0.6, mu: float=0.3, max_pass: int=15):
L838         S, best, improved, passes, N = sorted(idx), cls._obj_with_cross(C_within,C_cross,score,idx,lam,mu), True, 0, len(score)
L839         while improved and passes<max_pass:
L840             improved, passes = False, passes+1
L841             for i,out in enumerate(list(S)):
L842                 for inn in range(N):
L843                     if inn in S: continue
L844                     cand = sorted(S[:i]+[inn]+S[i+1:]); v = cls._obj_with_cross(C_within,C_cross,score,cand,lam,mu)
L845                     if v>best+1e-10: S, best, improved = cand, v, True; break
L846                 if improved: break
L847         return S, best
L848
L849     @staticmethod
L850     def avg_corr(C: np.ndarray, idx) -> float:
L851         k = len(idx); P = C[np.ix_(idx, idx)]
L852         return float((P.sum()-np.trace(P))/(k*(k-1)+1e-12))
L853
L854     @classmethod
L855     def select_bucket_drrs(cls, returns_df: pd.DataFrame, score_ser: pd.Series, pool_tickers: list[str], k: int, *, n_pc: int, gamma: float, lam: float, lookback: int, shrink: float=0.10, g_fixed_tickers: list[str]|None=None, mu: float=0.0):
L856         g_fixed = [t for t in (g_fixed_tickers or []) if t in returns_df.columns]
L857         union = [t for t in pool_tickers if t in returns_df.columns]
L858         for t in g_fixed:
L859             if t not in union: union.append(t)
L860         Rdf_all = returns_df[union]; Rdf_all = Rdf_all.iloc[-lookback:] if len(Rdf_all)>=lookback else Rdf_all; Rdf_all = Rdf_all.dropna()
L861         pool_eff, g_eff = [t for t in pool_tickers if t in Rdf_all.columns], [t for t in g_fixed if t in Rdf_all.columns]
L862         if len(pool_eff)==0: return dict(idx=[], tickers=[], avg_res_corr=np.nan, sum_score=0.0, objective=-np.inf)
L863         score = score_ser.reindex(pool_eff).to_numpy(dtype=np.float32)
L864         C_all = cls.residual_corr(Rdf_all.to_numpy(), n_pc=n_pc, shrink=shrink)
L865         col_pos = {c:i for i,c in enumerate(Rdf_all.columns)}; pool_pos = [col_pos[t] for t in pool_eff]
L866         C_within, C_cross = C_all[np.ix_(pool_pos,pool_pos)], None
L867         if len(g_eff)>0 and mu>0.0:
L868             g_pos = [col_pos[t] for t in g_eff]; C_cross = C_all[np.ix_(pool_pos,g_pos)]
L869         R_pool = Rdf_all[pool_eff].to_numpy(); S0 = cls.rrqr_like_det(R_pool, score, k, gamma=gamma)
L870         S, Jn = (cls.swap_local_det_cross(C_within, C_cross, score, S0, lam=lam, mu=mu, max_pass=15) if C_cross is not None else cls.swap_local_det(C_within, score, S0, lam=lam, max_pass=15))
L871         selected_tickers = [pool_eff[i] for i in S]
L872         return dict(idx=S, tickers=selected_tickers, avg_res_corr=cls.avg_corr(C_within,S), sum_score=float(score[S].sum()), objective=float(Jn))
L873
L874     # ---- é¸å®šï¼ˆã‚¹ã‚³ã‚¢ Series / returns ã ã‘ã‚’å—ã‘ã‚‹ï¼‰----
L875 # === Outputï¼šå‡ºåŠ›æ•´å½¢ã¨é€ä¿¡ï¼ˆè¡¨ç¤ºãƒ»Slackï¼‰ ===
L876 class Output:
L877
L878     def __init__(self, debug=None):
L879         # self.debug ã¯ä½¿ã‚ãªã„ï¼ˆäº’æ›ã®ãŸã‚å¼•æ•°ã¯å—ã‘ã‚‹ãŒç„¡è¦–ï¼‰
L880         self.miss_df = self.g_table = self.d_table = self.io_table = self.df_metrics_fmt = self.debug_table = None
L881         self.g_title = self.d_title = ""
L882         self.g_formatters = self.d_formatters = {}
L883         # ä½ã‚¹ã‚³ã‚¢ï¼ˆGSC+DSCï¼‰Top10 è¡¨ç¤º/é€ä¿¡ç”¨
L884         self.low10_table = None
L885         self.debug_text = ""   # ãƒ‡ãƒãƒƒã‚°ç”¨æœ¬æ–‡ã¯ã“ã“ã«ä¸€æœ¬åŒ–
L886         self._debug_logged = False
L887
L888     # --- è¡¨ç¤ºï¼ˆå…ƒ display_results ã®ãƒ­ã‚¸ãƒƒã‚¯ãã®ã¾ã¾ï¼‰ ---
L889     def display_results(self, *, exist, bench, df_z, g_score, d_score_all,
L890                         init_G, init_D, top_G, top_D, **kwargs):
L891         logger.info("ğŸ“Œ reached display_results")
L892         pd.set_option('display.float_format','{:.3f}'.format)
L893         print("ğŸ“ˆ ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼åˆ†æ•£æœ€é©åŒ–ã®çµæœ")
L894         if self.miss_df is not None and not self.miss_df.empty:
L895             print("Missing Data:")
L896             print(self.miss_df.to_string(index=False))
L897
L898         # ---- è¡¨ç¤ºç”¨ï¼šChanges/Near-Miss ã®ã‚¹ã‚³ã‚¢æºã‚’â€œæœ€çµ‚é›†è¨ˆâ€ã«çµ±ä¸€ã™ã‚‹ãƒ—ãƒ­ã‚­ã‚· ----
L899         try:
L900             sc = getattr(self, "_sc", None)
L901             agg_G = getattr(sc, "_agg_G", None)
L902             agg_D = getattr(sc, "_agg_D", None)
L903         except Exception:
L904             sc = agg_G = agg_D = None
L905         class _SeriesProxy:
L906             __slots__ = ("primary", "fallback")
L907             def __init__(self, primary, fallback): self.primary, self.fallback = primary, fallback
L908             def get(self, key, default=None):
L909                 try:
L910                     v = self.primary.get(key) if hasattr(self.primary, "get") else None
L911                     if v is not None and not (isinstance(v, float) and v != v):
L912                         return v
L913                 except Exception:
L914                     pass
L915                 try:
L916                     return self.fallback.get(key) if hasattr(self.fallback, "get") else default
L917                 except Exception:
L918                     return default
L919         g_score = _SeriesProxy(agg_G, g_score)
L920         d_score_all = _SeriesProxy(agg_D, d_score_all)
L921         near_G = getattr(sc, "_near_G", []) if sc else []
L922         near_D = getattr(sc, "_near_D", []) if sc else []
L923
L924         extra_G = [t for t in init_G if t not in top_G][:5]; G_UNI = top_G + extra_G
L925         gsc_series = pd.Series({t: g_score.get(t) for t in G_UNI}, name='GSC')
L926         self.g_table = pd.concat([df_z.loc[G_UNI,['GROWTH_F','MOM','TRD','VOL']], gsc_series], axis=1)
L927         self.g_table.index = [t + ("â­ï¸" if t in top_G else "") for t in G_UNI]
L928         self.g_formatters = {col:"{:.2f}".format for col in ['GROWTH_F','MOM','TRD','VOL']}; self.g_formatters['GSC'] = "{:.3f}".format
L929         self.g_title = (f"[Gæ  / {N_G} / {_fmt_w(g_weights)} / corrM={corrM} / "
L930                         f"LB={DRRS_G['lookback']} nPC={DRRS_G['n_pc']} Î³={DRRS_G['gamma']} Î»={DRRS_G['lam']} Î·={DRRS_G['eta']} shrink={DRRS_SHRINK}]")
L931         if near_G:
L932             add = [t for t in near_G if t not in set(G_UNI)][:10]
L933             if len(add) < 10:
L934                 try:
L935                     aggG = getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False)
L936                     out_now = sorted(set(exist) - set(top_G + top_D))  # ä»Šå› OUT
L937                     used = set(G_UNI + add)
L938                     def _push(lst):
L939                         nonlocal add, used
L940                         for t in lst:
L941                             if len(add) == 10: break
L942                             if t in aggG.index and t not in used:
L943                                 add.append(t); used.add(t)
L944                     _push(out_now)           # â‘  ä»Šå› OUT ã‚’å„ªå…ˆ
L945                     _push(list(aggG.index))  # â‘¡ ã¾ã è¶³ã‚Šãªã‘ã‚Œã°ä¸Šä½ã§å……å¡«
L946                 except Exception:
L947                     pass
L948             if add:
L949                 near_tbl = pd.concat([df_z.loc[add,['GROWTH_F','MOM','TRD','VOL']], pd.Series({t: g_score.get(t) for t in add}, name='GSC')], axis=1)
L950                 self.g_table = pd.concat([self.g_table, near_tbl], axis=0)
L951         print(self.g_title); print(self.g_table.to_string(formatters=self.g_formatters))
L952
L953         extra_D = [t for t in init_D if t not in top_D][:5]; D_UNI = top_D + extra_D
L954         cols_D = ['QAL','YLD','VOL','TRD']; d_disp = pd.DataFrame(index=D_UNI)
L955         d_disp['QAL'], d_disp['YLD'], d_disp['VOL'], d_disp['TRD'] = df_z.loc[D_UNI,'D_QAL'], df_z.loc[D_UNI,'D_YLD'], df_z.loc[D_UNI,'D_VOL_RAW'], df_z.loc[D_UNI,'D_TRD']
L956         dsc_series = pd.Series({t: d_score_all.get(t) for t in D_UNI}, name='DSC')
L957         self.d_table = pd.concat([d_disp, dsc_series], axis=1); self.d_table.index = [t + ("â­ï¸" if t in top_D else "") for t in D_UNI]
L958         self.d_formatters = {col:"{:.2f}".format for col in cols_D}; self.d_formatters['DSC']="{:.3f}".format
L959         import scorer
L960         dw_eff = scorer.D_WEIGHTS_EFF
L961         self.d_title = (f"[Dæ  / {N_D} / {_fmt_w(dw_eff)} / corrM={corrM} / "
L962                         f"LB={DRRS_D['lookback']} nPC={DRRS_D['n_pc']} Î³={DRRS_D['gamma']} Î»={DRRS_D['lam']} Î¼={CROSS_MU_GD} Î·={DRRS_D['eta']} shrink={DRRS_SHRINK}]")
L963         if near_D:
L964             add = [t for t in near_D if t not in set(D_UNI)][:10]
L965             if add:
L966                 d_disp2 = pd.DataFrame(index=add)
L967                 d_disp2['QAL'], d_disp2['YLD'], d_disp2['VOL'], d_disp2['TRD'] = df_z.loc[add,'D_QAL'], df_z.loc[add,'D_YLD'], df_z.loc[add,'D_VOL_RAW'], df_z.loc[add,'D_TRD']
L968                 near_tbl = pd.concat([d_disp2, pd.Series({t: d_score_all.get(t) for t in add}, name='DSC')], axis=1)
L969                 self.d_table = pd.concat([self.d_table, near_tbl], axis=0)
L970         print(self.d_title); print(self.d_table.to_string(formatters=self.d_formatters))
L971
L972         # === Changesï¼ˆIN ã® GSC/DSC ã‚’è¡¨ç¤ºã€‚OUT ã¯éŠ˜æŸ„åã®ã¿ï¼‰ ===
L973         in_list = sorted(set(list(top_G)+list(top_D)) - set(exist))
L974         out_list = sorted(set(exist) - set(list(top_G)+list(top_D)))
L975
L976         self.io_table = pd.DataFrame({
L977             'IN': pd.Series(in_list),
L978             '/ OUT': pd.Series(out_list)
L979         })
L980         g_list = [f"{g_score.get(t):.3f}" if pd.notna(g_score.get(t)) else 'â€”' for t in out_list]
L981         d_list = [f"{d_score_all.get(t):.3f}" if pd.notna(d_score_all.get(t)) else 'â€”' for t in out_list]
L982         self.io_table['GSC'] = pd.Series(g_list)
L983         self.io_table['DSC'] = pd.Series(d_list)
L984
L985         print("Changes:")
L986         print(self.io_table.to_string(index=False))
L987
L988         all_tickers = list(set(exist + list(top_G) + list(top_D) + [bench])); prices = yf.download(all_tickers, period='1y', auto_adjust=True, progress=False, threads=False)['Close'].ffill(limit=2)
L989         ret = prices.pct_change(); portfolios = {'CUR':exist,'NEW':list(top_G)+list(top_D)}; metrics={}
L990         for name,ticks in portfolios.items():
L991             pr = ret[ticks].mean(axis=1, skipna=True).dropna(); cum = (1+pr).cumprod()-1; n = len(pr)
L992             if n>=252: ann_ret, ann_vol = (1+cum.iloc[-1])**(252/n)-1, pr.std()*np.sqrt(252)
L993             else: ann_ret, ann_vol = cum.iloc[-1], pr.std()*np.sqrt(n)
L994             sharpe, drawdown = ann_ret/ann_vol, (cum - cum.cummax()).min()
L995             if len(ticks)>=2:
L996                 C_raw = ret[ticks].corr(); RAW_rho = C_raw.mask(np.eye(len(ticks), dtype=bool)).stack().mean()
L997                 R = ret[ticks].dropna().to_numpy(); C_resid = Selector.residual_corr(R, n_pc=3, shrink=DRRS_SHRINK)
L998                 RESID_rho = float((C_resid.sum()-np.trace(C_resid))/(C_resid.shape[0]*(C_resid.shape[0]-1)))
L999             else: RAW_rho = RESID_rho = np.nan
L1000             divy = ttm_div_yield_portfolio(ticks); metrics[name] = {'RET':ann_ret,'VOL':ann_vol,'SHP':sharpe,'MDD':drawdown,'RAWÏ':RAW_rho,'RESIDÏ':RESID_rho,'DIVY':divy}
L1001         df_metrics = pd.DataFrame(metrics).T; df_metrics_pct = df_metrics.copy(); self.df_metrics = df_metrics
L1002         for col in ['RET','VOL','MDD','DIVY']: df_metrics_pct[col] = df_metrics_pct[col]*100
L1003         cols_order = ['RET','VOL','SHP','MDD','RAWÏ','RESIDÏ','DIVY']; df_metrics_pct = df_metrics_pct.reindex(columns=cols_order)
L1004         def _fmt_row(s):
L1005             return pd.Series({'RET':f"{s['RET']:.1f}%",'VOL':f"{s['VOL']:.1f}%",'SHP':f"{s['SHP']:.1f}",'MDD':f"{s['MDD']:.1f}%",'RAWÏ':(f"{s['RAWÏ']:.2f}" if pd.notna(s['RAWÏ']) else "NaN"),'RESIDÏ':(f"{s['RESIDÏ']:.2f}" if pd.notna(s['RESIDÏ']) else "NaN"),'DIVY':f"{s['DIVY']:.1f}%"})
L1006         self.df_metrics_fmt = df_metrics_pct.apply(_fmt_row, axis=1); print("Performance Comparison:"); print(self.df_metrics_fmt.to_string())
L1007         # === è¿½åŠ : GSC+DSC ãŒä½ã„é † TOP10 ===
L1008         try:
L1009             all_scores = pd.DataFrame({'GSC': df_z['GSC'], 'DSC': df_z['DSC']}).copy()
L1010             all_scores['G_plus_D'] = all_scores['GSC'] + all_scores['DSC']
L1011             all_scores = all_scores.dropna(subset=['G_plus_D'])
L1012             self.low10_table = all_scores.sort_values('G_plus_D', ascending=True).head(10).round(3)
L1013             print("Low Score Candidates (GSC+DSC bottom 10):")
L1014             print(self.low10_table.to_string())
L1015         except Exception as e:
L1016             print(f"[warn] low-score ranking failed: {e}")
L1017             self.low10_table = None
L1018         self.debug_text = ""
L1019         if debug_mode:
L1020             logger.info("debug_mode=True: df_z dump handled in scorer; skipping factor-side debug output")
L1021         else:
L1022             logger.debug(
L1023                 "skip debug log: debug_mode=%s debug_text_empty=%s",
L1024                 debug_mode, True
L1025             )
L1026         self._debug_logged = True
L1027
L1028     # --- Slacké€ä¿¡ï¼ˆå…ƒ notify_slack ã®ãƒ­ã‚¸ãƒƒã‚¯ãã®ã¾ã¾ï¼‰ ---
L1029     def notify_slack(self):
L1030         SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")
L1031
L1032         if not SLACK_WEBHOOK_URL:
L1033             print("âš ï¸ SLACK_WEBHOOK_URL not set (main report skipped)")
L1034             return
L1035
L1036         def _filter_suffix_from(spec: dict, group: str) -> str:
L1037             g = spec.get(group, {})
L1038             parts = [str(m) for m in g.get("pre_mask", [])]
L1039             for k, v in (g.get("pre_filter", {}) or {}).items():
L1040                 base, op = (k[:-4], "<") if k.endswith("_max") else ((k[:-4], ">") if k.endswith("_min") else (k, "="))
L1041                 name = {"beta": "Î²"}.get(base, base)
L1042                 try:
L1043                     val = f"{float(v):g}"
L1044                 except Exception:
L1045                     val = str(v)
L1046                 parts.append(f"{name}{op}{val}")
L1047             return "" if not parts else " / filter:" + " & ".join(parts)
L1048
L1049         def _inject_filter_suffix(title: str, group: str) -> str:
L1050             suf = _filter_suffix_from(FILTER_SPEC, group)
L1051             return f"{title[:-1]}{suf}]" if suf and title.endswith("]") else (title + suf)
L1052
L1053         def _blk(title, tbl, fmt=None, drop=()):
L1054             if tbl is None or getattr(tbl, 'empty', False):
L1055                 return f"{title}\n(é¸å®šãªã—)\n"
L1056             if drop and hasattr(tbl, 'columns'):
L1057                 keep = [c for c in tbl.columns if c not in drop]
L1058                 tbl, fmt = tbl[keep], {k: v for k, v in (fmt or {}).items() if k in keep}
L1059             return f"{title}\n```{tbl.to_string(formatters=fmt)}```\n"
L1060
L1061         message = "ğŸ“ˆ ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼åˆ†æ•£æœ€é©åŒ–ã®çµæœ\n"
L1062         if self.miss_df is not None and not self.miss_df.empty:
L1063             message += "Missing Data\n```" + self.miss_df.to_string(index=False) + "```\n"
L1064         message += _blk(_inject_filter_suffix(self.g_title, "G"), self.g_table, self.g_formatters, drop=("TRD",))
L1065         message += _blk(_inject_filter_suffix(self.d_title, "D"), self.d_table, self.d_formatters)
L1066         message += "Changes\n" + ("(å¤‰æ›´ãªã—)\n" if self.io_table is None or getattr(self.io_table, 'empty', False) else f"```{self.io_table.to_string(index=False)}```\n")
L1067         message += "Performance Comparison:\n```" + self.df_metrics_fmt.to_string() + "```"
L1068
L1069         try:
L1070             r = requests.post(SLACK_WEBHOOK_URL, json={"text": message})
L1071             print(f"[DBG] main_post status={getattr(r, 'status_code', None)} size={len(message)}")
L1072             if r is not None:
L1073                 r.raise_for_status()
L1074         except Exception as e:
L1075             print(f"[ERR] main_post_failed: {e}")
L1076
L1077 def _infer_g_universe(feature_df, selected12=None, near5=None):
L1078     try:
L1079         out = feature_df.index[feature_df['group'].astype(str).str.upper().eq('G')].tolist()
L1080         if out: return out
L1081     except Exception:
L1082         pass
L1083     base = set()
L1084     for lst in (selected12 or []), (near5 or []):
L1085         for x in (lst or []): base.add(x)
L1086     return list(base) if base else list(feature_df.index)
L1087
L1088 def _fmt_with_fire_mark(tickers, feature_df):
L1089     out = []
L1090     for t in tickers or []:
L1091         try:
L1092             br = bool(feature_df.at[t, "G_BREAKOUT_recent_5d"])
L1093             pb = bool(feature_df.at[t, "G_PULLBACK_recent_5d"])
L1094             out.append(f"{t}{' ğŸ”¥' if (br or pb) else ''}")
L1095         except Exception:
L1096             out.append(t)
L1097     return out
L1098
L1099 def _label_recent_event(t, feature_df):
L1100     try:
L1101         br = bool(feature_df.at[t, "G_BREAKOUT_recent_5d"]); dbr = str(feature_df.at[t, "G_BREAKOUT_last_date"]) if br else ""
L1102         pb = bool(feature_df.at[t, "G_PULLBACK_recent_5d"]); dpb = str(feature_df.at[t, "G_PULLBACK_last_date"]) if pb else ""
L1103         if   br and not pb: return f"{t}ï¼ˆãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®š {dbr}ï¼‰"
L1104         elif pb and not br: return f"{t}ï¼ˆæŠ¼ã—ç›®åç™º {dpb}ï¼‰"
L1105         elif br and pb:     return f"{t}ï¼ˆãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®š {dbr}ï¼æŠ¼ã—ç›®åç™º {dpb}ï¼‰"
L1106     except Exception:
L1107         pass
L1108     return t
L1109
L1110 # === ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å¯è¦–åŒ–ï¼šG/Då…±é€šãƒ•ãƒ­ãƒ¼ï¼ˆå‡ºåŠ›ã¯ä¸å¤‰ï¼‰ ===
L1111
L1112 def io_build_input_bundle() -> InputBundle:
L1113     """
L1114     æ—¢å­˜ã®ã€ãƒ‡ãƒ¼ã‚¿å–å¾—â†’å‰å‡¦ç†ã€ã‚’å®Ÿè¡Œã—ã€InputBundle ã‚’è¿”ã™ã€‚
L1115     å‡¦ç†å†…å®¹ãƒ»åˆ—åãƒ»ä¸¸ã‚ãƒ»ä¾‹å¤–ãƒ»ãƒ­ã‚°æ–‡è¨€ã¯ç¾è¡Œã©ãŠã‚Šï¼ˆå¤‰æ›´ç¦æ­¢ï¼‰ã€‚
L1116     """
L1117     state = Input(cand=cand, exist=exist, bench=bench, price_max=CAND_PRICE_MAX, finnhub_api_key=FINNHUB_API_KEY).prepare_data()
L1118     return InputBundle(cand=state["cand"], tickers=state["tickers"], bench=bench, data=state["data"], px=state["px"], spx=state["spx"], tickers_bulk=state["tickers_bulk"], info=state["info"], eps_df=state["eps_df"], fcf_df=state["fcf_df"], returns=state["returns"])
L1119
L1120 def run_group(sc: Scorer, group: str, inb: InputBundle, cfg: PipelineConfig,
L1121               n_target: int) -> tuple[list, float, float, float]:
L1122     """
L1123     G/Dã‚’åŒä¸€æ‰‹é †ã§å‡¦ç†ï¼šæ¡ç‚¹â†’ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼â†’é¸å®šï¼ˆç›¸é–¢ä½æ¸›è¾¼ã¿ï¼‰ã€‚
L1124     æˆ»ã‚Šå€¤ï¼š(pick, avg_res_corr, sum_score, objective)
L1125     JSONä¿å­˜ã¯æ—¢å­˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆã‚­ãƒ¼åãƒ»ä¸¸ã‚æ¡ãƒ»é †åºï¼‰ã‚’è¸è¥²ã€‚
L1126     """
L1127     sc.cfg = cfg
L1128
L1129     if hasattr(sc, "score_build_features"):
L1130         feat = sc.score_build_features(inb)
L1131         if not hasattr(sc, "_feat_logged"):
L1132             T.log("features built (scorer)")
L1133             sc._feat_logged = True
L1134         agg = sc.score_aggregate(feat, group, cfg) if hasattr(sc, "score_aggregate") else feat
L1135     else:
L1136         fb = sc.aggregate_scores(inb, cfg)
L1137         if not hasattr(sc, "_feat_logged"):
L1138             T.log("features built (scorer)")
L1139             sc._feat_logged = True
L1140         sc._feat = fb
L1141         agg = fb.g_score if group == "G" else fb.d_score_all
L1142         if group == "D" and hasattr(fb, "df"):
L1143             agg = agg[fb.df['BETA'] < D_BETA_MAX]
L1144
L1145     if hasattr(sc, "filter_candidates"):
L1146         agg = agg[sc.filter_candidates(inb, agg, group, cfg)]
L1147
L1148     selector = Selector()
L1149     if hasattr(sc, "select_diversified"):
L1150         pick, avg_r, sum_sc, obj = sc.select_diversified(agg, group, cfg, n_target,
L1151             selector=selector, prev_tickers=None,
L1152             corrM=cfg.drrs.corrM, shrink=cfg.drrs.shrink,
L1153             cross_mu=cfg.drrs.cross_mu_gd)
L1154     else:
L1155         if group == "G":
L1156             init = agg.nlargest(min(cfg.drrs.corrM, len(agg))).index.tolist()
L1157             res = selector.select_bucket_drrs(returns_df=inb.returns, score_ser=agg, pool_tickers=init, k=n_target,
L1158                 n_pc=cfg.drrs.G.get("n_pc", 3), gamma=cfg.drrs.G.get("gamma", 1.2),
L1159                 lam=cfg.drrs.G.get("lam", 0.68),
L1160                 lookback=cfg.drrs.G.get("lookback", 252),
L1161                 shrink=cfg.drrs.shrink, g_fixed_tickers=None, mu=0.0)
L1162         else:
L1163             init = agg.nlargest(min(cfg.drrs.corrM, len(agg))).index.tolist()
L1164             g_fixed = getattr(sc, "_top_G", None)
L1165             res = selector.select_bucket_drrs(returns_df=inb.returns, score_ser=agg, pool_tickers=init, k=n_target,
L1166                 n_pc=cfg.drrs.D.get("n_pc", 4), gamma=cfg.drrs.D.get("gamma", 0.8),
L1167                 lam=cfg.drrs.D.get("lam", 0.85),
L1168                 lookback=cfg.drrs.D.get("lookback", 504),
L1169                 shrink=cfg.drrs.shrink, g_fixed_tickers=g_fixed,
L1170                 mu=cfg.drrs.cross_mu_gd)
L1171         pick = res["tickers"]; avg_r = res["avg_res_corr"]
L1172         sum_sc = res["sum_score"]; obj = res["objective"]
L1173         if group == "D":
L1174             _, pick = _disjoint_keepG(getattr(sc, "_top_G", []), pick, init)
L1175             T.log("selection finalized (G/D)")
L1176     try:
L1177         inc = [t for t in exist if t in agg.index]
L1178         pick = _sticky_keep_current(
L1179             agg=agg, pick=pick, incumbents=inc, n_target=n_target,
L1180             delta_z=SWAP_DELTA_Z, keep_buffer=SWAP_KEEP_BUFFER
L1181         )
L1182     except Exception as _e:
L1183         print(f"[warn] sticky_keep_current skipped: {str(_e)}")
L1184     # --- Near-Miss: æƒœã—ãã‚‚é¸ã°ã‚Œãªã‹ã£ãŸä¸Šä½10ã‚’ä¿æŒï¼ˆSlackè¡¨ç¤ºç”¨ï¼‰ ---
L1185     # 5) Near-Miss ã¨æœ€çµ‚é›†è¨ˆSeriesã‚’ä¿æŒï¼ˆè¡¨ç¤ºå°‚ç”¨ã€‚è¨ˆç®—ã¸å½±éŸ¿ãªã—ï¼‰
L1186     try:
L1187         pool = agg.drop(index=[t for t in pick if t in agg.index], errors="ignore")
L1188         near10 = list(pool.sort_values(ascending=False).head(10).index)
L1189         setattr(sc, f"_near_{group}", near10)
L1190         setattr(sc, f"_agg_{group}", agg)
L1191     except Exception:
L1192         pass
L1193
L1194     if group == "D":
L1195         T.log("save done")
L1196     if group == "G":
L1197         sc._top_G = pick
L1198     return pick, avg_r, sum_sc, obj
L1199
L1200 def run_pipeline() -> SelectionBundle:
L1201     """
L1202     G/Då…±é€šãƒ•ãƒ­ãƒ¼ã®å…¥å£ã€‚I/Oã¯ã“ã“ã ã‘ã§å®Ÿæ–½ã—ã€è¨ˆç®—ã¯Scorerã«å§”è­²ã€‚
L1203     Slackæ–‡è¨€ãƒ»ä¸¸ã‚ãƒ»é †åºã¯æ—¢å­˜ã® Output ã‚’ç”¨ã„ã¦å¤‰æ›´ã—ãªã„ã€‚
L1204     """
L1205     inb = io_build_input_bundle()
L1206     cfg = PipelineConfig(
L1207         weights=WeightsConfig(g=g_weights, d=D_weights),
L1208         drrs=DRRSParams(
L1209             corrM=corrM, shrink=DRRS_SHRINK,
L1210             G=DRRS_G, D=DRRS_D, cross_mu_gd=CROSS_MU_GD
L1211         ),
L1212         price_max=CAND_PRICE_MAX,
L1213         debug_mode=debug_mode
L1214     )
L1215     sc = Scorer()
L1216     top_G, avgG, sumG, objG = run_group(sc, "G", inb, cfg, N_G)
L1217     poolG = list(getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False).index)
L1218     alpha = Scorer.spx_to_alpha(inb.spx)
L1219     sectors = {t:(inb.info.get(t,{}).get("sector") or "U") for t in poolG}; scores = {t:Scorer.g_score.get(t,0.0) for t in poolG}
L1220     top_G = Scorer.pick_top_softcap(scores, sectors, N=N_G, cap=2, alpha=alpha, hard=5)
L1221     sc._top_G = top_G
L1222     try:
L1223         aggG = getattr(sc, "_agg_G", pd.Series(dtype=float)).sort_values(ascending=False)
L1224         sc._near_G = [t for t in aggG.index if t not in set(top_G)][:10]
L1225     except Exception:
L1226         pass
L1227     base = sum(Scorer.g_score.get(t,0.0) for t in poolG[:N_G])
L1228     effs = sum(Scorer.g_score.get(t,0.0) for t in top_G)
L1229     print(f"[soft_cap2] score_cost={(base-effs)/max(1e-9,abs(base)):.2%}, alpha={alpha:.3f}")
L1230     top_D, avgD, sumD, objD = run_group(sc, "D", inb, cfg, N_D)
L1231     fb = getattr(sc, "_feat", None)
L1232     near_G = getattr(sc, "_near_G", [])
L1233     selected12 = list(top_G)
L1234     df = fb.df if fb is not None else pd.DataFrame()
L1235     guni = _infer_g_universe(df, selected12, near_G)
L1236     try:
L1237         fire_recent = [t for t in guni
L1238                        if (str(df.at[t, "G_BREAKOUT_recent_5d"]) == "True") or
L1239                           (str(df.at[t, "G_PULLBACK_recent_5d"]) == "True")]
L1240     except Exception: fire_recent = []
L1241
L1242     lines = [
L1243         "ã€Gæ ãƒ¬ãƒãƒ¼ãƒˆï½œé€±æ¬¡ãƒ¢ãƒ‹ã‚¿ï¼ˆç›´è¿‘5å–¶æ¥­æ—¥ï¼‰ã€‘",
L1244         "ã€å‡¡ä¾‹ã€‘ğŸ”¥=ç›´è¿‘5å–¶æ¥­æ—¥å†…ã«ã€Œãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®šã€ã¾ãŸã¯ã€ŒæŠ¼ã—ç›®åç™ºã€ã‚’æ¤œçŸ¥",
L1245         f"é¸å®š{N_G}: {', '.join(_fmt_with_fire_mark(selected12, df))}" if selected12 else f"é¸å®š{N_G}: ãªã—",
L1246         f"æ¬¡ç‚¹10: {', '.join(_fmt_with_fire_mark(near_G, df))}" if near_G else "æ¬¡ç‚¹10: ãªã—",]
L1247
L1248     if fire_recent:
L1249         fire_list = ", ".join([_label_recent_event(t, df) for t in fire_recent])
L1250         lines.append(f"éå»5å–¶æ¥­æ—¥ã®æ¤œçŸ¥: {fire_list}")
L1251     else:
L1252         lines.append("éå»5å–¶æ¥­æ—¥ã®æ¤œçŸ¥: ãªã—")
L1253
L1254     try:
L1255         webhook = os.environ.get("SLACK_WEBHOOK_URL", "")
L1256         if webhook:
L1257             requests.post(webhook, json={"text": "\n".join([s for s in lines if s != ""])}, timeout=10)
L1258     except Exception:
L1259         pass
L1260
L1261     out = Output()
L1262     # è¡¨ç¤ºå´ã‹ã‚‰é¸å®šæ™‚ã®é›†è¨ˆã¸ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†ã«ä¿æŒï¼ˆè¡¨ç¤ºå°‚ç”¨ãƒ»å‰¯ä½œç”¨ãªã—ï¼‰
L1263     try: out._sc = sc
L1264     except Exception: pass
L1265     if hasattr(sc, "_feat"):
L1266         try:
L1267             fb = sc._feat
L1268             out.miss_df = fb.missing_logs
L1269             out.display_results(
L1270                 exist=exist,
L1271                 bench=bench,
L1272                 df_z=fb.df_z,
L1273                 g_score=fb.g_score,
L1274                 d_score_all=fb.d_score_all,
L1275                 init_G=top_G,
L1276                 init_D=top_D,
L1277                 top_G=top_G,
L1278                 top_D=top_D,
L1279                 df_full_z=getattr(fb, "df_full_z", None),
L1280                 prev_G=getattr(sc, "_prev_G", exist),
L1281                 prev_D=getattr(sc, "_prev_D", exist),
L1282             )
L1283         except Exception:
L1284             pass
L1285     out.notify_slack()
L1286     sb = SelectionBundle(resG={"tickers": top_G, "avg_res_corr": avgG,
L1287               "sum_score": sumG, "objective": objG},
L1288         resD={"tickers": top_D, "avg_res_corr": avgD,
L1289               "sum_score": sumD, "objective": objD},
L1290         top_G=top_G, top_D=top_D, init_G=top_G, init_D=top_D)
L1291
L1292     # --- Low Score Candidates (GSC+DSC bottom 10) : send before debug dump ---
L1293     try:
L1294         _low_df = (pd.DataFrame({"GSC": fb.g_score, "DSC": fb.d_score_all})
L1295               .assign(G_plus_D=lambda x: x["GSC"] + x["DSC"])
L1296               .sort_values("G_plus_D")
L1297               .head(10)
L1298               .round(3))
L1299         low_msg = "Low Score Candidates (GSC+DSC bottom 10)\n" + _low_df.to_string(index=True, index_names=False)
L1300         _post_slack({"text": f"```{low_msg}```"})
L1301     except Exception as _e:
L1302         _post_slack({"text": f"```Low Score Candidates: ä½œæˆå¤±æ•—: {_e}```"})
L1303
L1304     return sb
L1305
L1306 if __name__ == "__main__":
L1307     run_pipeline()
```

## <scorer.py>
```text
L1 # scorer.py
L2 # kawatest
L3 # =============================================================================
L4 # Scorer: ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼/æŒ‡æ¨™ã®ç”Ÿæˆã¨åˆæˆã‚¹ã‚³ã‚¢ç®—å‡ºã‚’æ‹…ã†ç´”ç²‹å±¤
L5 #
L6 # ã€ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã ã‘èª­ã‚ã°åˆ†ã‹ã‚‹ãƒã‚¤ãƒ³ãƒˆã€‘
L7 # - å…¥åŠ›(InputBundle)ã¯ã€Œä¾¡æ ¼/å‡ºæ¥é«˜/ãƒ™ãƒ³ãƒ/åŸºæœ¬æƒ…å ±/EPS/FCF/ãƒªã‚¿ãƒ¼ãƒ³ã€ã‚’å«ã‚€DTO
L8 # - å‡ºåŠ›(FeatureBundle)ã¯ã€Œrawç‰¹å¾´é‡ dfã€ã€Œæ¨™æº–åŒ– df_zã€ã€ŒG/D ã‚¹ã‚³ã‚¢ã€ã€Œæ¬ æãƒ­ã‚°ã€
L9 # - é‡ã¿ç­‰ã®ã‚³ãƒ³ãƒ•ã‚£ã‚°(PipelineConfig)ã¯ factor ã‹ã‚‰æ¸¡ã™ï¼ˆcfg å¿…é ˆï¼‰
L10 # - æ—§ã‚«ãƒ©ãƒ åã¯ Scorer å†…ã§è‡ªå‹•ãƒªãƒãƒ¼ãƒ ã—ã¦å—ã‘å…¥ã‚Œï¼ˆå¾Œæ–¹äº’æ›ï¼‰
L11 #   ä¾‹) eps_ttm -> EPS_TTM, eps_q_recent -> EPS_Q_LastQ, fcf_ttm -> FCF_TTM
L12 #
L13 # ã€I/Oå¥‘ç´„ï¼ˆScorerãŒå‚ç…§ã™ã‚‹InputBundleãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰ã€‘
L14 #   - cand: List[str]    â€¦ å€™è£œéŠ˜æŸ„ï¼ˆå˜ä½“å®Ÿè¡Œã§ã¯æœªä½¿ç”¨ï¼‰
L15 #   - tickers: List[str] â€¦ å¯¾è±¡éŠ˜æŸ„ãƒªã‚¹ãƒˆ
L16 #   - bench: str         â€¦ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼ï¼ˆä¾‹ '^GSPC'ï¼‰
L17 #   - data: pd.DataFrame â€¦ yfinance downloadçµæœ ('Close','Volume' ç­‰ã®éšå±¤åˆ—)
L18 #   - px: pd.DataFrame   â€¦ data['Close'] ç›¸å½“ï¼ˆçµ‚å€¤ï¼‰
L19 #   - spx: pd.Series     â€¦ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®çµ‚å€¤
L20 #   - tickers_bulk: object         â€¦ yfinance.Tickers
L21 #   - info: Dict[str, dict]        â€¦ yfinance info per ticker
L22 #   - eps_df: pd.DataFrame         â€¦ å¿…é ˆåˆ—: EPS_TTM, EPS_Q_LastQï¼ˆæ—§åã‚‚å¯ï¼‰
L23 #   - fcf_df: pd.DataFrame         â€¦ å¿…é ˆåˆ—: FCF_TTMï¼ˆæ—§åã‚‚å¯ï¼‰
L24 #   - returns: pd.DataFrame        â€¦ px[tickers].pct_change() ç›¸å½“
L25 #
L26 # â€»å…¥å‡ºåŠ›ã®å½¢å¼ãƒ»ä¾‹å¤–æ–‡è¨€ã¯æ—¢å­˜å®Ÿè£…ã‚’å¤‰ãˆã¾ã›ã‚“ï¼ˆå®‰å…¨ãªçŸ­ç¸®ã®ã¿ï¼‰
L27 # =============================================================================
L28
L29 import logging
L30 import os, sys, warnings
L31 import json
L32 import requests
L33 import numpy as np
L34 import pandas as pd
L35 import yfinance as yf
L36 from typing import Any, TYPE_CHECKING
L37 from scipy.stats import zscore
L38 from datetime import datetime as _dt
L39
L40 FACTOR_COLUMNS = {
L41     "GRW": [
L42         "GROWTH_F",
L43         "GRW_FLEX_SCORE",
L44         "GRW_REV_YOY_Q",
L45         "GRW_REV_ACC_Q",
L46         "GRW_REV_QOQ",
L47         "GRW_REV_TTM2",
L48         "GRW_REV_YOY_Y",
L49         "GRW_PRICE_PROXY",
L50         "GRW_PATH",
L51     ],
L52     "MOM": ["MOM", "MOM_RAW", "MOM_P12M", "MOM_P6M", "MOM_P1M"],
L53     "VOL": ["VOL", "VOL_SD", "VOL_BETA"],
L54     "QUAL": ["QUAL", "ROE", "ROA", "FCF_MGN"],
L55     "VAL": ["VAL", "PE", "PB", "PS", "EVEBITDA"],
L56 }
L57
L58 if TYPE_CHECKING:
L59     from factor import PipelineConfig  # type: ignore  # å®Ÿè¡Œæ™‚importãªã—ï¼ˆå¾ªç’°å›é¿ï¼‰
L60
L61 logger = logging.getLogger(__name__)
L62
L63
L64 def _log(stage, msg):
L65     try:
L66         print(f"[DBG][{_dt.utcnow().isoformat(timespec='seconds')}Z][{stage}] {msg}")
L67     except Exception:
L68         print(f"[DBG][{stage}] {msg}")
L69
L70
L71 def _collect_relevant_cols(df, df_z, *, weight_dict=None):
L72     use: set[str] = set()
L73     if weight_dict:
L74         try:
L75             items = weight_dict.items() if hasattr(weight_dict, "items") else []
L76         except Exception:
L77             items = []
L78         for col, w in items:
L79             try:
L80                 weight = float(w or 0)
L81             except Exception:
L82                 weight = 0.0
L83             if abs(weight) > 0 and col in getattr(df_z, "columns", []):
L84                 use.add(col)
L85     for fac in ("GRW",):
L86         for c in FACTOR_COLUMNS.get(fac, []):
L87             if c in getattr(df, "columns", []) or c in getattr(df_z, "columns", []):
L88                 use.add(c)
L89     for fac in ["MOM", "VOL", "QUAL", "VAL"]:
L90         for c in FACTOR_COLUMNS.get(fac, []):
L91             if c in getattr(df, "columns", []) or c in getattr(df_z, "columns", []):
L92                 use.add(c)
L93     return sorted(use)
L94
L95
L96 def _reorder_for_debug(df, df_z, factor_cols=FACTOR_COLUMNS):
L97     cols: list[str] = []
L98     for fac in ["GRW", "MOM", "VOL", "QUAL", "VAL"]:
L99         for c in factor_cols.get(fac, []):
L100             if c in getattr(df_z, "columns", []) or c in getattr(df, "columns", []):
L101                 cols.append(c)
L102     seen: set[str] = set()
L103     ordered: list[str] = []
L104     for c in cols:
L105         if c not in seen:
L106             ordered.append(c)
L107             seen.add(c)
L108     return ordered
L109
L110
L111 def dump_dfz_scoped(df, df_z, *, weight_dict=None, topk=20, logger=None):
L112     import pandas as pd, logging
L113
L114     logger = logger or logging.getLogger(__name__)
L115     rel = _collect_relevant_cols(df, df_z, weight_dict=weight_dict)
L116     rel_df = [c for c in rel if c in getattr(df_z, "columns", [])]
L117     dfz = df_z[rel_df].copy() if rel_df else df_z.copy()
L118     logger.info("DEBUG scope: %d relevant cols (of %d total).", dfz.shape[1], df_z.shape[1])
L119     nan_top = dfz.isna().sum().sort_values(ascending=False).head(topk)
L120     logger.info("scorer:NaN columns (top%d):", topk)
L121     for c, n in nan_top.items():
L122         logger.info("%s\t%d", c, int(n))
L123     num_dfz = dfz.select_dtypes(include=["number"])
L124     if not num_dfz.empty:
L125         ztop = (num_dfz == 0).mean().sort_values(ascending=False).head(topk)
L126         logger.info("scorer:Zero-dominated columns (top%d):", topk)
L127         for c, r in ztop.items():
L128             logger.info("%s\t%.2f%%", c, 100.0 * float(r))
L129
L130
L131 def save_factor_debug_csv(df, df_z, path="out/factor_debug_latest.csv"):
L132     import os, pandas as pd, logging
L133
L134     lg = logging.getLogger(__name__)
L135     try:
L136         cols = _reorder_for_debug(df, df_z)
L137         dump = pd.DataFrame(index=df.index)
L138         for c in cols:
L139             if c in getattr(df, "columns", []):
L140                 dump[c] = df[c]
L141             if c in getattr(df_z, "columns", []):
L142                 dump[c] = df_z[c]
L143         dump.reset_index(names=["symbol"], inplace=True)
L144         if path:
L145             dirpath = os.path.dirname(path) or "."
L146             os.makedirs(dirpath, exist_ok=True)
L147             dump.to_csv(path, index=False)
L148         lg.info(
L149             "factor debug CSV saved: %s (cols=%d rows=%d)",
L150             path,
L151             dump.shape[1],
L152             dump.shape[0],
L153         )
L154     except Exception as e:
L155         lg.warning("factor debug CSV failed: %s", e)
L156
L157
L158 def log_grw_stats(df, df_z, logger):
L159     import numpy as np, pandas as pd
L160
L161     try:
L162         s = pd.to_numeric(df.get("GRW_FLEX_SCORE", pd.Series(dtype=float)), errors="coerce")
L163         z = pd.to_numeric(df_z.get("GROWTH_F", pd.Series(dtype=float)), errors="coerce")
L164         if s.size:
L165             logger.info(
L166                 "GRW raw stats: n=%d, median=%.3f, mad=%.3f, std=%.3f",
L167                 s.count(),
L168                 np.nanmedian(s),
L169                 np.nanmedian(np.abs(s - np.nanmedian(s))),
L170                 np.nanstd(s),
L171             )
L172         if z.size and not z.dropna().empty:
L173             clip_hi = float((z >= 2.95).mean() * 100.0)
L174             clip_lo = float((z <= -2.95).mean() * 100.0)
L175             logger.info(
L176                 "GRW z stats: min=%.2f, p25=%.2f, med=%.2f, p75=%.2f, max=%.2f, clipped_hi=%.1f%%, clipped_lo=%.1f%%",
L177                 np.nanmin(z),
L178                 np.nanpercentile(z.dropna(), 25),
L179                 np.nanmedian(z),
L180                 np.nanpercentile(z.dropna(), 75),
L181                 np.nanmax(z),
L182                 clip_hi,
L183                 clip_lo,
L184             )
L185         if "GRW_PATH" in getattr(df, "columns", []):
L186             logger.info(
L187                 "GRW path breakdown: %s",
L188                 df["GRW_PATH"].value_counts(dropna=False).to_dict(),
L189             )
L190     except Exception as e:
L191         logger.warning("GRW stats log failed: %s", e)
L192
L193
L194 def _grw_record_to_df(t: str, info_t: dict, df):
L195     if not isinstance(df, pd.DataFrame):
L196         return
L197     raw_parts = info_t.get("DEBUG_GRW_PARTS") if isinstance(info_t, dict) else None
L198     parts: dict[str, Any] = {}
L199     if isinstance(raw_parts, str):
L200         try:
L201             parts = json.loads(raw_parts)
L202         except Exception:
L203             parts = {}
L204     elif isinstance(raw_parts, dict):
L205         parts = raw_parts
L206     path = info_t.get("DEBUG_GRW_PATH") if isinstance(info_t, dict) else None
L207     score = info_t.get("GRW_SCORE") if isinstance(info_t, dict) else None
L208
L209     def _part_value(key: str):
L210         value = parts.get(key) if isinstance(parts, dict) else None
L211         if value is None:
L212             return np.nan
L213         try:
L214             return float(value)
L215         except Exception:
L216             return np.nan
L217
L218     df.loc[t, "GRW_PATH"] = path
L219     df.loc[t, "GRW_FLEX_SCORE"] = np.nan if score is None else float(score)
L220     df.loc[t, "GRW_REV_YOY_Q"] = _part_value("rev_yoy_q")
L221     df.loc[t, "GRW_REV_ACC_Q"] = _part_value("rev_acc_q")
L222     df.loc[t, "GRW_REV_QOQ"] = _part_value("rev_qoq")
L223     df.loc[t, "GRW_REV_TTM2"] = _part_value("rev_ttm2")
L224     df.loc[t, "GRW_REV_YOY_Y"] = _part_value("rev_yoy_y")
L225     df.loc[t, "GRW_PRICE_PROXY"] = _part_value("price_proxy")
L226
L227 # ---- Dividend Helpers -------------------------------------------------------
L228 def _last_close(t, price_map=None):
L229     if price_map and (c := price_map.get(t)) is not None: return float(c)
L230     try:
L231         h = yf.Ticker(t).history(period="5d")["Close"].dropna()
L232         return float(h.iloc[-1]) if len(h) else np.nan
L233     except Exception:
L234         return np.nan
L235
L236 def _ttm_div_sum(t, lookback_days=400):
L237     try:
L238         div = yf.Ticker(t).dividends
L239         if div is None or len(div) == 0: return 0.0
L240         cutoff = pd.Timestamp.utcnow().tz_localize(None) - pd.Timedelta(days=lookback_days)
L241         ttm = float(div[div.index.tz_localize(None) >= cutoff].sum())
L242         return ttm if ttm > 0 else float(div.tail(4).sum())
L243     except Exception:
L244         return 0.0
L245
L246 def ttm_div_yield_portfolio(tickers, price_map=None):
L247     ys = [(lambda c, s: (s/c) if (np.isfinite(c) and c>0 and s>0) else 0.0)(_last_close(t, price_map), _ttm_div_sum(t)) for t in tickers]
L248     return float(np.mean(ys)) if ys else 0.0
L249
L250 # ---- ç°¡æ˜“ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆå®‰å…¨ãªçŸ­ç¸®ã®ã¿ï¼‰ -----------------------------------
L251 def winsorize_s(s: pd.Series, p=0.02):
L252     if s is None or s.dropna().empty: return s
L253     lo, hi = np.nanpercentile(s.astype(float), [100*p, 100*(1-p)]); return s.clip(lo, hi)
L254
L255 def robust_z(s: pd.Series, p=0.02):
L256     s2 = winsorize_s(s,p); return np.nan_to_num(zscore(s2.fillna(s2.mean())))
L257
L258 def robust_z_keepnan(s: pd.Series) -> pd.Series:
L259     """robust_z variant that preserves NaNs and falls back to rank-z when needed."""
L260     if s is None:
L261         return pd.Series(dtype=float)
L262     v = pd.to_numeric(s, errors="coerce")
L263     m = np.nanmedian(v)
L264     mad = np.nanmedian(np.abs(v - m))
L265     z = (v - m) / (1.4826 * mad + 1e-9)
L266     if np.nanstd(z) < 1e-9:
L267         r = v.rank(method="average", na_option="keep")
L268         z = (r - np.nanmean(r)) / (np.nanstd(r) + 1e-9)
L269     return pd.Series(z, index=v.index, dtype=float)
L270
L271
L272 def _dump_dfz(
L273     df: pd.DataFrame,
L274     df_z: pd.DataFrame,
L275     debug_mode: bool,
L276     max_rows: int = 400,
L277     ndigits: int = 3,
L278     *,
L279     weight_dict=None,
L280 ) -> None:
L281     """df_z ã‚’ System log(INFO) ã¸ãƒ€ãƒ³ãƒ—ã™ã‚‹ç°¡æ½”ãªãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£."""
L282
L283     if not debug_mode:
L284         return
L285     try:
L286         rel = _collect_relevant_cols(df, df_z, weight_dict=weight_dict)
L287         ordered = _reorder_for_debug(df, df_z)
L288         rel_set = set(rel)
L289         view_cols = [c for c in ordered if c in rel_set and c in df_z.columns]
L290         if not view_cols:
L291             view_cols = list(df_z.columns)
L292         view = df_z[view_cols].copy()
L293         view = view.apply(
L294             lambda s: s.round(ndigits)
L295             if getattr(getattr(s, "dtype", None), "kind", "") in ("f", "i")
L296             else s
L297         )
L298         if len(view) > max_rows:
L299             view = view.iloc[:max_rows]
L300
L301         dump_dfz_scoped(df, df_z, weight_dict=weight_dict, topk=20, logger=logger)
L302
L303         logger.info("===== DF_Z DUMP START =====")
L304         logger.info("\n%s", view.to_string(max_rows=None, max_cols=None))
L305         logger.info("===== DF_Z DUMP END =====")
L306     except Exception as exc:
L307         logger.warning("df_z dump failed: %s", exc)
L308
L309 def _safe_div(a, b):
L310     try: return np.nan if (b is None or float(b)==0 or pd.isna(b)) else float(a)/float(b)
L311     except Exception: return np.nan
L312
L313 def _safe_last(series: pd.Series, default=np.nan):
L314     try: return float(series.iloc[-1])
L315     except Exception: return default
L316
L317
L318 def _ensure_series(x):
L319     if x is None:
L320         return pd.Series(dtype=float)
L321     if isinstance(x, pd.Series):
L322         return x.dropna()
L323     if isinstance(x, (list, tuple)):
L324         if len(x) and isinstance(x[0], (tuple, list)) and len(x[0]) == 2:
L325             dt = pd.to_datetime([d for d, _ in x], errors="coerce")
L326             v = pd.to_numeric([_v for _, _v in x], errors="coerce")
L327             return pd.Series(v, index=dt).dropna()
L328         return pd.Series(pd.to_numeric(list(x), errors="coerce")).dropna()
L329     try:
L330         return pd.Series(x).dropna()
L331     except Exception:
L332         return pd.Series(dtype=float)
L333
L334
L335 def _to_quarterly(s: pd.Series) -> pd.Series:
L336     if s.empty or not isinstance(s.index, pd.DatetimeIndex):
L337         return s
L338     return s.resample("Q").last().dropna()
L339
L340
L341 def _ttm_yoy_from_quarterly(qs: pd.Series) -> pd.Series:
L342     if qs is None or qs.empty:
L343         return pd.Series(dtype=float)
L344     ttm = qs.rolling(4, min_periods=2).sum()
L345     yoy = ttm.pct_change(4)
L346     return yoy
L347
L348
L349 def _nz(x) -> float:
L350     if x is None:
L351         return 0.0
L352     try:
L353         value = float(x)
L354     except Exception:
L355         return 0.0
L356     if not np.isfinite(value):
L357         return 0.0
L358     return value
L359
L360
L361 def _winsor(x, lo=-2.0, hi=2.0) -> float:
L362     v = _nz(x)
L363     if v < lo:
L364         return float(lo)
L365     if v > hi:
L366         return float(hi)
L367     return float(v)
L368
L369
L370 def _round_debug(x, ndigits: int = 4):
L371     try:
L372         value = float(x)
L373     except Exception:
L374         return None
L375     if not np.isfinite(value):
L376         return None
L377     return round(value, ndigits)
L378
L379
L380 def _calc_grw_flexible(
L381     ticker: str,
L382     info_entry: dict | None,
L383     close_series: pd.Series | None,
L384     volume_series: pd.Series | None,
L385 ):
L386     info_entry = info_entry if isinstance(info_entry, dict) else {}
L387
L388     s_rev_q = _ensure_series(info_entry.get("SEC_REV_Q_SERIES"))
L389     s_eps_q = _ensure_series(info_entry.get("SEC_EPS_Q_SERIES"))
L390     s_rev_y = _ensure_series(info_entry.get("SEC_REV_Y_SERIES"))
L391
L392     nQ = int(getattr(s_rev_q, "size", 0))
L393     nY = int(getattr(s_rev_y, "size", 0))
L394
L395     parts: dict[str, Any] = {"nQ": nQ, "nY": nY}
L396     path = "NONE"
L397     w = 0.0
L398
L399     def _valid_ratio(a, b):
L400         try:
L401             na, nb = float(a), float(b)
L402         except Exception:
L403             return None
L404         if not np.isfinite(na) or not np.isfinite(nb) or nb == 0:
L405             return None
L406         return na, nb
L407
L408     def yoy_q(series: pd.Series) -> float | None:
L409         s = _ensure_series(series)
L410         if s.empty:
L411             return None
L412         s = s.sort_index()
L413         if isinstance(s.index, pd.DatetimeIndex):
L414             last_idx = s.index[-1]
L415             window_start = last_idx - pd.DateOffset(months=15)
L416             window_end = last_idx - pd.DateOffset(months=9)
L417             candidates = s.loc[(s.index >= window_start) & (s.index <= window_end)]
L418             if candidates.empty:
L419                 candidates = s.loc[s.index <= window_end]
L420             if candidates.empty:
L421                 return None
L422             v1 = candidates.iloc[-1]
L423             v0 = s.iloc[-1]
L424         else:
L425             if s.size < 5:
L426                 return None
L427             v0 = s.iloc[-1]
L428             v1 = s.iloc[-5]
L429         pair = _valid_ratio(v0, v1)
L430         if pair is None:
L431             return None
L432         a, b = pair
L433         return float(a / b - 1.0)
L434
L435     def qoq(series: pd.Series) -> float | None:
L436         s = _ensure_series(series)
L437         if s.size < 2:
L438             return None
L439         s = s.sort_index()
L440         v0, v1 = s.iloc[-1], s.iloc[-2]
L441         pair = _valid_ratio(v0, v1)
L442         if pair is None:
L443             return None
L444         a, b = pair
L445         return float(a / b - 1.0)
L446
L447     def ttm_delta(series: pd.Series) -> float | None:
L448         s = _ensure_series(series)
L449         if s.size < 2:
L450             return None
L451         s = s.sort_index()
L452         k = int(min(4, s.size))
L453         cur_slice = s.iloc[-k:]
L454         prev_slice = s.iloc[:-k]
L455         if prev_slice.empty:
L456             return None
L457         prev_k = int(min(k, prev_slice.size))
L458         cur_sum = float(cur_slice.sum())
L459         prev_sum = float(prev_slice.iloc[-prev_k:].sum())
L460         pair = _valid_ratio(cur_sum, prev_sum)
L461         if pair is None:
L462             return None
L463         a, b = pair
L464         return float(a / b - 1.0)
L465
L466     def yoy_y(series: pd.Series) -> float | None:
L467         s = _ensure_series(series)
L468         if s.size < 2:
L469             return None
L470         s = s.sort_index()
L471         v0, v1 = s.iloc[-1], s.iloc[-2]
L472         pair = _valid_ratio(v0, v1)
L473         if pair is None:
L474             return None
L475         a, b = pair
L476         return float(a / b - 1.0)
L477
L478     def price_proxy_growth() -> float | None:
L479         if not isinstance(close_series, pd.Series):
L480             return None
L481         close = close_series.sort_index().dropna()
L482         if close.empty:
L483             return None
L484         hh_window = int(min(126, len(close)))
L485         if hh_window < 20:
L486             return None
L487         hh = close.rolling(hh_window).max().iloc[-1]
L488         prox = None
L489         if np.isfinite(hh) and hh > 0:
L490             prox = float(close.iloc[-1] / hh)
L491         rs6 = None
L492         if len(close) >= 63:
L493             rs6 = float(close.pct_change(63).iloc[-1])
L494         rs12 = None
L495         if len(close) >= 126:
L496             rs12 = float(close.pct_change(126).iloc[-1])
L497         vexp = None
L498         if isinstance(volume_series, pd.Series):
L499             vol = volume_series.reindex(close.index).dropna()
L500             if len(vol) >= 50:
L501                 v20 = vol.rolling(20).mean().iloc[-1]
L502                 v50 = vol.rolling(50).mean().iloc[-1]
L503                 if np.isfinite(v20) and np.isfinite(v50) and v50 > 0:
L504                     vexp = float(v20 / v50 - 1.0)
L505         prox = 0.0 if prox is None or not np.isfinite(prox) else prox
L506         rs6 = 0.0 if rs6 is None or not np.isfinite(rs6) else rs6
L507         rs12 = 0.0 if rs12 is None or not np.isfinite(rs12) else rs12
L508         vexp = 0.0 if vexp is None or not np.isfinite(vexp) else vexp
L509         return 0.5 * prox + 0.3 * rs6 + 0.2 * rs12 + 0.2 * vexp
L510
L511     price_alt = price_proxy_growth() or 0.0
L512     core = 0.0
L513     core_raw = 0.0
L514     price_raw = price_alt
L515
L516     if nQ >= 5:
L517         path = "P5"
L518         yq = yoy_q(s_rev_q)
L519         parts["rev_yoy_q"] = yq
L520         tmp_prev = s_rev_q.iloc[:-1] if s_rev_q.size > 1 else s_rev_q
L521         acc = None
L522         if tmp_prev.size >= 5 and yq is not None:
L523             yq_prev = yoy_q(tmp_prev)
L524             if yq_prev is not None:
L525                 acc = float(yq - yq_prev)
L526         parts["rev_acc_q"] = acc
L527         eps_yoy = yoy_q(s_eps_q) if s_eps_q.size >= 5 else None
L528         parts["eps_yoy_q"] = eps_yoy
L529         eps_acc = None
L530         if eps_yoy is not None and s_eps_q.size > 5:
L531             eps_prev = s_eps_q.iloc[:-1]
L532             if eps_prev.size >= 5:
L533                 eps_prev_yoy = yoy_q(eps_prev)
L534                 if eps_prev_yoy is not None:
L535                     eps_acc = float(eps_yoy - eps_prev_yoy)
L536         parts["eps_acc_q"] = eps_acc
L537         w = 1.0
L538         core_raw = (
L539             0.60 * _nz(yq)
L540             + 0.20 * _nz(acc)
L541             + 0.15 * _nz(eps_yoy)
L542             + 0.05 * _nz(eps_acc)
L543         )
L544         price_alt = 0.0
L545     elif 2 <= nQ <= 4:
L546         path = "P24"
L547         rev_qoq = qoq(s_rev_q)
L548         rev_ttm2 = ttm_delta(s_rev_q)
L549         parts["rev_qoq"] = rev_qoq
L550         parts["rev_ttm2"] = rev_ttm2
L551         eps_qoq = qoq(s_eps_q) if s_eps_q.size >= 2 else None
L552         parts["eps_qoq"] = eps_qoq
L553         w = min(1.0, nQ / 5.0)
L554         core_raw = 0.6 * _nz(rev_qoq) + 0.3 * _nz(rev_ttm2) + 0.1 * _nz(eps_qoq)
L555     else:
L556         path = "P1Y"
L557         rev_yoy_y = yoy_y(s_rev_y) if nY >= 2 else None
L558         parts["rev_yoy_y"] = rev_yoy_y
L559         w = 0.6 * min(1.0, nY / 3.0) if nY >= 2 else 0.4
L560         core_raw = _nz(rev_yoy_y)
L561         if nQ <= 1 and nY < 2 and price_alt == 0.0:
L562             price_alt = price_proxy_growth() or 0.0
L563
L564     core = _winsor(core_raw, lo=-1.5, hi=1.5)
L565     price_alt = _winsor(price_alt, lo=-1.5, hi=1.5)
L566     grw = _winsor(w * core + (1.0 - w) * (0.5 * _nz(price_alt)), lo=-2.0, hi=2.0)
L567
L568     parts.update(
L569         {
L570             "core_raw": core_raw,
L571             "core": core,
L572             "price_proxy_raw": price_raw,
L573             "price_proxy": price_alt,
L574             "weight": w,
L575             "score": grw,
L576         }
L577     )
L578
L579     parts_out: dict[str, Any] = {
L580         "nQ": nQ,
L581         "nY": nY,
L582     }
L583     for key, value in parts.items():
L584         if key in ("nQ", "nY"):
L585             continue
L586         rounded = _round_debug(value)
L587         parts_out[key] = rounded
L588
L589     info_entry["DEBUG_GRW_PATH"] = path
L590     info_entry["DEBUG_GRW_PARTS"] = json.dumps(parts_out, ensure_ascii=False, sort_keys=True)
L591     info_entry["GRW_SCORE"] = grw
L592     info_entry["GRW_WEIGHT"] = w
L593     info_entry["GRW_CORE"] = core
L594     info_entry["GRW_PRICE_PROXY"] = price_alt
L595
L596     return {
L597         "score": grw,
L598         "path": path,
L599         "parts": info_entry["DEBUG_GRW_PARTS"],
L600         "weight": w,
L601         "core": core,
L602         "price_proxy": price_alt,
L603     }
L604
L605
L606 D_WEIGHTS_EFF = None  # å‡ºåŠ›è¡¨ç¤ºäº’æ›ã®ãŸã‚
L607
L608
L609 def _scalar(v):
L610     """å˜ä¸€ã‚»ãƒ«ä»£å…¥ç”¨ã«å€¤ã‚’ã‚¹ã‚«ãƒ©ãƒ¼ã¸æ­£è¦åŒ–ã™ã‚‹ã€‚
L611
L612     - pandas Series -> .iloc[-1]ï¼ˆæœ€å¾Œã‚’æ¡ç”¨ï¼‰
L613     - list/tuple/ndarray -> æœ€å¾Œã®è¦ç´ 
L614     - ãã‚Œä»¥å¤–          -> ãã®ã¾ã¾
L615     å–å¾—å¤±æ•—æ™‚ã¯ np.nan ã‚’è¿”ã™ã€‚
L616     """
L617     import numpy as _np
L618     import pandas as _pd
L619     try:
L620         if isinstance(v, _pd.Series):
L621             return v.iloc[-1] if len(v) else _np.nan
L622         if isinstance(v, (list, tuple, _np.ndarray)):
L623             return v[-1] if len(v) else _np.nan
L624         return v
L625     except Exception:
L626         return _np.nan
L627
L628
L629 # ---- Scorer æœ¬ä½“ -------------------------------------------------------------
L630 class Scorer:
L631     """
L632     - factor.py ã‹ã‚‰ã¯ `aggregate_scores(ib, cfg)` ã‚’å‘¼ã¶ã ã‘ã§OKã€‚
L633     - cfg ã¯å¿…é ˆï¼ˆfactor.PipelineConfig ã‚’æ¸¡ã™ï¼‰ã€‚
L634     - æ—§ã‚«ãƒ©ãƒ åã‚’è‡ªå‹•ãƒªãƒãƒ¼ãƒ ã—ã¦æ–°ã‚¹ã‚­ãƒ¼ãƒã«å¸åã—ã¾ã™ã€‚
L635     """
L636
L637     # === å…ˆé ­ã§æ—§â†’æ–°ã‚«ãƒ©ãƒ åãƒãƒƒãƒ—ï¼ˆç§»è¡Œç”¨ï¼‰ ===
L638     EPS_RENAME = {"eps_ttm":"EPS_TTM", "eps_q_recent":"EPS_Q_LastQ"}
L639     FCF_RENAME = {"fcf_ttm":"FCF_TTM"}
L640
L641     # === ã‚¹ã‚­ãƒ¼ãƒç°¡æ˜“ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€ä½é™ï¼‰ ===
L642     @staticmethod
L643     def _validate_ib_for_scorer(ib: Any):
L644         miss = [a for a in ["tickers","bench","data","px","spx","tickers_bulk","info","eps_df","fcf_df","returns"] if not hasattr(ib,a) or getattr(ib,a) is None]
L645         if miss: raise ValueError(f"InputBundle is missing required attributes for Scorer: {miss}")
L646         if any(c in ib.eps_df.columns for c in Scorer.EPS_RENAME): ib.eps_df.rename(columns=Scorer.EPS_RENAME, inplace=True)
L647         if any(c in ib.fcf_df.columns for c in Scorer.FCF_RENAME): ib.fcf_df.rename(columns=Scorer.FCF_RENAME, inplace=True)
L648         need_eps, need_fcf = {"EPS_TTM","EPS_Q_LastQ"},{"FCF_TTM"}
L649         if not need_eps.issubset(ib.eps_df.columns): raise ValueError(f"eps_df must contain columns {need_eps} (accepts old names via auto-rename). Got: {list(ib.eps_df.columns)}")
L650         if not need_fcf.issubset(ib.fcf_df.columns): raise ValueError(f"fcf_df must contain columns {need_fcf} (accepts old names via auto-rename). Got: {list(ib.fcf_df.columns)}")
L651
L652     # ----ï¼ˆScorerå°‚ç”¨ï¼‰ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ»æŒ‡æ¨™ç³» ----
L653     @staticmethod
L654     def trend(s: pd.Series):
L655         if len(s)<200: return np.nan
L656         sma50, sma150, sma200 = s.rolling(50).mean().iloc[-1], s.rolling(150).mean().iloc[-1], s.rolling(200).mean().iloc[-1]
L657         prev200, p = s.rolling(200).mean().iloc[-21], s.iloc[-1]
L658         lo_52 = s[-252:].min() if len(s)>=252 else s.min(); hi_52 = s[-252:].max() if len(s)>=252 else s.max()
L659         rng = (hi_52 - lo_52) if hi_52>lo_52 else np.nan
L660         clip = lambda x,lo,hi: (np.nan if pd.isna(x) else max(lo,min(hi,x)))
L661         a = clip(p/(s.rolling(50).mean().iloc[-1]) - 1, -0.5, 0.5)
L662         b = clip(sma50/sma150 - 1, -0.5, 0.5)
L663         c = clip(sma150/sma200 - 1, -0.5, 0.5)
L664         d = clip(sma200/prev200 - 1, -0.2, 0.2)
L665         e = clip((p - lo_52) / (rng if rng and rng>0 else np.nan) - 0.5, -0.5, 0.5)
L666         parts = [0.0 if pd.isna(x) else x for x in (a,b,c,d,e)]
L667         return 0.30*parts[0] + 0.20*parts[1] + 0.15*parts[2] + 0.15*parts[3] + 0.20*parts[4]
L668
L669     @staticmethod
L670     def rs(s, b):
L671         n, nb = len(s), len(b)
L672         if n<60 or nb<60: return np.nan
L673         L12 = 252 if n>=252 and nb>=252 else min(n,nb)-1; L1 = 22 if n>=22 and nb>=22 else max(5, min(n,nb)//3)
L674         r12, r1, br12, br1 = s.iloc[-1]/s.iloc[-L12]-1, s.iloc[-1]/s.iloc[-L1]-1, b.iloc[-1]/b.iloc[-L12]-1, b.iloc[-1]/b.iloc[-L1]-1
L675         return (r12 - br12)*0.7 + (r1 - br1)*0.3
L676
L677     @staticmethod
L678     def tr_str(s):
L679         if s is None:
L680             return np.nan
L681         s = s.ffill(limit=2).dropna()
L682         if len(s) < 50:
L683             return np.nan
L684         ma50 = s.rolling(50, min_periods=50).mean()
L685         last_ma = ma50.iloc[-1]
L686         last_px = s.iloc[-1]
L687         return float(last_px/last_ma - 1.0) if pd.notna(last_ma) and pd.notna(last_px) else np.nan
L688
L689     @staticmethod
L690     def rs_line_slope(s: pd.Series, b: pd.Series, win: int) -> float:
L691         r = (s/b).dropna()
L692         if len(r) < win: return np.nan
L693         y, x = np.log(r.iloc[-win:]), np.arange(win, dtype=float)
L694         try: return float(np.polyfit(x, y, 1)[0])
L695         except Exception: return np.nan
L696
L697     @staticmethod
L698     def ev_fallback(info_t: dict, tk: yf.Ticker) -> float:
L699         ev = info_t.get('enterpriseValue', np.nan)
L700         if pd.notna(ev) and ev>0: return float(ev)
L701         mc, debt, cash = info_t.get('marketCap', np.nan), np.nan, np.nan
L702         try:
L703             bs = tk.quarterly_balance_sheet
L704             if bs is not None and not bs.empty:
L705                 c = bs.columns[0]
L706                 for k in ("Total Debt","Long Term Debt","Short Long Term Debt"):
L707                     if k in bs.index: debt = float(bs.loc[k,c]); break
L708                 for k in ("Cash And Cash Equivalents","Cash And Cash Equivalents And Short Term Investments","Cash"):
L709                     if k in bs.index: cash = float(bs.loc[k,c]); break
L710         except Exception: pass
L711         if pd.notna(mc): return float(mc + (0 if pd.isna(debt) else debt) - (0 if pd.isna(cash) else cash))
L712         return np.nan
L713
L714     @staticmethod
L715     def dividend_status(ticker: str) -> str:
L716         t = yf.Ticker(ticker)
L717         try:
L718             if not t.dividends.empty: return "has"
L719         except Exception: return "unknown"
L720         try:
L721             a = t.actions
L722             if (a is not None and not a.empty and "Stock Splits" in a.columns and a["Stock Splits"].abs().sum()>0): return "none_confident"
L723         except Exception: pass
L724         try:
L725             fi = t.fast_info
L726             if any(getattr(fi,k,None) for k in ("last_dividend_date","dividend_rate","dividend_yield")): return "maybe_missing"
L727         except Exception: pass
L728         return "unknown"
L729
L730     @staticmethod
L731     def div_streak(t):
L732         try:
L733             divs = yf.Ticker(t).dividends.dropna(); ann = divs.groupby(divs.index.year).sum(); ann = ann[ann.index<pd.Timestamp.today().year]
L734             years, streak = sorted(ann.index), 0
L735             for i in range(len(years)-1,0,-1):
L736                 if ann[years[i]] > ann[years[i-1]]: streak += 1
L737                 else: break
L738             return streak
L739         except Exception: return 0
L740
L741     @staticmethod
L742     def fetch_finnhub_metrics(symbol):
L743         api_key = os.environ.get("FINNHUB_API_KEY")
L744         if not api_key: return {}
L745         url, params = "https://finnhub.io/api/v1/stock/metric", {"symbol":symbol,"metric":"all","token":api_key}
L746         try:
L747             r = requests.get(url, params=params, timeout=10); r.raise_for_status(); m = r.json().get("metric",{})
L748             return {'EPS':m.get('epsGrowthTTMYoy'),'REV':m.get('revenueGrowthTTMYoy'),'ROE':m.get('roeTTM'),'BETA':m.get('beta'),'DIV':m.get('dividendYieldIndicatedAnnual'),'FCF':(m.get('freeCashFlowTTM')/m.get('enterpriseValue')) if m.get('freeCashFlowTTM') and m.get('enterpriseValue') else None}
L749         except Exception: return {}
L750
L751     @staticmethod
L752     def calc_beta(series: pd.Series, market: pd.Series, lookback=252):
L753         r, m = series.pct_change().dropna(), market.pct_change().dropna()
L754         n = min(len(r), len(m), lookback)
L755         if n<60: return np.nan
L756         r, m = r.iloc[-n:], m.iloc[-n:]; cov, var = np.cov(r, m)[0,1], np.var(m)
L757         return np.nan if var==0 else cov/var
L758
L759     @staticmethod
L760     def spx_to_alpha(spx: pd.Series, bands=(0.03,0.10), w=(0.6,0.4),
L761                      span=5, q=(0.20,0.40), alphas=(0.05,0.08,0.10)) -> float:
L762         """
L763         S&P500æŒ‡æ•°ã®ã¿ã‹ã‚‰æ“¬ä¼¼breadthã‚’ä½œã‚Šã€å±¥æ­´åˆ†ä½ã§Î±ã‚’æ®µéšæ±ºå®šã€‚
L764         bands=(Â±3%, Â±10%), w=(50DMA,200DMA), åˆ†ä½q=(20%,40%), alphas=(ä½,ä¸­,é«˜)
L765         """
L766         ma50, ma200 = spx.rolling(50).mean(), spx.rolling(200).mean()
L767         b50, b200 = ((spx/ma50 - 1)+bands[0])/(2*bands[0]), ((spx/ma200 - 1)+bands[1])/(2*bands[1])
L768         hist = (w[0]*b50 + w[1]*b200).clip(0,1).ewm(span=span).mean()
L769         b, (lo, mid) = float(hist.iloc[-1]), (float(hist.quantile(q[0])), float(hist.quantile(q[1])))
L770         return alphas[0] if b < lo else alphas[1] if b < mid else alphas[2]
L771
L772     @staticmethod
L773     def soft_cap_effective_scores(scores: pd.Series|dict, sectors: dict, cap=2, alpha=0.08) -> pd.Series:
L774         """
L775         åŒä¸€ã‚»ã‚¯ã‚¿ãƒ¼capè¶…éï¼ˆ3æœ¬ç›®ä»¥é™ï¼‰ã« Î±Ã—æ®µéšæ¸›ç‚¹ã‚’èª²ã—ãŸâ€œæœ‰åŠ¹ã‚¹ã‚³ã‚¢â€Seriesã‚’è¿”ã™ã€‚
L776         æˆ»ã‚Šå€¤ã¯é™é †ã‚½ãƒ¼ãƒˆæ¸ˆã¿ã€‚
L777         """
L778         s = pd.Series(scores, dtype=float); order = s.sort_values(ascending=False).index
L779         cnt, pen = {}, {}
L780         for t in order:
L781             sec = sectors.get(t, "U"); cnt[sec] = cnt.get(sec,0) + 1; pen[t] = alpha*max(0, cnt[sec]-cap)
L782         return (s - pd.Series(pen)).sort_values(ascending=False)
L783
L784     @staticmethod
L785     def pick_top_softcap(scores: pd.Series|dict, sectors: dict, N: int, cap=2, alpha=0.08, hard: int|None=5) -> list[str]:
L786         """
L787         soft-capé©ç”¨å¾Œã®ä¸Šä½Nãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚’è¿”ã™ã€‚hard>0ãªã‚‰éå¸¸ç”¨ãƒãƒ¼ãƒ‰ä¸Šé™ã§åŒä¸€ã‚»ã‚¯ã‚¿ãƒ¼è¶…éã‚’é–“å¼•ãï¼ˆæ—¢å®š=5ï¼‰ã€‚
L788         """
L789         eff = Scorer.soft_cap_effective_scores(scores, sectors, cap, alpha)
L790         if not hard:
L791             return list(eff.head(N).index)
L792         pick, used = [], {}
L793         for t in eff.index:
L794             s = sectors.get(t, "U")
L795             if used.get(s,0) < hard:
L796                 pick.append(t); used[s] = used.get(s,0) + 1
L797             if len(pick) == N: break
L798         return pick
L799
L800     @staticmethod
L801     def trend_template_breadth_series(px: pd.DataFrame, spx: pd.Series, win_days: int | None = None) -> pd.Series:
L802         """
L803         å„å–¶æ¥­æ—¥ã® trend_template åˆæ ¼æœ¬æ•°ï¼ˆåˆæ ¼â€œæœ¬æ•°â€=Cï¼‰ã‚’è¿”ã™ã€‚
L804         - px: åˆ—=tickerï¼ˆãƒ™ãƒ³ãƒã¯å«ã‚ãªã„ï¼‰
L805         - spx: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ Seriesï¼ˆpx.index ã«æ•´åˆ—ï¼‰
L806         - win_days: æœ«å°¾ã®è¨ˆç®—å¯¾è±¡å–¶æ¥­æ—¥æ•°ï¼ˆNoneâ†’å…¨ä½“ã€æ—¢å®š600ã¯å‘¼ã³å‡ºã—å´æŒ‡å®šï¼‰
L807         ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼†rollingã®ã¿ã§è»½é‡ã€‚æ¬ æã¯ False æ‰±ã„ã€‚
L808         """
L809         import numpy as np, pandas as pd
L810         if px is None or px.empty:
L811             return pd.Series(dtype=int)
L812         px = px.dropna(how="all", axis=1)
L813         if win_days and win_days > 0:
L814             px = px.tail(win_days)
L815         if px.empty:
L816             return pd.Series(dtype=int)
L817         spx = spx.reindex(px.index).ffill()
L818
L819         ma50  = px.rolling(50).mean()
L820         ma150 = px.rolling(150).mean()
L821         ma200 = px.rolling(200).mean()
L822
L823         tt = (px > ma150)
L824         tt &= (px > ma200)
L825         tt &= (ma150 > ma200)
L826         tt &= (ma200 - ma200.shift(21) > 0)
L827         tt &= (ma50  > ma150)
L828         tt &= (ma50  > ma200)
L829         tt &= (px    > ma50)
L830
L831         lo252 = px.rolling(252).min()
L832         hi252 = px.rolling(252).max()
L833         tt &= (px.divide(lo252).sub(1.0) >= 0.30)   # P_OVER_LOW52 >= 0.30
L834         tt &= (px >= (0.75 * hi252))                # NEAR_52W_HIGH >= -0.25
L835
L836         r12  = px.divide(px.shift(252)).sub(1.0)
L837         br12 = spx.divide(spx.shift(252)).sub(1.0)
L838         r1   = px.divide(px.shift(22)).sub(1.0)
L839         br1  = spx.divide(spx.shift(22)).sub(1.0)
L840         rs   = 0.7*(r12.sub(br12, axis=0)) + 0.3*(r1.sub(br1, axis=0))
L841         tt &= (rs >= 0.10)
L842
L843         return tt.fillna(False).sum(axis=1).astype(int)
L844
L845     # ---- ã‚¹ã‚³ã‚¢é›†è¨ˆï¼ˆDTO/Configã‚’å—ã‘å–ã‚Šã€FeatureBundleã‚’è¿”ã™ï¼‰ ----
L846     def aggregate_scores(self, ib: Any, cfg):
L847         if cfg is None:
L848             raise ValueError("cfg is required; pass factor.PipelineConfig")
L849         self._validate_ib_for_scorer(ib)
L850
L851         px, spx, tickers = ib.px, ib.spx, ib.tickers
L852         tickers_bulk, info, eps_df, fcf_df = ib.tickers_bulk, ib.info, ib.eps_df, ib.fcf_df
L853
L854         df, missing_logs = pd.DataFrame(index=tickers), []
L855         for t in tickers:
L856             d, s = info[t], px[t]; ev = self.ev_fallback(d, tickers_bulk.tickers[t])
L857             try:
L858                 volume_series_full = ib.data['Volume'][t]
L859             except Exception:
L860                 volume_series_full = None
L861
L862             grw_result = _calc_grw_flexible(t, d, s, volume_series_full)
L863             _grw_record_to_df(t, d, df)
L864             df.loc[t,'GRW_FLEX_SCORE'] = grw_result.get('score')
L865             df.loc[t,'GRW_FLEX_WEIGHT'] = grw_result.get('weight')
L866             df.loc[t,'GRW_FLEX_CORE'] = grw_result.get('core')
L867             df.loc[t,'GRW_FLEX_PRICE'] = grw_result.get('price_proxy')
L868             df.loc[t,'DEBUG_GRW_PATH'] = grw_result.get('path')
L869             df.loc[t,'DEBUG_GRW_PARTS'] = grw_result.get('parts')
L870
L871             # --- åŸºæœ¬ç‰¹å¾´ ---
L872             df.loc[t,'TR']   = self.trend(s)
L873             df.loc[t,'EPS']  = _scalar(eps_df.loc[t,'EPS_TTM']) if t in eps_df.index else np.nan
L874             df.loc[t,'EPS_Q'] = _scalar(eps_df.loc[t,'EPS_Q_LastQ']) if t in eps_df.index else np.nan
L875             df.loc[t,'REV_TTM'] = _scalar(eps_df.loc[t,'REV_TTM']) if t in eps_df.index else np.nan
L876             df.loc[t,'REV_Q']   = _scalar(eps_df.loc[t,'REV_Q_LastQ']) if t in eps_df.index else np.nan
L877             df.loc[t,'REV']  = d.get('revenueGrowth',np.nan)
L878             df.loc[t,'ROE']  = d.get('returnOnEquity',np.nan)
L879             df.loc[t,'BETA'] = self.calc_beta(s, spx, lookback=252)
L880
L881             # --- é…å½“ï¼ˆæ¬ æè£œå®Œå«ã‚€ï¼‰ ---
L882             div = d.get('dividendYield') if d.get('dividendYield') is not None else d.get('trailingAnnualDividendYield')
L883             if div is None or pd.isna(div):
L884                 try:
L885                     divs = yf.Ticker(t).dividends
L886                     if divs is not None and not divs.empty:
L887                         last_close = s.iloc[-1]; div_1y = divs[divs.index >= (divs.index.max() - pd.Timedelta(days=365))].sum()
L888                         if last_close and last_close>0: div = float(div_1y/last_close)
L889                 except Exception: pass
L890             df.loc[t,'DIV'] = 0.0 if (div is None or pd.isna(div)) else float(div)
L891
L892             # --- FCF/EV ---
L893             fcf_val = fcf_df.loc[t,'FCF_TTM'] if t in fcf_df.index else np.nan
L894             df.loc[t,'FCF'] = (fcf_val/ev) if (pd.notna(fcf_val) and pd.notna(ev) and ev>0) else np.nan
L895
L896             # --- ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ãƒ»ãƒœãƒ©é–¢é€£ ---
L897             df.loc[t,'RS'], df.loc[t,'TR_str'] = self.rs(s, spx), self.tr_str(s)
L898             r, rm = s.pct_change().dropna(), spx.pct_change().dropna()
L899             n = int(min(len(r), len(rm)))
L900
L901             DOWNSIDE_DEV = np.nan
L902             if n>=60:
L903                 r6 = r.iloc[-min(len(r),126):]; neg = r6[r6<0]
L904                 if len(neg)>=10: DOWNSIDE_DEV = float(neg.std(ddof=0)*np.sqrt(252))
L905             df.loc[t,'DOWNSIDE_DEV'] = DOWNSIDE_DEV
L906
L907             MDD_1Y = np.nan
L908             try:
L909                 w = s.iloc[-min(len(s),252):].dropna()
L910                 if len(w)>=30:
L911                     roll_max = w.cummax(); MDD_1Y = float((w/roll_max - 1.0).min())
L912             except Exception: pass
L913             df.loc[t,'MDD_1Y'] = MDD_1Y
L914
L915             RESID_VOL = np.nan
L916             if n>=120:
L917                 rr, rrm = r.iloc[-n:].align(rm.iloc[-n:], join='inner')
L918                 if len(rr)==len(rrm) and len(rr)>=120 and rrm.var()>0:
L919                     beta = float(np.cov(rr, rrm)[0,1]/np.var(rrm)); resid = rr - beta*rrm
L920                     RESID_VOL = float(resid.std(ddof=0)*np.sqrt(252))
L921             df.loc[t,'RESID_VOL'] = RESID_VOL
L922
L923             DOWN_OUTPERF = np.nan
L924             if n>=60:
L925                 m, x = rm.iloc[-n:], r.iloc[-n:]; mask = m<0
L926                 if mask.sum()>=10:
L927                     mr, sr = float(m[mask].mean()), float(x[mask].mean())
L928                     DOWN_OUTPERF = (sr - mr)/abs(mr) if mr!=0 else np.nan
L929             df.loc[t,'DOWN_OUTPERF'] = DOWN_OUTPERF
L930
L931             # --- é•·æœŸç§»å‹•å¹³å‡/ä½ç½® ---
L932             sma200 = s.rolling(200).mean(); df.loc[t,'EXT_200'] = np.nan
L933             if pd.notna(sma200.iloc[-1]) and sma200.iloc[-1]!=0: df.loc[t,'EXT_200'] = abs(float(s.iloc[-1]/sma200.iloc[-1]-1.0))
L934
L935             # --- é…å½“ã®è©³ç´°ç³» ---
L936             DIV_TTM_PS=DIV_VAR5=DIV_YOY=DIV_FCF_COVER=np.nan
L937             try:
L938                 divs = yf.Ticker(t).dividends.dropna()
L939                 if not divs.empty:
L940                     last_close = s.iloc[-1]; div_1y = float(divs[divs.index >= (divs.index.max()-pd.Timedelta(days=365))].sum())
L941                     DIV_TTM_PS = div_1y if div_1y>0 else np.nan
L942                     ann = divs.groupby(divs.index.year).sum()
L943                     if len(ann)>=2 and ann.iloc[-2]!=0: DIV_YOY = float(ann.iloc[-1]/ann.iloc[-2]-1.0)
L944                     tail = ann.iloc[-5:] if len(ann)>=5 else ann
L945                     if len(tail)>=3 and tail.mean()!=0: DIV_VAR5 = float(tail.std(ddof=1)/abs(tail.mean()))
L946                 so = d.get('sharesOutstanding',None)
L947                 if so and pd.notna(DIV_TTM_PS) and pd.notna(fcf_val) and fcf_val!=0:
L948                     DIV_FCF_COVER = float((fcf_val)/(DIV_TTM_PS*float(so)))
L949             except Exception: pass
L950             df.loc[t,'DIV_TTM_PS'], df.loc[t,'DIV_VAR5'], df.loc[t,'DIV_YOY'], df.loc[t,'DIV_FCF_COVER'] = DIV_TTM_PS, DIV_VAR5, DIV_YOY, DIV_FCF_COVER
L951
L952             # --- è²¡å‹™å®‰å®šæ€§ ---
L953             df.loc[t,'DEBT2EQ'], df.loc[t,'CURR_RATIO'] = d.get('debtToEquity',np.nan), d.get('currentRatio',np.nan)
L954
L955             # --- EPS å¤‰å‹• ---
L956             EPS_VAR_8Q = np.nan
L957             try:
L958                 qe, so = tickers_bulk.tickers[t].quarterly_earnings, d.get('sharesOutstanding',None)
L959                 if qe is not None and not qe.empty and so:
L960                     eps_q = (qe['Earnings'].dropna().astype(float)/float(so)).replace([np.inf,-np.inf],np.nan)
L961                     if len(eps_q)>=4: EPS_VAR_8Q = float(eps_q.iloc[-min(8,len(eps_q)):].std(ddof=1))
L962             except Exception: pass
L963             df.loc[t,'EPS_VAR_8Q'] = EPS_VAR_8Q
L964
L965             # --- ã‚µã‚¤ã‚º/æµå‹•æ€§ ---
L966             df.loc[t,'MARKET_CAP'] = d.get('marketCap',np.nan); adv60 = np.nan
L967             try:
L968                 if isinstance(volume_series_full, pd.Series):
L969                     vol_series = volume_series_full.reindex(s.index).dropna()
L970                     if len(vol_series) >= 5:
L971                         aligned_px = s.reindex(vol_series.index).dropna()
L972                         if len(aligned_px) == len(vol_series):
L973                             dv = (vol_series*aligned_px).rolling(60).mean()
L974                             if not dv.dropna().empty:
L975                                 adv60 = float(dv.dropna().iloc[-1])
L976             except Exception:
L977                 pass
L978             df.loc[t,'ADV60_USD'] = adv60
L979
L980             # --- Rule of 40 ã‚„å‘¨è¾º ---
L981             total_rev_ttm = d.get('totalRevenue',np.nan)
L982             FCF_MGN = _safe_div(fcf_val, total_rev_ttm)
L983             df.loc[t,'FCF_MGN'] = FCF_MGN
L984             rule40 = np.nan
L985             try:
L986                 r = df.loc[t,'REV']; rule40 = (r if pd.notna(r) else np.nan) + (FCF_MGN if pd.notna(FCF_MGN) else np.nan)
L987             except Exception: pass
L988             df.loc[t,'RULE40'] = rule40
L989
L990             # --- ãƒˆãƒ¬ãƒ³ãƒ‰è£œåŠ© ---
L991             sma50  = s.rolling(50).mean()
L992             sma150 = s.rolling(150).mean()
L993             sma200 = s.rolling(200).mean()
L994             p = _safe_last(s)
L995
L996             df.loc[t,'MA50_OVER_150'] = (_safe_last(sma50)/_safe_last(sma150) - 1
L997                 if pd.notna(_safe_last(sma50)) and pd.notna(_safe_last(sma150)) and _safe_last(sma150)!=0 else np.nan)
L998             df.loc[t,'MA150_OVER_200'] = (_safe_last(sma150)/_safe_last(sma200) - 1
L999                 if pd.notna(_safe_last(sma150)) and pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan)
L1000
L1001             lo52 = s[-252:].min() if len(s)>=252 else s.min()
L1002             df.loc[t,'P_OVER_LOW52'] = (p/lo52 - 1) if (lo52 and lo52>0 and pd.notna(p)) else np.nan
L1003
L1004             df.loc[t,'MA200_SLOPE_1M'] = np.nan
L1005             if len(sma200.dropna()) >= 21:
L1006                 cur200 = _safe_last(sma200)
L1007                 old2001 = float(sma200.iloc[-21])
L1008                 if old2001:
L1009                     df.loc[t,'MA200_SLOPE_1M'] = cur200/old2001 - 1
L1010
L1011             df.loc[t,'P_OVER_150'] = p/_safe_last(sma150)-1 if pd.notna(_safe_last(sma150)) and _safe_last(sma150)!=0 else np.nan
L1012             df.loc[t,'P_OVER_200'] = p/_safe_last(sma200)-1 if pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L1013             df.loc[t,'MA50_OVER_200'] = _safe_last(sma50)/_safe_last(sma200)-1 if pd.notna(_safe_last(sma50)) and pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L1014             df.loc[t,'MA200_SLOPE_5M'] = np.nan
L1015             if len(sma200.dropna())>=105:
L1016                 cur200, old200 = _safe_last(sma200), float(sma200.iloc[-105])
L1017                 if old200 and old200!=0: df.loc[t,'MA200_SLOPE_5M'] = cur200/old200 - 1
L1018             # NEW: 200æ—¥ç·šãŒé€£ç¶šã§ä¸Šå‘ãã®ã€Œæ—¥æ•°ã€
L1019             df.loc[t,'MA200_UP_STREAK_D'] = np.nan
L1020             try:
L1021                 s200 = sma200.dropna()
L1022                 if len(s200) >= 2:
L1023                     diff200 = s200.diff()
L1024                     up = 0
L1025                     for v in diff200.iloc[::-1]:
L1026                         if pd.isna(v) or v <= 0:
L1027                             break
L1028                         up += 1
L1029                     df.loc[t,'MA200_UP_STREAK_D'] = float(up)
L1030             except Exception:
L1031                 pass
L1032             df.loc[t,'LOW52PCT25_EXCESS'] = np.nan if (lo52 is None or lo52<=0 or pd.isna(p)) else (p/(lo52*1.25)-1)
L1033             hi52 = s[-252:].max() if len(s)>=252 else s.max(); df.loc[t,'NEAR_52W_HIGH'] = np.nan
L1034             if hi52 and hi52>0 and pd.notna(p):
L1035                 d_hi = (p/hi52)-1.0; df.loc[t,'NEAR_52W_HIGH'] = -abs(min(0.0, d_hi))
L1036             df.loc[t,'RS_SLOPE_6W'] = self.rs_line_slope(s, ib.spx, 30)
L1037             df.loc[t,'RS_SLOPE_13W'] = self.rs_line_slope(s, ib.spx, 65)
L1038
L1039             df.loc[t,'DIV_STREAK'] = self.div_streak(t)
L1040
L1041             # --- æ¬ æãƒ¡ãƒ¢ ---
L1042             fin_cols = ['REV','ROE','BETA','DIV','FCF']
L1043             need_finnhub = [col for col in fin_cols if pd.isna(df.loc[t,col])]
L1044             if need_finnhub:
L1045                 fin_data = self.fetch_finnhub_metrics(t)
L1046                 for col in need_finnhub:
L1047                     val = fin_data.get(col)
L1048                     if val is not None and not pd.isna(val): df.loc[t,col] = val
L1049             for col in fin_cols + ['EPS','RS','TR_str','DIV_STREAK']:
L1050                 if pd.isna(df.loc[t,col]):
L1051                     if col=='DIV':
L1052                         status = self.dividend_status(t)
L1053                         if status!='none_confident': missing_logs.append({'Ticker':t,'Column':col,'Status':status})
L1054                     else:
L1055                         missing_logs.append({'Ticker':t,'Column':col})
L1056
L1057         def _pick_series(entry: dict, keys: list[str]):
L1058             for k in keys:
L1059                 val = entry.get(k) if isinstance(entry, dict) else None
L1060                 if val is None:
L1061                     continue
L1062                 try:
L1063                     if hasattr(val, "empty") and getattr(val, "empty"):
L1064                         continue
L1065                 except Exception:
L1066                     pass
L1067                 if isinstance(val, (list, tuple)) and len(val) == 0:
L1068                     continue
L1069                 return val
L1070             return None
L1071
L1072         def _has_sec_series(val) -> bool:
L1073             try:
L1074                 if isinstance(val, pd.Series):
L1075                     return not val.dropna().empty
L1076                 if isinstance(val, (list, tuple)):
L1077                     return any(pd.notna(v) for v in val)
L1078                 return bool(val)
L1079             except Exception:
L1080                 return False
L1081
L1082         def _series_len(val) -> int:
L1083             try:
L1084                 if isinstance(val, pd.Series):
L1085                     return int(val.dropna().size)
L1086                 if isinstance(val, (list, tuple)):
L1087                     return len(val)
L1088                 return int(bool(val))
L1089             except Exception:
L1090                 return 0
L1091
L1092         cnt_rev_series = sum(1 for _t, d in info.items() if _has_sec_series(d.get("SEC_REV_Q_SERIES")))
L1093         cnt_eps_series = sum(1 for _t, d in info.items() if _has_sec_series(d.get("SEC_EPS_Q_SERIES")))
L1094         logger.info(
L1095             "[DERIV] SEC series presence: REV_Q=%d, EPS_Q=%d (universe=%d)",
L1096             cnt_rev_series,
L1097             cnt_eps_series,
L1098             len(info),
L1099         )
L1100
L1101         rev_q_ge5 = 0
L1102         ttm_yoy_avail = 0
L1103         wrote_growth = 0
L1104
L1105         for t in tickers:
L1106             try:
L1107                 d = info.get(t, {}) or {}
L1108                 rev_series = d.get("SEC_REV_Q_SERIES")
L1109                 eps_series = d.get("SEC_EPS_Q_SERIES")
L1110                 fallback_qearn = False
L1111                 try:
L1112                     qe = tickers_bulk.tickers[t].quarterly_earnings
L1113                     fallback_qearn = bool(qe is not None and not getattr(qe, "empty", True))
L1114                 except Exception:
L1115                     qe = None
L1116                 logger.debug(
L1117                     "[DERIV] %s: rev_q_len=%s eps_q_len=%s fallback_qearn=%s",
L1118                     t,
L1119                     _series_len(rev_series),
L1120                     _series_len(eps_series),
L1121                     fallback_qearn,
L1122                 )
L1123
L1124                 r_src = _pick_series(d, ["SEC_REV_Q_SERIES", "rev_q_series_pairs", "rev_q_series"])
L1125                 e_src = _pick_series(d, ["SEC_EPS_Q_SERIES", "eps_q_series_pairs", "eps_q_series"])
L1126                 r_raw = _ensure_series(r_src)
L1127                 e_raw = _ensure_series(e_src)
L1128                 _log("DERIV_SRC", f"{t} rev_raw_len={r_raw.size} eps_raw_len={e_raw.size}")
L1129
L1130                 r_q = _to_quarterly(r_raw)
L1131                 e_q = _to_quarterly(e_raw)
L1132                 _log("DERIV_Q", f"{t} rev_q_len={r_q.size} eps_q_len={e_q.size}")
L1133                 if r_q.size >= 5:
L1134                     rev_q_ge5 += 1
L1135
L1136                 r_yoy_ttm = _ttm_yoy_from_quarterly(r_q)
L1137                 e_yoy_ttm = _ttm_yoy_from_quarterly(e_q)
L1138                 has_ttm = int(not r_yoy_ttm.dropna().empty)
L1139                 ttm_yoy_avail += has_ttm
L1140                 _log("DERIV_TTM", f"{t} rev_ttm_yoy_len={r_yoy_ttm.dropna().size} eps_ttm_yoy_len={e_yoy_ttm.dropna().size}")
L1141
L1142                 def _q_yoy(qs):
L1143                     return np.nan if qs is None or len(qs) < 5 else float(qs.iloc[-1] / qs.iloc[-5] - 1.0)
L1144
L1145                 rev_q_yoy = _q_yoy(r_q)
L1146                 eps_q_yoy = _q_yoy(e_q)
L1147
L1148                 def _annual_from(qs: pd.Series, yoy_ttm: pd.Series):
L1149                     if isinstance(qs.index, pd.DatetimeIndex) and len(qs) >= 8:
L1150                         ann = qs.groupby(qs.index.year).last().pct_change()
L1151                         ann_dn = ann.dropna()
L1152                         if not ann_dn.empty:
L1153                             y = float(ann_dn.iloc[-1])
L1154                             acc = float(ann_dn.tail(3).mean()) if ann_dn.size >= 3 else np.nan
L1155                             var = float(ann_dn.tail(4).var()) if ann_dn.size >= 4 else np.nan
L1156                             return y, acc, var
L1157                     yoy_dn = yoy_ttm.dropna()
L1158                     if yoy_dn.empty:
L1159                         return np.nan, np.nan, np.nan
L1160                     return (
L1161                         float(yoy_dn.iloc[-1]),
L1162                         float(yoy_dn.tail(3).mean() if yoy_dn.size >= 3 else np.nan),
L1163                         float(yoy_dn.tail(4).var() if yoy_dn.size >= 4 else np.nan),
L1164                     )
L1165
L1166                 rev_yoy, rev_acc, rev_var = _annual_from(r_q, r_yoy_ttm)
L1167                 eps_yoy, _, _ = _annual_from(e_q, e_yoy_ttm)
L1168
L1169                 def _pos_streak(s: pd.Series):
L1170                     s = s.dropna()
L1171                     if s.empty:
L1172                         return np.nan
L1173                     b = (s > 0).astype(int).to_numpy()[::-1]
L1174                     k = 0
L1175                     for v in b:
L1176                         if v == 1:
L1177                             k += 1
L1178                         else:
L1179                             break
L1180                     return float(k)
L1181
L1182                 rev_ann_streak = _pos_streak(r_yoy_ttm)
L1183
L1184                 df.loc[t, "REV_Q_YOY"] = rev_q_yoy
L1185                 df.loc[t, "EPS_Q_YOY"] = eps_q_yoy
L1186                 df.loc[t, "REV_YOY"] = rev_yoy
L1187                 df.loc[t, "EPS_YOY"] = eps_yoy
L1188                 df.loc[t, "REV_YOY_ACC"] = rev_acc
L1189                 df.loc[t, "REV_YOY_VAR"] = rev_var
L1190                 df.loc[t, "REV_ANN_STREAK"] = rev_ann_streak
L1191
L1192                 wrote_growth += 1
L1193                 _log(
L1194                     "DERIV_WRITE",
L1195                     f"{t} wrote: Q_YOY(rev={rev_q_yoy}, eps={eps_q_yoy}) ANN(rev_yoy={rev_yoy}, acc={rev_acc}, var={rev_var}) streak={rev_ann_streak}",
L1196                 )
L1197
L1198             except Exception as e:
L1199                 logger.warning("[DERIV_WARN] %s growth-derivatives failed: %s", t, e)
L1200                 _log("DERIV_WARN", f"{t} {type(e).__name__}: {e}")
L1201
L1202         _log("DERIV_SUMMARY", f"rev_q_len>=5: {rev_q_ge5}/{len(tickers)}  ttm_yoy_available: {ttm_yoy_avail}  wrote_growth_for: {wrote_growth}")
L1203
L1204         try:
L1205             cols = [
L1206                 "REV_Q_YOY",
L1207                 "EPS_Q_YOY",
L1208                 "REV_YOY",
L1209                 "EPS_YOY",
L1210                 "REV_YOY_ACC",
L1211                 "REV_YOY_VAR",
L1212                 "REV_ANN_STREAK",
L1213             ]
L1214             cnt = {c: int(df[c].count()) for c in cols if c in df.columns}
L1215             _log("DERIV_NONNAN_COUNTS", str(cnt))
L1216         except Exception as e:
L1217             _log("DERIV_NONNAN_COUNTS", f"error: {e}")
L1218
L1219         def _trend_template_pass(row, rs_alpha_thresh=0.10):
L1220             c1 = (row.get('P_OVER_150', np.nan) > 0) and (row.get('P_OVER_200', np.nan) > 0)
L1221             c2 = (row.get('MA150_OVER_200', np.nan) > 0)
L1222             c3 = (row.get('MA200_SLOPE_1M', np.nan) > 0)
L1223             c4 = (row.get('MA50_OVER_150', np.nan) > 0) and (row.get('MA50_OVER_200', np.nan) > 0)
L1224             c5 = (row.get('TR_str', np.nan) > 0)
L1225             c6 = (row.get('P_OVER_LOW52', np.nan) >= 0.30)
L1226             c7 = (row.get('NEAR_52W_HIGH', np.nan) >= -0.25)
L1227             c8 = (row.get('RS', np.nan) >= 0.10)
L1228             return bool(c1 and c2 and c3 and c4 and c5 and c6 and c7 and c8)
L1229
L1230         if 'trend_template' not in df.columns: df['trend_template'] = df.apply(_trend_template_pass, axis=1).fillna(False)
L1231         assert 'trend_template' in df.columns
L1232
L1233         # === ZåŒ–ã¨åˆæˆ ===
L1234         for col in ['ROE','FCF','REV','EPS']: df[f'{col}_W'] = winsorize_s(df[col], 0.02)
L1235
L1236         df_z = pd.DataFrame(index=df.index)
L1237         for col in ['EPS','REV','ROE','FCF','RS','TR_str','BETA','DIV','DIV_STREAK']: df_z[col] = robust_z(df[col])
L1238         df_z['REV'], df_z['EPS'], df_z['TR'] = robust_z(df['REV_W']), robust_z(df['EPS_W']), robust_z(df['TR'])
L1239         for col in ['P_OVER_150','P_OVER_200','MA50_OVER_200','MA200_SLOPE_5M','LOW52PCT25_EXCESS','NEAR_52W_HIGH','RS_SLOPE_6W','RS_SLOPE_13W','MA200_UP_STREAK_D']: df_z[col] = robust_z(df[col])
L1240
L1241         # === Growthæ·±æ˜ã‚Šç³»ï¼ˆæ¬ æä¿æŒz + RAWä½µè¼‰ï¼‰ ===
L1242         grw_cols = ['REV_Q_YOY','EPS_Q_YOY','REV_YOY','EPS_YOY','REV_YOY_ACC','REV_YOY_VAR','FCF_MGN','RULE40','REV_ANN_STREAK']
L1243         for col in grw_cols:
L1244             if col in df.columns:
L1245                 raw = pd.to_numeric(df[col], errors="coerce")
L1246                 df_z[col] = robust_z_keepnan(raw)
L1247                 df_z[f'{col}_RAW'] = raw
L1248         for k in ("TREND_SLOPE_EPS", "TREND_SLOPE_REV"):
L1249             if k in df.columns and k not in df_z.columns:
L1250                 raw = pd.to_numeric(df[k], errors="coerce")
L1251                 df_z[k] = robust_z_keepnan(raw)
L1252                 df_z[f'{k}_RAW'] = raw
L1253         for col in ['DOWNSIDE_DEV','MDD_1Y','RESID_VOL','DOWN_OUTPERF','EXT_200','DIV_TTM_PS','DIV_VAR5','DIV_YOY','DIV_FCF_COVER','DEBT2EQ','CURR_RATIO','EPS_VAR_8Q','MARKET_CAP','ADV60_USD']: df_z[col] = robust_z(df[col])
L1254
L1255         df_z['SIZE'], df_z['LIQ'] = robust_z(np.log1p(df['MARKET_CAP'])), robust_z(np.log1p(df['ADV60_USD']))
L1256         df_z['QUALITY_F'] = robust_z(0.6*df['FCF_W'] + 0.4*df['ROE_W']).clip(-3.0,3.0)
L1257         df_z['YIELD_F']   = 0.3*df_z['DIV'] + 0.7*df_z['DIV_STREAK']
L1258
L1259         # EPSãŒèµ¤å­—ã§ã‚‚FCFãŒé»’å­—ãªã‚‰å®Ÿè³ªé»’å­—ã¨ã¿ãªã™
L1260         eps_pos_mask = (df['EPS'] > 0) | (df['FCF_MGN'] > 0)
L1261         df_z['EPS_POS'] = df_z['EPS'].where(eps_pos_mask, 0.0)
L1262
L1263         # ===== ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ãƒ­ãƒ¼ãƒ—ç®—å‡º =====
L1264         def zpos(x):
L1265             arr = robust_z(x)
L1266             idx = getattr(x, 'index', df_z.index)
L1267             return pd.Series(arr, index=idx).fillna(0.0)
L1268
L1269         def relu(x):
L1270             ser = x if isinstance(x, pd.Series) else pd.Series(x, index=df_z.index)
L1271             return ser.clip(lower=0).fillna(0.0)
L1272
L1273         # å£²ä¸Šãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ãƒ­ãƒ¼ãƒ—ï¼ˆå››åŠæœŸï¼‰
L1274         slope_rev = 0.70*zpos(df_z['REV_Q_YOY']) + 0.30*zpos(df_z['REV_YOY_ACC'])
L1275         noise_rev = relu(robust_z(df_z['REV_YOY_VAR']) - 0.8)
L1276         slope_rev_combo = slope_rev - 0.25*noise_rev
L1277         df_z['TREND_SLOPE_REV_RAW'] = slope_rev_combo
L1278         df_z['TREND_SLOPE_REV'] = slope_rev_combo.clip(-3.0, 3.0)
L1279
L1280         # EPSãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ãƒ­ãƒ¼ãƒ—ï¼ˆå››åŠæœŸï¼‰
L1281         slope_eps = 0.60*zpos(df_z['EPS_Q_YOY']) + 0.40*zpos(df_z['EPS_POS'])
L1282         df_z['TREND_SLOPE_EPS_RAW'] = slope_eps
L1283         df_z['TREND_SLOPE_EPS'] = slope_eps.clip(-3.0, 3.0)
L1284
L1285         # å¹´æ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆã‚µãƒ–ï¼‰
L1286         slope_rev_yr = zpos(df_z['REV_YOY'])
L1287         slope_eps_yr = zpos(df_z.get('EPS_YOY', pd.Series(0.0, index=df.index)))
L1288         streak_base = df['REV_ANN_STREAK'].clip(lower=0).fillna(0)
L1289         streak_yr = streak_base / (streak_base.abs() + 1.0)
L1290         slope_rev_yr_combo = 0.7*slope_rev_yr + 0.3*streak_yr
L1291         df_z['TREND_SLOPE_REV_YR_RAW'] = slope_rev_yr_combo
L1292         df_z['TREND_SLOPE_REV_YR'] = slope_rev_yr_combo.clip(-3.0, 3.0)
L1293         df_z['TREND_SLOPE_EPS_YR_RAW'] = slope_eps_yr
L1294         df_z['TREND_SLOPE_EPS_YR'] = slope_eps_yr.clip(-3.0, 3.0)
L1295
L1296         # ===== GRW flexible score (variable data paths) =====
L1297         grw_raw = pd.to_numeric(df.get('GRW_FLEX_SCORE'), errors="coerce")
L1298         df_z['GRW_FLEX_SCORE_RAW'] = grw_raw
L1299         df_z['GROWTH_F_RAW'] = grw_raw
L1300         df_z['GROWTH_F'] = robust_z_keepnan(grw_raw).clip(-3.0, 3.0)
L1301         df_z['GRW_FLEX_WEIGHT'] = pd.to_numeric(df.get('GRW_FLEX_WEIGHT'), errors="coerce")
L1302         df_z['GRW_FLEX_CORE_RAW'] = pd.to_numeric(df.get('GRW_FLEX_CORE'), errors="coerce")
L1303         df_z['GRW_FLEX_PRICE_RAW'] = pd.to_numeric(df.get('GRW_FLEX_PRICE'), errors="coerce")
L1304
L1305         # Debug dump for GRW composition (console OFF by default; enable only with env)
L1306         if bool(os.getenv("GRW_CONSOLE_DEBUG")):
L1307             try:
L1308                 cols = ['GROWTH_F', 'GROWTH_F_RAW', 'GRW_FLEX_WEIGHT']
L1309                 use_cols = [c for c in cols if c in df_z.columns]
L1310                 i = df_z[use_cols].copy() if use_cols else pd.DataFrame(index=df_z.index)
L1311                 i.sort_values('GROWTH_F', ascending=False, inplace=True)
L1312                 limit = max(0, min(40, len(i)))
L1313                 print("[DEBUG: GRW]")
L1314                 for t in i.index[:limit]:
L1315                     row = i.loc[t]
L1316                     parts = []
L1317                     if pd.notna(row.get('GROWTH_F')):
L1318                         parts.append(f"GROWTH_F={row.get('GROWTH_F'):.3f}")
L1319                     raw_val = row.get('GROWTH_F_RAW')
L1320                     if pd.notna(raw_val):
L1321                         parts.append(f"GROWTH_F_RAW={raw_val:.3f}")
L1322                     weight_val = row.get('GRW_FLEX_WEIGHT')
L1323                     if pd.notna(weight_val):
L1324                         parts.append(f"w={weight_val:.2f}")
L1325                     path_val = None
L1326                     try:
L1327                         path_val = info.get(t, {}).get('DEBUG_GRW_PATH')
L1328                     except Exception:
L1329                         path_val = None
L1330                     if not path_val and 'DEBUG_GRW_PATH' in df.columns:
L1331                         path_val = df.at[t, 'DEBUG_GRW_PATH']
L1332                     if path_val:
L1333                         parts.append(f"PATH={path_val}")
L1334                     parts_json = None
L1335                     try:
L1336                         parts_json = info.get(t, {}).get('DEBUG_GRW_PARTS')
L1337                     except Exception:
L1338                         parts_json = None
L1339                     if not parts_json and 'DEBUG_GRW_PARTS' in df.columns:
L1340                         parts_json = df.at[t, 'DEBUG_GRW_PARTS']
L1341                     if parts_json:
L1342                         parts.append(f"PARTS={parts_json}")
L1343                     if not parts:
L1344                         parts.append('no-data')
L1345                     print(f"Ticker: {t} | " + " ".join(parts))
L1346                 print()
L1347             except Exception as exc:
L1348                 print(f"[ERR] GRW debug dump failed: {exc}")
L1349
L1350         df_z['MOM_F'] = robust_z(0.40*df_z['RS']
L1351             + 0.15*df_z['TR_str']
L1352             + 0.15*df_z['RS_SLOPE_6W']
L1353             + 0.15*df_z['RS_SLOPE_13W']
L1354             + 0.10*df_z['MA200_SLOPE_5M']
L1355             + 0.10*df_z['MA200_UP_STREAK_D']).clip(-3.0,3.0)
L1356         df_z['VOL'] = robust_z(df['BETA'])
L1357         df_z['QAL'], df_z['YLD'], df_z['MOM'] = df_z['QUALITY_F'], df_z['YIELD_F'], df_z['MOM_F']
L1358         df_z.drop(columns=['QUALITY_F','YIELD_F','MOM_F'], inplace=True, errors='ignore')
L1359
L1360         _dump_dfz(
L1361             df=df,
L1362             df_z=df_z,
L1363             debug_mode=getattr(cfg, "debug_mode", False),
L1364             weight_dict=getattr(getattr(cfg, "weights", None), "g", None),
L1365         )
L1366         if getattr(cfg, "debug_mode", False):
L1367             log_grw_stats(df, df_z, logger)
L1368         save_factor_debug_csv(df, df_z)
L1369
L1370         # === begin: BIO LOSS PENALTY =====================================
L1371         try:
L1372             penalty_z = float(os.getenv("BIO_LOSS_PENALTY_Z", "0.8"))
L1373         except Exception:
L1374             penalty_z = 0.8
L1375
L1376         def _is_bio_like(t: str) -> bool:
L1377             inf = info.get(t, {}) if isinstance(info, dict) else {}
L1378             sec = str(inf.get("sector", "")).lower()
L1379             ind = str(inf.get("industry", "")).lower()
L1380             if "health" not in sec:
L1381                 return False
L1382             keys = ("biotech", "biopharma", "pharma")
L1383             return any(k in ind for k in keys)
L1384
L1385         tickers_s = pd.Index(df_z.index)
L1386         is_bio = pd.Series({t: _is_bio_like(t) for t in tickers_s})
L1387         is_loss = pd.Series({t: (pd.notna(df.loc[t,"EPS"]) and df.loc[t,"EPS"] <= 0) for t in tickers_s})
L1388         mask_bio_loss = (is_bio & is_loss).reindex(df_z.index).fillna(False)
L1389
L1390         if bool(mask_bio_loss.any()) and penalty_z > 0:
L1391             df_z.loc[mask_bio_loss, "GROWTH_F"] = df_z.loc[mask_bio_loss, "GROWTH_F"] - penalty_z
L1392             df_z["GROWTH_F"] = df_z["GROWTH_F"].clip(-3.0, 3.0)
L1393         # === end: BIO LOSS PENALTY =======================================
L1394
L1395         df_z['TRD'] = 0.0  # TRDã¯ã‚¹ã‚³ã‚¢å¯„ä¸ã‹ã‚‰å¤–ã—ã€ãƒ†ãƒ³ãƒ—ãƒ¬åˆ¤å®šã¯ãƒ•ã‚£ãƒ«ã‚¿ã§è¡Œã†ï¼ˆåˆ—ã¯è¡¨ç¤ºäº’æ›ã®ãŸã‚æ®‹ã™ï¼‰
L1396         if 'BETA' not in df_z.columns: df_z['BETA'] = robust_z(df['BETA'])
L1397
L1398         df_z['D_VOL_RAW'] = robust_z(0.40*df_z['DOWNSIDE_DEV'] + 0.22*df_z['RESID_VOL'] + 0.18*df_z['MDD_1Y'] - 0.10*df_z['DOWN_OUTPERF'] - 0.05*df_z['EXT_200'] - 0.08*df_z['SIZE'] - 0.10*df_z['LIQ'] + 0.10*df_z['BETA'])
L1399         df_z['D_QAL']     = robust_z(0.35*df_z['QAL'] + 0.20*df_z['FCF'] + 0.15*df_z['CURR_RATIO'] - 0.15*df_z['DEBT2EQ'] - 0.15*df_z['EPS_VAR_8Q'])
L1400         df_z['D_YLD']     = robust_z(0.45*df_z['DIV'] + 0.25*df_z['DIV_STREAK'] + 0.20*df_z['DIV_FCF_COVER'] - 0.10*df_z['DIV_VAR5'])
L1401         df_z['D_TRD']     = robust_z(0.40*df_z.get('MA200_SLOPE_5M',0) - 0.30*df_z.get('EXT_200',0) + 0.15*df_z.get('NEAR_52W_HIGH',0) + 0.15*df_z['TR'])
L1402
L1403         # --- é‡ã¿ã¯ cfg ã‚’å„ªå…ˆï¼ˆå¤–éƒ¨ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ï¼‰ ---
L1404         # â‘  å…¨éŠ˜æŸ„ã§ G/D ã‚¹ã‚³ã‚¢ã‚’ç®—å‡ºï¼ˆunmaskedï¼‰
L1405         g_score_all = df_z.mul(pd.Series(cfg.weights.g)).sum(axis=1)
L1406
L1407         d_comp = pd.concat({
L1408             'QAL': df_z['D_QAL'],
L1409             'YLD': df_z['D_YLD'],
L1410             'VOL': df_z['D_VOL_RAW'],
L1411             'TRD': df_z['D_TRD']
L1412         }, axis=1)
L1413         dw = pd.Series(cfg.weights.d, dtype=float).reindex(['QAL','YLD','VOL','TRD']).fillna(0.0)
L1414         globals()['D_WEIGHTS_EFF'] = dw.copy()
L1415         d_score_all = d_comp.mul(dw, axis=1).sum(axis=1)
L1416
L1417         # â‘¡ ãƒ†ãƒ³ãƒ—ãƒ¬åˆ¤å®šï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ãã®ã¾ã¾ï¼‰
L1418         mask = df['trend_template']
L1419         if not bool(mask.any()):
L1420             mask = ((df.get('P_OVER_LOW52', np.nan) >= 0.25) &
L1421                 (df.get('NEAR_52W_HIGH', np.nan) >= -0.30) &
L1422                 (df.get('RS', np.nan) >= 0.08) &
L1423                 (df.get('MA200_SLOPE_1M', np.nan) > 0) &
L1424                 (df.get('P_OVER_150', np.nan) > 0) & (df.get('P_OVER_200', np.nan) > 0) &
L1425                 (df.get('MA150_OVER_200', np.nan) > 0) &
L1426                 (df.get('MA50_OVER_150', np.nan) > 0) & (df.get('MA50_OVER_200', np.nan) > 0) &
L1427                 (df.get('TR_str', np.nan) > 0)).fillna(False)
L1428             df['trend_template'] = mask
L1429
L1430         # â‘¢ æ¡ç”¨ç”¨ã¯ maskã€è¡¨ç¤º/åˆ†æç”¨ã¯åˆ—ã§å…¨éŠ˜æŸ„ä¿å­˜
L1431         g_score = g_score_all.loc[mask]
L1432         Scorer.g_score = g_score
L1433         df_z['GSC'] = g_score_all
L1434         df_z['DSC'] = d_score_all
L1435
L1436         try:
L1437             current = (pd.read_csv("current_tickers.csv")
L1438                   .iloc[:, 0]
L1439                   .str.upper()
L1440                   .tolist())
L1441         except FileNotFoundError:
L1442             warnings.warn("current_tickers.csv not found â€” bonus skipped")
L1443             current = []
L1444
L1445         mask_bonus = g_score.index.isin(current)
L1446         if mask_bonus.any():
L1447             # 1) factor.BONUS_COEFF ã‹ã‚‰ k ã‚’æ±ºã‚ã€ç„¡ã‘ã‚Œã° 0.4
L1448             k = float(getattr(sys.modules.get("factor"), "BONUS_COEFF", 0.4))
L1449             # 2) g å´ã® Ïƒ ã‚’å–ã‚Šã€NaN ãªã‚‰ 0 ã«ä¸¸ã‚ã‚‹
L1450             sigma_g = g_score.std()
L1451             if pd.isna(sigma_g):
L1452                 sigma_g = 0.0
L1453             bonus_g = round(k * sigma_g, 3)
L1454             g_score.loc[mask_bonus] += bonus_g
L1455             Scorer.g_score = g_score
L1456             # 3) D å´ã‚‚åŒæ§˜ã« Ïƒ ã® NaN ã‚’ã‚±ã‚¢
L1457             sigma_d = d_score_all.std()
L1458             if pd.isna(sigma_d):
L1459                 sigma_d = 0.0
L1460             bonus_d = round(k * sigma_d, 3)
L1461             d_score_all.loc[d_score_all.index.isin(current)] += bonus_d
L1462
L1463         try:
L1464             df = _apply_growth_entry_flags(df, ib, self, win_breakout=5, win_pullback=5)
L1465         except Exception:
L1466             pass
L1467
L1468         df_full = df.copy()
L1469         df_full_z = df_z.copy()
L1470
L1471         from factor import FeatureBundle  # type: ignore  # å®Ÿè¡Œæ™‚importãªã—ï¼ˆå¾ªç’°å›é¿ï¼‰
L1472         return FeatureBundle(df=df,
L1473             df_z=df_z,
L1474             g_score=g_score,
L1475             d_score_all=d_score_all,
L1476             missing_logs=pd.DataFrame(missing_logs),
L1477             df_full=df_full,
L1478             df_full_z=df_full_z,
L1479             scaler=None)
L1480
L1481 def _apply_growth_entry_flags(feature_df, bundle, self_obj, win_breakout=5, win_pullback=5):
L1482     """
L1483     Gæ ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã«å¯¾ã—ã€ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®š/æŠ¼ã—ç›®åç™ºã®ã€Œç›´è¿‘Nå–¶æ¥­æ—¥å†…ã®ç™ºç«ã€ã‚’åˆ¤å®šã—ã€
L1484     æ¬¡ã®åˆ—ã‚’ feature_df ã«è¿½åŠ ã™ã‚‹ï¼ˆindex=tickerï¼‰ã€‚
L1485       - G_BREAKOUT_recent_5d : bool
L1486       - G_BREAKOUT_last_date : str "YYYY-MM-DD"
L1487       - G_PULLBACK_recent_5d : bool
L1488       - G_PULLBACK_last_date : str "YYYY-MM-DD"
L1489       - G_PIVOT_price        : float
L1490     å¤±æ•—ã—ã¦ã‚‚ä¾‹å¤–ã¯æ¡ã‚Šæ½°ã—ã€æ—¢å­˜å‡¦ç†ã‚’é˜»å®³ã—ãªã„ã€‚
L1491     """
L1492     try:
L1493         px   = bundle.px                      # çµ‚å€¤ DataFrame
L1494         hi   = bundle.data['High']
L1495         lo   = bundle.data['Low']
L1496         vol  = bundle.data['Volume']
L1497         bench= bundle.spx                     # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ Series
L1498
L1499         # Gãƒ¦ãƒ‹ãƒãƒ¼ã‚¹æ¨å®šï¼šself.g_universe å„ªå…ˆ â†’ feature_df['group']=='G' â†’ å…¨éŠ˜æŸ„
L1500         g_universe = getattr(self_obj, "g_universe", None)
L1501         if g_universe is None:
L1502             try:
L1503                 g_universe = feature_df.index[feature_df['group'].astype(str).str.upper().eq('G')].tolist()
L1504             except Exception:
L1505                 g_universe = list(feature_df.index)
L1506         if not g_universe:
L1507             return feature_df
L1508
L1509         # æŒ‡æ¨™
L1510         px = px.ffill(limit=2)
L1511         ema21 = px[g_universe].ewm(span=21, adjust=False).mean()
L1512         ma50  = px[g_universe].rolling(50).mean()
L1513         ma150 = px[g_universe].rolling(150).mean()
L1514         ma200 = px[g_universe].rolling(200).mean()
L1515         atr20 = (hi[g_universe] - lo[g_universe]).rolling(20).mean()
L1516         vol20 = vol[g_universe].rolling(20).mean()
L1517         vol50 = vol[g_universe].rolling(50).mean()
L1518
L1519         # ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆåˆæ ¼
L1520         trend_template_ok = (px[g_universe] > ma50) & (px[g_universe] > ma150) & (px[g_universe] > ma200) \
L1521                             & (ma150 > ma200) & (ma200.diff() > 0)
L1522
L1523         # æ±ç”¨ãƒ”ãƒœãƒƒãƒˆï¼šç›´è¿‘65å–¶æ¥­æ—¥ã®é«˜å€¤ï¼ˆå½“æ—¥é™¤å¤–ï¼‰
L1524         pivot_price = hi[g_universe].rolling(65).max().shift(1)
L1525
L1526         # ç›¸å¯¾åŠ›ï¼šå¹´å†…é«˜å€¤æ›´æ–°
L1527         bench_aligned = bench.reindex(px.index).ffill()
L1528         rs = px[g_universe].div(bench_aligned, axis=0)
L1529         rs_high = rs.rolling(252).max().shift(1)
L1530
L1531         # ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆã€Œç™ºç”Ÿæ—¥ã€ï¼šæ¡ä»¶ç«‹ã¡ä¸ŠãŒã‚Š
L1532         breakout_today = trend_template_ok & (px[g_universe] > pivot_price) \
L1533                          & (vol[g_universe] >= 1.5 * vol50) & (rs > rs_high)
L1534         breakout_event = breakout_today & ~breakout_today.shift(1).fillna(False)
L1535
L1536         # æŠ¼ã—ç›®åç™ºã€Œç™ºç”Ÿæ—¥ã€ï¼šEMA21å¸¯Ã—å‡ºæ¥é«˜ãƒ‰ãƒ©ã‚¤ã‚¢ãƒƒãƒ—Ã—å‰æ—¥é«˜å€¤è¶ŠãˆÃ—çµ‚å€¤EMA21ä¸Š
L1537         near_ema21_band = px[g_universe].between(ema21 - atr20, ema21 + atr20)
L1538         volume_dryup = (vol20 / vol50) <= 1.0
L1539         pullback_bounce_confirmed = (px[g_universe] > hi[g_universe].shift(1)) & (px[g_universe] > ema21)
L1540         pullback_today = trend_template_ok & near_ema21_band & volume_dryup & pullback_bounce_confirmed
L1541         pullback_event = pullback_today & ~pullback_today.shift(1).fillna(False)
L1542
L1543         # ç›´è¿‘Nå–¶æ¥­æ—¥å†…ã®ç™ºç« / æœ€çµ‚ç™ºç”Ÿæ—¥
L1544         rows = []
L1545         for t in g_universe:
L1546             def _recent_and_date(s, win):
L1547                 sw = s[t].iloc[-win:]
L1548                 if sw.any():
L1549                     d = sw[sw].index[-1]
L1550                     return True, d.strftime("%Y-%m-%d")
L1551                 return False, ""
L1552             br_recent, br_date = _recent_and_date(breakout_event, win_breakout)
L1553             pb_recent, pb_date = _recent_and_date(pullback_event, win_pullback)
L1554             rows.append((t, {
L1555                 "G_BREAKOUT_recent_5d": br_recent,
L1556                 "G_BREAKOUT_last_date": br_date,
L1557                 "G_PULLBACK_recent_5d": pb_recent,
L1558                 "G_PULLBACK_last_date": pb_date,
L1559                 "G_PIVOT_price": float(pivot_price[t].iloc[-1]) if t in pivot_price.columns else float('nan'),
L1560             }))
L1561         flags = pd.DataFrame({k: v for k, v in rows}).T
L1562
L1563         # åˆ—ã‚’ä½œæˆãƒ»ä¸Šæ›¸ã
L1564         cols = ["G_BREAKOUT_recent_5d","G_BREAKOUT_last_date","G_PULLBACK_recent_5d","G_PULLBACK_last_date","G_PIVOT_price"]
L1565         for c in cols:
L1566             if c not in feature_df.columns:
L1567                 feature_df[c] = np.nan
L1568         feature_df.loc[flags.index, flags.columns] = flags
L1569
L1570     except Exception:
L1571         pass
L1572     return feature_df
L1573
```

## <.github/workflows/weekly-report.yml>
```text
L1 name: Weekly Stock Report
L2
L3 on:
L4   push:
L5     branches: [ main ]
L6     paths-ignore:
L7       - 'CodeForChat/**'
L8   schedule:
L9     - cron: '0 0 * * 6'  # UTC 00:00 â†’ JST 09:00ï¼ˆåœŸï¼‰
L10   workflow_dispatch:
L11
L12 jobs:
L13   build-and-report:
L14     runs-on: ubuntu-latest
L15     permissions:
L16       contents: write
L17
L18     steps:
L19       - name: Debug start
L20         run: echo 'ğŸš€ DEBUGstarted'
L21               
L22       - name: Checkout repository
L23         uses: actions/checkout@v3
L24
L25       - name: Setup Python
L26         uses: actions/setup-python@v5
L27         with:
L28           python-version: '3.x'
L29           cache: 'pip'
L30           cache-dependency-path: requirements.txt
L31
L32       - name: Install dependencies
L33         run: pip install -r requirements.txt
L34
L35       - name: Prepare results directory
L36         run: mkdir -p results
L37
L38       - name: Run factor & scoring
L39         env:
L40           SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
L41           FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
L42           FIN_THREADS: "8"
L43         run: python factor.py
```

## <documents/README.md>
```text
L1 # é‹ç”¨ãƒ«ãƒ¼ãƒ«
L2
L3 ## åŸºæœ¬æ§‹æˆ
L4 - 20éŠ˜æŸ„ã‚’å‡ç­‰é…åˆ†ï¼ˆç¾é‡‘ã‚’é™¤ã1éŠ˜æŸ„ã‚ãŸã‚Š5%ï¼‰
L5 - moomooè¨¼åˆ¸ã§é‹ç”¨
L6 - **Growthæ  12éŠ˜æŸ„ / Defenseæ  8éŠ˜æŸ„**ï¼ˆNORMAL åŸºæº–ï¼‰
L7
L8 ## Barbell Growth-Defenseæ–¹é‡
L9 - Growthæ  **12éŠ˜æŸ„**ï¼šé«˜æˆé•·ã§ä¹–é›¢æºã¨ãªã‚‹æ”»ã‚ã®éŠ˜æŸ„
L10 - Defenseæ  **8éŠ˜æŸ„**ï¼šä½ãƒœãƒ©ã§å®‰å®šæˆé•·ã—é…å½“ã‚’å¢—ã‚„ã™å®ˆã‚Šã®éŠ˜æŸ„
L11 - ã€ŒçŒ›çƒˆã«ä¼¸ã³ã‚‹æ”»ã‚ Ã— ç€å®Ÿã«ç¨¼ãç›¾ã€ã®çµ„åˆã›ã§ä¹–é›¢â†’åŠæˆ»ã—ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ã‚’ç‹™ã†
L12
L13 ## ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¤å®šï¼ˆtrend_template åˆæ ¼â€œæœ¬æ•°â€ã§åˆ¤å®šï¼‰
L14 - åˆæ ¼æœ¬æ•° = current+candidate å…¨ä½“ã®ã†ã¡ã€trend_template æ¡ä»¶ã‚’æº€ãŸã—ãŸéŠ˜æŸ„ã®**æœ¬æ•°(C)**ï¼ˆåŸºæº– N_G=12ï¼‰
L15 - ã—ãã„å€¤ã¯éå»~600å–¶æ¥­æ—¥ã®åˆ†å¸ƒã‹ã‚‰**æ¯å›è‡ªå‹•æ¡ç”¨**ï¼ˆåˆ†ä½ç‚¹ã¨é‹ç”¨â€œåºŠâ€ã®maxï¼‰
L16   - ç·Šæ€¥å…¥ã‚Š: `max(q05, 12æœ¬)`ï¼ˆ= N_Gï¼‰
L17   - ç·Šæ€¥è§£é™¤: `max(q20, 18æœ¬)`ï¼ˆ= ceil(1.5Ã—12)ï¼‰
L18   - é€šå¸¸å¾©å¸°: `max(q60, 36æœ¬)`ï¼ˆ= 3Ã—N_Gï¼‰
L19 - ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹: å‰å›ãƒ¢ãƒ¼ãƒ‰ã«ä¾å­˜ï¼ˆEMERGâ†’è§£é™¤ã¯23æœ¬ä»¥ä¸Šã€CAUTIONâ†’é€šå¸¸ã¯45æœ¬ä»¥ä¸Šï¼‰
L20
L21 ## ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¥ã®ç¾é‡‘ãƒ»ãƒ‰ãƒªãƒ•ãƒˆ
L22  - **é€šå¸¸(NORMAL)** : ç¾é‡‘ **10%** / ãƒ‰ãƒªãƒ•ãƒˆé–¾å€¤ **12%**
L23  - **è­¦æˆ’(CAUTION)** : ç¾é‡‘ **12.5%** / ãƒ‰ãƒªãƒ•ãƒˆé–¾å€¤ **14%**
L24  - **ç·Šæ€¥(EMERG)** : ç¾é‡‘ **20%** / **ãƒ‰ãƒªãƒ•ãƒˆå£²è²·åœæ­¢**ï¼ˆ20Ã—5%ã«å…¨æˆ»ã—ã®ã¿ï¼‰
L25
L26 ## ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®æ¨å¥¨â€œä¿æœ‰éŠ˜æŸ„æ•°â€ï¼ˆMMFâ‰’ç¾é‡‘ï¼‰
L27 *å„æ =5%ï¼ˆ20éŠ˜æŸ„å‡ç­‰ï¼‰ã€‚ãƒ¢ãƒ¼ãƒ‰ç§»è¡Œæ™‚ã¯**Gã®æ æ•°ã®ã¿**èª¿æ•´ã—ã€å¤–ã—ãŸæ ã¯ç¾é‡‘ã¨ã—ã¦ä¿æŒã€‚*
L28
L29 - **NORMAL:** G **12** / D **8** / ç¾é‡‘åŒ–æ  **0**  
L30 - **CAUTION:** G **10** / D **8** / ç¾é‡‘åŒ–æ  **2**ï¼ˆ= 10%ï¼‰  
L31 - **EMERG:** G **8**  / D **8** / ç¾é‡‘åŒ–æ  **4**ï¼ˆ= 20%ï¼‰  
L32
L33 > å®Ÿé‹ç”¨ï¼šâ­ï¸ä½ã‚¹ã‚³ã‚¢ã®Gã‹ã‚‰é †ã«å¤–ã™ã€‚è§£é™¤æ™‚ã¯factorä¸Šä½ã‹ã‚‰è£œå……ã€‚
L34
L35 ## ãƒˆãƒ¬ãƒ¼ãƒªãƒ³ã‚°ã‚¹ãƒˆãƒƒãƒ—
L36 - **åŸºæœ¬TS (ãƒ¢ãƒ¼ãƒ‰åˆ¥):** NORMAL **15%** / CAUTION **13%** / EMERG **10%**
L37 - å«ã¿ç›ŠãŒ **+30% / +60% / +100%** åˆ°é”ã§ã€åŸºæœ¬ã‹ã‚‰ **-3pt / -6pt / -8pt** å¼•ãä¸Šã’
L38 - TSç™ºå‹•ã§æ¸›å°‘ã—ãŸéŠ˜æŸ„ã¯ç¿Œæ—¥ä»¥é™ã«è£œå……ï¼ˆâ€»ç·Šæ€¥ãƒ¢ãƒ¼ãƒ‰ä¸­ã¯è£œå……ã—ãªã„ï¼‰
L39
L40 ## åŠæˆ»ã—ï¼ˆãƒªãƒãƒ©ãƒ³ã‚¹ï¼‰æ‰‹é †
L41 ãƒ‰ãƒªãƒ•ãƒˆãƒã‚§ãƒƒã‚¯ã§**ã‚¢ãƒ©ãƒ¼ãƒˆ**ãŒå‡ºãŸå ´åˆï¼ˆåˆè¨ˆ|drift| ãŒãƒ¢ãƒ¼ãƒ‰é–¾å€¤ã‚’è¶…éã€EMERGé™¤ãï¼‰ã€ç¿Œå–¶æ¥­æ—¥ã®ç±³å›½å¯„ä»˜ãã§ä¸‹è¨˜ã‚’å®Ÿæ–½ã™ã‚‹ã€‚
L42
L43 1. **å£²å´ï¼ˆå¿…é ˆï¼‰**  
L44    Slackãƒ†ãƒ¼ãƒ–ãƒ«ã® **Î”qty ãŒãƒã‚¤ãƒŠã‚¹ã®éŠ˜æŸ„ã‚’å£²å´** ã™ã‚‹ï¼ˆå¯„ä»˜ãæˆè¡Œæ¨å¥¨ï¼‰ã€‚  
L45    ã“ã‚Œã¯ã€ŒåŠæˆ»ã—ã€è¨ˆç®—ã«åŸºã¥ãéé‡é‡ã®å‰Šæ¸›ã‚’æ„å‘³ã™ã‚‹ã€‚
L46
L47 2. **è³¼å…¥ï¼ˆä»»æ„ãƒ»åŠæˆ»ã—ç›®å®‰ï¼‰**  
L48    åŠæˆ»ã—å¾Œã®åˆè¨ˆ|drift|ã‚’**ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤ï¼ˆSlackãƒ˜ãƒƒãƒ€ã«è¡¨ç¤ºï¼‰**ã«è¿‘ã¥ã‘ã‚‹ã“ã¨ã‚’ç›®å®‰ã«ã€  
L49    **ä»»æ„ã®éŠ˜æŸ„ã‚’è²·ã„å¢—ã—**ã—ã¦ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹ï¼ˆÎ”qtyãŒãƒ—ãƒ©ã‚¹ã®éŠ˜æŸ„ã‚’å„ªå…ˆã—ã¦ã‚‚ã‚ˆã„ï¼‰ã€‚
L50
L51 3. **ãƒˆãƒ¬ãƒ¼ãƒªãƒ³ã‚°ã‚¹ãƒˆãƒƒãƒ—ã®å†è¨­å®šï¼ˆå¿…é ˆï¼‰**  
L52    ã™ã¹ã¦ã®ä¿æœ‰éŠ˜æŸ„ã«ã¤ã„ã¦ã€æœ€æ–°ã®è©•ä¾¡é¡ã«åˆã‚ã›ã¦TSã‚’**å†ç™ºæ³¨ï¼æ›´æ–°**ã™ã‚‹ã€‚  
L53    ãƒ«ãƒ¼ãƒ«ã¯ä¸‹è¨˜ï¼ˆåˆ©ç›Šåˆ°é”ã§æ®µéšçš„ã«ã‚¿ã‚¤ãƒˆåŒ–ï¼‰ï¼š  
L54    - **åŸºæœ¬TS:** -15%  
L55    - **+30% åˆ°é” â†’ TS -12%**  
L56    - **+60% åˆ°é” â†’ TS -9%**  
L57    - **+100% åˆ°é” â†’ TS -7%**  
L58    â€»ã‚¹ãƒˆãƒƒãƒ—ä¾¡æ ¼ã®å¼•ãä¸Šã’ã¯è¨±å¯ã€**å¼•ãä¸‹ã’ã¯ä¸å¯**ï¼ˆåˆ©ç›Šä¿å…¨ã®åŸå‰‡ï¼‰ã€‚
L59
L60 4. **ä¾‹å¤–ï¼ˆEMERGãƒ¢ãƒ¼ãƒ‰ï¼‰**  
L61    ç·Šæ€¥(EMERG)ã§ã¯**ãƒ‰ãƒªãƒ•ãƒˆç”±æ¥ã®å£²è²·ã¯åœæ­¢ï¼ˆâˆï¼‰**ã€‚20éŠ˜æŸ„Ã—å„5%ã¸ã®**å…¨æˆ»ã—**ã®ã¿è¨±å®¹ã€‚
L62
L63 5. **å®Ÿè¡Œã‚¿ã‚¤ãƒŸãƒ³ã‚°**
L64    - åˆ¤å®šï¼šç±³å›½å¸‚å ´çµ‚å€¤ç›´å¾Œ
L65    - åŸ·è¡Œï¼šç¿Œå–¶æ¥­æ—¥ã®ç±³å›½å¯„ä»˜ãæˆè¡Œ
L66
L67 ## ãƒ¢ãƒ¼ãƒ‰ç§»è¡Œã®å®Ÿå‹™æ‰‹é †ï¼ˆè¶…ã‚·ãƒ³ãƒ—ãƒ«ï¼‰
L68 ãƒ¢ãƒ¼ãƒ‰ãŒå¤‰ã‚ã£ãŸã‚‰ã€**MMFâ‰’ç¾é‡‘**ã¨ã—ã¦æ‰±ã„ã€**Gã®æ æ•°ã ã‘**ã‚’èª¿æ•´ã™ã‚‹ï¼š
L69 1. **Gã‚’å‰Šã‚‹**ï¼ˆCAUTION/EMERGï¼‰  
L70    - â­ï¸ä½ã‚¹ã‚³ã‚¢ã®Gã‹ã‚‰é †ã«å¤–ã™ã€‚  
L71    - **`current_tickers.csv` ã‹ã‚‰å¤–ã™GéŠ˜æŸ„ã®è¡Œã‚’å‰Šé™¤**ï¼ˆï¼ãã®æ ã¯ç¾é‡‘åŒ–ï¼‰ã€‚
L72 2. **ç¾é‡‘ã¨ã—ã¦ä¿æŒ**  
L73    - å¤–ã—ãŸæ ã¯ç¾é‡‘ï¼ˆã¾ãŸã¯MMFç›¸å½“ï¼‰ã§ãƒ—ãƒ¼ãƒ«ã€‚  
L74 3. **å¾©å¸°æ™‚ã®è£œå……**ï¼ˆNORMALã¸ï¼‰  
L75    - **`current_tickers.csv` ã«éŠ˜æŸ„ã‚’è¿½åŠ **ï¼ˆfactorä¸Šä½ã‹ã‚‰ï¼‰ã€‚  
L76    - ä»¥é™ã¯æ—¥æ¬¡ãƒ‰ãƒªãƒ•ãƒˆ/TSãƒ«ãƒ¼ãƒ«ã«å¾“ã†ã€‚
L77
L78 > driftã¯ `target_ratio = 1/éŠ˜æŸ„æ•°` ã‚’è‡ªå‹•é©ç”¨ã€‚è¡Œæ•°ã«å¿œã˜ã¦è‡ªå‹•ã§å‡ç­‰æ¯”ç‡ãŒå†è¨ˆç®—ã•ã‚Œã‚‹ã€‚
L79
L80 ## å…¥æ›¿éŠ˜æŸ„é¸å®š
L81 - Oxfordã‚­ãƒ£ãƒ”ã‚¿ãƒ«ï¼ã‚¤ãƒ³ã‚«ãƒ ã€Alpha Investorã€Motley Fool Stock Advisorã€moomooã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ç­‰ã‚’å‚è€ƒã«chatGPTã§æ¤œè¨
L82 - å¹´é–“NISAæ ã¯Growthç¾¤ã®ä¸­ã‹ã‚‰ä½ãƒœãƒ©éŠ˜æŸ„ã‚’é¸å®šã—åˆ©ç”¨ã€‚é•·æœŸä¿æŒã«ã¯ã“ã ã‚ã‚‰ãªã„ã€‚
L83
L84 ## å†ã‚¨ãƒ³ãƒˆãƒªãƒ¼ï¼ˆã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ï¼‰
L85 - TSãƒ’ãƒƒãƒˆå¾Œã®åŒéŠ˜æŸ„å†INã¯ **8å–¶æ¥­æ—¥** ã®ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ã‚’è¨­ã‘ã‚‹ï¼ˆæœŸé–“ä¸­ã¯å†INç¦æ­¢ï¼‰
L86
L87 ## å®Ÿè¡Œã‚¿ã‚¤ãƒŸãƒ³ã‚°
L88 - åˆ¤å®šï¼šç±³å›½å¸‚å ´çµ‚å€¤ç›´å¾Œ
L89 - åŸ·è¡Œï¼šç¿Œå–¶æ¥­æ—¥ã®ç±³å›½å¯„ä»˜ãæˆè¡Œ
```

## <documents/factor_design.md>
```text
L1 # factor.py è©³ç´°è¨­è¨ˆæ›¸
L2
L3 ## æ¦‚è¦
L4 - æ—¢å­˜ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®éŠ˜æŸ„ã¨æ¤œè¨ä¸­ã®éŠ˜æŸ„ç¾¤ã‚’åŒæ™‚ã«æ‰±ã†éŠ˜æŸ„é¸å®šãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€‚
L5 - ä¾¡æ ¼ãƒ»è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šè¾¼ã¿ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã¨DRRSé¸å®šã‚’è¡Œã†ã“ã¨ã§ã€ä»¥ä¸‹ã®ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆã‚’å¾—ã‚‹ã€‚
L6   - æ¡ç”¨éŠ˜æŸ„ã¨æƒœã—ãã‚‚æ¼ã‚ŒãŸéŠ˜æŸ„ã®ã‚¹ã‚³ã‚¢ä¸€è¦§
L7   - IN/OUTã®ãƒ†ã‚£ãƒƒã‚«ãƒ¼ãƒªã‚¹ãƒˆã¨OUTå´ã®ä½ã‚¹ã‚³ã‚¢éŠ˜æŸ„
L8   - æ–°æ—§ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®æ¯”è¼ƒè¡¨
L9   - æ¤œè¨ä¸­éŠ˜æŸ„ã®ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆæ•´ç†ç”¨ï¼‰
L10
L11 ## å…¨ä½“ãƒ•ãƒ­ãƒ¼
L12 1. **Input** â€“ `current_tickers.csv`ã¨`candidate_tickers.csv`ã‚’èª­ã¿è¾¼ã¿ã€yfinanceã‚„Finnhubã®APIã‹ã‚‰ä¾¡æ ¼ãƒ»è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã¦`InputBundle`ã‚’æ•´å‚™ã€‚
L13 2. **Score Calculation** â€“ ScorerãŒç‰¹å¾´é‡ã‚’è¨ˆç®—ã—å› å­ã‚¹ã‚³ã‚¢ã‚’åˆæˆã—ã¦`FeatureBundle`ã‚’ç”Ÿæˆã€‚
L14 3. **Correlation Reduction & Selection** â€“ SelectorãŒDRRSãƒ­ã‚¸ãƒƒã‚¯ã§ç›¸é–¢ã‚’æŠ‘ãˆã¤ã¤G/DéŠ˜æŸ„ã‚’é¸å®šã—`SelectionBundle`ã‚’å¾—ã‚‹ã€‚
L15 4. **Output** â€“ æ¡ç”¨çµæœã¨å‘¨è¾ºæƒ…å ±ã‚’è¡¨ãƒ»Slacké€šçŸ¥ã¨ã—ã¦å‡ºåŠ›ã€‚
L16
L17 ```mermaid
L18 flowchart LR
L19   A[Input\nAPI & å‰å‡¦ç†] --> B[Score Calculation\nç‰¹å¾´é‡ãƒ»å› å­åˆæˆ]
L20   B --> C[Correlation Reduction\nDRRSé¸å®š]
L21   C --> D[Output\nSlacké€šçŸ¥]
L22 ```
L23
L24 ## å®šæ•°ãƒ»è¨­å®š
L25 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L26 | --- | --- | --- |
L27 | `exist` / `cand` | ç¾è¡Œãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã¨æ¤œè¨ä¸­éŠ˜æŸ„ã®ãƒ†ã‚£ãƒƒã‚«ãƒ¼ãƒªã‚¹ãƒˆ | ã‚¹ã‚³ã‚¢å¯¾è±¡ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã®æ§‹æˆã€å€™è£œæ•´ç† |
L28 | `bench` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼ | ç›¸å¯¾å¼·ã•ãƒ»Î²ç®—å‡ºã€ãƒãƒ¼ãƒˆæ¯”è¼ƒ |
L29 | `CAND_PRICE_MAX` | å€™è£œéŠ˜æŸ„ã®è¨±å®¹ä¾¡æ ¼ä¸Šé™ | é«˜é¡éŠ˜æŸ„ã®äº‹å‰é™¤å¤– |
L30 | `N_G` / `N_D` | G/Dæ¡ç”¨æ ã®ä»¶æ•°ï¼ˆ**æ—¢å®š: 12 / 8**ï¼‰ | æœ€çµ‚çš„ã«é¸ã¶éŠ˜æŸ„æ•°ã®åˆ¶ç´„ |
L31 | `g_weights` / `D_weights` | å„å› å­ã®é‡ã¿dict | G/Dã‚¹ã‚³ã‚¢åˆæˆ |
L32 | `D_BETA_MAX` | Dãƒã‚±ãƒƒãƒˆã®è¨±å®¹Î²ä¸Šé™ | é«˜Î²éŠ˜æŸ„ã®é™¤å¤–ãƒ•ã‚£ãƒ«ã‚¿ |
L33 | `FILTER_SPEC` | G/Dã”ã¨ã®å‰å‡¦ç†ãƒ•ã‚£ãƒ«ã‚¿ | ãƒˆãƒ¬ãƒ³ãƒ‰ãƒã‚¹ã‚¯ã‚„Î²ä¸Šé™è¨­å®š |
L34 | `corrM` | DRRSåˆæœŸãƒ—ãƒ¼ãƒ«ã®æœ€å¤§ä»¶æ•° | ç›¸é–¢è¡Œåˆ—ã‚µã‚¤ã‚ºåˆ¶å¾¡ |
L35 | `DRRS_G` / `DRRS_D` | DRRSãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿dict | ãƒã‚±ãƒƒãƒˆåˆ¥ã®ç›¸é–¢ä½æ¸›è¨­å®š |
L36 | `DRRS_SHRINK` | æ®‹å·®ç›¸é–¢ã®å¯¾è§’ã‚·ãƒ¥ãƒªãƒ³ã‚¯ç‡ | `residual_corr`ã®å®‰å®šåŒ– |
L37 | `CROSS_MU_GD` | G-Dé–“ã‚¯ãƒ­ã‚¹ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£Î¼ | 2ãƒã‚±ãƒƒãƒˆåŒæ™‚æœ€é©åŒ–ã§ç›¸é–¢æŠ‘åˆ¶ |
L38 | `RESULTS_DIR` | é¸å®šçµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª | `_save_sel`/`_load_prev`ã®å…¥å‡ºåŠ› |
L39
L40 é¸å®šçµæœã¯`results/`é…ä¸‹ã«JSONã¨ã—ã¦ä¿å­˜ã—ã€æ¬¡å›å®Ÿè¡Œæ™‚ã«`_load_prev`ã§èª­ã¿è¾¼ã‚“ã§é¸å®šæ¡ä»¶ã«åæ˜ ã€‚
L41
L42 ## DTO/Config
L43 å„ã‚¹ãƒ†ãƒƒãƒ—é–“ã§å—ã‘æ¸¡ã™ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¨è¨­å®šå€¤ã€‚å¤‰æ•°ã®æ„å‘³åˆã„ã¨åˆ©ç”¨ç®‡æ‰€ã‚’ä»¥ä¸‹ã«ç¤ºã™ã€‚
L44
L45 ### InputBundleï¼ˆInput â†’ Scorerï¼‰
L46 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L47 | --- | --- | --- |
L48 | `cand` | å€™è£œéŠ˜æŸ„ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã®ãƒªã‚¹ãƒˆ | OUTãƒ†ãƒ¼ãƒ–ãƒ«ã‚„ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°å¯¾è±¡ã®æ¯é›†å›£ |
L49 | `tickers` | ç¾è¡Œ+å€™è£œã‚’åˆã‚ã›ãŸãƒ†ã‚£ãƒƒã‚«ãƒ¼ä¸€è¦§ | ä¾¡æ ¼ãƒ»å‡ºæ¥é«˜ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€ãƒªã‚¿ãƒ¼ãƒ³è¨ˆç®— |
L50 | `bench` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼ | ç›¸å¯¾å¼·ã•ãƒ»Î²ç®—å‡ºã€ãƒãƒ¼ãƒˆæ¯”è¼ƒ |
L51 | `data` | yfinanceã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœï¼ˆéšå±¤åˆ—ï¼‰ | `px`/`spx`/ãƒªã‚¿ãƒ¼ãƒ³ç­‰ã®åŸºç¤ãƒ‡ãƒ¼ã‚¿ |
L52 | `px` | `data['Close']`ã ã‘ã‚’æŠœãå‡ºã—ãŸä¾¡æ ¼ç³»åˆ— | æŒ‡æ¨™è¨ˆç®—ãƒ»ãƒªã‚¿ãƒ¼ãƒ³ç”Ÿæˆ |
L53 | `spx` | `data['Close'][bench]` ã®Series | `rs`ã‚„`calc_beta`ã®åŸºæº–æŒ‡æ•° |
L54 | `tickers_bulk` | `yf.Tickers`ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ | `info`ç­‰ã®ä¸€æ‹¬å–å¾— |
L55 | `info` | ãƒ†ã‚£ãƒƒã‚«ãƒ¼åˆ¥ã®yfinanceæƒ…å ±dict | ã‚»ã‚¯ã‚¿ãƒ¼åˆ¤å®šã‚„EPSè£œå®Œ |
L56 | `eps_df` | EPS TTM/ç›´è¿‘EPSç­‰ã‚’ã¾ã¨ã‚ãŸè¡¨ | æˆé•·æŒ‡æ¨™ã®ç®—å‡º |
L57 | `fcf_df` | CFOãƒ»CapExãƒ»FCF TTMã¨æƒ…å ±æºãƒ•ãƒ©ã‚° | FCF/EVã‚„é…å½“ã‚«ãƒãƒ¬ãƒƒã‚¸ |
L58 | `returns` | `px.pct_change()`ã®ãƒªã‚¿ãƒ¼ãƒ³è¡¨ | ç›¸é–¢è¡Œåˆ—ãƒ»DRRSè¨ˆç®— |
L59
L60 ### FeatureBundleï¼ˆScorer â†’ Selectorï¼‰
L61 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L62 | --- | --- | --- |
L63 | `df` | è¨ˆç®—æ¸ˆã¿æŒ‡æ¨™ã®ç”Ÿå€¤ãƒ†ãƒ¼ãƒ–ãƒ« | ãƒ‡ãƒãƒƒã‚°ãƒ»å‡ºåŠ›è¡¨ç¤º |
L64 | `df_z` | ã‚¦ã‚£ãƒ³ã‚¶ãƒ¼å¾ŒZã‚¹ã‚³ã‚¢åŒ–ã—ãŸæŒ‡æ¨™è¡¨ | å› å­ã‚¹ã‚³ã‚¢åˆæˆã€é¸å®šåŸºæº– |
L65 | `g_score` | Gãƒã‚±ãƒƒãƒˆç·åˆã‚¹ã‚³ã‚¢ | Gé¸å®šã€IN/OUTæ¯”è¼ƒ |
L66 | `d_score_all` | Dãƒã‚±ãƒƒãƒˆç·åˆã‚¹ã‚³ã‚¢ï¼ˆå…¨éŠ˜æŸ„ï¼‰ | Dé¸å®šã€ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚° |
L67 | `missing_logs` | æ¬ ææŒ‡æ¨™ã¨è£œå®ŒçŠ¶æ³ã®ãƒ­ã‚° | ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ |
L68
L69 ### SelectionBundleï¼ˆSelector â†’ Outputï¼‰
L70 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L71 | --- | --- | --- |
L72 | `resG` | Gé¸å®šçµæœã®è©³ç´°dictï¼ˆ`tickers`ã€ç›®çš„å€¤ç­‰ï¼‰ | çµæœä¿å­˜ãƒ»å¹³å‡ç›¸é–¢ãªã©ã®æŒ‡æ¨™è¡¨ç¤º |
L73 | `resD` | Dé¸å®šçµæœã®è©³ç´°dict | åŒä¸Š |
L74 | `top_G` | æœ€çµ‚æ¡ç”¨Gãƒ†ã‚£ãƒƒã‚«ãƒ¼ | æ–°ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæ§‹ç¯‰ |
L75 | `top_D` | æœ€çµ‚æ¡ç”¨Dãƒ†ã‚£ãƒƒã‚«ãƒ¼ | åŒä¸Š |
L76 | `init_G` | DRRSå‰ã®GåˆæœŸå€™è£œ | æƒœã—ãã‚‚å¤–ã‚ŒãŸéŠ˜æŸ„è¡¨ç¤º |
L77 | `init_D` | DRRSå‰ã®DåˆæœŸå€™è£œ | åŒä¸Š |
L78
L79 ### WeightsConfig
L80 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L81 | --- | --- | --- |
L82 | `g` | Gå› å­ï¼ˆGRW/MOM/VOLï¼‰ã®é‡ã¿dict | `g_score`åˆæˆ |
L83 | `d` | Då› å­ï¼ˆD_QAL/D_YLD/D_VOL_RAW/D_TRDï¼‰ã®é‡ã¿dict | `d_score_all`åˆæˆ |
L84
L85 ### DRRSParams
L86 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L87 | --- | --- | --- |
L88 | `corrM` | DRRSåˆæœŸãƒ—ãƒ¼ãƒ«ã®æœ€å¤§ä»¶æ•° | ç›¸é–¢è¡Œåˆ—ã‚µã‚¤ã‚ºåˆ¶å¾¡ |
L89 | `shrink` | æ®‹å·®ç›¸é–¢ã®ã‚·ãƒ¥ãƒªãƒ³ã‚¯ç‡ | `residual_corr`ã®å¯¾è§’å¼·èª¿ |
L90 | `G` | Gãƒã‚±ãƒƒãƒˆç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿dictï¼ˆ`lookback`ç­‰ï¼‰ | `select_bucket_drrs`è¨­å®š |
L91 | `D` | Dãƒã‚±ãƒƒãƒˆç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿dict | åŒä¸Š |
L92 | `cross_mu_gd` | G-Dã‚¯ãƒ­ã‚¹ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£ä¿‚æ•°Î¼ | `select_buckets`ã®ç›®çš„é–¢æ•° |
L93
L94 ### PipelineConfig
L95 | å¤‰æ•° | å†…å®¹ | ä¸»ãªç”¨é€” |
L96 | --- | --- | --- |
L97 | `weights` | `WeightsConfig`ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ | ã‚¹ã‚³ã‚¢åˆæˆã®é‡ã¿å‚ç…§ |
L98 | `drrs` | `DRRSParams`ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ | é¸å®šã‚¹ãƒ†ãƒƒãƒ—ã®è¨­å®šå€¤ |
L99 | `price_max` | å€™è£œéŠ˜æŸ„ã®è¨±å®¹ä¾¡æ ¼ä¸Šé™ | Inputæ®µéšã§ã®ãƒ•ã‚£ãƒ«ã‚¿ |
L100
L101 ## å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
L102 - `winsorize_s` / `robust_z` : å¤–ã‚Œå€¤å‡¦ç†ã¨Zã‚¹ã‚³ã‚¢åŒ–ã€‚
L103 - `_safe_div` / `_safe_last` : ä¾‹å¤–ã‚’æ½°ã—ãŸåˆ†å‰²ãƒ»æœ«å°¾å–å¾—ã€‚
L104 - `_load_prev` / `_save_sel` : é¸å®šçµæœã®èª­ã¿æ›¸ãã€‚
L105
L106 ## ã‚¯ãƒ©ã‚¹è¨­è¨ˆ
L107 ### Step1: Input
L108 `current_tickers.csv`ã®ç¾è¡ŒéŠ˜æŸ„ã¨`candidate_tickers.csv`ã®æ¤œè¨ä¸­éŠ˜æŸ„ã‚’èµ·ç‚¹ã«ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„ã™ã‚‹ã€‚å¤–éƒ¨I/Oã¨å‰å‡¦ç†ã‚’æ‹…å½“ã—ã€`prepare_data`ã§`InputBundle`ã‚’ç”Ÿæˆã€‚ä¾¡æ ¼ãƒ»è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã¯**yfinanceã‚’å„ªå…ˆã—ã€æ¬ æãŒã‚ã‚‹æŒ‡æ¨™ã®ã¿Finnhub APIã§è£œå®Œ**ã™ã‚‹ã€‚
L109 ä¸»ãªãƒ¡ã‚½ãƒƒãƒ‰:
L110 - `impute_eps_ttm` : å››åŠæœŸEPSÃ—4ã§TTMã‚’æ¨å®šã—æ¬ ææ™‚ã®ã¿å·®ã—æ›¿ãˆã€‚
L111 - `fetch_cfo_capex_ttm_yf` : yfinanceã®å››åŠæœŸ/å¹´æ¬¡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼ã‹ã‚‰CFOãƒ»CapExãƒ»FCF TTMã‚’ç®—å‡ºã€‚
L112 - `fetch_cfo_capex_ttm_finnhub` : yfinanceã§æ¬ ã‘ãŸéŠ˜æŸ„ã®ã¿Finnhub APIã§è£œå®Œã€‚
L113 - `compute_fcf_with_fallback` : yfinanceå€¤ã‚’åŸºæº–ã«Finnhubå€¤ã§ç©´åŸ‹ã‚ã—ã€CFO/CapEx/FCFã¨æƒ…å ±æºãƒ•ãƒ©ã‚°ã‚’è¿”ã™ã€‚
L114 - `_build_eps_df` : `info`ã‚„`quarterly_earnings`ã‹ã‚‰EPS TTMã¨ç›´è¿‘EPSã‚’è¨ˆç®—ã—ã€`impute_eps_ttm`ã§è£œå®Œã€‚
L115 - `prepare_data` :
L116     0. CSVã‹ã‚‰ç¾è¡ŒéŠ˜æŸ„ã¨å€™è£œéŠ˜æŸ„ã®ãƒ†ã‚£ãƒƒã‚«ãƒ¼ä¸€è¦§ã‚’èª­ã¿è¾¼ã‚€ã€‚
L117     1. å€™è£œéŠ˜æŸ„ã®ç¾åœ¨å€¤ã‚’å–å¾—ã—ä¾¡æ ¼ä¸Šé™ã§ãƒ•ã‚£ãƒ«ã‚¿ã€‚
L118     2. æ—¢å­˜+å€™è£œã‹ã‚‰å¯¾è±¡ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚’æ±ºå®šã—ã€ä¾¡æ ¼ãƒ»å‡ºæ¥é«˜ã‚’ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆyfinanceï¼‰ã€‚
L119     3. yfinanceå€¤ã‚’åŸºã«EPS/FCFãƒ†ãƒ¼ãƒ–ãƒ«ã‚„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç³»åˆ—ã€ãƒªã‚¿ãƒ¼ãƒ³ã‚’æ§‹ç¯‰ã—ã€æ¬ æã‚»ãƒ«ã¯Finnhubå‘¼ã³å‡ºã—ã§ç©´åŸ‹ã‚ã€‚
L120     4. ä¸Šè¨˜ã‚’`InputBundle`ã«æ ¼ç´ã—ã¦è¿”ã™ã€‚
L121
L122 ### Step2: Score Calculation (Scorer)
L123 ç‰¹å¾´é‡è¨ˆç®—ã¨ã‚¹ã‚³ã‚¢åˆæˆã‚’æ‹…å½“ã—ã€`FeatureBundle`ã‚’è¿”ã™ã€‚
L124
L125 #### è£œåŠ©é–¢æ•°
L126 - `trend(s)` : 50/150/200æ—¥ç§»å‹•å¹³å‡ã‚„52é€±ãƒ¬ãƒ³ã‚¸ã‹ã‚‰-0.5ã€œ0.5ã§æ§‹æˆã•ã‚ŒãŸãƒˆãƒ¬ãƒ³ãƒ‰æŒ‡æ¨™ã€‚
L127 - `rs(s,b)` / `tr_str(s)` / `rs_line_slope(s,b,win)` : ç›¸å¯¾å¼·ã•ã‚„çŸ­æœŸãƒˆãƒ¬ãƒ³ãƒ‰ã€RSå›å¸°å‚¾ãã‚’ç®—å‡ºã€‚
L128 - `ev_fallback` : `enterpriseValue`æ¬ ææ™‚ã«è² å‚µãƒ»ç¾é‡‘ã‹ã‚‰EVã‚’æ¨å®šã€‚
L129 - `dividend_status` / `div_streak` : é…å½“æœªè¨­å®šçŠ¶æ³ã®åˆ¤å®šã¨å¢—é…å¹´æ•°ã‚«ã‚¦ãƒ³ãƒˆã€‚
L130 - `fetch_finnhub_metrics` : Finnhub APIã‹ã‚‰EPSæˆé•·ãƒ»ROEãƒ»Î²ãªã©ä¸è¶³æŒ‡æ¨™ã‚’å–å¾—ã€‚
L131 - `calc_beta` : ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ã®å…±åˆ†æ•£ã‹ã‚‰Î²ã‚’ç®—å‡ºã€‚
L132 - `spx_to_alpha` : S&P500ã®ä½ç½®æƒ…å ±ã‹ã‚‰DRRSã§ç”¨ã„ã‚‹Î±ã‚’æ¨å®šã€‚
L133 - `soft_cap_effective_scores` / `pick_top_softcap` : ã‚»ã‚¯ã‚¿ãƒ¼ã‚½ãƒ•ãƒˆã‚­ãƒ£ãƒƒãƒ—ä»˜ãã‚¹ã‚³ã‚¢èª¿æ•´ã¨ä¸Šä½æŠ½å‡ºã€‚
L134
L135 **è£œåŠ©é–¢æ•°ã¨ç”ŸæˆæŒ‡æ¨™**
L136
L137 | è£œåŠ©é–¢æ•° | ç”ŸæˆæŒ‡æ¨™ | ç•¥ç§° |
L138 | --- | --- | --- |
L139 | `trend` | ãƒˆãƒ¬ãƒ³ãƒ‰ç·åˆå€¤ | `TR` |
L140 | `rs` | ç›¸å¯¾å¼·ã• | `RS` |
L141 | `tr_str` | ä¾¡æ ¼ã¨50æ—¥ç·šã®ä¹–é›¢ | `TR_str` |
L142 | `rs_line_slope` | RSç·šã®å›å¸°å‚¾ã | `RS_SLOPE_*` |
L143 | `calc_beta` | Î² | `BETA` |
L144 | `div_streak` | é€£ç¶šå¢—é…å¹´æ•° | `DIV_STREAK` |
L145
L146 #### `aggregate_scores` è©³ç´°
L147 1. å„éŠ˜æŸ„ã®ä¾¡æ ¼ç³»åˆ—ã‚„`info`ã‚’åŸºã«ä»¥ä¸‹ã‚’ç®—å‡ºã€‚
L148    - **ãƒˆãƒ¬ãƒ³ãƒ‰/ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ **: `TR`ã€`RS`ã€`TR_str`ã€å¤šæ§˜ãªç§»å‹•å¹³å‡æ¯”ã€`RS_SLOPE_*`ãªã©ã€‚
L149    - **ãƒªã‚¹ã‚¯**: `BETA`ã€`DOWNSIDE_DEV`ã€`MDD_1Y`ã€`RESID_VOL`ã€`DOWN_OUTPERF`ã€`EXT_200`ç­‰ã€‚
L150    - **é…å½“**: `DIV`ã€`DIV_TTM_PS`ã€`DIV_VAR5`ã€`DIV_YOY`ã€`DIV_FCF_COVER`ã€`DIV_STREAK`ã€‚
L151    - **è²¡å‹™ãƒ»æˆé•·**: `EPS`ã€`REV`ã€`ROE`ã€`FCF/EV`ã€`REV_Q_YOY`ã€`EPS_Q_YOY`ã€`REV_YOY_ACC`ã€`REV_YOY_VAR`ã€`REV_ANN_STREAK`ã€`RULE40`ã€`FCF_MGN` ç­‰ã€‚
L152    - **å®‰å®šæ€§/ã‚µã‚¤ã‚º**: `DEBT2EQ`ã€`CURR_RATIO`ã€`MARKET_CAP`ã€`ADV60_USD`ã€`EPS_VAR_8Q`ãªã©ã€‚
L153 2. æŒ‡æ¨™æ¬ æã¯Finnhub APIç­‰ã§è£œå®Œã—ã€æœªå–å¾—é …ç›®ã‚’`missing_logs`ã«è¨˜éŒ²ã€‚
L154 3. `winsorize_s`â†’`robust_z`ã§æ¨™æº–åŒ–ã—`df_z`ã¸ä¿å­˜ã€‚ã‚µã‚¤ã‚ºãƒ»æµå‹•æ€§ã¯å¯¾æ•°å¤‰æ›ã€‚
L155 4. æ­£è¦åŒ–æ¸ˆæŒ‡æ¨™ã‹ã‚‰å› å­ã‚¹ã‚³ã‚¢ã‚’åˆæˆã€‚
L156    - å„å› å­ã®æ§‹æˆã¨é‡ã¿ã¯ä»¥ä¸‹ã®é€šã‚Šã€‚
L157      - **GRW**: 0.30Ã—`REV` + 0.20Ã—`EPS_Q_YOY` + 0.15Ã—`REV_Q_YOY` + 0.15Ã—`REV_YOY_ACC` + 0.10Ã—`RULE40` + 0.10Ã—`FCF_MGN` + 0.10Ã—`REV_ANN_STREAK` âˆ’ 0.05Ã—`REV_YOY_VAR`ã€‚
L158      - **MOM**: 0.40Ã—`RS` + 0.15Ã—`TR_str` + 0.15Ã—`RS_SLOPE_6W` + 0.15Ã—`RS_SLOPE_13W` + 0.10Ã—`MA200_SLOPE_5M` + 0.10Ã—`MA200_UP_STREAK_D`ã€‚
L159      - **VOL**: `BETA`å˜ä½“ã‚’ä½¿ç”¨ã€‚
L160      - **QAL**: 0.60Ã—`FCF_W` + 0.40Ã—`ROE_W`ã§ä½œæˆã€‚
L161      - **YLD**: 0.30Ã—`DIV` + 0.70Ã—`DIV_STREAK`ã€‚
L162      - **D_QAL**: 0.35Ã—`QAL` + 0.20Ã—`FCF` + 0.15Ã—`CURR_RATIO` âˆ’ 0.15Ã—`DEBT2EQ` âˆ’ 0.15Ã—`EPS_VAR_8Q`ã€‚
L163      - **D_YLD**: 0.45Ã—`DIV` + 0.25Ã—`DIV_STREAK` + 0.20Ã—`DIV_FCF_COVER` âˆ’ 0.10Ã—`DIV_VAR5`ã€‚
L164      - **D_VOL_RAW**: 0.40Ã—`DOWNSIDE_DEV` + 0.22Ã—`RESID_VOL` + 0.18Ã—`MDD_1Y` âˆ’ 0.10Ã—`DOWN_OUTPERF` âˆ’ 0.05Ã—`EXT_200` âˆ’ 0.08Ã—`SIZE` âˆ’ 0.10Ã—`LIQ` + 0.10Ã—`BETA`ã€‚
L165      - **D_TRD**: 0.40Ã—`MA200_SLOPE_5M` âˆ’ 0.30Ã—`EXT_200` + 0.15Ã—`NEAR_52W_HIGH` + 0.15Ã—`TR`ã€‚
L166     - ä¸»ãªæŒ‡æ¨™ã®ç•¥ç§°ã¨æ„å‘³:
L167
L168       | ç•¥ç§° | è£œåŠ©é–¢æ•° | æ¦‚è¦ |
L169       | --- | --- | --- |
L170       | TR | `trend` | 50/150/200æ—¥ç§»å‹•å¹³å‡ã¨52é€±ãƒ¬ãƒ³ã‚¸ã‚’çµ„ã¿åˆã‚ã›ãŸãƒˆãƒ¬ãƒ³ãƒ‰ç·åˆå€¤ |
L171       | RS | `rs` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«å¯¾ã™ã‚‹ç›¸å¯¾å¼·ã•ï¼ˆ12M/1Mãƒªã‚¿ãƒ¼ãƒ³å·®ï¼‰ |
L172       | TR_str | `tr_str` | ä¾¡æ ¼ã¨50æ—¥ç§»å‹•å¹³å‡ã®ä¹–é›¢ |
L173       | RS_SLOPE_6W | `rs_line_slope` | ç›¸å¯¾å¼·ã•ç·šã®6é€±å›å¸°å‚¾ã |
L174       | RS_SLOPE_13W | `rs_line_slope` | ç›¸å¯¾å¼·ã•ç·šã®13é€±å›å¸°å‚¾ã |
L175       | MA200_SLOPE_5M | - | 200æ—¥ç§»å‹•å¹³å‡ã®5ã‹æœˆé¨°è½ç‡ |
L176       | MA200_UP_STREAK_D | - | 200æ—¥ç·šãŒé€£ç¶šã§ä¸Šå‘ã„ãŸæ—¥æ•° |
L177       | BETA | `calc_beta` | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«å¯¾ã™ã‚‹Î² |
L178       | DOWNSIDE_DEV | - | ä¸‹æ–¹ãƒªã‚¿ãƒ¼ãƒ³ã®ã¿ã®å¹´ç‡åŒ–æ¨™æº–åå·® |
L179       | RESID_VOL | - | Î²ã§èª¿æ•´ã—ãŸæ®‹å·®ãƒªã‚¿ãƒ¼ãƒ³ã®å¹´ç‡åŒ–æ¨™æº–åå·® |
L180       | MDD_1Y | - | éå»1å¹´ã®æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³ |
L181       | DOWN_OUTPERF | - | å¸‚å ´ä¸‹è½æ—¥ã«å¯¾ã™ã‚‹å¹³å‡è¶…éãƒªã‚¿ãƒ¼ãƒ³ |
L182       | EXT_200 | - | 200æ—¥ç§»å‹•å¹³å‡ã‹ã‚‰ã®çµ¶å¯¾ä¹–é›¢ç‡ |
L183       | NEAR_52W_HIGH | - | 52é€±é«˜å€¤ã¾ã§ã®ä¸‹æ–¹è·é›¢ï¼ˆ0=é«˜å€¤ï¼‰ |
L184       | FCF_W | - | ã‚¦ã‚£ãƒ³ã‚¶ãƒ¼å‡¦ç†å¾Œã®FCF/EV |
L185       | ROE_W | - | ã‚¦ã‚£ãƒ³ã‚¶ãƒ¼å‡¦ç†å¾Œã®ROE |
L186       | FCF | - | FCF/EV |
L187       | QAL | - | FCF_Wã¨ROE_Wã‚’çµ„ã¿åˆã‚ã›ãŸå“è³ªã‚¹ã‚³ã‚¢ |
L188       | CURR_RATIO | - | æµå‹•æ¯”ç‡ |
L189       | DEBT2EQ | - | è² å‚µè³‡æœ¬å€ç‡ |
L190       | EPS_VAR_8Q | - | EPSã®8å››åŠæœŸæ¨™æº–åå·® |
L191       | DIV | - | å¹´ç‡æ›ç®—é…å½“åˆ©å›ã‚Š |
L192       | DIV_STREAK | `div_streak` | é€£ç¶šå¢—é…å¹´æ•° |
L193       | DIV_FCF_COVER | - | é…å½“ã®FCFã‚«ãƒãƒ¬ãƒƒã‚¸ |
L194       | DIV_VAR5 | - | 5å¹´é…å½“å¤‰å‹•ç‡ |
L195       | DIV_TTM_PS | - | 1æ ªå½“ãŸã‚ŠTTMé…å½“ |
L196       | DIV_YOY | - | å‰å¹´æ¯”é…å½“æˆé•·ç‡ |
L197       | REV | - | å£²ä¸Šæˆé•·ç‡TTM |
L198       | EPS_Q_YOY | - | å››åŠæœŸEPSã®å‰å¹´åŒæœŸæ¯” |
L199       | REV_Q_YOY | - | å››åŠæœŸå£²ä¸Šã®å‰å¹´åŒæœŸæ¯” |
L200       | REV_YOY_ACC | - | å£²ä¸Šæˆé•·ç‡ã®åŠ é€Ÿåˆ† |
L201       | RULE40 | - | å£²ä¸Šæˆé•·ç‡ã¨FCFãƒãƒ¼ã‚¸ãƒ³ã®åˆè¨ˆ |
L202       | FCF_MGN | - | FCFãƒãƒ¼ã‚¸ãƒ³ |
L203       | REV_ANN_STREAK | - | å¹´æ¬¡å£²ä¸Šæˆé•·ã®é€£ç¶šå¹´æ•° |
L204       | REV_YOY_VAR | - | å¹´æ¬¡å£²ä¸Šæˆé•·ç‡ã®å¤‰å‹•æ€§ |
L205       | SIZE | - | æ™‚ä¾¡ç·é¡ã®å¯¾æ•°å€¤ |
L206       | LIQ | - | 60æ—¥å¹³å‡å‡ºæ¥é«˜ãƒ‰ãƒ«ã®å¯¾æ•°å€¤ |
L207    - Gãƒã‚±ãƒƒãƒˆ: `GRW`ã€`MOM`ã€`VOL`ã‚’`cfg.weights.g`ï¼ˆ0.40/0.45/-0.15ï¼‰ã§åŠ é‡ã—`g_score`ã‚’å¾—ã‚‹ã€‚
L208    - Dãƒã‚±ãƒƒãƒˆ: `D_QAL`ã€`D_YLD`ã€`D_VOL_RAW`ã€`D_TRD`ã‚’`cfg.weights.d`ï¼ˆ0.15/0.15/-0.45/0.25ï¼‰ã§åŠ é‡ã—`d_score_all`ã‚’ç®—å‡ºã€‚
L209    - ã‚»ã‚¯ã‚¿ãƒ¼capã«ã‚ˆã‚‹`soft_cap_effective_scores`ã‚’é©ç”¨ã—ã€Gæ¡ç”¨éŠ˜æŸ„ã«ã¯ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨ã€‚
L210 5. `_apply_growth_entry_flags`ã§ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆ/æŠ¼ã—ç›®ç™ºç«çŠ¶æ³ã‚’ä»˜åŠ ã—ã€`FeatureBundle`ã‚’è¿”ã™ã€‚
L211
L212 ### Step3: Correlation Reduction & Selection (Selector)
L213 DRRSã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ç›¸é–¢ã‚’æŠ‘ãˆãŸéŠ˜æŸ„é¸å®šã‚’è¡Œã„ã€`SelectionBundle`ã‚’è¿”ã™ã€‚`results/`ã«ä¿å­˜ã•ã‚ŒãŸå‰å›é¸å®šï¼ˆ`G_selection.json` / `D_selection.json`ï¼‰ã‚’`_load_prev`ã§èª­ã¿è¾¼ã¿ã€ç›®çš„å€¤ãŒå¤§ããæ‚ªåŒ–ã—ãªã„é™ã‚Šç¶­æŒã™ã‚‹ã€‚æ–°ã—ã„æ¡ç”¨é›†åˆã¯`_save_sel`ã§JSONã«æ›¸ãå‡ºã—æ¬¡å›ä»¥é™ã®å…¥åŠ›ã«å‚™ãˆã‚‹ã€‚
L214 ä¸»ãªãƒ¡ã‚½ãƒƒãƒ‰:
L215 - `residual_corr` : åç›Šç‡è¡Œåˆ—ã‚’Zã‚¹ã‚³ã‚¢åŒ–ã—ã€ä¸Šä½ä¸»æˆåˆ†ã‚’é™¤å»ã—ãŸæ®‹å·®ã‹ã‚‰ç›¸é–¢è¡Œåˆ—ã‚’æ±‚ã‚ã€å¹³å‡ç›¸é–¢ã«å¿œã˜ã¦ã‚·ãƒ¥ãƒªãƒ³ã‚¯ã€‚
L216 - `rrqr_like_det` : ã‚¹ã‚³ã‚¢ã‚’é‡ã¿ä»˜ã‘ã—ãŸQRåˆ†è§£é¢¨ã®æ‰‹é †ã§åˆæœŸå€™è£œã‚’kä»¶æŠ½å‡ºã—ã€ã‚¹ã‚³ã‚¢ã®é«˜ã„éç›¸é–¢ãªé›†åˆã‚’å¾—ã‚‹ã€‚
L217 - `swap_local_det` / `swap_local_det_cross` : `sum(score) - Î»*within_corr - Î¼*cross_corr`ã‚’ç›®çš„é–¢æ•°ã¨ã—ã¦ã€å…¥ã‚Œæ›¿ãˆæ¢ç´¢ã§å±€æ‰€çš„ã«æœ€é©åŒ–ã€‚
L218 - `select_bucket_drrs` : ãƒ—ãƒ¼ãƒ«éŠ˜æŸ„ã¨ã‚¹ã‚³ã‚¢ã‹ã‚‰æ®‹å·®ç›¸é–¢ã‚’è¨ˆç®—ã—ã€ä¸Šè¨˜2æ®µéš(åˆæœŸé¸æŠâ†’å…¥ã‚Œæ›¿ãˆ)ã§kéŠ˜æŸ„ã‚’æ±ºå®šã€‚éå»æ¡ç”¨éŠ˜æŸ„ã¨ã®æ¯”è¼ƒã§ç›®çš„å€¤ãŒåŠ£åŒ–ã—ãªã‘ã‚Œã°ç¶­æŒã™ã‚‹ã€‚
L219 - `select_buckets` : Gãƒã‚±ãƒƒãƒˆã‚’é¸å®šå¾Œã€ãã®çµæœã‚’é™¤ã„ãŸå€™è£œã‹ã‚‰Dãƒã‚±ãƒƒãƒˆã‚’é¸ã¶ã€‚Dé¸å®šæ™‚ã¯Gã¨ã®ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£Î¼ã‚’ä»˜ä¸ã—ã€ä¸¡ãƒã‚±ãƒƒãƒˆã®åˆ†æ•£ã‚’åˆ¶å¾¡ã™ã‚‹ã€‚
L220
L221 #### ç›¸é–¢ä½æ¸›ãƒ­ã‚¸ãƒƒã‚¯è©³ç´°
L222 1. **æ®‹å·®ç›¸é–¢è¡Œåˆ—ã®æ§‹ç¯‰ (`residual_corr`)**
L223    - ãƒªã‚¿ãƒ¼ãƒ³è¡Œåˆ—`R`ã‚’Zã‚¹ã‚³ã‚¢åŒ–ã€‚
L224    - SVDã§ä¸Šä½`n_pc`ä¸»æˆåˆ†`F`ã‚’æ±‚ã‚ã€æœ€å°äºŒä¹—ã§ä¿‚æ•°`B`ã‚’ç®—å‡ºã—æ®‹å·®`E = Z - F@B`ã‚’å¾—ã‚‹ã€‚
L225    - `E`ã®ç›¸é–¢è¡Œåˆ—`C`ã‚’è¨ˆç®—ã—ã€å¹³å‡çµ¶å¯¾ç›¸é–¢ã«å¿œã˜ã¦ã‚·ãƒ¥ãƒªãƒ³ã‚¯é‡`shrink_eff`ã‚’è£œæ­£ã—ã¦å¯¾è§’ã‚’å¼·èª¿ã€‚
L226 2. **åˆæœŸå€™è£œã®æŠ½å‡º (`rrqr_like_det`)**
L227    - ã‚¹ã‚³ã‚¢ã‚’0-1æ­£è¦åŒ–ã—ãŸé‡ã¿`w`ã¨ã—ã€`Z*(1+Î³w)`ã§åˆ—ãƒãƒ«ãƒ ã‚’å¼·èª¿ã€‚
L228    - æ®‹å·®ãƒãƒ«ãƒ æœ€å¤§ã®åˆ—ã‚’é€æ¬¡é¸ã³ã€QRãƒ©ã‚¤ã‚¯ãªãƒ‡ãƒ•ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã£ã¦éç›¸é–¢ã‹ã¤é«˜ã‚¹ã‚³ã‚¢ãª`k`éŠ˜æŸ„é›†åˆ`S0`ã‚’å¾—ã‚‹ã€‚
L229 3. **å±€æ‰€æ¢ç´¢ (`swap_local_det` / `swap_local_det_cross`)**
L230    - ç›®çš„é–¢æ•°`Î£z_score âˆ’ Î»Â·within_corr âˆ’ Î¼Â·cross_corr`ã‚’æœ€å¤§åŒ–ã€‚
L231    - é¸æŠé›†åˆã®å„éŠ˜æŸ„ã‚’ä»–å€™è£œã¨å…¥ã‚Œæ›¿ãˆã€æ”¹å–„ãŒãªããªã‚‹ã¾ã§ã¾ãŸã¯`max_pass`å›ã¾ã§æ¢ç´¢ã€‚
L232    - `swap_local_det_cross`ã¯Gãƒã‚±ãƒƒãƒˆã¨ã®ã‚¯ãƒ­ã‚¹ç›¸é–¢è¡Œåˆ—`C_cross`ã‚’ä½¿ç”¨ã—ã€ãƒšãƒŠãƒ«ãƒ†ã‚£`Î¼`ã‚’ä»˜ä¸ã€‚
L233 4. **éå»æ¡ç”¨ã®ç¶­æŒã¨ã‚¯ãƒ­ã‚¹ãƒšãƒŠãƒ«ãƒ†ã‚£ (`select_bucket_drrs` / `select_buckets`)**
L234    - å±€æ‰€æ¢ç´¢çµæœ`S`ã¨éå»é›†åˆ`P`ã®ç›®çš„å€¤ã‚’æ¯”è¼ƒã—ã€`S`ãŒ`P`ã‚ˆã‚Š`Î·`æœªæº€ã®æ”¹å–„ãªã‚‰`P`ã‚’ç¶­æŒã€‚
L235    - `select_buckets`ã§ã¯Gã‚’å…ˆã«æ±ºå®šã—ã€Dé¸å®šæ™‚ã«Gã¨ã®ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£`Î¼`ã‚’åŠ ãˆã¦ã‚¯ãƒ­ã‚¹åˆ†æ•£ã‚’æŠ‘åˆ¶ã€‚
L236
L237 ### Step4: Output
L238 é¸å®šçµæœã‚’å¯è¦–åŒ–ã—å…±æœ‰ã™ã‚‹å·¥ç¨‹ã€‚ä»¥ä¸‹ã®å†…å®¹ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«åŒ–ã—ã¦æ¨™æº–å‡ºåŠ›ã¨Slackã¸é€ã‚‹ã€‚
L239 - æ¡ç”¨éŠ˜æŸ„ã¨æƒœã—ãã‚‚é¸å¤–ã¨ãªã£ãŸéŠ˜æŸ„ã®ã‚¹ã‚³ã‚¢ä¸€è¦§
L240 - IN/OUTãƒªã‚¹ãƒˆã¨OUTéŠ˜æŸ„ã®ã‚¹ã‚³ã‚¢ï¼ˆä½å¾—ç‚¹éŠ˜æŸ„ã‚’ç¢ºèªã—ã‚„ã™ãï¼‰
L241 - æ–°æ—§ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®æ¯”è¼ƒè¡¨ï¼ˆçµ„å…¥ã‚Œãƒ»é™¤å¤–ã€ã‚¹ã‚³ã‚¢å¤‰åŒ–ï¼‰
L242 - æ¤œè¨ä¸­éŠ˜æŸ„ã®ä½ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°
L243
L244 ä¸»ãªãƒ¡ã‚½ãƒƒãƒ‰:
L245 - `display_results` : ä¸Šè¨˜ãƒ†ãƒ¼ãƒ–ãƒ«ã«åŠ ãˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã‚„åˆ†æ•£åŒ–æŒ‡æ¨™ã‚’è¡¨ç¤ºã€‚
L246 - `notify_slack` : Slack Webhookã¸åŒå†…å®¹ã‚’é€ä¿¡ã€‚
L247 - è£œåŠ©:`_avg_offdiag`ã€`_resid_avg_rho`ã€`_raw_avg_rho`ã€`_cross_block_raw_rho`ã€‚
L248
L249 ## ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
L250 1. `PipelineConfig`ã‚’æ§‹ç¯‰ã€‚
L251 2. **Step1** `Input.prepare_data`ã§`InputBundle`ã‚’ç”Ÿæˆã€‚
L252 3. **Step2** `Scorer.aggregate_scores`ã§`FeatureBundle`ã‚’å–å¾—ã€‚
L253 4. **Step3** `Selector.select_buckets`ã§`SelectionBundle`ã‚’ç®—å‡ºã€‚
L254 5. **Step4** `Output.display_results`ã¨`notify_slack`ã§çµæœã‚’å‡ºåŠ›ã€‚
```

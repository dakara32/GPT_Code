# === Chat Paste Pack ===
# Repo: dakara32/GPT_Code @ main
# Files: config.py, factor.py, scorer.py, .github/workflows/weekly-report.yml, documents/README.md, documents/factor_design.md
# 作成日時: 2025-09-21 23:43:13 (JST)
# 使い方: 下のチャンクを順に貼ればこのチャットで全体把握できます。
# 注記: 各ファイルは個別に L1.. で行番号付与。
---

## <config.py>
```text
L1 # 共通設定（factor / drift から参照）
L2 TOTAL_TARGETS = 20
L3
L4 # 基準のバケット数（NORMAL）
L5 COUNTS_BASE = {"G": 12, "D": 8}
L6
L7 # モード別の推奨バケット数
L8 COUNTS_BY_MODE = {
L9     "NORMAL": {"G": 12, "D": 8},
L10     "CAUTION": {"G": 10, "D": 8},
L11     "EMERG": {"G": 8,  "D": 8},
L12 }
L13
L14 # モード別のドリフト閾値（%）
L15 DRIFT_THRESHOLD_BY_MODE = {"NORMAL": 12, "CAUTION": 14, "EMERG": float("inf")}
L16
L17 # モード別のTS（基本幅, 小数=割合）
L18 TS_BASE_BY_MODE = {"NORMAL": 0.15, "CAUTION": 0.13, "EMERG": 0.10}
L19 # 利益到達(+30/+60/+100%)時の段階タイト化（ポイント差）
L20 TS_STEP_DELTAS_PT = (3, 6, 8)
L21
L22 # Breadthの校正は N_G に連動（緊急解除=ceil(1.5*N_G), 通常復帰=3*N_G）
L23 N_G = COUNTS_BASE["G"]
L24 N_D = COUNTS_BASE["D"]
L25
```

## <factor.py>
```text
L1 '''ROLE: Orchestration ONLY（外部I/O・SSOT・Slack出力）, 計算は scorer.py'''
L2 # === NOTE: 機能・入出力・ログ文言・例外挙動は不変。安全な短縮（import統合/複数代入/内包表記/メソッドチェーン/一行化/空行圧縮など）のみ適用 ===
L3 BONUS_COEFF = 0.55  # 推奨: 攻め=0.45 / 中庸=0.55 / 守り=0.65
L4 SWAP_DELTA_Z = 0.15   # 僅差判定: σの15%。(緩め=0.10 / 標準=0.15 / 固め=0.20)
L5 SWAP_KEEP_BUFFER = 3  # n_target+この順位以内の現行は保持。(粘り弱=2 / 標準=3 / 粘り強=4〜5)
L6 import logging, os, time, requests
L7 from concurrent.futures import ThreadPoolExecutor
L8 from dataclasses import dataclass
L9 from time import perf_counter
L10 from typing import Any, Dict, List, Tuple
L11
L12 import numpy as np
L13 import pandas as pd
L14 import yfinance as yf
L15 from scipy.stats import zscore  # used via scorer
L16
L17 from scorer import Scorer, ttm_div_yield_portfolio, _log, _as_numeric_series
L18 import config
L19
L20 import warnings, atexit, threading
L21 from collections import Counter, defaultdict
L22
L23 # ---------- 重複警告の集約ロジック ----------
L24 _warn_lock = threading.Lock()
L25 _warn_seen = set()                     # 初回表示済みキー
L26 _warn_count = Counter()                # (category, message, module) → 件数
L27 _warn_first_ctx = {}                   # 初回の (filename, lineno)
L28
L29 def _warn_key(message, category, filename, lineno, *_args, **_kwargs):
L30     # "同じ警告" を定義: カテゴリ + 正規化メッセージ + モジュールパス(先頭数階層)
L31     mod = filename.split("/site-packages/")[-1] if "/site-packages/" in filename else filename
L32     mod = mod.rsplit("/", 3)[-1]  # 長すぎ抑制（末尾3階層まで）
L33     msg = str(message).strip()
L34     return (category.__name__, msg, mod)
L35
L36 _orig_showwarning = warnings.showwarning
L37
L38 def _compact_showwarning(message, category, filename, lineno, file=None, line=None):
L39     key = _warn_key(message, category, filename, lineno)
L40     with _warn_lock:
L41         _warn_count[key] += 1
L42         if key not in _warn_seen:
L43             # 初回だけ1行で出す（カテゴリ | モジュール | メッセージ）
L44             _warn_seen.add(key)
L45             _warn_first_ctx[key] = (filename, lineno)
L46             # 1行フォーマット（行数節約）
L47             txt = f"[WARN][{category.__name__}] {message} | {filename}:{lineno}"
L48             print(txt)
L49         # 2回目以降は出さない（集約）
L50
L51 warnings.showwarning = _compact_showwarning
L52
L53 # ベースポリシー: 通常は警告を出す（default）→ ただし同一メッセージは集約
L54 warnings.resetwarnings()
L55 warnings.simplefilter("default")
L56
L57 # 2) ピンポイント間引き: yfinance 'Ticker.earnings' は "once"（初回のみ可視化）
L58 warnings.filterwarnings(
L59     "once",
L60     message="'Ticker.earnings' is deprecated",
L61     category=DeprecationWarning,
L62     module="yfinance"
L63 )
L64
L65 # 3) 最終サマリ: 同一警告が何回出たかを最後に1行で
L66 @atexit.register
L67 def _print_warning_summary():
L68     suppressed = []
L69     for key, cnt in _warn_count.items():
L70         if cnt > 1:
L71             (cat, msg, mod) = key
L72             filename, lineno = _warn_first_ctx.get(key, ("", 0))
L73             suppressed.append((cnt, cat, msg, mod, filename, lineno))
L74     if suppressed:
L75         suppressed.sort(reverse=True)  # 件数降順
L76         # 最多上位だけ出す（必要なら上限制御：ここでは上位10件）
L77         top = suppressed[:10]
L78         print(f"[WARN-SUMMARY] duplicated warning groups: {len(suppressed)}")
L79         for cnt, cat, msg, mod, filename, lineno in top:
L80             print(f"[WARN-SUMMARY] {cnt-1} more | [{cat}] {msg} | {mod} ({filename}:{lineno})")
L81         if len(suppressed) > len(top):
L82             print(f"[WARN-SUMMARY] ... and {len(suppressed)-len(top)} more groups suppressed")
L83
L84 # 4) 追加（任意）: 1ジョブあたりの総警告上限を設定したい場合
L85 #    例: 上限1000を超えたら以降は完全サイレント
L86 _WARN_HARD_LIMIT = int(os.getenv("WARN_HARD_LIMIT", "0") or "0")  # 0なら無効
L87 if _WARN_HARD_LIMIT > 0:
L88     _orig_warn_func = warnings.warn
L89     def _limited_warn(*a, **k):
L90         total = sum(_warn_count.values())
L91         if total < _WARN_HARD_LIMIT:
L92             return _orig_warn_func(*a, **k)
L93         # 超過後は捨てる（最後にsummaryだけ残る）
L94     warnings.warn = _limited_warn
L95
L96 # ---------- ここまでで警告の“可視性は維持”しつつ“重複で行数爆発”を抑止 ----------
L97
L98 # その他
L99 debug_mode, FINNHUB_API_KEY = True, os.environ.get("FINNHUB_API_KEY")
L100
L101 logger = logging.getLogger(__name__)
L102 logging.basicConfig(level=(logging.INFO if debug_mode else logging.WARNING), force=True)
L103
L104 class T:
L105     t = perf_counter()
L106
L107     @staticmethod
L108     def log(tag):
L109         now = perf_counter()
L110         print(f"[T] {tag}: {now - T.t:.2f}s")
L111         T.t = now
L112
L113 T.log("start")
L114
L115 # === ユニバースと定数（冒頭に固定） ===
L116 exist, cand = [pd.read_csv(f, header=None)[0].tolist() for f in ("current_tickers.csv","candidate_tickers.csv")]
L117 T.log(f"csv loaded: exist={len(exist)} cand={len(cand)}")
L118 CAND_PRICE_MAX, bench = 450, '^GSPC'  # 価格上限・ベンチマーク
L119 N_G, N_D = config.N_G, config.N_D  # G/D枠サイズ（NORMAL基準: G12/D8）
L120 g_weights = {'GROWTH_F':0.35,'MOM':0.55,'VOL':-0.10}
L121 D_BETA_MAX = float(os.environ.get("D_BETA_MAX", "0.8"))
L122 FILTER_SPEC = {"G":{"pre_mask":["trend_template"]},"D":{"pre_filter":{"beta_max":D_BETA_MAX}}}
L123 D_weights = {'QAL':0.1,'YLD':0.3,'VOL':-0.5,'TRD':0.1}
L124 _fmt_w = lambda w: " ".join(f"{k}{int(v*100)}" for k, v in w.items())
L125
L126 # DRRS 初期プール・各種パラメータ
L127 corrM = 45
L128 DRRS_G, DRRS_D = dict(lookback=252,n_pc=3,gamma=1.2,lam=0.68,eta=0.8), dict(lookback=504,n_pc=4,gamma=0.8,lam=0.85,eta=0.5)
L129 DRRS_SHRINK = 0.10  # 残差相関の対角シュリンク（基礎）
L130
L131 # クロス相関ペナルティ（未定義なら設定）
L132 try: CROSS_MU_GD
L133 except NameError: CROSS_MU_GD = 0.40  # 推奨 0.35–0.45（lam=0.85想定）
L134
L135 # 出力関連
L136 RESULTS_DIR = "results"
L137 os.makedirs(RESULTS_DIR, exist_ok=True)
L138
L139 # === 共有DTO（クラス間I/O契約）＋ Config ===
L140 @dataclass(frozen=True)
L141 class InputBundle:
L142     # Input → Scorer で受け渡す素材（I/O禁止の生データ）
L143     cand: List[str]
L144     tickers: List[str]
L145     bench: str
L146     data: pd.DataFrame              # yfinance download結果（'Close','Volume'等の階層列）
L147     px: pd.DataFrame                # data['Close']
L148     spx: pd.Series                  # data['Close'][bench]
L149     tickers_bulk: object            # yfinance.Tickers
L150     info: Dict[str, dict]           # yfinance info per ticker
L151     eps_df: pd.DataFrame            # ['eps_ttm','eps_q_recent',...]
L152     fcf_df: pd.DataFrame            # ['fcf_ttm', ...]
L153     returns: pd.DataFrame           # px[tickers].pct_change()
L154     missing_logs: pd.DataFrame
L155
L156 @dataclass(frozen=True)
L157 class FeatureBundle:
L158     df: pd.DataFrame
L159     df_z: pd.DataFrame
L160     g_score: pd.Series
L161     d_score_all: pd.Series
L162     missing_logs: pd.DataFrame
L163     df_full: pd.DataFrame | None = None
L164     df_full_z: pd.DataFrame | None = None
L165     scaler: Any | None = None
L166
L167 @dataclass(frozen=True)
L168 class SelectionBundle:
L169     resG: dict
L170     resD: dict
L171     top_G: List[str]
L172     top_D: List[str]
L173     init_G: List[str]
L174     init_D: List[str]
L175
L176 @dataclass(frozen=True)
L177 class WeightsConfig:
L178     g: Dict[str,float]
L179     d: Dict[str,float]
L180
L181 @dataclass(frozen=True)
L182 class DRRSParams:
L183     corrM: int
L184     shrink: float
L185     G: Dict[str,float]   # lookback, n_pc, gamma, lam, eta
L186     D: Dict[str,float]
L187     cross_mu_gd: float
L188
L189 @dataclass(frozen=True)
L190 class PipelineConfig:
L191     weights: WeightsConfig
L192     drrs: DRRSParams
L193     price_max: float
L194     debug_mode: bool = False
L195
L196 # === 共通ユーティリティ（複数クラスで使用） ===
L197 # (unused local utils removed – use scorer.py versions if needed)
L198
L199 def _build_missing_logs_after_impute(eps_df: pd.DataFrame) -> pd.DataFrame:
L200     df = eps_df.copy()
L201     required_cols = [
L202         "EPS_TTM",
L203         "EPS_Q_LastQ",
L204         "EPS_A_LATEST",
L205         "REV_TTM",
L206         "REV_Q_LastQ",
L207         "REV_A_LATEST",
L208     ]
L209     for col in required_cols:
L210         if col not in df.columns:
L211             df[col] = np.nan
L212
L213     miss_eps = df["EPS_TTM"].isna() & df["EPS_Q_LastQ"].isna() & df["EPS_A_LATEST"].isna()
L214     miss_rev = df["REV_TTM"].isna() & df["REV_Q_LastQ"].isna() & df["REV_A_LATEST"].isna()
L215
L216     rows: list[dict] = []
L217     for ticker, row in df.iterrows():
L218         eps_missing = bool(miss_eps.loc[ticker])
L219         rev_missing = bool(miss_rev.loc[ticker])
L220         if not (eps_missing or rev_missing):
L221             continue
L222         rows.append({
L223             "ticker": ticker,
L224             "EPS_missing": eps_missing,
L225             "REV_missing": rev_missing,
L226             "eps_imputed": bool(row.get("eps_imputed", False)),
L227             "EPS_TTM": row.get("EPS_TTM"),
L228             "EPS_Q_LastQ": row.get("EPS_Q_LastQ"),
L229             "EPS_A_LATEST": row.get("EPS_A_LATEST"),
L230             "REV_TTM": row.get("REV_TTM"),
L231             "REV_Q_LastQ": row.get("REV_Q_LastQ"),
L232             "REV_A_LATEST": row.get("REV_A_LATEST"),
L233         })
L234
L235     if not rows:
L236         return pd.DataFrame(
L237             columns=[
L238                 "ticker",
L239                 "EPS_missing",
L240                 "REV_missing",
L241                 "eps_imputed",
L242                 "EPS_TTM",
L243                 "EPS_Q_LastQ",
L244                 "EPS_A_LATEST",
L245                 "REV_TTM",
L246                 "REV_Q_LastQ",
L247                 "REV_A_LATEST",
L248             ]
L249         )
L250
L251     return pd.DataFrame(rows)
L252
L253 _env_true = lambda name, default=False: (os.getenv(name) or str(default)).strip().lower() == "true"
L254
L255 def _post_slack(payload: dict):
L256     url = os.getenv("SLACK_WEBHOOK_URL")
L257     if not url: print("⚠️ SLACK_WEBHOOK_URL 未設定"); return
L258     try:
L259         requests.post(url, json=payload).raise_for_status()
L260     except Exception as e:
L261         print(f"⚠️ Slack通知エラー: {e}")
L262
L263 def _slack_send_text_chunks(url: str, text: str, chunk: int = 2800) -> None:
L264     """Slackへテキストを分割送信（コードブロック形式）。"""
L265
L266     def _post_text(payload: str) -> None:
L267         try:
L268             resp = requests.post(url, json={"text": payload})
L269             print(f"[DBG] debug_post status={getattr(resp,'status_code',None)} size={len(payload)}")
L270             if resp is not None:
L271                 resp.raise_for_status()
L272         except Exception as e:
L273             print(f"[ERR] debug_post_failed: {e}")
L274
L275     body = (text or "").strip()
L276     if not body:
L277         print("[DBG] skip debug send: empty body")
L278         return
L279
L280     block, block_len = [], 0
L281
L282     def _flush():
L283         nonlocal block, block_len
L284         if block:
L285             _post_text("```" + "\n".join(block) + "```")
L286             block, block_len = [], 0
L287
L288     for raw in body.splitlines():
L289         line = raw or ""
L290         while len(line) > chunk:
L291             head, line = line[:chunk], line[chunk:]
L292             _flush()
L293             _post_text("```" + head + "```")
L294         add_len = len(line) if not block else len(line) + 1
L295         if block and block_len + add_len > chunk:
L296             _flush(); add_len = len(line)
L297         block.append(line)
L298         block_len += add_len
L299     _flush()
L300
L301 def _disjoint_keepG(top_G, top_D, poolD):
L302     """G重複をDから除去し、poolDで順次補充（枯渇時は元銘柄維持）。"""
L303     used, D, i = set(top_G), list(top_D), 0
L304     for j, t in enumerate(D):
L305         if t not in used:
L306             continue
L307         while i < len(poolD) and (poolD[i] in used or poolD[i] in D):
L308             i += 1
L309         if i < len(poolD):
L310             D[j] = poolD[i]; used.add(D[j]); i += 1
L311     return top_G, D
L312
L313
L314 def _sticky_keep_current(agg: pd.Series, pick: list[str], incumbents: list[str],
L315                          n_target: int, delta_z: float, keep_buffer: int) -> list[str]:
L316     import pandas as pd, numpy as np
L317     sel = list(pick)
L318     if not sel: return sel
L319     ranked_sel = agg.reindex(sel).sort_values(ascending=False)
L320     kth = ranked_sel.iloc[min(len(sel), n_target)-1]
L321     std = agg.std()
L322     sigma = float(std) if pd.notna(std) else 0.0
L323     thresh = kth - delta_z * sigma
L324     ranked_all = agg.sort_values(ascending=False)
L325     cand = [t for t in incumbents if (t not in sel) and (t in agg.index)]
L326     for t in cand:
L327         within_score = pd.notna(agg[t]) and agg[t] >= thresh
L328         within_rank = t in ranked_all.index and ranked_all.index.get_loc(t) < n_target + keep_buffer
L329         if not (within_score or within_rank):
L330             continue
L331         non_inc = [x for x in sel if x not in incumbents]
L332         if not non_inc:
L333             break
L334         weakest = min(non_inc, key=lambda x: agg.get(x, -np.inf))
L335         if weakest in sel and agg.get(t, -np.inf) >= agg.get(weakest, -np.inf):
L336             sel.remove(weakest); sel.append(t)
L337     if len(sel) > n_target:
L338         sel = sorted(sel, key=lambda x: agg.get(x, -1e9), reverse=True)[:n_target]
L339     return sel
L340
L341
L342 # === Input：外部I/Oと前処理（CSV/API・欠損補完） ===
L343 class Input:
L344     def __init__(self, cand, exist, bench, price_max, finnhub_api_key=None):
L345         self.cand, self.exist, self.bench, self.price_max = cand, exist, bench, price_max
L346         self.api_key = finnhub_api_key or os.environ.get("FINNHUB_API_KEY")
L347
L348     # ---- （Input専用）EPS補完・FCF算出系 ----
L349     @staticmethod
L350     def _sec_headers():
L351         mail = (os.getenv("SEC_CONTACT_EMAIL") or "yasonba55@gmail.com").strip()
L352         app = (os.getenv("SEC_APP_NAME") or "FactorBot/1.0").strip()
L353         return {"User-Agent": f"{app} ({mail})", "From": mail, "Accept": "application/json"}
L354
L355     @staticmethod
L356     def _sec_get(url: str, retries: int = 3, backoff: float = 0.5):
L357         for i in range(retries):
L358             r = requests.get(url, headers=Input._sec_headers(), timeout=20)
L359             if r.status_code in (429, 503, 403):
L360                 time.sleep(min(2 ** i * backoff, 8.0))
L361                 continue
L362             r.raise_for_status(); return r.json()
L363         r.raise_for_status()
L364
L365     @staticmethod
L366     def _sec_ticker_map():
L367         import requests
L368
L369         url_primary = "https://data.sec.gov/api/xbrl/company_tickers.json"
L370         url_fallback = "https://www.sec.gov/files/company_tickers.json"
L371         mp = {}
L372         try:
L373             j = Input._sec_get(url_primary)  # 既存の堅牢GET（リトライ・バックオフ）
L374         except Exception:
L375             r = requests.get(url_fallback, headers=Input._sec_headers(), timeout=20)
L376             r.raise_for_status()
L377             j = r.json()
L378         # 形状A: {"0": {"ticker":..., "cik_str":...}, ...}
L379         if isinstance(j, dict) and "0" in j:
L380             for _, v in (j or {}).items():
L381                 try:
L382                     mp[str(v["ticker"]).upper()] = f"{int(v['cik_str']):010d}"
L383                 except Exception:
L384                     pass
L385         # 形状B: [{"ticker":..., "cik_str":...}, ...]
L386         elif isinstance(j, list):
L387             for v in j:
L388                 try:
L389                     mp[str(v.get("ticker")).upper()] = f"{int(v.get('cik_str')):010d}"
L390                 except Exception:
L391                     pass
L392         # 形状C: {"data":[[idx,ticker,title,cik_str],...]}
L393         elif isinstance(j, dict) and "data" in j:
L394             for row in j.get("data") or []:
L395                 try:
L396                     t = str(row[1]).upper()
L397                     c = int(row[3])
L398                     mp[t] = f"{c:010d}"
L399                 except Exception:
L400                     pass
L401         return mp
L402
L403     # --- 追加: ADR/OTC向けの簡易正規化（末尾Y/F, ドット等） ---
L404     @staticmethod
L405     def _normalize_ticker(sym: str) -> list[str]:
L406         s = (sym or "").upper().strip()
L407         # 追加: 先頭の$や全角の記号を除去
L408         s = s.lstrip("$").replace("＄", "").replace("．", ".").replace("－", "-")
L409         cand: list[str] = []
L410
L411         def add(x: str) -> None:
L412             if x and x not in cand:
L413                 cand.append(x)
L414
L415         # 1) 原文を最優先（SECは BRK.B, BF.B など . を正式採用）
L416         add(s)
L417         # 2) Yahoo系バリアント（. と - の揺れを相互に）
L418         if "." in s:
L419             add(s.replace(".", "-"))
L420             add(s.replace(".", ""))
L421         if "-" in s:
L422             add(s.replace("-", "."))
L423             add(s.replace("-", ""))
L424         # 3) ドット・ハイフン・ピリオド無し版（最後の保険）
L425         add(s.replace("-", "").replace(".", ""))
L426         # 4) ADR簡易：末尾Y/Fの除去（SECマップは本体ティッカーを持つことがある）
L427         if len(s) >= 2 and s[-1] in {"Y", "F"}:
L428             add(s[:-1])
L429         return cand
L430
L431     @staticmethod
L432     def _sec_companyfacts(cik: str):
L433         return Input._sec_get(f"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json")
L434
L435     @staticmethod
L436     def _units_for_tags(facts: dict, namespaces: list[str], tags: list[str]) -> list[dict]:
L437         """facts から namespace/tag を横断して units 配列を収集（存在順に連結）。"""
L438         out: list[dict] = []
L439         facts = (facts or {}).get("facts", {})
L440         for ns in namespaces:
L441             node = facts.get(ns, {}) if isinstance(facts, dict) else {}
L442             for tg in tags:
L443                 try:
L444                     units = node[tg]["units"]
L445                 except Exception:
L446                     continue
L447                 picks: list[dict] = []
L448                 if "USD/shares" in units:
L449                     picks.extend(list(units["USD/shares"]))
L450                 if "USD" in units:
L451                     picks.extend(list(units["USD"]))
L452                 if not picks:
L453                     for arr in units.values():
L454                         picks.extend(list(arr))
L455                 out.extend(picks)
L456         return out
L457
L458     @staticmethod
L459     def _only_quarterly(arr: list[dict]) -> list[dict]:
L460         """companyfactsの混在配列から『四半期』だけを抽出。
L461
L462         - frame に "Q" を含む（例: CY2024Q2I）
L463         - fp が Q1/Q2/Q3/Q4
L464         - form が 10-Q/10-Q/A/6-K
L465         """
L466         if not arr:
L467             return []
L468         q_forms = {"10-Q", "10-Q/A", "6-K"}
L469         out = [
L470             x
L471             for x in arr
L472             if (
L473                 "Q" in (x.get("frame") or "").upper()
L474                 or (x.get("fp") or "").upper() in {"Q1", "Q2", "Q3", "Q4"}
L475                 or (x.get("form") or "").upper() in q_forms
L476             )
L477         ]
L478         out.sort(key=lambda x: (x.get("end") or ""), reverse=True)
L479         return out
L480
L481     @staticmethod
L482     def _series_from_facts_with_dates(arr, key_val="val", key_dt="end", normalize=float):
L483         """companyfactsアイテム配列から (date,value) を返す。dateはYYYY-MM-DDを想定。"""
L484         out: List[Tuple[str, float]] = []
L485         for x in (arr or []):
L486             try:
L487                 d = x.get(key_dt)
L488                 if d is None:
L489                     continue
L490                 v = x.get(key_val)
L491                 out.append((str(d), normalize(v) if v is not None else float("nan")))
L492             except Exception:
L493                 continue
L494         out.sort(key=lambda t: t[0], reverse=True)
L495         return out
L496
L497     def _series_q_and_a(self, facts: list[dict]) -> tuple[list[Tuple[str, float]], list[Tuple[str, float]]]:
L498         """四半期・年次の両seriesを抽出して返す（formで簡易判定）。"""
L499         if not facts:
L500             return [], []
L501         q_items = self._only_quarterly(list(facts))
L502         annual_forms = {"10-K", "10-K/A", "20-F", "20-F/A"}
L503         a_items = [x for x in facts if str((x or {}).get("form", "")).upper() in annual_forms]
L504         a_items.sort(key=lambda x: (x.get("end") or ""), reverse=True)
L505         return self._series_from_facts_with_dates(q_items), self._series_from_facts_with_dates(a_items)
L506
L507     @staticmethod
L508     def _ttm_from_q_or_a(q_vals: list[float], a_vals: list[float]) -> float:
L509         """四半期TTM（4本合算）を優先し、欠損時は年次値で補完。"""
L510         import math
L511
L512         def _clean(vals: list[float]) -> list[float]:
L513             out: list[float] = []
L514             for v in vals:
L515                 try:
L516                     f = float(v)
L517                 except Exception:
L518                     continue
L519                 if math.isfinite(f):
L520                     out.append(f)
L521                 else:
L522                     out.append(float("nan"))
L523             return out
L524
L525         def _sum4(vs: list[float]) -> float:
L526             filtered = [v for v in vs[:4] if v == v]
L527             if len(filtered) >= 2:
L528                 return float(sum(filtered))
L529             if len(filtered) == 1:
L530                 return float(filtered[0])
L531             return float("nan")
L532
L533         q_clean = _clean(q_vals or [])
L534         ttm_q = _sum4(q_clean)
L535         if ttm_q == ttm_q:
L536             return ttm_q
L537         for v in _clean(a_vals or []):
L538             if v == v:
L539                 return float(v)
L540         return float("nan")
L541
L542     def fetch_eps_rev_from_sec(self, tickers: list[str]) -> dict:
L543         out = {}
L544         t2cik = self._sec_ticker_map()
L545         n_map = n_rev = n_eps = 0
L546         miss_map: list[str] = []
L547         miss_facts: list[str] = []
L548         for t in tickers:
L549             base = (t or "").upper()
L550             candidates: list[str] = []
L551             for key in [base, *self._normalize_ticker(t)]:
L552                 if key and key not in candidates:
L553                     candidates.append(key)
L554             cik = next((t2cik.get(key) for key in candidates if t2cik.get(key)), None)
L555             if not cik:
L556                 out[t] = {}
L557                 miss_map.append(t)
L558                 continue
L559             try:
L560                 j = self._sec_companyfacts(cik)
L561                 facts = j or {}
L562                 rev_tags = [
L563                     "Revenues",
L564                     "RevenueFromContractWithCustomerExcludingAssessedTax",
L565                     "SalesRevenueNet",
L566                     "SalesRevenueGoodsNet",
L567                     "SalesRevenueServicesNet",
L568                     "Revenue",
L569                 ]
L570                 eps_tags = [
L571                     "EarningsPerShareDiluted",
L572                     "EarningsPerShareBasicAndDiluted",
L573                     "EarningsPerShare",
L574                     "EarningsPerShareBasic",
L575                 ]
L576                 rev_arr = self._units_for_tags(facts, ["us-gaap", "ifrs-full"], rev_tags)
L577                 eps_arr = self._units_for_tags(facts, ["us-gaap", "ifrs-full"], eps_tags)
L578                 rev_q_pairs, rev_a_pairs = self._series_q_and_a(rev_arr)
L579                 eps_q_pairs, eps_a_pairs = self._series_q_and_a(eps_arr)
L580
L581                 rev_q_pairs = rev_q_pairs[:12]
L582                 eps_q_pairs = eps_q_pairs[:12]
L583                 rev_a_pairs = rev_a_pairs[:6]
L584                 eps_a_pairs = eps_a_pairs[:6]
L585
L586                 def _vals(pairs: list[tuple[str, float]]) -> list[float]:
L587                     vals: list[float] = []
L588                     for _d, v in pairs:
L589                         try:
L590                             vals.append(float(v))
L591                         except Exception:
L592                             vals.append(float("nan"))
L593                     return vals
L594
L595                 rev_q_vals = _vals(rev_q_pairs)
L596                 eps_q_vals = _vals(eps_q_pairs)
L597                 rev_a_vals = _vals(rev_a_pairs)
L598                 eps_a_vals = _vals(eps_a_pairs)
L599
L600                 def _first_valid(vals: list[float]) -> float:
L601                     for v in vals:
L602                         if v == v:
L603                             return float(v)
L604                     return float("nan")
L605
L606                 def _nth_valid(vals: list[float], n: int) -> float:
L607                     idx = 0
L608                     for v in vals:
L609                         if v == v:
L610                             if idx == n:
L611                                 return float(v)
L612                             idx += 1
L613                     return float("nan")
L614
L615                 def _quarter_from_annual(vals: list[float]) -> float:
L616                     v = _first_valid(vals)
L617                     return float(v / 4.0) if v == v else float("nan")
L618
L619                 def _quarter_from_annual_prev(vals: list[float]) -> float:
L620                     v = _nth_valid(vals, 1)
L621                     return float(v / 4.0) if v == v else float("nan")
L622
L623                 rev_lastq = _first_valid(rev_q_vals)
L624                 if rev_lastq != rev_lastq:
L625                     rev_lastq = _quarter_from_annual(rev_a_vals)
L626                 eps_lastq = _first_valid(eps_q_vals)
L627                 if eps_lastq != eps_lastq:
L628                     eps_lastq = _quarter_from_annual(eps_a_vals)
L629
L630                 rev_lastq_prev = _nth_valid(rev_q_vals, 4)
L631                 if rev_lastq_prev != rev_lastq_prev:
L632                     rev_lastq_prev = _quarter_from_annual_prev(rev_a_vals)
L633                 eps_lastq_prev = _nth_valid(eps_q_vals, 4)
L634                 if eps_lastq_prev != eps_lastq_prev:
L635                     eps_lastq_prev = _quarter_from_annual_prev(eps_a_vals)
L636
L637                 rev_ttm = self._ttm_from_q_or_a(rev_q_vals, rev_a_vals)
L638                 eps_ttm = self._ttm_from_q_or_a(eps_q_vals, eps_a_vals)
L639                 rev_ttm_prev = self._ttm_from_q_or_a(rev_q_vals[4:], rev_a_vals[1:])
L640                 eps_ttm_prev = self._ttm_from_q_or_a(eps_q_vals[4:], eps_a_vals[1:])
L641
L642                 rev_annual_latest = _first_valid(rev_a_vals)
L643                 rev_annual_prev = _nth_valid(rev_a_vals, 1)
L644                 eps_annual_latest = _first_valid(eps_a_vals)
L645                 eps_annual_prev = _nth_valid(eps_a_vals, 1)
L646
L647                 def _cagr3(vals: list[float]) -> float:
L648                     vals_valid = [v for v in vals if v == v]
L649                     if len(vals_valid) >= 3:
L650                         latest, base = float(vals_valid[0]), float(vals_valid[2])
L651                         if latest > 0 and base > 0:
L652                             try:
L653                                 return float((latest / base) ** (1 / 2) - 1.0)
L654                             except Exception:
L655                                 return float("nan")
L656                     return float("nan")
L657
L658                 rev_cagr3 = _cagr3(rev_a_vals)
L659                 eps_cagr3 = _cagr3(eps_a_vals)
L660
L661                 out[t] = {
L662                     "eps_q_recent": eps_lastq,
L663                     "eps_ttm": eps_ttm,
L664                     "eps_ttm_prev": eps_ttm_prev,
L665                     "eps_lastq_prev": eps_lastq_prev,
L666                     "rev_q_recent": rev_lastq,
L667                     "rev_ttm": rev_ttm,
L668                     "rev_ttm_prev": rev_ttm_prev,
L669                     "rev_lastq_prev": rev_lastq_prev,
L670                     # 後段でDatetimeIndex化できるよう (date,value) を保持。値だけの互換キーも残す。
L671                     # 3年運用に合わせて四半期は直近12本のみ保持（約3年=12Q）
L672                     "eps_q_series_pairs": eps_q_pairs,
L673                     "rev_q_series_pairs": rev_q_pairs,
L674                     "eps_q_series": eps_q_vals,
L675                     "rev_q_series": rev_q_vals,
L676                     "eps_a_series_pairs": eps_a_pairs,
L677                     "rev_a_series_pairs": rev_a_pairs,
L678                     "eps_a_series": eps_a_vals,
L679                     "rev_a_series": rev_a_vals,
L680                     "eps_annual_latest": eps_annual_latest,
L681                     "eps_annual_prev": eps_annual_prev,
L682                     "rev_annual_latest": rev_annual_latest,
L683                     "rev_annual_prev": rev_annual_prev,
L684                     "eps_cagr3": eps_cagr3,
L685                     "rev_cagr3": rev_cagr3,
L686                 }
L687                 n_map += 1
L688                 if any(v == v for v in rev_q_vals) or any(v == v for v in rev_a_vals):
L689                     n_rev += 1
L690                 if any(v == v for v in eps_q_vals) or any(v == v for v in eps_a_vals):
L691                     n_eps += 1
L692             except Exception:
L693                 out[t] = {}
L694                 miss_facts.append(t)
L695             time.sleep(0.30)
L696         # 取得サマリをログ（Actionsで確認しやすいよう print）
L697         try:
L698             total = len(tickers)
L699             print(f"[SEC] map={n_map}/{total}  rev_q_hit={n_rev}  eps_q_hit={n_eps}")
L700             # デバッグ: 取得本数の分布（先頭のみ）
L701             try:
L702                 lens = [len((out.get(t, {}) or {}).get("rev_q_series", [])) for t in tickers]
L703                 print(f"[SEC] rev_q_series length: min={min(lens) if lens else 0} "
L704                       f"p25={np.percentile(lens,25) if lens else 0} median={np.median(lens) if lens else 0} "
L705                       f"p75={np.percentile(lens,75) if lens else 0} max={max(lens) if lens else 0}")
L706             except Exception:
L707                 pass
L708             if miss_map:
L709                 print(f"[SEC] no CIK map: {len(miss_map)} (サンプル例) {miss_map[:20]}")
L710             if miss_facts:
L711                 print(f"[SEC] CIKあり だが対象factなし: {len(miss_facts)} (サンプル例) {miss_facts[:20]}")
L712         except Exception:
L713             pass
L714         return out
L715
L716     def sec_dryrun_sample(self, tickers: list[str] | None = None) -> None:
L717         if not _env_true("SEC_DRYRUN_SAMPLE", False):
L718             return
L719         sample = tickers or ["BRK.B", "BF.B", "GOOGL", "META", "UBER", "PBR.A", "TSM", "NARI", "EVBN", "SWAV"]
L720         print(f"[SEC-DRYRUN] sample tickers: {sample}")
L721         try:
L722             t2cik = self._sec_ticker_map()
L723             hits = 0
L724             for sym in sample:
L725                 candidates: list[str] = []
L726
L727                 def add(key: str) -> None:
L728                     if key and key not in candidates:
L729                         candidates.append(key)
L730
L731                 add((sym or "").upper())
L732                 for alt in self._normalize_ticker(sym):
L733                     add(alt)
L734                 if any(t2cik.get(key) for key in candidates):
L735                     hits += 1
L736             sec_data = self.fetch_eps_rev_from_sec(sample)
L737             rev_hits = sum(1 for v in sec_data.values() if v.get("rev_q_series"))
L738             eps_hits = sum(1 for v in sec_data.values() if v.get("eps_q_series"))
L739             total = len(sample)
L740             print(f"[SEC-DRYRUN] CIK map hit: {hits}/{total}  rev_q_series hits: {rev_hits}  eps_q_series hits: {eps_hits}")
L741         except Exception as e:
L742             print(f"[SEC-DRYRUN] error: {e}")
L743     @staticmethod
L744     def impute_eps_ttm(df: pd.DataFrame, ttm_col: str="eps_ttm", q_col: str="eps_q_recent", out_col: str|None=None) -> pd.DataFrame:
L745         out_col = out_col or ttm_col; df = df.copy(); df["eps_imputed"] = False
L746         cand = df[q_col]*4; ok = df[ttm_col].isna() & cand.replace([np.inf,-np.inf], np.nan).notna()
L747         df.loc[ok, out_col], df.loc[ok,"eps_imputed"] = cand[ok], True; return df
L748
L749     _CF_ALIASES = {"cfo":["Operating Cash Flow","Total Cash From Operating Activities"], "capex":["Capital Expenditure","Capital Expenditures"]}
L750
L751     @staticmethod
L752     def _pick_row(df: pd.DataFrame, names: list[str]) -> pd.Series|None:
L753         if df is None or df.empty: return None
L754         idx_lower={str(i).lower():i for i in df.index}
L755         for n in names:
L756             k=n.lower()
L757             if k in idx_lower: return df.loc[idx_lower[k]]
L758         return None
L759
L760     @staticmethod
L761     def _sum_last_n(s: pd.Series|None, n: int) -> float|None:
L762         if s is None or s.empty: return None
L763         v=s.dropna().astype(float); return None if v.empty else v.iloc[:n].sum()
L764
L765     @staticmethod
L766     def _latest(s: pd.Series|None) -> float|None:
L767         if s is None or s.empty: return None
L768         v=s.dropna().astype(float); return v.iloc[0] if not v.empty else None
L769
L770     def fetch_cfo_capex_ttm_yf(self, tickers: list[str]) -> pd.DataFrame:
L771         from concurrent.futures import ThreadPoolExecutor, as_completed
L772         pick, sumn, latest, aliases = self._pick_row, self._sum_last_n, self._latest, self._CF_ALIASES
L773
L774         def one(t: str):
L775             try:
L776                 tk = yf.Ticker(t)  # ★ セッションは渡さない（YFがcurl_cffiで管理）
L777                 qcf = tk.quarterly_cashflow
L778                 cfo_q, capex_q = pick(qcf, aliases["cfo"]), pick(qcf, aliases["capex"])
L779                 fcf_q = pick(qcf, ["Free Cash Flow","FreeCashFlow","Free cash flow"])
L780                 cfo, capex, fcf = sumn(cfo_q,4), sumn(capex_q,4), sumn(fcf_q,4)
L781                 if any(v is None for v in (cfo, capex, fcf)):
L782                     acf = tk.cashflow
L783                     if cfo   is None: cfo   = latest(pick(acf, aliases["cfo"]))
L784                     if capex is None: capex = latest(pick(acf, aliases["capex"]))
L785                     if fcf   is None: fcf   = latest(pick(acf, ["Free Cash Flow","FreeCashFlow","Free cash flow"]))
L786             except Exception as e:
L787                 print(f"[warn] yf financials error: {t}: {e}"); cfo=capex=fcf=None
L788             n=np.nan
L789             return {"ticker":t,
L790                     "cfo_ttm_yf":   n if cfo   is None else cfo,
L791                     "capex_ttm_yf": n if capex is None else capex,
L792                     "fcf_ttm_yf_direct": n if fcf is None else fcf}
L793
L794         rows, mw = [], int(os.getenv("FIN_THREADS","8"))
L795         with ThreadPoolExecutor(max_workers=mw) as ex:
L796             rows=[f.result() for f in as_completed(ex.submit(one,t) for t in tickers)]
L797         return pd.DataFrame(rows).set_index("ticker")
L798
L799     _FINN_CFO_KEYS = ["netCashProvidedByOperatingActivities","netCashFromOperatingActivities","cashFlowFromOperatingActivities","operatingCashFlow"]
L800     _FINN_CAPEX_KEYS = ["capitalExpenditure","capitalExpenditures","purchaseOfPPE","investmentsInPropertyPlantAndEquipment"]
L801
L802     @staticmethod
L803     def _first_key(d: dict, keys: list[str]):
L804         for k in keys:
L805             if k in d and d[k] is not None: return d[k]
L806         return None
L807
L808     @staticmethod
L809     def _finn_get(session: requests.Session, url: str, params: dict, retries: int=3, sleep_s: float=0.5):
L810         for i in range(retries):
L811             r = session.get(url, params=params, timeout=15)
L812             if r.status_code==429: time.sleep(min(2**i*sleep_s,4.0)); continue
L813             r.raise_for_status(); return r.json()
L814         r.raise_for_status()
L815
L816     def fetch_cfo_capex_ttm_finnhub(self, tickers: list[str], api_key: str|None=None) -> pd.DataFrame:
L817         api_key = api_key or os.getenv("FINNHUB_API_KEY")
L818         if not api_key: raise ValueError("Finnhub API key not provided. Set FINNHUB_API_KEY or pass api_key=")
L819         base, s, rows = "https://finnhub.io/api/v1", requests.Session(), []
L820         for sym in tickers:
L821             cfo_ttm = capex_ttm = None
L822             try:
L823                 j = self._finn_get(s, f"{base}/stock/cash-flow", {"symbol":sym,"frequency":"quarterly","limit":8,"token":api_key})
L824                 arr = j.get("cashFlow") or []; cfo_vals, capex_vals = [], []
L825                 for item in arr[:4]:
L826                     cfo_vals.append(self._first_key(item,self._FINN_CFO_KEYS)); capex_vals.append(self._first_key(item,self._FINN_CAPEX_KEYS))
L827                 if any(v is not None for v in cfo_vals): cfo_ttm = float(np.nansum([np.nan if v is None else float(v) for v in cfo_vals]))
L828                 if any(v is not None for v in capex_vals): capex_ttm = float(np.nansum([np.nan if v is None else float(v) for v in capex_vals]))
L829             except Exception: pass
L830             if cfo_ttm is None or capex_ttm is None:
L831                 try:
L832                     j = self._finn_get(s, f"{base}/stock/cash-flow", {"symbol":sym,"frequency":"annual","limit":1,"token":api_key})
L833                     arr = j.get("cashFlow") or []
L834                     if arr:
L835                         item0 = arr[0]
L836                         if cfo_ttm is None:
L837                             v = self._first_key(item0,self._FINN_CFO_KEYS)
L838                             if v is not None: cfo_ttm = float(v)
L839                         if capex_ttm is None:
L840                             v = self._first_key(item0,self._FINN_CAPEX_KEYS)
L841                             if v is not None: capex_ttm = float(v)
L842                 except Exception: pass
L843             rows.append({"ticker":sym,"cfo_ttm_fh":np.nan if cfo_ttm is None else cfo_ttm,"capex_ttm_fh":np.nan if capex_ttm is None else capex_ttm})
L844         return pd.DataFrame(rows).set_index("ticker")
L845
L846     def compute_fcf_with_fallback(self, tickers: list[str], finnhub_api_key: str|None=None) -> pd.DataFrame:
L847         yf_df = self.fetch_cfo_capex_ttm_yf(tickers)
L848         T.log("financials (yf) done")
L849         miss_mask = yf_df[["cfo_ttm_yf","capex_ttm_yf","fcf_ttm_yf_direct"]].isna().any(axis=1)
L850         need = yf_df.index[miss_mask].tolist(); print(f"[T] yf financials missing: {len(need)} {need[:10]}{'...' if len(need)>10 else ''}")
L851         if need:
L852             fh_df = self.fetch_cfo_capex_ttm_finnhub(need, api_key=finnhub_api_key)
L853             df = yf_df.join(fh_df, how="left")
L854             for col_yf, col_fh in [("cfo_ttm_yf","cfo_ttm_fh"),("capex_ttm_yf","capex_ttm_fh")]:
L855                 df[col_yf] = df[col_yf].fillna(df[col_fh])
L856             print("[T] financials (finnhub) done (fallback only)")
L857         else:
L858             df = yf_df.assign(cfo_ttm_fh=np.nan, capex_ttm_fh=np.nan)
L859             print("[T] financials (finnhub) skipped (no missing)")
L860         df["cfo_ttm"]  = df["cfo_ttm_yf"].where(df["cfo_ttm_yf"].notna(), df["cfo_ttm_fh"])
L861         df["capex_ttm"] = df["capex_ttm_yf"].where(df["capex_ttm_yf"].notna(), df["capex_ttm_fh"])
L862         cfo, capex = pd.to_numeric(df["cfo_ttm"], errors="coerce"), pd.to_numeric(df["capex_ttm"], errors="coerce").abs()
L863         fcf_calc = cfo - capex
L864         fcf_direct = pd.to_numeric(df.get("fcf_ttm_yf_direct"), errors="coerce")
L865         df["fcf_ttm"] = fcf_calc.where(fcf_calc.notna(), fcf_direct)
L866         df["cfo_source"]  = np.where(df["cfo_ttm_yf"].notna(),"yfinance",np.where(df["cfo_ttm_fh"].notna(),"finnhub",""))
L867         df["capex_source"] = np.where(df["capex_ttm_yf"].notna(),"yfinance",np.where(df["capex_ttm_fh"].notna(),"finnhub",""))
L868         df["fcf_imputed"] = df[["cfo_ttm","capex_ttm"]].isna().any(axis=1) & df["fcf_ttm"].notna()
L869         cols = ["cfo_ttm_yf","capex_ttm_yf","cfo_ttm_fh","capex_ttm_fh","cfo_ttm","capex_ttm","fcf_ttm","fcf_ttm_yf_direct","cfo_source","capex_source","fcf_imputed"]
L870         return df[cols].sort_index()
L871
L872     def _build_eps_df(self, tickers, tickers_bulk, info, sec_map: dict | None = None):
L873         eps_rows=[]
L874         for t in tickers:
L875             info_t = info[t]
L876             sec_t = (sec_map or {}).get(t, {})
L877             eps_ttm = sec_t.get("eps_ttm", info_t.get("trailingEps", np.nan))
L878             eps_q = sec_t.get("eps_q_recent", np.nan)
L879             try:
L880                 tk = tickers_bulk.tickers.get(t)
L881                 if tk is None:
L882                     sym = info_t.get("_yf_symbol") if isinstance(info_t, dict) else None
L883                     if sym:
L884                         tk = tickers_bulk.tickers.get(sym)
L885                 qearn = tk.quarterly_earnings if tk is not None else None
L886                 so = info_t.get("sharesOutstanding")
L887                 if so and qearn is not None and not qearn.empty and "Earnings" in qearn.columns:
L888                     eps_ttm_q = qearn["Earnings"].head(4).sum()/so
L889                     if pd.notna(eps_ttm_q) and (pd.isna(eps_ttm) or (abs(eps_ttm)>0 and abs(eps_ttm/eps_ttm_q)>3)): eps_ttm = eps_ttm_q
L890                     if pd.isna(eps_q):
L891                         eps_q = qearn["Earnings"].iloc[-1]/so
L892             except Exception: pass
L893             rev_ttm = sec_t.get("rev_ttm", np.nan)
L894             rev_q = sec_t.get("rev_q_recent", np.nan)
L895             if (not sec_t) or pd.isna(rev_ttm):
L896                 try:
L897                     tk = tickers_bulk.tickers.get(t)
L898                     if tk is None and isinstance(info_t, dict):
L899                         sym = info_t.get("_yf_symbol")
L900                         if sym:
L901                             tk = tickers_bulk.tickers.get(sym)
L902                     qfin = getattr(tk, "quarterly_financials", None)
L903                     if qfin is not None and not qfin.empty:
L904                         idx_lower = {str(i).lower(): i for i in qfin.index}
L905                         rev_idx = None
L906                         for name in ("Total Revenue", "TotalRevenue"):
L907                             key = name.lower()
L908                             if key in idx_lower:
L909                                 rev_idx = idx_lower[key]
L910                                 break
L911                         if rev_idx is not None:
L912                             rev_series = pd.to_numeric(qfin.loc[rev_idx], errors="coerce").dropna()
L913                             if not rev_series.empty:
L914                                 rev_ttm_yf = float(rev_series.head(4).sum())
L915                                 if pd.isna(rev_ttm):
L916                                     rev_ttm = rev_ttm_yf
L917                                 if pd.isna(rev_q):
L918                                     rev_q = float(rev_series.iloc[0])
L919                 except Exception:
L920                     pass
L921             eps_rows.append({
L922                 "ticker": t,
L923                 "eps_ttm": eps_ttm,
L924                 "eps_ttm_prev": sec_t.get("eps_ttm_prev", np.nan),
L925                 "eps_q_recent": eps_q,
L926                 "eps_q_prev": sec_t.get("eps_lastq_prev", np.nan),
L927                 "rev_ttm": rev_ttm,
L928                 "rev_ttm_prev": sec_t.get("rev_ttm_prev", np.nan),
L929                 "rev_q_recent": rev_q,
L930                 "rev_q_prev": sec_t.get("rev_lastq_prev", np.nan),
L931                 "eps_annual_latest": sec_t.get("eps_annual_latest", np.nan),
L932                 "eps_annual_prev": sec_t.get("eps_annual_prev", np.nan),
L933                 "rev_annual_latest": sec_t.get("rev_annual_latest", np.nan),
L934                 "rev_annual_prev": sec_t.get("rev_annual_prev", np.nan),
L935                 "eps_cagr3": sec_t.get("eps_cagr3", np.nan),
L936                 "rev_cagr3": sec_t.get("rev_cagr3", np.nan),
L937             })
L938         return self.impute_eps_ttm(pd.DataFrame(eps_rows).set_index("ticker"))
L939
L940     def prepare_data(self):
L941         """Fetch price and fundamental data for all tickers."""
L942         self.sec_dryrun_sample()
L943         # --- yfinance 用にティッカーを正規化（"$"剥がし、"."→"-"） ---
L944         def _to_yf(sym: str) -> str:
L945             s = (sym or "").strip().lstrip("$").replace("＄", "")
L946             # BRK.B / PBR.A などは Yahoo では '-' を使用
L947             yf_sym = s.replace("．", ".").replace(".", "-")
L948             return yf_sym or (sym or "")
L949
L950         cand_y = [_to_yf(t) for t in self.cand]
L951         cand_info = yf.Tickers(" ".join(cand_y))
L952
L953         def _price(orig: str, ysym: str) -> float:
L954             try:
L955                 return cand_info.tickers[ysym].fast_info.get("lastPrice", np.inf)
L956             except Exception as e:
L957                 print(f"{orig}: price fetch failed ({e})")
L958                 return np.inf
L959
L960         cand_prices = {orig: _price(orig, ysym) for orig, ysym in zip(self.cand, cand_y)}
L961         cand_f = [t for t, p in cand_prices.items() if p <= self.price_max]
L962         T.log("price cap filter done (CAND_PRICE_MAX)")
L963         # 入力ティッカーの重複を除去し、現行→候補の順序を維持
L964         # ユニバース確定（元ティッカー保持）。yfinance には後で変換して渡す
L965         tickers = list(dict.fromkeys(self.exist + cand_f))
L966         yf_map = {t: _to_yf(t) for t in tickers}
L967         yf_list = list(dict.fromkeys([yf_map[t] for t in tickers]))
L968         T.log(f"universe prepared: unique={len(tickers)} bench={self.bench}")
L969         data = yf.download(yf_list + [self.bench], period="600d",
L970                            auto_adjust=True, progress=False, threads=False)
L971         T.log("yf.download done")
L972         inv = {v: k for k, v in yf_map.items()}
L973         px = data["Close"].dropna(how="all", axis=1).ffill(limit=2)
L974         px = px.rename(columns=inv)
L975         try:
L976             if isinstance(data.columns, pd.MultiIndex):
L977                 data = data.rename(columns=inv, level=1)
L978             else:
L979                 data = data.rename(columns=inv)
L980         except Exception:
L981             pass
L982         spx = data["Close"][self.bench].reindex(px.index).ffill()
L983         clip_days = int(os.getenv("PRICE_CLIP_DAYS", "0"))   # 0なら無効（既定）
L984         if clip_days > 0:
L985             px, spx = px.tail(clip_days + 1), spx.tail(clip_days + 1)
L986             logger.info("[T] price window clipped by env: %d rows (PRICE_CLIP_DAYS=%d)", len(px), clip_days)
L987         else:
L988             logger.info("[T] price window clip skipped; rows=%d", len(px))
L989         tickers_bulk, info = yf.Tickers(" ".join(yf_list)), {}
L990         for orig, ysym in yf_map.items():
L991             if ysym in tickers_bulk.tickers:
L992                 tickers_bulk.tickers[orig] = tickers_bulk.tickers[ysym]
L993         for t in tickers:
L994             try:
L995                 tk = tickers_bulk.tickers.get(t) or tickers_bulk.tickers.get(yf_map[t])
L996                 info_entry = tk.info if tk is not None else {}
L997                 if not isinstance(info_entry, dict):
L998                     info_entry = {}
L999                 info_entry.setdefault("_yf_symbol", getattr(tk, "ticker", yf_map.get(t)))
L1000                 info[t] = info_entry
L1001             except Exception as e:
L1002                 logger.info("[warn] %s: info fetch failed (%s)", t, e)
L1003                 info[t] = {}
L1004         try:
L1005             sec_map = self.fetch_eps_rev_from_sec(tickers)
L1006         except Exception as e:
L1007             logger.warning("[SEC] fetch_eps_rev_from_sec failed: %s", e)
L1008             sec_map = {}
L1009
L1010         def _brief_len(s):
L1011             try:
L1012                 if isinstance(s, pd.Series):
L1013                     return int(s.dropna().size)
L1014                 if isinstance(s, (list, tuple)):
L1015                     return len([v for v in s if pd.notna(v)])
L1016                 if isinstance(s, np.ndarray):
L1017                     return int(np.count_nonzero(~pd.isna(s)))
L1018                 return int(bool(s))
L1019             except Exception:
L1020                 return 0
L1021
L1022         def _has_entries(val) -> bool:
L1023             try:
L1024                 if isinstance(val, pd.Series):
L1025                     return not val.dropna().empty
L1026                 if isinstance(val, (list, tuple)):
L1027                     return any(pd.notna(v) for v in val)
L1028                 return bool(val)
L1029             except Exception:
L1030                 return False
L1031
L1032         have_rev = 0
L1033         have_eps = 0
L1034         rev_lens: list[int] = []
L1035         eps_lens: list[int] = []
L1036         rev_y_lens: list[int] = []
L1037         samples: list[tuple[str, int, str, float | None, int, str, float | None]] = []
L1038
L1039         for t in tickers:
L1040             entry = info.get(t, {})
L1041             m = (sec_map or {}).get(t) or {}
L1042             if entry is None or not isinstance(entry, dict):
L1043                 entry = {}
L1044                 info[t] = entry
L1045
L1046             if m:
L1047                 pairs_r = m.get("rev_q_series_pairs") or []
L1048                 pairs_e = m.get("eps_q_series_pairs") or []
L1049                 if pairs_r:
L1050                     idx = pd.to_datetime([d for (d, _v) in pairs_r], errors="coerce")
L1051                     val = pd.to_numeric([v for (_d, v) in pairs_r], errors="coerce")
L1052                     s = pd.Series(val, index=idx).sort_index()
L1053                     entry["SEC_REV_Q_SERIES"] = s
L1054                 else:
L1055                     entry["SEC_REV_Q_SERIES"] = m.get("rev_q_series") or []
L1056                 if pairs_e:
L1057                     idx = pd.to_datetime([d for (d, _v) in pairs_e], errors="coerce")
L1058                     val = pd.to_numeric([v for (_d, v) in pairs_e], errors="coerce")
L1059                     s = pd.Series(val, index=idx).sort_index()
L1060                     entry["SEC_EPS_Q_SERIES"] = s
L1061                 else:
L1062                     entry["SEC_EPS_Q_SERIES"] = m.get("eps_q_series") or []
L1063
L1064             r = entry.get("SEC_REV_Q_SERIES")
L1065             e = entry.get("SEC_EPS_Q_SERIES")
L1066             # 年次は直近3件（約3年）だけ保持。重み分岐の nY 判定は従来通り。
L1067             try:
L1068                 if hasattr(r, "index") and isinstance(r.index, pd.DatetimeIndex):
L1069                     y = r.resample("Y").sum().dropna()
L1070                     entry["SEC_REV_Y_SERIES"] = y.tail(3)
L1071                 else:
L1072                     entry["SEC_REV_Y_SERIES"] = []
L1073             except Exception:
L1074                 entry["SEC_REV_Y_SERIES"] = []
L1075             ry = entry.get("SEC_REV_Y_SERIES")
L1076             if _has_entries(r):
L1077                 have_rev += 1
L1078             if _has_entries(e):
L1079                 have_eps += 1
L1080             lr = _brief_len(r)
L1081             le = _brief_len(e)
L1082             rev_lens.append(lr)
L1083             eps_lens.append(le)
L1084             rev_y_lens.append(_brief_len(ry))
L1085             if len(samples) < 8:
L1086                 try:
L1087                     rd = getattr(r, "index", [])[-1] if lr > 0 else None
L1088                     rv = float(r.iloc[-1]) if lr > 0 else None
L1089                     ed = getattr(e, "index", [])[-1] if le > 0 else None
L1090                     ev = float(e.iloc[-1]) if le > 0 else None
L1091                     samples.append((t, lr, str(rd) if rd is not None else "-", rv, le, str(ed) if ed is not None else "-", ev))
L1092                 except Exception:
L1093                     samples.append((t, lr, "-", None, le, "-", None))
L1094
L1095         logger.info("[SEC] series attach: rev_q=%d/%d, eps_q=%d/%d", have_rev, len(tickers), have_eps, len(tickers))
L1096         logger.info(
L1097             "[SEC_SERIES] rev_q=%d (<=12), eps_q=%d (<=12), rev_y=%d (<=3)",
L1098             max(rev_lens) if rev_lens else 0,
L1099             max(eps_lens) if eps_lens else 0,
L1100             max(rev_y_lens) if rev_y_lens else 0,
L1101         )
L1102
L1103         if rev_lens:
L1104             rev_lens_sorted = sorted(rev_lens)
L1105             eps_lens_sorted = sorted(eps_lens)
L1106             _log(
L1107                 "SEC_SERIES",
L1108                 f"rev_len min/med/max={rev_lens_sorted[0]}/{rev_lens_sorted[len(rev_lens)//2]}/{rev_lens_sorted[-1]} "
L1109                 f"eps_len min/med/max={eps_lens_sorted[0]}/{eps_lens_sorted[len(eps_lens)//2]}/{eps_lens_sorted[-1]}",
L1110             )
L1111         for (t, lr, rd, rv, le, ed, ev) in samples:
L1112             _log("SEC_SERIES_SMP", f"{t}  rev_len={lr} last=({rd},{rv})  eps_len={le} last=({ed},{ev})")
L1113         eps_df = self._build_eps_df(tickers, tickers_bulk, info, sec_map=sec_map)
L1114         # index 重複があると .loc[t, col] が Series になり代入時に ValueError を誘発する
L1115         if not eps_df.index.is_unique:
L1116             eps_df = eps_df[~eps_df.index.duplicated(keep="last")]
L1117         eps_df = eps_df.assign(
L1118             EPS_TTM=eps_df["eps_ttm"],
L1119             EPS_TTM_PREV=eps_df.get("eps_ttm_prev", np.nan),
L1120             EPS_Q_LastQ=eps_df["eps_q_recent"],
L1121             EPS_Q_Prev=eps_df.get("eps_q_prev", np.nan),
L1122             REV_TTM=eps_df["rev_ttm"],
L1123             REV_TTM_PREV=eps_df.get("rev_ttm_prev", np.nan),
L1124             REV_Q_LastQ=eps_df["rev_q_recent"],
L1125             REV_Q_Prev=eps_df.get("rev_q_prev", np.nan),
L1126             EPS_A_LATEST=eps_df.get("eps_annual_latest", np.nan),
L1127             EPS_A_PREV=eps_df.get("eps_annual_prev", np.nan),
L1128             REV_A_LATEST=eps_df.get("rev_annual_latest", np.nan),
L1129             REV_A_PREV=eps_df.get("rev_annual_prev", np.nan),
L1130             EPS_A_CAGR3=eps_df.get("eps_cagr3", np.nan),
L1131             REV_A_CAGR3=eps_df.get("rev_cagr3", np.nan),
L1132         )
L1133         missing_logs = _build_missing_logs_after_impute(eps_df)
L1134         # ここで非NaN件数をサマリ表示（欠損状況の即時把握用）
L1135         try:
L1136             n = len(eps_df)
L1137             c_eps = int(eps_df["EPS_TTM"].notna().sum())
L1138             c_rev = int(eps_df["REV_TTM"].notna().sum())
L1139             print(f"[SEC] eps_ttm non-NaN: {c_eps}/{n}  rev_ttm non-NaN: {c_rev}/{n}")
L1140         except Exception:
L1141             pass
L1142         fcf_df = self.compute_fcf_with_fallback(tickers, finnhub_api_key=self.api_key)
L1143         T.log("eps/fcf prep done")
L1144         returns = px[tickers].pct_change()
L1145         T.log("price prep/returns done")
L1146         return dict(cand=cand_f, tickers=tickers, data=data, px=px, spx=spx, tickers_bulk=tickers_bulk, info=info, eps_df=eps_df, fcf_df=fcf_df, returns=returns, missing_logs=missing_logs)
L1147
L1148 # === Selector：相関低減・選定（スコア＆リターンだけ読む） ===
L1149 class Selector:
L1150     # ---- DRRS helpers（Selector専用） ----
L1151     @staticmethod
L1152     def _z_np(X: np.ndarray) -> np.ndarray:
L1153         X = np.asarray(X, dtype=np.float32)
L1154         m = np.nanmean(X, axis=0, keepdims=True)
L1155         s = np.nanstd(X, axis=0, keepdims=True)
L1156         # 分母0/全NaN列の安全化：std==0 を 1 に置換（z=0に収束）
L1157         s = np.where(np.isfinite(s) & (s > 0), s, 1.0).astype(np.float32)
L1158         with np.errstate(invalid="ignore", divide="ignore"):
L1159             Z = (np.nan_to_num(X) - np.nan_to_num(m)) / s
L1160         return np.nan_to_num(Z)
L1161
L1162     @classmethod
L1163     def residual_corr(cls, R: np.ndarray, n_pc: int=3, shrink: float=0.1) -> np.ndarray:
L1164         Z = cls._z_np(R); U,S,_ = np.linalg.svd(Z, full_matrices=False); F = U[:,:n_pc]*S[:n_pc]; B = np.linalg.lstsq(F, Z, rcond=None)[0]
L1165         E = Z - F@B; C = np.corrcoef(E, rowvar=False)
L1166         off = C - np.diag(np.diag(C)); iu = np.triu_indices_from(off,1); avg_abs = np.nanmean(np.abs(off[iu])) if iu[0].size else 0.0
L1167         shrink_eff = float(np.clip(shrink + 0.5*avg_abs, 0.1, 0.6)); N = C.shape[0]
L1168         return (1.0 - shrink_eff)*C + shrink_eff*np.eye(N, dtype=C.dtype)
L1169
L1170     @classmethod
L1171     def rrqr_like_det(cls, R: np.ndarray, score: np.ndarray, k: int, gamma: float=1.0):
L1172         Z, w = cls._z_np(R), (score-score.min())/(np.ptp(score)+1e-12); X = Z*(1.0 + gamma*w)
L1173         N, k = X.shape[1], max(0, min(k, X.shape[1]))
L1174         if k==0: return []
L1175         S, selected, Rres = [], np.zeros(N, dtype=bool), X.copy()
L1176         for _ in range(k):
L1177             norms = (Rres*Rres).sum(axis=0); cand = np.where(~selected)[0]
L1178             j = sorted(cand, key=lambda c:(-norms[c], -w[c], c))[0]
L1179             S.append(j); selected[j]=True; u = X[:,j:j+1]; u/=(np.linalg.norm(u)+1e-12); Rres = Rres - u @ (u.T @ Rres)
L1180         return sorted(S)
L1181
L1182     @staticmethod
L1183     def _obj(corrM: np.ndarray, score: np.ndarray, idx, lam: float) -> float:
L1184         idx = list(idx); P = corrM[np.ix_(idx, idx)]; s = (score-score.mean())/(score.std()+1e-9)
L1185         return float(s[idx].sum() - lam*((P.sum()-np.trace(P))/2.0))
L1186
L1187     @classmethod
L1188     def swap_local_det(cls, corrM: np.ndarray, score: np.ndarray, idx, lam: float=0.6, max_pass: int=15):
L1189         S, best, improved, passes = sorted(idx), cls._obj(corrM, score, idx, lam), True, 0
L1190         while improved and passes<max_pass:
L1191             improved, passes = False, passes+1
L1192             for i,out in enumerate(list(S)):
L1193                 for inn in range(len(score)):
L1194                     if inn in S: continue
L1195                     cand = sorted(S[:i]+[inn]+S[i+1:]); v = cls._obj(corrM, score, cand, lam)
L1196                     if v>best+1e-10: S, best, improved = cand, v, True; break
L1197                 if improved: break
L1198         return S, best
L1199
L1200     @staticmethod
L1201     def _obj_with_cross(C_within: np.ndarray, C_cross: np.ndarray|None, score: np.ndarray, idx, lam: float, mu: float) -> float:
L1202         idx = list(idx); P = C_within[np.ix_(idx, idx)]; s = (score-score.mean())/(score.std()+1e-9)
L1203         within = (P.sum()-np.trace(P))/2.0; cross = 0.0
L1204         if C_cross is not None and C_cross.size>0: cross = C_cross[idx,:].sum()
L1205         return float(s[idx].sum() - lam*within - mu*cross)
L1206
L1207     @classmethod
L1208     def swap_local_det_cross(cls, C_within: np.ndarray, C_cross: np.ndarray|None, score: np.ndarray, idx, lam: float=0.6, mu: float=0.3, max_pass: int=15):
L1209         S, best, improved, passes, N = sorted(idx), cls._obj_with_cross(C_within,C_cross,score,idx,lam,mu), True, 0, len(score)
L1210         while improved and passes<max_pass:
L1211             improved, passes = False, passes+1
L1212             for i,out in enumerate(list(S)):
L1213                 for inn in range(N):
L1214                     if inn in S: continue
L1215                     cand = sorted(S[:i]+[inn]+S[i+1:]); v = cls._obj_with_cross(C_within,C_cross,score,cand,lam,mu)
L1216                     if v>best+1e-10: S, best, improved = cand, v, True; break
L1217                 if improved: break
L1218         return S, best
L1219
L1220     @staticmethod
L1221     def avg_corr(C: np.ndarray, idx) -> float:
L1222         k = len(idx); P = C[np.ix_(idx, idx)]
L1223         return float((P.sum()-np.trace(P))/(k*(k-1)+1e-12))
L1224
L1225     @classmethod
L1226     def select_bucket_drrs(cls, returns_df: pd.DataFrame, score_ser: pd.Series, pool_tickers: list[str], k: int, *, n_pc: int, gamma: float, lam: float, lookback: int, shrink: float=0.10, g_fixed_tickers: list[str]|None=None, mu: float=0.0):
L1227         g_fixed = [t for t in (g_fixed_tickers or []) if t in returns_df.columns]
L1228         union = [t for t in pool_tickers if t in returns_df.columns]
L1229         for t in g_fixed:
L1230             if t not in union: union.append(t)
L1231         Rdf_all = returns_df[union]; Rdf_all = Rdf_all.iloc[-lookback:] if len(Rdf_all)>=lookback else Rdf_all; Rdf_all = Rdf_all.dropna()
L1232         pool_eff, g_eff = [t for t in pool_tickers if t in Rdf_all.columns], [t for t in g_fixed if t in Rdf_all.columns]
L1233         if len(pool_eff)==0: return dict(idx=[], tickers=[], avg_res_corr=np.nan, sum_score=0.0, objective=-np.inf)
L1234         score = score_ser.reindex(pool_eff).to_numpy(dtype=np.float32)
L1235         C_all = cls.residual_corr(Rdf_all.to_numpy(), n_pc=n_pc, shrink=shrink)
L1236         col_pos = {c:i for i,c in enumerate(Rdf_all.columns)}; pool_pos = [col_pos[t] for t in pool_eff]
L1237         C_within, C_cross = C_all[np.ix_(pool_pos,pool_pos)], None
L1238         if len(g_eff)>0 and mu>0.0:
L1239             g_pos = [col_pos[t] for t in g_eff]; C_cross = C_all[np.ix_(pool_pos,g_pos)]
L1240         R_pool = Rdf_all[pool_eff].to_numpy(); S0 = cls.rrqr_like_det(R_pool, score, k, gamma=gamma)
L1241         S, Jn = (cls.swap_local_det_cross(C_within, C_cross, score, S0, lam=lam, mu=mu, max_pass=15) if C_cross is not None else cls.swap_local_det(C_within, score, S0, lam=lam, max_pass=15))
L1242         selected_tickers = [pool_eff[i] for i in S]
L1243         return dict(idx=S, tickers=selected_tickers, avg_res_corr=cls.avg_corr(C_within,S), sum_score=float(score[S].sum()), objective=float(Jn))
L1244
L1245     # ---- 選定（スコア Series / returns だけを受ける）----
L1246 # === Output：出力整形と送信（表示・Slack） ===
L1247 class Output:
L1248
L1249     def __init__(self, debug=None):
L1250         # self.debug は使わない（互換のため引数は受けるが無視）
L1251         self.miss_df = self.g_table = self.d_table = self.io_table = self.df_metrics_fmt = self.debug_table = None
L1252         self.g_title = self.d_title = ""
L1253         self.g_formatters = self.d_formatters = {}
L1254         # 低スコア（GSC+DSC）Top10 表示/送信用
L1255         self.low10_table = None
L1256         self.debug_text = ""   # デバッグ用本文はここに一本化
L1257         self._debug_logged = False
L1258         self._miss_disp_info: Tuple[pd.DataFrame, bool, int] | None = None
L1259
L1260     @staticmethod
L1261     def _prepare_missing_display(df: pd.DataFrame | None) -> Tuple[pd.DataFrame, bool, int]:
L1262         if df is None or df.empty:
L1263             return pd.DataFrame(), False, 0
L1264         work = df.copy()
L1265         if 'ticker' not in work.columns:
L1266             work = work.reset_index()
L1267             if 'ticker' not in work.columns and 'index' in work.columns:
L1268                 work = work.rename(columns={'index': 'ticker'})
L1269         bool_cols = [c for c in ['EPS_missing', 'REV_missing'] if c in work.columns]
L1270         if bool_cols:
L1271             work = work.loc[work[bool_cols].any(axis=1)]
L1272         if work.empty:
L1273             return pd.DataFrame(columns=work.columns), False, 0
L1274         cols_order = [
L1275             col for col in [
L1276                 'ticker',
L1277                 'EPS_missing',
L1278                 'REV_missing',
L1279                 'eps_imputed',
L1280                 'EPS_TTM',
L1281                 'EPS_Q_LastQ',
L1282                 'EPS_A_LATEST',
L1283                 'REV_TTM',
L1284                 'REV_Q_LastQ',
L1285                 'REV_A_LATEST',
L1286             ]
L1287             if col in work.columns
L1288         ]
L1289         if cols_order:
L1290             work = work.loc[:, cols_order]
L1291         total = len(work)
L1292         truncated = False
L1293         if total > 50:
L1294             work = work.head(20)
L1295             truncated = True
L1296         return work, truncated, total
L1297
L1298     # --- 表示（元 display_results のロジックそのまま） ---
L1299     def display_results(self, *, exist, bench, df_z, g_score, d_score_all,
L1300                         init_G, init_D, top_G, top_D, **kwargs):
L1301         logger.info("📌 reached display_results")
L1302         pd.set_option('display.float_format','{:.3f}'.format)
L1303         print("📈 ファクター分散最適化の結果")
L1304         miss_df, truncated, total = self._prepare_missing_display(self.miss_df)
L1305         self._miss_disp_info = (miss_df, truncated, total)
L1306         if not miss_df.empty:
L1307             print("Missing Data:")
L1308             print(miss_df.to_string(index=False))
L1309             if truncated:
L1310                 print(f"...省略 ({total}件中 上位20件のみ表示)")
L1311
L1312         # ---- 表示用：Changes/Near-Miss のスコア源を“最終集計”に統一するプロキシ ----
L1313         try:
L1314             sc = getattr(self, "_sc", None)
L1315             agg_G = getattr(sc, "_agg_G", None)
L1316             agg_D = getattr(sc, "_agg_D", None)
L1317         except Exception:
L1318             sc = agg_G = agg_D = None
L1319         class _SeriesProxy:
L1320             __slots__ = ("primary", "fallback")
L1321             def __init__(self, primary, fallback): self.primary, self.fallback = primary, fallback
L1322             def get(self, key, default=None):
L1323                 try:
L1324                     v = self.primary.get(key) if hasattr(self.primary, "get") else None
L1325                 except Exception:
L1326                     v = None
L1327                 if v is not None and not (isinstance(v, float) and v != v):
L1328                     return v
L1329                 try:
L1330                     return self.fallback.get(key) if hasattr(self.fallback, "get") else default
L1331                 except Exception:
L1332                     return default
L1333         g_score = _SeriesProxy(agg_G, g_score)
L1334         d_score_all = _SeriesProxy(agg_D, d_score_all)
L1335         near_G = getattr(sc, "_near_G", []) if sc else []
L1336         near_D = getattr(sc, "_near_D", []) if sc else []
L1337
L1338         extra_G = [t for t in init_G if t not in top_G][:5]; G_UNI = top_G + extra_G
L1339         gsc_series = pd.Series({t: g_score.get(t) for t in G_UNI}, name='GSC')
L1340         self.g_table = pd.concat([df_z.loc[G_UNI,['GROWTH_F','MOM','TRD','VOL']], gsc_series], axis=1)
L1341         self.g_table.index = [t + ("⭐️" if t in top_G else "") for t in G_UNI]
L1342         self.g_formatters = {col:"{:.2f}".format for col in ['GROWTH_F','MOM','TRD','VOL']}; self.g_formatters['GSC'] = "{:.3f}".format
L1343         self.g_title = (f"[G枠 / {N_G} / {_fmt_w(g_weights)} / corrM={corrM} / "
L1344                         f"LB={DRRS_G['lookback']} nPC={DRRS_G['n_pc']} γ={DRRS_G['gamma']} λ={DRRS_G['lam']} η={DRRS_G['eta']} shrink={DRRS_SHRINK}]")
L1345         if near_G:
L1346             add = [t for t in near_G if t not in set(G_UNI)][:10]
L1347             if len(add) < 10:
L1348                 try:
L1349                     aggG = getattr(sc, "_agg_G", pd.Series(dtype=float)).dropna().sort_values(ascending=False)
L1350                     out_now = sorted(set(exist) - set(top_G + top_D))  # 今回 OUT
L1351                     used = set(G_UNI + add)
L1352                     def _push(lst):
L1353                         nonlocal add, used
L1354                         for t in lst:
L1355                             if len(add) == 10: break
L1356                             if t in aggG.index and t not in used:
L1357                                 add.append(t); used.add(t)
L1358                     _push(out_now)           # ① 今回 OUT を優先
L1359                     _push(list(aggG.index))  # ② まだ足りなければ上位で充填
L1360                 except Exception:
L1361                     pass
L1362             if add:
L1363                 near_tbl = pd.concat([df_z.loc[add,['GROWTH_F','MOM','TRD','VOL']], pd.Series({t: g_score.get(t) for t in add}, name='GSC')], axis=1)
L1364                 self.g_table = pd.concat([self.g_table, near_tbl], axis=0)
L1365         print(self.g_title); print(self.g_table.to_string(formatters=self.g_formatters))
L1366
L1367         extra_D = [t for t in init_D if t not in top_D][:5]; D_UNI = top_D + extra_D
L1368         cols_D = ['QAL','YLD','VOL','TRD']; d_disp = pd.DataFrame(index=D_UNI)
L1369         d_disp['QAL'], d_disp['YLD'], d_disp['VOL'], d_disp['TRD'] = df_z.loc[D_UNI,'D_QAL'], df_z.loc[D_UNI,'D_YLD'], df_z.loc[D_UNI,'D_VOL_RAW'], df_z.loc[D_UNI,'D_TRD']
L1370         dsc_series = pd.Series({t: d_score_all.get(t) for t in D_UNI}, name='DSC')
L1371         self.d_table = pd.concat([d_disp, dsc_series], axis=1); self.d_table.index = [t + ("⭐️" if t in top_D else "") for t in D_UNI]
L1372         self.d_formatters = {col:"{:.2f}".format for col in cols_D}; self.d_formatters['DSC']="{:.3f}".format
L1373         import scorer
L1374         dw_eff = scorer.D_WEIGHTS_EFF
L1375         self.d_title = (f"[D枠 / {N_D} / {_fmt_w(dw_eff)} / corrM={corrM} / "
L1376                         f"LB={DRRS_D['lookback']} nPC={DRRS_D['n_pc']} γ={DRRS_D['gamma']} λ={DRRS_D['lam']} μ={CROSS_MU_GD} η={DRRS_D['eta']} shrink={DRRS_SHRINK}]")
L1377         if near_D:
L1378             add = [t for t in near_D if t not in set(D_UNI)][:10]
L1379             if add:
L1380                 d_disp2 = pd.DataFrame(index=add)
L1381                 d_disp2['QAL'], d_disp2['YLD'], d_disp2['VOL'], d_disp2['TRD'] = df_z.loc[add,'D_QAL'], df_z.loc[add,'D_YLD'], df_z.loc[add,'D_VOL_RAW'], df_z.loc[add,'D_TRD']
L1382                 near_tbl = pd.concat([d_disp2, pd.Series({t: d_score_all.get(t) for t in add}, name='DSC')], axis=1)
L1383                 self.d_table = pd.concat([self.d_table, near_tbl], axis=0)
L1384         print(self.d_title); print(self.d_table.to_string(formatters=self.d_formatters))
L1385
L1386         # === Changes（IN の GSC/DSC を表示。OUT は銘柄名のみ） ===
L1387         in_list = sorted(set(list(top_G)+list(top_D)) - set(exist))
L1388         out_list = sorted(set(exist) - set(list(top_G)+list(top_D)))
L1389
L1390         self.io_table = pd.DataFrame({
L1391             'IN': pd.Series(in_list),
L1392             '/ OUT': pd.Series(out_list)
L1393         })
L1394         g_list = [f"{g_score.get(t):.3f}" if pd.notna(g_score.get(t)) else '—' for t in out_list]
L1395         d_list = [f"{d_score_all.get(t):.3f}" if pd.notna(d_score_all.get(t)) else '—' for t in out_list]
L1396         self.io_table['GSC'] = pd.Series(g_list)
L1397         self.io_table['DSC'] = pd.Series(d_list)
L1398
L1399         print("Changes:")
L1400         print(self.io_table.to_string(index=False))
L1401
L1402         all_tickers = list(set(exist + list(top_G) + list(top_D) + [bench])); prices = yf.download(all_tickers, period='1y', auto_adjust=True, progress=False, threads=False)['Close'].ffill(limit=2)
L1403         ret = prices.pct_change(); portfolios = {'CUR':exist,'NEW':list(top_G)+list(top_D)}; metrics={}
L1404         for name,ticks in portfolios.items():
L1405             pr = ret[ticks].mean(axis=1, skipna=True).dropna(); cum = (1+pr).cumprod()-1; n = len(pr)
L1406             if n>=252: ann_ret, ann_vol = (1+cum.iloc[-1])**(252/n)-1, pr.std()*np.sqrt(252)
L1407             else: ann_ret, ann_vol = cum.iloc[-1], pr.std()*np.sqrt(n)
L1408             sharpe, drawdown = ann_ret/ann_vol, (cum - cum.cummax()).min()
L1409             if len(ticks)>=2:
L1410                 C_raw = ret[ticks].corr(); RAW_rho = C_raw.mask(np.eye(len(ticks), dtype=bool)).stack().mean()
L1411                 R = ret[ticks].dropna().to_numpy(); C_resid = Selector.residual_corr(R, n_pc=3, shrink=DRRS_SHRINK)
L1412                 RESID_rho = float((C_resid.sum()-np.trace(C_resid))/(C_resid.shape[0]*(C_resid.shape[0]-1)))
L1413             else: RAW_rho = RESID_rho = np.nan
L1414             divy = ttm_div_yield_portfolio(ticks); metrics[name] = {'RET':ann_ret,'VOL':ann_vol,'SHP':sharpe,'MDD':drawdown,'RAWρ':RAW_rho,'RESIDρ':RESID_rho,'DIVY':divy}
L1415         df_metrics = pd.DataFrame(metrics).T; df_metrics_pct = df_metrics.copy(); self.df_metrics = df_metrics
L1416         for col in ['RET','VOL','MDD','DIVY']: df_metrics_pct[col] = df_metrics_pct[col]*100
L1417         cols_order = ['RET','VOL','SHP','MDD','RAWρ','RESIDρ','DIVY']; df_metrics_pct = df_metrics_pct.reindex(columns=cols_order)
L1418         def _fmt_row(s):
L1419             return pd.Series({'RET':f"{s['RET']:.1f}%",'VOL':f"{s['VOL']:.1f}%",'SHP':f"{s['SHP']:.1f}",'MDD':f"{s['MDD']:.1f}%",'RAWρ':(f"{s['RAWρ']:.2f}" if pd.notna(s['RAWρ']) else "NaN"),'RESIDρ':(f"{s['RESIDρ']:.2f}" if pd.notna(s['RESIDρ']) else "NaN"),'DIVY':f"{s['DIVY']:.1f}%"})
L1420         self.df_metrics_fmt = df_metrics_pct.apply(_fmt_row, axis=1); print("Performance Comparison:"); print(self.df_metrics_fmt.to_string())
L1421         # === 追加: GSC+DSC が低い順 TOP10 ===
L1422         try:
L1423             all_scores = pd.DataFrame({'GSC': df_z['GSC'], 'DSC': df_z['DSC']}).copy()
L1424             all_scores['G_plus_D'] = all_scores['GSC'] + all_scores['DSC']
L1425             all_scores = all_scores.dropna(subset=['G_plus_D'])
L1426             self.low10_table = all_scores.sort_values('G_plus_D', ascending=True).head(10).round(3)
L1427             print("Low Score Candidates (GSC+DSC bottom 10):")
L1428             print(self.low10_table.to_string())
L1429         except Exception as e:
L1430             print(f"[warn] low-score ranking failed: {e}")
L1431             self.low10_table = None
L1432         self.debug_text = ""
L1433         if debug_mode:
L1434             logger.info("debug_mode=True: df_z dump handled in scorer; skipping factor-side debug output")
L1435         else:
L1436             logger.debug(
L1437                 "skip debug log: debug_mode=%s debug_text_empty=%s",
L1438                 debug_mode, True
L1439             )
L1440         self._debug_logged = True
L1441
L1442     # --- Slack送信（元 notify_slack のロジックそのまま） ---
L1443     def notify_slack(self):
L1444         SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")
L1445
L1446         if not SLACK_WEBHOOK_URL:
L1447             print("⚠️ SLACK_WEBHOOK_URL not set (main report skipped)")
L1448             return
L1449
L1450         def _filter_suffix_from(spec: dict, group: str) -> str:
L1451             g = spec.get(group, {})
L1452             parts = [str(m) for m in g.get("pre_mask", [])]
L1453             for k, v in (g.get("pre_filter", {}) or {}).items():
L1454                 base, op = (k[:-4], "<") if k.endswith("_max") else ((k[:-4], ">") if k.endswith("_min") else (k, "="))
L1455                 name = {"beta": "β"}.get(base, base)
L1456                 try:
L1457                     val = f"{float(v):g}"
L1458                 except Exception:
L1459                     val = str(v)
L1460                 parts.append(f"{name}{op}{val}")
L1461             return "" if not parts else " / filter:" + " & ".join(parts)
L1462
L1463         def _inject_filter_suffix(title: str, group: str) -> str:
L1464             suf = _filter_suffix_from(FILTER_SPEC, group)
L1465             return f"{title[:-1]}{suf}]" if suf and title.endswith("]") else (title + suf)
L1466
L1467         def _blk(title, tbl, fmt=None, drop=()):
L1468             if tbl is None or getattr(tbl, 'empty', False):
L1469                 return f"{title}\n(選定なし)\n"
L1470             if drop and hasattr(tbl, 'columns'):
L1471                 keep = [c for c in tbl.columns if c not in drop]
L1472                 tbl, fmt = tbl[keep], {k: v for k, v in (fmt or {}).items() if k in keep}
L1473             return f"{title}\n```{tbl.to_string(formatters=fmt)}```\n"
L1474
L1475         message = "📈 ファクター分散最適化の結果\n"
L1476         miss_df, truncated, total = self._miss_disp_info or self._prepare_missing_display(self.miss_df)
L1477         if not miss_df.empty:
L1478             message += "Missing Data\n```" + miss_df.to_string(index=False) + "```\n"
L1479             if truncated:
L1480                 message += f"...省略 ({total}件中 上位20件のみ表示)\n"
L1481         message += _blk(_inject_filter_suffix(self.g_title, "G"), self.g_table, self.g_formatters, drop=("TRD",))
L1482         message += _blk(_inject_filter_suffix(self.d_title, "D"), self.d_table, self.d_formatters)
L1483         message += "Changes\n" + ("(変更なし)\n" if self.io_table is None or getattr(self.io_table, 'empty', False) else f"```{self.io_table.to_string(index=False)}```\n")
L1484         message += "Performance Comparison:\n```" + self.df_metrics_fmt.to_string() + "```"
L1485
L1486         try:
L1487             r = requests.post(SLACK_WEBHOOK_URL, json={"text": message})
L1488             print(f"[DBG] main_post status={getattr(r, 'status_code', None)} size={len(message)}")
L1489             if r is not None:
L1490                 r.raise_for_status()
L1491         except Exception as e:
L1492             print(f"[ERR] main_post_failed: {e}")
L1493
L1494 def _infer_g_universe(feature_df, selected12=None, near5=None):
L1495     try:
L1496         out = feature_df.index[feature_df['group'].astype(str).str.upper().eq('G')].tolist()
L1497         if out: return out
L1498     except Exception:
L1499         pass
L1500     base = set()
L1501     for lst in (selected12 or []), (near5 or []):
L1502         for x in (lst or []): base.add(x)
L1503     return list(base) if base else list(feature_df.index)
L1504
L1505 def _fmt_with_fire_mark(tickers, feature_df):
L1506     out = []
L1507     for t in tickers or []:
L1508         try:
L1509             br = bool(feature_df.at[t, "G_BREAKOUT_recent_5d"])
L1510             pb = bool(feature_df.at[t, "G_PULLBACK_recent_5d"])
L1511             out.append(f"{t}{' 🔥' if (br or pb) else ''}")
L1512         except Exception:
L1513             out.append(t)
L1514     return out
L1515
L1516 def _label_recent_event(t, feature_df):
L1517     try:
L1518         br = bool(feature_df.at[t, "G_BREAKOUT_recent_5d"]); dbr = str(feature_df.at[t, "G_BREAKOUT_last_date"]) if br else ""
L1519         pb = bool(feature_df.at[t, "G_PULLBACK_recent_5d"]); dpb = str(feature_df.at[t, "G_PULLBACK_last_date"]) if pb else ""
L1520         if   br and not pb: return f"{t}（ブレイクアウト確定 {dbr}）"
L1521         elif pb and not br: return f"{t}（押し目反発 {dpb}）"
L1522         elif br and pb:     return f"{t}（ブレイクアウト確定 {dbr}／押し目反発 {dpb}）"
L1523     except Exception:
L1524         pass
L1525     return t
L1526
L1527 # === パイプライン可視化：G/D共通フロー（出力は不変） ===
L1528
L1529 def io_build_input_bundle() -> InputBundle:
L1530     """
L1531     既存の『データ取得→前処理』を実行し、InputBundle を返す。
L1532     処理内容・列名・丸め・例外・ログ文言は現行どおり（変更禁止）。
L1533     """
L1534     state = Input(cand=cand, exist=exist, bench=bench, price_max=CAND_PRICE_MAX, finnhub_api_key=FINNHUB_API_KEY).prepare_data()
L1535     return InputBundle(cand=state["cand"], tickers=state["tickers"], bench=bench, data=state["data"], px=state["px"], spx=state["spx"], tickers_bulk=state["tickers_bulk"], info=state["info"], eps_df=state["eps_df"], fcf_df=state["fcf_df"], returns=state["returns"], missing_logs=state["missing_logs"])
L1536
L1537 def run_group(sc: Scorer, group: str, inb: InputBundle, cfg: PipelineConfig,
L1538               n_target: int) -> tuple[list, float, float, float]:
L1539     """
L1540     G/Dを同一手順で処理：採点→フィルター→選定（相関低減込み）。
L1541     戻り値：(pick, avg_res_corr, sum_score, objective)
L1542     JSON保存は既存フォーマット（キー名・丸め桁・順序）を踏襲。
L1543     """
L1544     sc.cfg = cfg
L1545
L1546     if hasattr(sc, "score_build_features"):
L1547         feat = sc.score_build_features(inb)
L1548         if not hasattr(sc, "_feat_logged"):
L1549             T.log("features built (scorer)")
L1550             sc._feat_logged = True
L1551         agg = sc.score_aggregate(feat, group, cfg) if hasattr(sc, "score_aggregate") else feat
L1552     else:
L1553         fb = sc.aggregate_scores(inb, cfg)
L1554         if not hasattr(sc, "_feat_logged"):
L1555             T.log("features built (scorer)")
L1556             sc._feat_logged = True
L1557         sc._feat = fb
L1558         agg = fb.g_score if group == "G" else fb.d_score_all
L1559         if group == "D" and hasattr(fb, "df"):
L1560             agg = agg[fb.df['BETA'] < D_BETA_MAX]
L1561
L1562     if hasattr(sc, "filter_candidates"):
L1563         agg = agg[sc.filter_candidates(inb, agg, group, cfg)]
L1564
L1565     if isinstance(agg, pd.Series):
L1566         agg = _as_numeric_series(agg)
L1567
L1568     selector = Selector()
L1569     if hasattr(sc, "select_diversified"):
L1570         pick, avg_r, sum_sc, obj = sc.select_diversified(agg, group, cfg, n_target,
L1571             selector=selector, prev_tickers=None,
L1572             corrM=cfg.drrs.corrM, shrink=cfg.drrs.shrink,
L1573             cross_mu=cfg.drrs.cross_mu_gd)
L1574     else:
L1575         if group == "G":
L1576             init = agg.nlargest(min(cfg.drrs.corrM, len(agg))).index.tolist()
L1577             res = selector.select_bucket_drrs(returns_df=inb.returns, score_ser=agg, pool_tickers=init, k=n_target,
L1578                 n_pc=cfg.drrs.G.get("n_pc", 3), gamma=cfg.drrs.G.get("gamma", 1.2),
L1579                 lam=cfg.drrs.G.get("lam", 0.68),
L1580                 lookback=cfg.drrs.G.get("lookback", 252),
L1581                 shrink=cfg.drrs.shrink, g_fixed_tickers=None, mu=0.0)
L1582         else:
L1583             init = agg.nlargest(min(cfg.drrs.corrM, len(agg))).index.tolist()
L1584             g_fixed = getattr(sc, "_top_G", None)
L1585             res = selector.select_bucket_drrs(returns_df=inb.returns, score_ser=agg, pool_tickers=init, k=n_target,
L1586                 n_pc=cfg.drrs.D.get("n_pc", 4), gamma=cfg.drrs.D.get("gamma", 0.8),
L1587                 lam=cfg.drrs.D.get("lam", 0.85),
L1588                 lookback=cfg.drrs.D.get("lookback", 504),
L1589                 shrink=cfg.drrs.shrink, g_fixed_tickers=g_fixed,
L1590                 mu=cfg.drrs.cross_mu_gd)
L1591         pick = res["tickers"]; avg_r = res["avg_res_corr"]
L1592         sum_sc = res["sum_score"]; obj = res["objective"]
L1593         if group == "D":
L1594             _, pick = _disjoint_keepG(getattr(sc, "_top_G", []), pick, init)
L1595             T.log("selection finalized (G/D)")
L1596     try:
L1597         inc = [t for t in exist if t in agg.index]
L1598         pick = _sticky_keep_current(
L1599             agg=agg, pick=pick, incumbents=inc, n_target=n_target,
L1600             delta_z=SWAP_DELTA_Z, keep_buffer=SWAP_KEEP_BUFFER
L1601         )
L1602     except Exception as _e:
L1603         print(f"[warn] sticky_keep_current skipped: {str(_e)}")
L1604     # --- Near-Miss: 惜しくも選ばれなかった上位10を保持（Slack表示用） ---
L1605     # 5) Near-Miss と最終集計Seriesを保持（表示専用。計算へ影響なし）
L1606     try:
L1607         pool = agg.drop(index=[t for t in pick if t in agg.index], errors="ignore")
L1608         near10 = list(pool.sort_values(ascending=False).head(10).index)
L1609         setattr(sc, f"_near_{group}", near10)
L1610         setattr(sc, f"_agg_{group}", agg)
L1611     except Exception:
L1612         pass
L1613
L1614     if group == "D":
L1615         T.log("save done")
L1616     if group == "G":
L1617         sc._top_G = pick
L1618     return pick, avg_r, sum_sc, obj
L1619
L1620 def run_pipeline() -> SelectionBundle:
L1621     """
L1622     G/D共通フローの入口。I/Oはここだけで実施し、計算はScorerに委譲。
L1623     Slack文言・丸め・順序は既存の Output を用いて変更しない。
L1624     """
L1625     inb = io_build_input_bundle()
L1626     cfg = PipelineConfig(
L1627         weights=WeightsConfig(g=g_weights, d=D_weights),
L1628         drrs=DRRSParams(
L1629             corrM=corrM, shrink=DRRS_SHRINK,
L1630             G=DRRS_G, D=DRRS_D, cross_mu_gd=CROSS_MU_GD
L1631         ),
L1632         price_max=CAND_PRICE_MAX,
L1633         debug_mode=debug_mode
L1634     )
L1635     sc = Scorer()
L1636     top_G, avgG, sumG, objG = run_group(sc, "G", inb, cfg, N_G)
L1637     poolG = list(getattr(sc, "_agg_G", pd.Series(dtype=float)).dropna().sort_values(ascending=False).index)
L1638     alpha = Scorer.spx_to_alpha(inb.spx)
L1639     sectors = {t:(inb.info.get(t,{}).get("sector") or "U") for t in poolG}; scores = {t:Scorer.g_score.get(t,0.0) for t in poolG}
L1640     top_G = Scorer.pick_top_softcap(scores, sectors, N=N_G, cap=2, alpha=alpha, hard=5)
L1641     sc._top_G = top_G
L1642     try:
L1643         aggG = getattr(sc, "_agg_G", pd.Series(dtype=float)).dropna().sort_values(ascending=False)
L1644         sc._near_G = [t for t in aggG.index if t not in set(top_G)][:10]
L1645     except Exception:
L1646         pass
L1647     base = sum(Scorer.g_score.get(t,0.0) for t in poolG[:N_G])
L1648     effs = sum(Scorer.g_score.get(t,0.0) for t in top_G)
L1649     print(f"[soft_cap2] score_cost={(base-effs)/max(1e-9,abs(base)):.2%}, alpha={alpha:.3f}")
L1650     top_D, avgD, sumD, objD = run_group(sc, "D", inb, cfg, N_D)
L1651     poolD = list(getattr(sc, "_agg_D", pd.Series(dtype=float)).dropna().sort_values(ascending=False).index)
L1652     fb = getattr(sc, "_feat", None)
L1653     near_G = getattr(sc, "_near_G", [])
L1654     selected12 = list(top_G)
L1655     df = fb.df if fb is not None else pd.DataFrame()
L1656     guni = _infer_g_universe(df, selected12, near_G)
L1657     try:
L1658         fire_recent = [t for t in guni
L1659                        if (str(df.at[t, "G_BREAKOUT_recent_5d"]) == "True") or
L1660                           (str(df.at[t, "G_PULLBACK_recent_5d"]) == "True")]
L1661     except Exception: fire_recent = []
L1662
L1663     lines = [
L1664         "【G枠レポート｜週次モニタ（直近5営業日）】",
L1665         "【凡例】🔥=直近5営業日内に「ブレイクアウト確定」または「押し目反発」を検知",
L1666         f"選定{N_G}: {', '.join(_fmt_with_fire_mark(selected12, df))}" if selected12 else f"選定{N_G}: なし",
L1667         f"次点10: {', '.join(_fmt_with_fire_mark(near_G, df))}" if near_G else "次点10: なし",]
L1668
L1669     if fire_recent:
L1670         fire_list = ", ".join([_label_recent_event(t, df) for t in fire_recent])
L1671         lines.append(f"過去5営業日の検知: {fire_list}")
L1672     else:
L1673         lines.append("過去5営業日の検知: なし")
L1674
L1675     try:
L1676         webhook = os.environ.get("SLACK_WEBHOOK_URL", "")
L1677         if webhook:
L1678             requests.post(webhook, json={"text": "\n".join([s for s in lines if s != ""])}, timeout=10)
L1679     except Exception:
L1680         pass
L1681
L1682     out = Output()
L1683     # 表示側から選定時の集計へアクセスできるように保持（表示専用・副作用なし）
L1684     try:
L1685         out._sc = sc
L1686     except Exception:
L1687         pass
L1688     if hasattr(sc, "_feat"):
L1689         try:
L1690             fb = sc._feat
L1691             out.miss_df = fb.missing_logs
L1692             out.display_results(
L1693                 exist=exist,
L1694                 bench=bench,
L1695                 df_z=fb.df_z,
L1696                 g_score=fb.g_score,
L1697                 d_score_all=fb.d_score_all,
L1698                 init_G=top_G,
L1699                 init_D=top_D,
L1700                 top_G=top_G,
L1701                 top_D=top_D,
L1702                 df_full_z=getattr(fb, "df_full_z", None),
L1703                 prev_G=getattr(sc, "_prev_G", exist),
L1704                 prev_D=getattr(sc, "_prev_D", exist),
L1705             )
L1706             try:
L1707                 DBG_COLS = ["GSC", "GROWTH_F", "MOM", "VOL", "DBGRW.GROWTH_F", "DBGRW.MOM", "DBGRW.VOL"]
L1708                 cols = [c for c in DBG_COLS if c in fb.df_z.columns]
L1709                 idx = [t for t in top_G if t in fb.df_z.index]
L1710                 out.debug_table = fb.df_z.loc[idx, cols].round(2) if idx and cols else None
L1711             except Exception:
L1712                 out.debug_table = None
L1713         except Exception:
L1714             pass
L1715     out.notify_slack()
L1716     sb = SelectionBundle(resG={"tickers": top_G, "avg_res_corr": avgG,
L1717               "sum_score": sumG, "objective": objG},
L1718         resD={"tickers": top_D, "avg_res_corr": avgD,
L1719               "sum_score": sumD, "objective": objD},
L1720         top_G=top_G, top_D=top_D, init_G=top_G, init_D=top_D)
L1721
L1722     # --- Low Score Candidates (GSC+DSC bottom 10) : send before debug dump ---
L1723     try:
L1724         _low_df = (pd.DataFrame({"GSC": fb.g_score, "DSC": fb.d_score_all})
L1725               .assign(G_plus_D=lambda x: x["GSC"] + x["DSC"])
L1726               .sort_values("G_plus_D")
L1727               .head(10)
L1728               .round(3))
L1729         low_msg = "Low Score Candidates (GSC+DSC bottom 10)\n" + _low_df.to_string(index=True, index_names=False)
L1730         _post_slack({"text": f"```{low_msg}```"})
L1731     except Exception as _e:
L1732         _post_slack({"text": f"```Low Score Candidates: 作成失敗: {_e}```"})
L1733
L1734     return sb
L1735
L1736 if __name__ == "__main__":
L1737     run_pipeline()
```

## <scorer.py>
```text
L1 # scorer.py
L2 # kawatest
L3 # =============================================================================
L4 # Scorer: ファクター/指標の生成と合成スコア算出を担う純粋層
L5 #
L6 # 【このファイルだけ読めば分かるポイント】
L7 # - 入力(InputBundle)は「価格/出来高/ベンチ/基本情報/EPS/FCF/リターン」を含むDTO
L8 # - 出力(FeatureBundle)は「raw特徴量 df」「標準化 df_z」「G/D スコア」「欠損ログ」
L9 # - 重み等のコンフィグ(PipelineConfig)は factor から渡す（cfg 必須）
L10 # - 旧カラム名は Scorer 内で自動リネームして受け入れ（後方互換）
L11 #   例) eps_ttm -> EPS_TTM, eps_q_recent -> EPS_Q_LastQ, fcf_ttm -> FCF_TTM
L12 #
L13 # 【I/O契約（Scorerが参照するInputBundleフィールド）】
L14 #   - cand: List[str]    … 候補銘柄（単体実行では未使用）
L15 #   - tickers: List[str] … 対象銘柄リスト
L16 #   - bench: str         … ベンチマークティッカー（例 '^GSPC'）
L17 #   - data: pd.DataFrame … yfinance download結果 ('Close','Volume' 等の階層列)
L18 #   - px: pd.DataFrame   … data['Close'] 相当（終値）
L19 #   - spx: pd.Series     … ベンチマークの終値
L20 #   - tickers_bulk: object         … yfinance.Tickers
L21 #   - info: Dict[str, dict]        … yfinance info per ticker
L22 #   - eps_df: pd.DataFrame         … 必須列: EPS_TTM, EPS_Q_LastQ（旧名も可）
L23 #   - fcf_df: pd.DataFrame         … 必須列: FCF_TTM（旧名も可）
L24 #   - returns: pd.DataFrame        … px[tickers].pct_change() 相当
L25 #   - missing_logs: pd.DataFrame   … 補完後の欠損ログ
L26 #
L27 # ※入出力の形式・例外文言は既存実装を変えません（安全な短縮のみ）
L28 # =============================================================================
L29
L30 import json, logging, os, requests, sys, warnings
L31 import numpy as np
L32 import pandas as pd
L33 import yfinance as yf
L34 from typing import Any, TYPE_CHECKING
L35 from scipy.stats import zscore
L36 from datetime import datetime as _dt
L37
L38 if TYPE_CHECKING:
L39     from factor import PipelineConfig  # type: ignore  # 実行時importなし（循環回避）
L40
L41 logger = logging.getLogger(__name__)
L42
L43
L44 def _log(stage, msg):
L45     try:
L46         print(f"[DBG][{_dt.utcnow().isoformat(timespec='seconds')}Z][{stage}] {msg}")
L47     except Exception:
L48         print(f"[DBG][{stage}] {msg}")
L49
L50
L51 # ---- Dividend Helpers -------------------------------------------------------
L52 def _last_close(t, price_map=None):
L53     if price_map and (c := price_map.get(t)) is not None: return float(c)
L54     try:
L55         h = yf.Ticker(t).history(period="5d")["Close"].dropna()
L56         return float(h.iloc[-1]) if len(h) else np.nan
L57     except Exception:
L58         return np.nan
L59
L60 def _ttm_div_sum(t, lookback_days=400):
L61     try:
L62         div = yf.Ticker(t).dividends
L63         if div is None or len(div) == 0: return 0.0
L64         cutoff = pd.Timestamp.utcnow().tz_localize(None) - pd.Timedelta(days=lookback_days)
L65         ttm = float(div[div.index.tz_localize(None) >= cutoff].sum())
L66         return ttm if ttm > 0 else float(div.tail(4).sum())
L67     except Exception:
L68         return 0.0
L69
L70 def ttm_div_yield_portfolio(tickers, price_map=None):
L71     ys = [(lambda c, s: (s/c) if (np.isfinite(c) and c>0 and s>0) else 0.0)(_last_close(t, price_map), _ttm_div_sum(t)) for t in tickers]
L72     return float(np.mean(ys)) if ys else 0.0
L73
L74 # ---- 簡易ユーティリティ（安全な短縮のみ） -----------------------------------
L75 def _as_numeric_series(s: pd.Series) -> pd.Series:
L76     """Series を float dtype に強制変換し、index を保持する。"""
L77     if s is None:
L78         return pd.Series(dtype=float)
L79     v = pd.to_numeric(s, errors="coerce")
L80     return pd.Series(v.values, index=getattr(s, "index", None), dtype=float, name=getattr(s, "name", None))
L81
L82
L83 def _scalar(x):
L84     """
L85     入力を安全に float スカラへ変換する。
L86
L87     許容する入力パターン:
L88       - pandas.Series: 非NaNの最後の値を採用
L89       - numpy スカラ/配列: 最後の要素を採用
L90       - その他の数値っぽい値: float へ変換
L91
L92     変換できない場合は np.nan を返す。
L93     """
L94
L95     if x is None:
L96         return np.nan
L97
L98     # pandas.Series の場合は非NaNの最後の値を採用
L99     if isinstance(x, pd.Series):
L100         s = pd.to_numeric(x, errors="coerce").dropna()
L101         return float(s.iloc[-1]) if not s.empty else np.nan
L102
L103     # numpy スカラ (item() を持つ) ※文字列は除外
L104     if hasattr(x, "item") and not isinstance(x, (str, bytes)):
L105         try:
L106             return float(x.item())
L107         except Exception:
L108             pass
L109
L110     # 配列様のオブジェクト
L111     try:
L112         arr = np.asarray(x, dtype=float).ravel()
L113         return float(arr[-1]) if arr.size else np.nan
L114     except Exception:
L115         pass
L116
L117     # 最後に素直に float 変換を試す
L118     try:
L119         return float(x)
L120     except Exception:
L121         return np.nan
L122
L123
L124 def winsorize_s(s: pd.Series, p=0.02):
L125     if s is None or s.dropna().empty: return s
L126     lo, hi = np.nanpercentile(s.astype(float), [100*p, 100*(1-p)]); return s.clip(lo, hi)
L127
L128 def robust_z(s: pd.Series, p=0.02):
L129     s2 = winsorize_s(s,p); return np.nan_to_num(zscore(s2.fillna(s2.mean())))
L130
L131 def robust_z_keepnan(s: pd.Series) -> pd.Series:
L132     """robust_z variant that preserves NaNs and falls back to rank-z when needed."""
L133     if s is None:
L134         return pd.Series(dtype=float)
L135     v = pd.to_numeric(s, errors="coerce")
L136     m = np.nanmedian(v)
L137     mad = np.nanmedian(np.abs(v - m))
L138     z = (v - m) / (1.4826 * mad + 1e-9)
L139     if np.nanstd(z) < 1e-9:
L140         r = v.rank(method="average", na_option="keep")
L141         z = (r - np.nanmean(r)) / (np.nanstd(r) + 1e-9)
L142     return pd.Series(z, index=v.index, dtype=float)
L143
L144
L145 def _safe_div(a, b):
L146     try: return np.nan if (b is None or float(b)==0 or pd.isna(b)) else float(a)/float(b)
L147     except Exception: return np.nan
L148
L149 def _safe_last(series: pd.Series, default=np.nan):
L150     try: return float(series.iloc[-1])
L151     except Exception: return default
L152
L153
L154 def _ensure_series(x):
L155     if x is None:
L156         return pd.Series(dtype=float)
L157     if isinstance(x, pd.Series):
L158         return x.dropna()
L159     if isinstance(x, (list, tuple)):
L160         if len(x) and isinstance(x[0], (tuple, list)) and len(x[0]) == 2:
L161             dt = pd.to_datetime([d for d, _ in x], errors="coerce")
L162             v = pd.to_numeric([_v for _, _v in x], errors="coerce")
L163             return pd.Series(v, index=dt).dropna()
L164         return pd.Series(pd.to_numeric(list(x), errors="coerce")).dropna()
L165     try:
L166         return pd.Series(x).dropna()
L167     except Exception:
L168         return pd.Series(dtype=float)
L169
L170
L171 def _to_quarterly(s: pd.Series) -> pd.Series:
L172     if s.empty or not isinstance(s.index, pd.DatetimeIndex):
L173         return s
L174     return s.resample("Q").last().dropna()
L175
L176
L177 def _ttm_yoy_from_quarterly(qs: pd.Series) -> pd.Series:
L178     if qs is None or qs.empty:
L179         return pd.Series(dtype=float)
L180     ttm = qs.rolling(4, min_periods=2).sum()
L181     yoy = ttm.pct_change(4)
L182     return yoy
L183
L184
L185
L186
L187 class Scorer:
L188     """
L189     - factor.py からは `aggregate_scores(ib, cfg)` を呼ぶだけでOK。
L190     - cfg は必須（factor.PipelineConfig を渡す）。
L191     - 旧カラム名を自動リネームして新スキーマに吸収します。
L192     """
L193
L194     # === 先頭で旧→新カラム名マップ（移行用） ===
L195     EPS_RENAME = {"eps_ttm":"EPS_TTM", "eps_q_recent":"EPS_Q_LastQ"}
L196     FCF_RENAME = {"fcf_ttm":"FCF_TTM"}
L197
L198     # === スキーマ簡易チェック（最低限） ===
L199     @staticmethod
L200     def _validate_ib_for_scorer(ib: Any):
L201         miss = [a for a in ["tickers","bench","data","px","spx","tickers_bulk","info","eps_df","fcf_df","returns"] if not hasattr(ib,a) or getattr(ib,a) is None]
L202         if miss: raise ValueError(f"InputBundle is missing required attributes for Scorer: {miss}")
L203         if any(c in ib.eps_df.columns for c in Scorer.EPS_RENAME): ib.eps_df.rename(columns=Scorer.EPS_RENAME, inplace=True)
L204         if any(c in ib.fcf_df.columns for c in Scorer.FCF_RENAME): ib.fcf_df.rename(columns=Scorer.FCF_RENAME, inplace=True)
L205         need_eps, need_fcf = {"EPS_TTM","EPS_Q_LastQ"},{"FCF_TTM"}
L206         if not need_eps.issubset(ib.eps_df.columns): raise ValueError(f"eps_df must contain columns {need_eps} (accepts old names via auto-rename). Got: {list(ib.eps_df.columns)}")
L207         if not need_fcf.issubset(ib.fcf_df.columns): raise ValueError(f"fcf_df must contain columns {need_fcf} (accepts old names via auto-rename). Got: {list(ib.fcf_df.columns)}")
L208
L209     # ----（Scorer専用）テクニカル・指標系 ----
L210     @staticmethod
L211     def trend(s: pd.Series):
L212         if len(s)<200: return np.nan
L213         sma50, sma150, sma200 = s.rolling(50).mean().iloc[-1], s.rolling(150).mean().iloc[-1], s.rolling(200).mean().iloc[-1]
L214         prev200, p = s.rolling(200).mean().iloc[-21], s.iloc[-1]
L215         lo_52 = s[-252:].min() if len(s)>=252 else s.min(); hi_52 = s[-252:].max() if len(s)>=252 else s.max()
L216         rng = (hi_52 - lo_52) if hi_52>lo_52 else np.nan
L217         clip = lambda x,lo,hi: (np.nan if pd.isna(x) else max(lo,min(hi,x)))
L218         a = clip(p/(s.rolling(50).mean().iloc[-1]) - 1, -0.5, 0.5)
L219         b = clip(sma50/sma150 - 1, -0.5, 0.5)
L220         c = clip(sma150/sma200 - 1, -0.5, 0.5)
L221         d = clip(sma200/prev200 - 1, -0.2, 0.2)
L222         e = clip((p - lo_52) / (rng if rng and rng>0 else np.nan) - 0.5, -0.5, 0.5)
L223         parts = [0.0 if pd.isna(x) else x for x in (a,b,c,d,e)]
L224         return 0.30*parts[0] + 0.20*parts[1] + 0.15*parts[2] + 0.15*parts[3] + 0.20*parts[4]
L225
L226     @staticmethod
L227     def rs(s, b):
L228         n, nb = len(s), len(b)
L229         if n<60 or nb<60: return np.nan
L230         L12 = 252 if n>=252 and nb>=252 else min(n,nb)-1; L1 = 22 if n>=22 and nb>=22 else max(5, min(n,nb)//3)
L231         r12, r1, br12, br1 = s.iloc[-1]/s.iloc[-L12]-1, s.iloc[-1]/s.iloc[-L1]-1, b.iloc[-1]/b.iloc[-L12]-1, b.iloc[-1]/b.iloc[-L1]-1
L232         return (r12 - br12)*0.7 + (r1 - br1)*0.3
L233
L234     @staticmethod
L235     def tr_str(s):
L236         if s is None:
L237             return np.nan
L238         s = s.ffill(limit=2).dropna()
L239         if len(s) < 50:
L240             return np.nan
L241         ma50 = s.rolling(50, min_periods=50).mean()
L242         last_ma = ma50.iloc[-1]
L243         last_px = s.iloc[-1]
L244         return float(last_px/last_ma - 1.0) if pd.notna(last_ma) and pd.notna(last_px) else np.nan
L245
L246     @staticmethod
L247     def rs_line_slope(s: pd.Series, b: pd.Series, win: int) -> float:
L248         r = (s/b).dropna()
L249         if len(r) < win: return np.nan
L250         y, x = np.log(r.iloc[-win:]), np.arange(win, dtype=float)
L251         try: return float(np.polyfit(x, y, 1)[0])
L252         except Exception: return np.nan
L253
L254     @staticmethod
L255     def ev_fallback(info_t: dict, tk: yf.Ticker) -> float:
L256         ev = info_t.get('enterpriseValue', np.nan)
L257         if pd.notna(ev) and ev>0: return float(ev)
L258         mc, debt, cash = info_t.get('marketCap', np.nan), np.nan, np.nan
L259         try:
L260             bs = tk.quarterly_balance_sheet
L261             if bs is not None and not bs.empty:
L262                 c = bs.columns[0]
L263                 for k in ("Total Debt","Long Term Debt","Short Long Term Debt"):
L264                     if k in bs.index: debt = float(bs.loc[k,c]); break
L265                 for k in ("Cash And Cash Equivalents","Cash And Cash Equivalents And Short Term Investments","Cash"):
L266                     if k in bs.index: cash = float(bs.loc[k,c]); break
L267         except Exception: pass
L268         if pd.notna(mc): return float(mc + (0 if pd.isna(debt) else debt) - (0 if pd.isna(cash) else cash))
L269         return np.nan
L270
L271     @staticmethod
L272     def dividend_status(ticker: str) -> str:
L273         t = yf.Ticker(ticker)
L274         try:
L275             if not t.dividends.empty: return "has"
L276         except Exception: return "unknown"
L277         try:
L278             a = t.actions
L279             if (a is not None and not a.empty and "Stock Splits" in a.columns and a["Stock Splits"].abs().sum()>0): return "none_confident"
L280         except Exception: pass
L281         try:
L282             fi = t.fast_info
L283             if any(getattr(fi,k,None) for k in ("last_dividend_date","dividend_rate","dividend_yield")): return "maybe_missing"
L284         except Exception: pass
L285         return "unknown"
L286
L287     @staticmethod
L288     def div_streak(t):
L289         try:
L290             divs = yf.Ticker(t).dividends.dropna(); ann = divs.groupby(divs.index.year).sum(); ann = ann[ann.index<pd.Timestamp.today().year]
L291             years, streak = sorted(ann.index), 0
L292             for i in range(len(years)-1,0,-1):
L293                 if ann[years[i]] > ann[years[i-1]]: streak += 1
L294                 else: break
L295             return streak
L296         except Exception: return 0
L297
L298     @staticmethod
L299     def fetch_finnhub_metrics(symbol):
L300         api_key = os.environ.get("FINNHUB_API_KEY")
L301         if not api_key: return {}
L302         url, params = "https://finnhub.io/api/v1/stock/metric", {"symbol":symbol,"metric":"all","token":api_key}
L303         try:
L304             r = requests.get(url, params=params, timeout=10); r.raise_for_status(); m = r.json().get("metric",{})
L305             return {'EPS':m.get('epsGrowthTTMYoy'),'REV':m.get('revenueGrowthTTMYoy'),'ROE':m.get('roeTTM'),'BETA':m.get('beta'),'DIV':m.get('dividendYieldIndicatedAnnual'),'FCF':(m.get('freeCashFlowTTM')/m.get('enterpriseValue')) if m.get('freeCashFlowTTM') and m.get('enterpriseValue') else None}
L306         except Exception: return {}
L307
L308     @staticmethod
L309     def calc_beta(series: pd.Series, market: pd.Series, lookback=252):
L310         r, m = series.pct_change().dropna(), market.pct_change().dropna()
L311         n = min(len(r), len(m), lookback)
L312         if n<60: return np.nan
L313         r, m = r.iloc[-n:], m.iloc[-n:]; cov, var = np.cov(r, m)[0,1], np.var(m)
L314         return np.nan if var==0 else cov/var
L315
L316     @staticmethod
L317     def spx_to_alpha(spx: pd.Series, bands=(0.03,0.10), w=(0.6,0.4),
L318                      span=5, q=(0.20,0.40), alphas=(0.05,0.08,0.10)) -> float:
L319         """
L320         S&P500指数のみから擬似breadthを作り、履歴分位でαを段階決定。
L321         bands=(±3%, ±10%), w=(50DMA,200DMA), 分位q=(20%,40%), alphas=(低,中,高)
L322         """
L323         ma50, ma200 = spx.rolling(50).mean(), spx.rolling(200).mean()
L324         b50, b200 = ((spx/ma50 - 1)+bands[0])/(2*bands[0]), ((spx/ma200 - 1)+bands[1])/(2*bands[1])
L325         hist = (w[0]*b50 + w[1]*b200).clip(0,1).ewm(span=span).mean()
L326         b, (lo, mid) = float(hist.iloc[-1]), (float(hist.quantile(q[0])), float(hist.quantile(q[1])))
L327         return alphas[0] if b < lo else alphas[1] if b < mid else alphas[2]
L328
L329     @staticmethod
L330     def soft_cap_effective_scores(scores: pd.Series|dict, sectors: dict, cap=2, alpha=0.08) -> pd.Series:
L331         """
L332         同一セクターcap超過（3本目以降）に α×段階減点を課した“有効スコア”Seriesを返す。
L333         戻り値は降順ソート済み。
L334         """
L335         s = pd.Series(scores, dtype=float); order = s.sort_values(ascending=False).index
L336         cnt, pen = {}, {}
L337         for t in order:
L338             sec = sectors.get(t, "U"); cnt[sec] = cnt.get(sec,0) + 1; pen[t] = alpha*max(0, cnt[sec]-cap)
L339         return (s - pd.Series(pen)).sort_values(ascending=False)
L340
L341     @staticmethod
L342     def pick_top_softcap(scores: pd.Series|dict, sectors: dict, N: int, cap=2, alpha=0.08, hard: int|None=5) -> list[str]:
L343         """
L344         soft-cap適用後の上位Nティッカーを返す。hard>0なら非常用ハード上限で同一セクター超過を間引く（既定=5）。
L345         """
L346         eff = Scorer.soft_cap_effective_scores(scores, sectors, cap, alpha)
L347         eff = eff.dropna()
L348         if not hard:
L349             return list(eff.head(N).index)
L350         pick, used = [], {}
L351         for t in eff.index:
L352             s = sectors.get(t, "U")
L353             if used.get(s,0) < hard:
L354                 pick.append(t); used[s] = used.get(s,0) + 1
L355             if len(pick) == N: break
L356         return pick
L357
L358     @staticmethod
L359     def trend_template_breadth_series(px: pd.DataFrame, spx: pd.Series, win_days: int | None = None) -> pd.Series:
L360         """
L361         各営業日の trend_template 合格本数（合格“本数”=C）を返す。
L362         - px: 列=ticker（ベンチは含めない）
L363         - spx: ベンチマーク Series（px.index に整列）
L364         - win_days: 末尾の計算対象営業日数（None→全体、既定600は呼び出し側指定）
L365         ベクトル化＆rollingのみで軽量。欠損は False 扱い。
L366         """
L367         import numpy as np, pandas as pd
L368         if px is None or px.empty:
L369             return pd.Series(dtype=int)
L370         px = px.dropna(how="all", axis=1)
L371         if win_days and win_days > 0:
L372             px = px.tail(win_days)
L373         if px.empty:
L374             return pd.Series(dtype=int)
L375         spx = spx.reindex(px.index).ffill()
L376
L377         ma50  = px.rolling(50).mean()
L378         ma150 = px.rolling(150).mean()
L379         ma200 = px.rolling(200).mean()
L380
L381         tt = (px > ma150)
L382         tt &= (px > ma200)
L383         tt &= (ma150 > ma200)
L384         tt &= (ma200 - ma200.shift(21) > 0)
L385         tt &= (ma50  > ma150)
L386         tt &= (ma50  > ma200)
L387         tt &= (px    > ma50)
L388
L389         lo252 = px.rolling(252).min()
L390         hi252 = px.rolling(252).max()
L391         tt &= (px.divide(lo252).sub(1.0) >= 0.30)   # P_OVER_LOW52 >= 0.30
L392         tt &= (px >= (0.75 * hi252))                # NEAR_52W_HIGH >= -0.25
L393
L394         r12  = px.divide(px.shift(252)).sub(1.0)
L395         br12 = spx.divide(spx.shift(252)).sub(1.0)
L396         r1   = px.divide(px.shift(22)).sub(1.0)
L397         br1  = spx.divide(spx.shift(22)).sub(1.0)
L398         rs   = 0.7*(r12.sub(br12, axis=0)) + 0.3*(r1.sub(br1, axis=0))
L399         tt &= (rs >= 0.10)
L400
L401         return tt.fillna(False).sum(axis=1).astype(int)
L402
L403     # ---- スコア集計（DTO/Configを受け取り、FeatureBundleを返す） ----
L404     def aggregate_scores(self, ib: Any, cfg):
L405         if cfg is None:
L406             raise ValueError("cfg is required; pass factor.PipelineConfig")
L407         self._validate_ib_for_scorer(ib)
L408
L409         px, spx, tickers = ib.px, ib.spx, ib.tickers
L410         tickers_bulk, info, eps_df, fcf_df = ib.tickers_bulk, ib.info, ib.eps_df, ib.fcf_df
L411
L412         df = pd.DataFrame(index=tickers)
L413         df['EPS_SERIES'] = pd.Series([None] * len(df), index=df.index, dtype=object)
L414         debug_mode = bool(getattr(cfg, "debug_mode", False))
L415         eps_cols = set(getattr(eps_df, "columns", []))
L416         for t in tickers:
L417             d, s = info[t], px[t]; ev = self.ev_fallback(d, tickers_bulk.tickers[t])
L418             try:
L419                 volume_series_full = ib.data['Volume'][t]
L420             except Exception:
L421                 volume_series_full = None
L422
L423             # --- 基本特徴 ---
L424             df.loc[t,'TR']   = self.trend(s)
L425
L426             def _eps_value(col: str) -> float:
L427                 if col not in eps_cols:
L428                     return np.nan
L429                 try:
L430                     return _scalar(eps_df[col].get(t, np.nan))
L431                 except Exception:
L432                     return np.nan
L433
L434             df.loc[t,'EPS']  = _eps_value('EPS_TTM')
L435             df.loc[t,'EPS_Q'] = _eps_value('EPS_Q_LastQ')
L436             df.loc[t,'REV_TTM'] = _eps_value('REV_TTM')
L437             df.loc[t,'REV_Q']   = _eps_value('REV_Q_LastQ')
L438             df.loc[t,'EPS_TTM_PREV'] = _eps_value('EPS_TTM_PREV')
L439             df.loc[t,'REV_TTM_PREV'] = _eps_value('REV_TTM_PREV')
L440             df.loc[t,'EPS_Q_PREV'] = _eps_value('EPS_Q_Prev')
L441             df.loc[t,'REV_Q_PREV'] = _eps_value('REV_Q_Prev')
L442             df.loc[t,'EPS_A_LATEST'] = _eps_value('EPS_A_LATEST')
L443             df.loc[t,'EPS_A_PREV'] = _eps_value('EPS_A_PREV')
L444             df.loc[t,'REV_A_LATEST'] = _eps_value('REV_A_LATEST')
L445             df.loc[t,'REV_A_PREV'] = _eps_value('REV_A_PREV')
L446             df.loc[t,'EPS_A_CAGR3'] = _eps_value('EPS_A_CAGR3')
L447             df.loc[t,'REV_A_CAGR3'] = _eps_value('REV_A_CAGR3')
L448             df.loc[t,'REV']  = d.get('revenueGrowth',np.nan)
L449             df.loc[t,'ROE']  = d.get('returnOnEquity',np.nan)
L450             df.loc[t,'BETA'] = self.calc_beta(s, spx, lookback=252)
L451
L452             # --- 配当（欠損補完含む） ---
L453             div = d.get('dividendYield') if d.get('dividendYield') is not None else d.get('trailingAnnualDividendYield')
L454             if div is None or pd.isna(div):
L455                 try:
L456                     divs = yf.Ticker(t).dividends
L457                     if divs is not None and not divs.empty:
L458                         last_close = s.iloc[-1]; div_1y = divs[divs.index >= (divs.index.max() - pd.Timedelta(days=365))].sum()
L459                         if last_close and last_close>0: div = float(div_1y/last_close)
L460                 except Exception: pass
L461             df.loc[t,'DIV'] = 0.0 if (div is None or pd.isna(div)) else float(div)
L462
L463             # --- FCF/EV ---
L464             fcf_val = fcf_df.loc[t,'FCF_TTM'] if t in fcf_df.index else np.nan
L465             df.loc[t,'FCF'] = (fcf_val/ev) if (pd.notna(fcf_val) and pd.notna(ev) and ev>0) else np.nan
L466
L467             # --- モメンタム・ボラ関連 ---
L468             df.loc[t,'RS'], df.loc[t,'TR_str'] = self.rs(s, spx), self.tr_str(s)
L469             r, rm = s.pct_change().dropna(), spx.pct_change().dropna()
L470             n = int(min(len(r), len(rm)))
L471
L472             DOWNSIDE_DEV = np.nan
L473             if n>=60:
L474                 r6 = r.iloc[-min(len(r),126):]; neg = r6[r6<0]
L475                 if len(neg)>=10: DOWNSIDE_DEV = float(neg.std(ddof=0)*np.sqrt(252))
L476             df.loc[t,'DOWNSIDE_DEV'] = DOWNSIDE_DEV
L477
L478             MDD_1Y = np.nan
L479             try:
L480                 w = s.iloc[-min(len(s),252):].dropna()
L481                 if len(w)>=30:
L482                     roll_max = w.cummax(); MDD_1Y = float((w/roll_max - 1.0).min())
L483             except Exception: pass
L484             df.loc[t,'MDD_1Y'] = MDD_1Y
L485
L486             RESID_VOL = np.nan
L487             if n>=120:
L488                 rr, rrm = r.iloc[-n:].align(rm.iloc[-n:], join='inner')
L489                 if len(rr)==len(rrm) and len(rr)>=120 and rrm.var()>0:
L490                     beta = float(np.cov(rr, rrm)[0,1]/np.var(rrm)); resid = rr - beta*rrm
L491                     RESID_VOL = float(resid.std(ddof=0)*np.sqrt(252))
L492             df.loc[t,'RESID_VOL'] = RESID_VOL
L493
L494             DOWN_OUTPERF = np.nan
L495             if n>=60:
L496                 m, x = rm.iloc[-n:], r.iloc[-n:]; mask = m<0
L497                 if mask.sum()>=10:
L498                     mr, sr = float(m[mask].mean()), float(x[mask].mean())
L499                     DOWN_OUTPERF = (sr - mr)/abs(mr) if mr!=0 else np.nan
L500             df.loc[t,'DOWN_OUTPERF'] = DOWN_OUTPERF
L501
L502             # --- 長期移動平均/位置 ---
L503             sma200 = s.rolling(200).mean(); df.loc[t,'EXT_200'] = np.nan
L504             if pd.notna(sma200.iloc[-1]) and sma200.iloc[-1]!=0: df.loc[t,'EXT_200'] = abs(float(s.iloc[-1]/sma200.iloc[-1]-1.0))
L505
L506             # --- 配当の詳細系 ---
L507             DIV_TTM_PS=DIV_VAR5=DIV_YOY=DIV_FCF_COVER=np.nan
L508             try:
L509                 divs = yf.Ticker(t).dividends.dropna()
L510                 if not divs.empty:
L511                     last_close = s.iloc[-1]; div_1y = float(divs[divs.index >= (divs.index.max()-pd.Timedelta(days=365))].sum())
L512                     DIV_TTM_PS = div_1y if div_1y>0 else np.nan
L513                     ann = divs.groupby(divs.index.year).sum()
L514                     if len(ann)>=2 and ann.iloc[-2]!=0: DIV_YOY = float(ann.iloc[-1]/ann.iloc[-2]-1.0)
L515                     tail = ann.iloc[-5:] if len(ann)>=5 else ann
L516                     if len(tail)>=3 and tail.mean()!=0: DIV_VAR5 = float(tail.std(ddof=1)/abs(tail.mean()))
L517                 so = d.get('sharesOutstanding',None)
L518                 if so and pd.notna(DIV_TTM_PS) and pd.notna(fcf_val) and fcf_val!=0:
L519                     DIV_FCF_COVER = float((fcf_val)/(DIV_TTM_PS*float(so)))
L520             except Exception: pass
L521             df.loc[t,'DIV_TTM_PS'], df.loc[t,'DIV_VAR5'], df.loc[t,'DIV_YOY'], df.loc[t,'DIV_FCF_COVER'] = DIV_TTM_PS, DIV_VAR5, DIV_YOY, DIV_FCF_COVER
L522
L523             # --- 財務安定性 ---
L524             df.loc[t,'DEBT2EQ'], df.loc[t,'CURR_RATIO'] = d.get('debtToEquity',np.nan), d.get('currentRatio',np.nan)
L525
L526             # --- EPS 変動 ---
L527             EPS_VAR_8Q = np.nan
L528             try:
L529                 qe, so = tickers_bulk.tickers[t].quarterly_earnings, d.get('sharesOutstanding',None)
L530                 if qe is not None and not qe.empty and so:
L531                     eps_q = (qe['Earnings'].dropna().astype(float)/float(so)).replace([np.inf,-np.inf],np.nan)
L532                     if len(eps_q)>=4: EPS_VAR_8Q = float(eps_q.iloc[-min(8,len(eps_q)):].std(ddof=1))
L533             except Exception: pass
L534             df.loc[t,'EPS_VAR_8Q'] = EPS_VAR_8Q
L535
L536             # --- サイズ/流動性 ---
L537             df.loc[t,'MARKET_CAP'] = d.get('marketCap',np.nan); adv60 = np.nan
L538             try:
L539                 if isinstance(volume_series_full, pd.Series):
L540                     vol_series = volume_series_full.reindex(s.index).dropna()
L541                     if len(vol_series) >= 5:
L542                         aligned_px = s.reindex(vol_series.index).dropna()
L543                         if len(aligned_px) == len(vol_series):
L544                             dv = (vol_series*aligned_px).rolling(60).mean()
L545                             if not dv.dropna().empty:
L546                                 adv60 = float(dv.dropna().iloc[-1])
L547             except Exception:
L548                 pass
L549             df.loc[t,'ADV60_USD'] = adv60
L550
L551             # --- Rule of 40 や周辺 ---
L552             total_rev_ttm = d.get('totalRevenue',np.nan)
L553             FCF_MGN = _safe_div(fcf_val, total_rev_ttm)
L554             df.loc[t,'FCF_MGN'] = FCF_MGN
L555             rule40 = np.nan
L556             try:
L557                 r = df.loc[t,'REV']; rule40 = (r if pd.notna(r) else np.nan) + (FCF_MGN if pd.notna(FCF_MGN) else np.nan)
L558             except Exception: pass
L559             df.loc[t,'RULE40'] = rule40
L560
L561             # --- トレンド補助 ---
L562             sma50  = s.rolling(50).mean()
L563             sma150 = s.rolling(150).mean()
L564             sma200 = s.rolling(200).mean()
L565             p = _safe_last(s)
L566
L567             df.loc[t,'MA50_OVER_150'] = (_safe_last(sma50)/_safe_last(sma150) - 1
L568                 if pd.notna(_safe_last(sma50)) and pd.notna(_safe_last(sma150)) and _safe_last(sma150)!=0 else np.nan)
L569             df.loc[t,'MA150_OVER_200'] = (_safe_last(sma150)/_safe_last(sma200) - 1
L570                 if pd.notna(_safe_last(sma150)) and pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan)
L571
L572             lo52 = s[-252:].min() if len(s)>=252 else s.min()
L573             df.loc[t,'P_OVER_LOW52'] = (p/lo52 - 1) if (lo52 and lo52>0 and pd.notna(p)) else np.nan
L574
L575             df.loc[t,'MA200_SLOPE_1M'] = np.nan
L576             if len(sma200.dropna()) >= 21:
L577                 cur200 = _safe_last(sma200)
L578                 old2001 = float(sma200.iloc[-21])
L579                 if old2001:
L580                     df.loc[t,'MA200_SLOPE_1M'] = cur200/old2001 - 1
L581
L582             df.loc[t,'P_OVER_150'] = p/_safe_last(sma150)-1 if pd.notna(_safe_last(sma150)) and _safe_last(sma150)!=0 else np.nan
L583             df.loc[t,'P_OVER_200'] = p/_safe_last(sma200)-1 if pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L584             df.loc[t,'MA50_OVER_200'] = _safe_last(sma50)/_safe_last(sma200)-1 if pd.notna(_safe_last(sma50)) and pd.notna(_safe_last(sma200)) and _safe_last(sma200)!=0 else np.nan
L585             df.loc[t,'MA200_SLOPE_5M'] = np.nan
L586             if len(sma200.dropna())>=105:
L587                 cur200, old200 = _safe_last(sma200), float(sma200.iloc[-105])
L588                 if old200 and old200!=0: df.loc[t,'MA200_SLOPE_5M'] = cur200/old200 - 1
L589             # NEW: 200日線が連続で上向きの「日数」
L590             df.loc[t,'MA200_UP_STREAK_D'] = np.nan
L591             try:
L592                 s200 = sma200.dropna()
L593                 if len(s200) >= 2:
L594                     diff200 = s200.diff()
L595                     up = 0
L596                     for v in diff200.iloc[::-1]:
L597                         if pd.isna(v) or v <= 0:
L598                             break
L599                         up += 1
L600                     df.loc[t,'MA200_UP_STREAK_D'] = float(up)
L601             except Exception:
L602                 pass
L603             df.loc[t,'LOW52PCT25_EXCESS'] = np.nan if (lo52 is None or lo52<=0 or pd.isna(p)) else (p/(lo52*1.25)-1)
L604             hi52 = s[-252:].max() if len(s)>=252 else s.max(); df.loc[t,'NEAR_52W_HIGH'] = np.nan
L605             if hi52 and hi52>0 and pd.notna(p):
L606                 d_hi = (p/hi52)-1.0; df.loc[t,'NEAR_52W_HIGH'] = -abs(min(0.0, d_hi))
L607             df.loc[t,'RS_SLOPE_6W'] = self.rs_line_slope(s, ib.spx, 30)
L608             df.loc[t,'RS_SLOPE_13W'] = self.rs_line_slope(s, ib.spx, 65)
L609
L610             df.loc[t,'DIV_STREAK'] = self.div_streak(t)
L611
L612             # --- 欠損メモ ---
L613             fin_cols = ['REV','ROE','BETA','DIV','FCF']
L614             need_finnhub = [col for col in fin_cols if pd.isna(df.loc[t,col])]
L615             if need_finnhub:
L616                 fin_data = self.fetch_finnhub_metrics(t)
L617                 for col in need_finnhub:
L618                     val = fin_data.get(col)
L619                     if val is not None and not pd.isna(val): df.loc[t,col] = val
L620             # 欠損ログは factor 側で補完後に集約する（ここでは検知のみ）
L621
L622         def _pick_series(entry: dict, keys: list[str]):
L623             for k in keys:
L624                 val = entry.get(k) if isinstance(entry, dict) else None
L625                 if val is None:
L626                     continue
L627                 try:
L628                     if hasattr(val, "empty") and getattr(val, "empty"):
L629                         continue
L630                 except Exception:
L631                     pass
L632                 if isinstance(val, (list, tuple)) and len(val) == 0:
L633                     continue
L634                 return val
L635             return None
L636
L637         def _has_sec_series(val) -> bool:
L638             try:
L639                 if isinstance(val, pd.Series):
L640                     return not val.dropna().empty
L641                 if isinstance(val, (list, tuple)):
L642                     return any(pd.notna(v) for v in val)
L643                 return bool(val)
L644             except Exception:
L645                 return False
L646
L647         def _series_len(val) -> int:
L648             try:
L649                 if isinstance(val, pd.Series):
L650                     return int(val.dropna().size)
L651                 if isinstance(val, (list, tuple)):
L652                     return len(val)
L653                 return int(bool(val))
L654             except Exception:
L655                 return 0
L656
L657         for t in tickers:
L658             try:
L659                 d = info.get(t, {}) or {}
L660                 rev_series = d.get("SEC_REV_Q_SERIES")
L661                 eps_series = d.get("SEC_EPS_Q_SERIES")
L662                 fallback_qearn = False
L663                 try:
L664                     qe = tickers_bulk.tickers[t].quarterly_earnings
L665                     fallback_qearn = bool(qe is not None and not getattr(qe, "empty", True))
L666                 except Exception:
L667                     qe = None
L668
L669                 r_src = _pick_series(d, ["SEC_REV_Q_SERIES", "rev_q_series_pairs", "rev_q_series"])
L670                 e_src = _pick_series(d, ["SEC_EPS_Q_SERIES", "eps_q_series_pairs", "eps_q_series"])
L671                 r_raw = _ensure_series(r_src)
L672                 e_raw = _ensure_series(e_src)
L673
L674                 r_q = _to_quarterly(r_raw)
L675                 e_q = _to_quarterly(e_raw)
L676
L677                 df.at[t, "EPS_SERIES"] = e_q
L678
L679                 r_yoy_ttm = _ttm_yoy_from_quarterly(r_q)
L680                 e_yoy_ttm = _ttm_yoy_from_quarterly(e_q)
L681
L682                 def _q_yoy(qs):
L683                     return np.nan if qs is None or len(qs) < 5 else float(qs.iloc[-1] / qs.iloc[-5] - 1.0)
L684
L685                 rev_q_yoy = _q_yoy(r_q)
L686                 eps_q_yoy = _q_yoy(e_q)
L687
L688                 def _annual_from(qs: pd.Series, yoy_ttm: pd.Series):
L689                     if isinstance(qs.index, pd.DatetimeIndex) and len(qs) >= 8:
L690                         ann = qs.groupby(qs.index.year).last().pct_change()
L691                         ann_dn = ann.dropna()
L692                         if not ann_dn.empty:
L693                             y = float(ann_dn.iloc[-1])
L694                             acc = float(ann_dn.tail(3).mean()) if ann_dn.size >= 3 else np.nan
L695                             var = float(ann_dn.tail(4).var()) if ann_dn.size >= 4 else np.nan
L696                             return y, acc, var
L697                     yoy_dn = yoy_ttm.dropna()
L698                     if yoy_dn.empty:
L699                         return np.nan, np.nan, np.nan
L700                     return (
L701                         float(yoy_dn.iloc[-1]),
L702                         float(yoy_dn.tail(3).mean() if yoy_dn.size >= 3 else np.nan),
L703                         float(yoy_dn.tail(4).var() if yoy_dn.size >= 4 else np.nan),
L704                     )
L705
L706                 rev_yoy, rev_acc, rev_var = _annual_from(r_q, r_yoy_ttm)
L707                 eps_yoy, _, _ = _annual_from(e_q, e_yoy_ttm)
L708
L709                 def _pos_streak(s: pd.Series):
L710                     s = s.dropna()
L711                     if s.empty:
L712                         return np.nan
L713                     b = (s > 0).astype(int).to_numpy()[::-1]
L714                     k = 0
L715                     for v in b:
L716                         if v == 1:
L717                             k += 1
L718                         else:
L719                             break
L720                     return float(k)
L721
L722                 rev_ann_streak = _pos_streak(r_yoy_ttm)
L723
L724                 df.loc[t, "REV_Q_YOY"] = rev_q_yoy
L725                 df.loc[t, "EPS_Q_YOY"] = eps_q_yoy
L726                 df.loc[t, "REV_YOY"] = rev_yoy
L727                 df.loc[t, "EPS_YOY"] = eps_yoy
L728                 df.loc[t, "REV_YOY_ACC"] = rev_acc
L729                 df.loc[t, "REV_YOY_VAR"] = rev_var
L730                 df.loc[t, "REV_ANN_STREAK"] = rev_ann_streak
L731
L732             except Exception as e:
L733                 logger.warning("growth-derivatives failed: %s: %s", t, e)
L734
L735         def _pct_change(new, old):
L736             try:
L737                 if np.isfinite(new) and np.isfinite(old) and float(old) != 0:
L738                     return float((new - old) / abs(old))
L739             except Exception:
L740                 pass
L741             return np.nan
L742
L743         def _pct_series(a: pd.Series, b: pd.Series) -> list[float]:
L744             a_vals = pd.to_numeric(a, errors="coerce") if a is not None else pd.Series(np.nan, index=df.index)
L745             b_vals = pd.to_numeric(b, errors="coerce") if b is not None else pd.Series(np.nan, index=df.index)
L746             return [_pct_change(x, y) for x, y in zip(a_vals.reindex(df.index), b_vals.reindex(df.index))]
L747
L748         def _mean_valid(vals: list[float]) -> float:
L749             arr = [float(v) for v in vals if np.isfinite(v)]
L750             return float(np.mean(arr)) if arr else np.nan
L751
L752         grw_q_eps_last = _pct_series(df['EPS_Q'], df.get('EPS_Q_PREV', pd.Series(np.nan, index=df.index)))
L753         grw_q_rev_last = _pct_series(df['REV_Q'], df.get('REV_Q_PREV', pd.Series(np.nan, index=df.index)))
L754         grw_q_eps_ttm = _pct_series(df['EPS'], df.get('EPS_TTM_PREV', pd.Series(np.nan, index=df.index)))
L755         grw_q_rev_ttm = _pct_series(df['REV_TTM'], df.get('REV_TTM_PREV', pd.Series(np.nan, index=df.index)))
L756
L757         grw_a_eps_yoy = _pct_series(df.get('EPS_A_LATEST', pd.Series(np.nan, index=df.index)), df.get('EPS_A_PREV', pd.Series(np.nan, index=df.index)))
L758         grw_a_rev_yoy = _pct_series(df.get('REV_A_LATEST', pd.Series(np.nan, index=df.index)), df.get('REV_A_PREV', pd.Series(np.nan, index=df.index)))
L759         grw_a_eps_cagr = pd.to_numeric(df.get('EPS_A_CAGR3', pd.Series(np.nan, index=df.index)), errors="coerce").reindex(df.index).tolist()
L760         grw_a_rev_cagr = pd.to_numeric(df.get('REV_A_CAGR3', pd.Series(np.nan, index=df.index)), errors="coerce").reindex(df.index).tolist()
L761
L762         grw_q_combined = [
L763             _mean_valid([a, b, c, d])
L764             for a, b, c, d in zip(grw_q_eps_last, grw_q_rev_last, grw_q_eps_ttm, grw_q_rev_ttm)
L765         ]
L766         grw_a_combined = [
L767             _mean_valid([a, b, c, d])
L768             for a, b, c, d in zip(grw_a_eps_yoy, grw_a_rev_yoy, grw_a_eps_cagr, grw_a_rev_cagr)
L769         ]
L770
L771         df['GRW_Q_RAW'] = pd.Series(grw_q_combined, index=df.index, dtype=float)
L772         df['GRW_A_RAW'] = pd.Series(grw_a_combined, index=df.index, dtype=float)
L773
L774         def _trend_template_pass(row, rs_alpha_thresh=0.10):
L775             c1 = (row.get('P_OVER_150', np.nan) > 0) and (row.get('P_OVER_200', np.nan) > 0)
L776             c2 = (row.get('MA150_OVER_200', np.nan) > 0)
L777             c3 = (row.get('MA200_SLOPE_1M', np.nan) > 0)
L778             c4 = (row.get('MA50_OVER_150', np.nan) > 0) and (row.get('MA50_OVER_200', np.nan) > 0)
L779             c5 = (row.get('TR_str', np.nan) > 0)
L780             c6 = (row.get('P_OVER_LOW52', np.nan) >= 0.30)
L781             c7 = (row.get('NEAR_52W_HIGH', np.nan) >= -0.25)
L782             c8 = (row.get('RS', np.nan) >= 0.10)
L783             return bool(c1 and c2 and c3 and c4 and c5 and c6 and c7 and c8)
L784
L785         if 'trend_template' not in df.columns: df['trend_template'] = df.apply(_trend_template_pass, axis=1).fillna(False)
L786         assert 'trend_template' in df.columns
L787
L788         def _calc_eps_abs_slope(eps_series, n=12):
L789             try:
L790                 if isinstance(eps_series, pd.Series):
L791                     series = pd.to_numeric(eps_series, errors="coerce").dropna()
L792                 elif isinstance(eps_series, (list, tuple, np.ndarray)):
L793                     series = pd.Series(eps_series, dtype=float).dropna()
L794                 else:
L795                     return 0.0
L796             except Exception:
L797                 return 0.0
L798
L799             if series.empty:
L800                 return 0.0
L801
L802             tail = series.tail(n).to_numpy(dtype=float)
L803             if tail.size < 2:
L804                 return 0.0
L805
L806             x = np.arange(tail.size, dtype=float)
L807             x = x - x.mean()
L808             y = tail - tail.mean()
L809             denom = np.dot(x, x)
L810             if denom == 0:
L811                 return 0.0
L812             slope = float(np.dot(x, y) / denom)
L813             return slope
L814
L815         df['EPS_ABS_SLOPE'] = df['EPS_SERIES'].apply(_calc_eps_abs_slope).astype(float)
L816         df.drop(columns=['EPS_SERIES'], inplace=True)
L817
L818         # === Z化と合成 ===
L819         for col in ['ROE','FCF','REV','EPS']: df[f'{col}_W'] = winsorize_s(df[col], 0.02)
L820
L821         df_z = pd.DataFrame(index=df.index)
L822         for col in ['EPS','REV','ROE','FCF','RS','TR_str','BETA','DIV','DIV_STREAK']: df_z[col] = robust_z(df[col])
L823         df_z['REV'], df_z['EPS'], df_z['TR'] = robust_z(df['REV_W']), robust_z(df['EPS_W']), robust_z(df['TR'])
L824         for col in ['P_OVER_150','P_OVER_200','MA200_SLOPE_5M','NEAR_52W_HIGH','RS_SLOPE_6W','RS_SLOPE_13W','MA200_UP_STREAK_D']: df_z[col] = robust_z(df[col])
L825
L826         df_z['EPS_ABS_SLOPE'] = robust_z(df['EPS_ABS_SLOPE']).clip(-3.0, 3.0)
L827
L828         # === Growth深掘り系（欠損保持z + RAW併載） ===
L829         grw_cols = ['REV_Q_YOY','EPS_Q_YOY','REV_YOY','EPS_YOY','REV_YOY_ACC','REV_YOY_VAR','FCF_MGN','RULE40','REV_ANN_STREAK']
L830         for col in grw_cols:
L831             if col in df.columns:
L832                 raw = pd.to_numeric(df[col], errors="coerce")
L833                 df_z[col] = robust_z_keepnan(raw)
L834         for k in ("TREND_SLOPE_EPS", "TREND_SLOPE_REV"):
L835             if k in df.columns and k not in df_z.columns:
L836                 raw = pd.to_numeric(df[k], errors="coerce")
L837                 df_z[k] = robust_z_keepnan(raw)
L838         for col in ['DOWNSIDE_DEV','MDD_1Y','RESID_VOL','DOWN_OUTPERF','EXT_200','DIV_VAR5','DIV_FCF_COVER','DEBT2EQ','CURR_RATIO','EPS_VAR_8Q','MARKET_CAP','ADV60_USD']: df_z[col] = robust_z(df[col])
L839
L840         df_z['SIZE'], df_z['LIQ'] = robust_z(np.log1p(df['MARKET_CAP'])), robust_z(np.log1p(df['ADV60_USD']))
L841         df_z['QUALITY_F'] = robust_z(0.6*df['FCF_W'] + 0.4*df['ROE_W']).clip(-3.0,3.0)
L842         df_z['YIELD_F']   = 0.3*df_z['DIV'] + 0.7*df_z['DIV_STREAK']
L843
L844         # EPSが赤字でもFCFが黒字なら実質黒字とみなす
L845         eps_pos_mask = (df['EPS'] > 0) | (df['FCF_MGN'] > 0)
L846         df_z['EPS_POS'] = df_z['EPS'].where(eps_pos_mask, 0.0)
L847
L848         # ===== トレンドスロープ算出 =====
L849         def zpos(x):
L850             arr = robust_z(x)
L851             idx = getattr(x, 'index', df_z.index)
L852             return pd.Series(arr, index=idx).fillna(0.0)
L853
L854         def relu(x):
L855             ser = x if isinstance(x, pd.Series) else pd.Series(x, index=df_z.index)
L856             return ser.clip(lower=0).fillna(0.0)
L857
L858         # 売上トレンドスロープ（四半期）
L859         slope_rev = 0.70*zpos(df_z['REV_Q_YOY']) + 0.30*zpos(df_z['REV_YOY_ACC'])
L860         noise_rev = relu(robust_z(df_z['REV_YOY_VAR']) - 0.8)
L861         slope_rev_combo = slope_rev - 0.25*noise_rev
L862         df_z['TREND_SLOPE_REV'] = slope_rev_combo.clip(-3.0, 3.0)
L863
L864         # EPSトレンドスロープ（四半期）
L865         slope_eps = (
L866             0.40*zpos(df_z['EPS_Q_YOY']) +
L867             0.20*zpos(df_z['EPS_POS']) +
L868             0.40*zpos(df_z['EPS_ABS_SLOPE'])
L869         )
L870         df_z['TREND_SLOPE_EPS'] = slope_eps.clip(-3.0, 3.0)
L871
L872         # 年次トレンド（サブ）
L873         slope_rev_yr = zpos(df_z['REV_YOY'])
L874         slope_eps_yr = zpos(df_z.get('EPS_YOY', pd.Series(0.0, index=df.index)))
L875         streak_base = df['REV_ANN_STREAK'].clip(lower=0).fillna(0)
L876         streak_yr = streak_base / (streak_base.abs() + 1.0)
L877         slope_rev_yr_combo = 0.7*slope_rev_yr + 0.3*streak_yr
L878         df_z['TREND_SLOPE_REV_YR'] = slope_rev_yr_combo.clip(-3.0, 3.0)
L879         df_z['TREND_SLOPE_EPS_YR'] = slope_eps_yr.clip(-3.0, 3.0)
L880
L881         grw_q_z = robust_z_keepnan(df['GRW_Q_RAW']).clip(-3.0, 3.0)
L882         grw_a_z = robust_z_keepnan(df['GRW_A_RAW']).clip(-3.0, 3.0)
L883         df_z['GRW_Q'] = grw_q_z
L884         df_z['GRW_A'] = grw_a_z
L885
L886         try:
L887             mix = float(os.environ.get("GRW_Q_ANNUAL_MIX", "0.7"))
L888         except Exception:
L889             mix = 0.7
L890         if not np.isfinite(mix):
L891             mix = 0.7
L892         mix = float(np.clip(mix, 0.0, 1.0))
L893
L894         weights_q: list[float] = []
L895         weights_a: list[float] = []
L896         grw_mix: list[float] = []
L897         for idx in df.index:
L898             q_val = grw_q_z.get(idx, np.nan)
L899             a_val = grw_a_z.get(idx, np.nan)
L900             q_ok = np.isfinite(q_val)
L901             a_ok = np.isfinite(a_val)
L902             if q_ok and a_ok:
L903                 wq, wa = mix, 1.0 - mix
L904             elif q_ok:
L905                 wq, wa = 1.0, 0.0
L906             elif a_ok:
L907                 wq, wa = 0.0, 1.0
L908             else:
L909                 wq = wa = np.nan
L910                 grw_mix.append(np.nan)
L911                 weights_q.append(wq)
L912                 weights_a.append(wa)
L913                 continue
L914             weights_q.append(wq)
L915             weights_a.append(wa)
L916             grw_mix.append(q_val * wq + a_val * wa)
L917
L918         wq_series = pd.Series(weights_q, index=df.index, dtype=float)
L919         wa_series = pd.Series(weights_a, index=df.index, dtype=float)
L920         grw_series = pd.Series(grw_mix, index=df.index, dtype=float).clip(-3.0, 3.0)
L921
L922         df_z['GROWTH_F'] = grw_series
L923         df_z['GRW_FLEX_WEIGHT'] = 1.0  # 現状は固定（SECの可用性に依らず）
L924
L925         if str(os.environ.get("GRW_DBG_DETAIL", "0")).strip().lower() in {"1", "true", "yes", "on"}:
L926             df_z['GRW_Q_DBG'] = pd.Series(df['GRW_Q_RAW'], index=df.index, dtype=float)
L927             df_z['GRW_A_DBG'] = pd.Series(df['GRW_A_RAW'], index=df.index, dtype=float)
L928             df_z['GRW_WQ_DBG'] = wq_series
L929             df_z['GRW_WA_DBG'] = wa_series
L930
L931         df_z['MOM_F'] = robust_z(0.40*df_z['RS']
L932             + 0.15*df_z['TR_str']
L933             + 0.15*df_z['RS_SLOPE_6W']
L934             + 0.15*df_z['RS_SLOPE_13W']
L935             + 0.10*df_z['MA200_SLOPE_5M']
L936             + 0.10*df_z['MA200_UP_STREAK_D']).clip(-3.0,3.0)
L937         df_z['VOL'] = robust_z(df['BETA'])
L938         df_z['QAL'], df_z['YLD'], df_z['MOM'] = df_z['QUALITY_F'], df_z['YIELD_F'], df_z['MOM_F']
L939         df_z.drop(columns=['QUALITY_F','YIELD_F','MOM_F'], inplace=True, errors='ignore')
L940
L941         # df_z 全明細をページングしてログ出力（最小版）
L942         if getattr(cfg, "debug_mode", False):
L943             pd.set_option("display.max_columns", None)
L944             pd.set_option("display.max_colwidth", None)
L945             pd.set_option("display.width", None)
L946             page = int(getattr(cfg, "debug_dfz_page", 50))  # デフォルト50行単位
L947             n = len(df_z)
L948             logger.info("=== df_z FULL DUMP start === rows=%d cols=%d page=%d", n, df_z.shape[1], page)
L949             for i in range(0, n, page):
L950                 j = min(i + page, n)
L951                 try:
L952                     chunk_str = df_z.iloc[i:j].to_string()
L953                 except Exception:
L954                     chunk_str = df_z.iloc[i:j].astype(str).to_string()
L955                 logger.info("--- df_z rows %d..%d ---\n%s", i, j-1, chunk_str)
L956             logger.info("=== df_z FULL DUMP end ===")
L957
L958         # === begin: BIO LOSS PENALTY =====================================
L959         try:
L960             penalty_z = float(os.getenv("BIO_LOSS_PENALTY_Z", "0.8"))
L961         except Exception:
L962             penalty_z = 0.8
L963
L964         def _is_bio_like(t: str) -> bool:
L965             inf = info.get(t, {}) if isinstance(info, dict) else {}
L966             sec = str(inf.get("sector", "")).lower()
L967             ind = str(inf.get("industry", "")).lower()
L968             if "health" not in sec:
L969                 return False
L970             keys = ("biotech", "biopharma", "pharma")
L971             return any(k in ind for k in keys)
L972
L973         tickers_s = pd.Index(df_z.index)
L974         is_bio = pd.Series({t: _is_bio_like(t) for t in tickers_s})
L975         is_loss = pd.Series({t: (pd.notna(df.loc[t,"EPS"]) and df.loc[t,"EPS"] <= 0) for t in tickers_s})
L976         mask_bio_loss = (is_bio & is_loss).reindex(df_z.index).fillna(False)
L977
L978         if bool(mask_bio_loss.any()) and penalty_z > 0:
L979             df_z.loc[mask_bio_loss, "GROWTH_F"] = df_z.loc[mask_bio_loss, "GROWTH_F"] - penalty_z
L980             df_z["GROWTH_F"] = df_z["GROWTH_F"].clip(-3.0, 3.0)
L981         # === end: BIO LOSS PENALTY =======================================
L982
L983         _debug_only_cols = [c for c in df_z.columns if c.endswith("_RAW")]
L984         _no_score_cols = ["DIV_TTM_PS", "DIV_YOY", "LOW52PCT25_EXCESS", "MA50_OVER_200"]
L985         _drop_cols = [c for c in (_debug_only_cols + _no_score_cols) if c in df_z.columns]
L986         if _drop_cols:
L987             df_z = df_z.drop(columns=_drop_cols, errors="ignore")
L988
L989         assert not any(c.endswith("_RAW") for c in df_z.columns)
L990         for c in ["DIV_TTM_PS","DIV_YOY","LOW52PCT25_EXCESS","MA50_OVER_200"]:
L991             assert c not in df_z.columns
L992
L993         df_z['TRD'] = 0.0  # TRDはスコア寄与から外し、テンプレ判定はフィルタで行う（列は表示互換のため残す）
L994         if 'BETA' not in df_z.columns: df_z['BETA'] = robust_z(df['BETA'])
L995
L996         df_z['D_VOL_RAW'] = robust_z(0.40*df_z['DOWNSIDE_DEV'] + 0.22*df_z['RESID_VOL'] + 0.18*df_z['MDD_1Y'] - 0.10*df_z['DOWN_OUTPERF'] - 0.05*df_z['EXT_200'] - 0.08*df_z['SIZE'] - 0.10*df_z['LIQ'] + 0.10*df_z['BETA'])
L997         df_z['D_QAL']     = robust_z(0.35*df_z['QAL'] + 0.20*df_z['FCF'] + 0.15*df_z['CURR_RATIO'] - 0.15*df_z['DEBT2EQ'] - 0.15*df_z['EPS_VAR_8Q'])
L998         df_z['D_YLD']     = robust_z(0.45*df_z['DIV'] + 0.25*df_z['DIV_STREAK'] + 0.20*df_z['DIV_FCF_COVER'] - 0.10*df_z['DIV_VAR5'])
L999         df_z['D_TRD']     = robust_z(0.40*df_z.get('MA200_SLOPE_5M',0) - 0.30*df_z.get('EXT_200',0) + 0.15*df_z.get('NEAR_52W_HIGH',0) + 0.15*df_z['TR'])
L1000
L1001         # --- 重みは cfg を優先（外部があればそれを使用） ---
L1002         # ① 全銘柄で G/D スコアを算出（unmasked）
L1003         g_weights = pd.Series(cfg.weights.g, dtype=float)
L1004         need_g = ["GROWTH_F", "MOM"]
L1005         dbg_cols = ["GROWTH_F", "MOM", "VOL"]
L1006         if all(c in df_z.columns for c in need_g):
L1007             mask_g = df_z[need_g].notna().all(axis=1)
L1008         else:
L1009             mask_g = pd.Series(False, index=df_z.index, dtype=bool)
L1010         for c in dbg_cols:
L1011             if c in df_z.columns:
L1012                 df_z[f"DBGRW.{c}"] = df_z[c]
L1013         df_fill_g = df_z.reindex(columns=g_weights.index, fill_value=np.nan).copy()
L1014         for c in df_fill_g.columns:
L1015             if c not in need_g:
L1016                 df_fill_g[c] = df_fill_g[c].fillna(0)
L1017         g_score_all = _as_numeric_series(
L1018             df_fill_g.mul(g_weights.reindex(df_fill_g.columns)).sum(axis=1, skipna=False)
L1019         )
L1020         g_score_all = g_score_all.where(mask_g)
L1021
L1022         d_comp = pd.concat({
L1023             'QAL': df_z['D_QAL'],
L1024             'YLD': df_z['D_YLD'],
L1025             'VOL': df_z['D_VOL_RAW'],
L1026             'TRD': df_z['D_TRD']
L1027         }, axis=1)
L1028         dw = pd.Series(cfg.weights.d, dtype=float).reindex(['QAL','YLD','VOL','TRD']).fillna(0.0)
L1029         globals()['D_WEIGHTS_EFF'] = dw.copy()
L1030         need_d_candidates = ["VOL", "QAL"]
L1031         mask_d = pd.Series(True, index=d_comp.index, dtype=bool)
L1032         for c in need_d_candidates:
L1033             if c in d_comp.columns:
L1034                 mask_d &= d_comp[c].notna()
L1035             else:
L1036                 mask_d &= False
L1037         df_fill_d = d_comp.copy()
L1038         for c in df_fill_d.columns:
L1039             if c not in need_d_candidates:
L1040                 df_fill_d[c] = df_fill_d[c].fillna(0)
L1041         d_score_all = _as_numeric_series(
L1042             df_fill_d.mul(dw, axis=1).sum(axis=1, skipna=False)
L1043         )
L1044         d_score_all = d_score_all.where(mask_d)
L1045
L1046         # ② テンプレ判定（既存ロジックそのまま）
L1047         mask = df['trend_template']
L1048         if not bool(mask.any()):
L1049             mask = ((df.get('P_OVER_LOW52', np.nan) >= 0.25) &
L1050                 (df.get('NEAR_52W_HIGH', np.nan) >= -0.30) &
L1051                 (df.get('RS', np.nan) >= 0.08) &
L1052                 (df.get('MA200_SLOPE_1M', np.nan) > 0) &
L1053                 (df.get('P_OVER_150', np.nan) > 0) & (df.get('P_OVER_200', np.nan) > 0) &
L1054                 (df.get('MA150_OVER_200', np.nan) > 0) &
L1055                 (df.get('MA50_OVER_150', np.nan) > 0) & (df.get('MA50_OVER_200', np.nan) > 0) &
L1056                 (df.get('TR_str', np.nan) > 0)).fillna(False)
L1057             df['trend_template'] = mask
L1058
L1059         # ③ 採用用は mask、表示/分析用は列で全銘柄保存
L1060         g_score = _as_numeric_series(g_score_all.loc[mask])
L1061         Scorer.g_score = g_score
L1062         df_z['GSC'] = g_score_all
L1063         df_z['DSC'] = d_score_all
L1064
L1065         try:
L1066             current = (pd.read_csv("current_tickers.csv")
L1067                   .iloc[:, 0]
L1068                   .str.upper()
L1069                   .tolist())
L1070         except FileNotFoundError:
L1071             warnings.warn("current_tickers.csv not found — bonus skipped")
L1072             current = []
L1073
L1074         mask_bonus = g_score.index.isin(current)
L1075         if mask_bonus.any():
L1076             # 1) factor.BONUS_COEFF から k を決め、無ければ 0.4
L1077             k = float(getattr(sys.modules.get("factor"), "BONUS_COEFF", 0.4))
L1078             # 2) g 側の σ を取り、NaN なら 0 に丸める
L1079             sigma_g = g_score.std()
L1080             if pd.isna(sigma_g):
L1081                 sigma_g = 0.0
L1082             bonus_g = round(k * sigma_g, 3)
L1083             g_score.loc[mask_bonus] += bonus_g
L1084             Scorer.g_score = g_score
L1085             # 3) D 側も同様に σ の NaN をケア
L1086             sigma_d = d_score_all.std()
L1087             if pd.isna(sigma_d):
L1088                 sigma_d = 0.0
L1089             bonus_d = round(k * sigma_d, 3)
L1090             d_score_all.loc[d_score_all.index.isin(current)] += bonus_d
L1091
L1092         try:
L1093             df = _apply_growth_entry_flags(df, ib, self, win_breakout=5, win_pullback=5)
L1094         except Exception:
L1095             pass
L1096
L1097         df_full = df.copy()
L1098         df_full_z = df_z.copy()
L1099
L1100         from factor import FeatureBundle  # type: ignore  # 実行時importなし（循環回避）
L1101         missing_logs_df = getattr(ib, "missing_logs", pd.DataFrame())
L1102         if not isinstance(missing_logs_df, pd.DataFrame):
L1103             try:
L1104                 missing_logs_df = pd.DataFrame(missing_logs_df)
L1105             except Exception:
L1106                 missing_logs_df = pd.DataFrame()
L1107
L1108         return FeatureBundle(df=df,
L1109             df_z=df_z,
L1110             g_score=g_score,
L1111             d_score_all=d_score_all,
L1112             missing_logs=missing_logs_df,
L1113             df_full=df_full,
L1114             df_full_z=df_full_z,
L1115             scaler=None)
L1116
L1117 def _apply_growth_entry_flags(feature_df, bundle, self_obj, win_breakout=5, win_pullback=5):
L1118     """
L1119     G枠ユニバースに対し、ブレイクアウト確定/押し目反発の「直近N営業日内の発火」を判定し、
L1120     次の列を feature_df に追加する（index=ticker）。
L1121       - G_BREAKOUT_recent_5d : bool
L1122       - G_BREAKOUT_last_date : str "YYYY-MM-DD"
L1123       - G_PULLBACK_recent_5d : bool
L1124       - G_PULLBACK_last_date : str "YYYY-MM-DD"
L1125       - G_PIVOT_price        : float
L1126     失敗しても例外は握り潰し、既存処理を阻害しない。
L1127     """
L1128     try:
L1129         px   = bundle.px                      # 終値 DataFrame
L1130         hi   = bundle.data['High']
L1131         lo   = bundle.data['Low']
L1132         vol  = bundle.data['Volume']
L1133         bench= bundle.spx                     # ベンチマーク Series
L1134
L1135         # Gユニバース推定：self.g_universe 優先 → feature_df['group']=='G' → 全銘柄
L1136         g_universe = getattr(self_obj, "g_universe", None)
L1137         if g_universe is None:
L1138             try:
L1139                 g_universe = feature_df.index[feature_df['group'].astype(str).str.upper().eq('G')].tolist()
L1140             except Exception:
L1141                 g_universe = list(feature_df.index)
L1142         if not g_universe:
L1143             return feature_df
L1144
L1145         # 指標
L1146         px = px.ffill(limit=2)
L1147         ema21 = px[g_universe].ewm(span=21, adjust=False).mean()
L1148         ma50  = px[g_universe].rolling(50).mean()
L1149         ma150 = px[g_universe].rolling(150).mean()
L1150         ma200 = px[g_universe].rolling(200).mean()
L1151         atr20 = (hi[g_universe] - lo[g_universe]).rolling(20).mean()
L1152         vol20 = vol[g_universe].rolling(20).mean()
L1153         vol50 = vol[g_universe].rolling(50).mean()
L1154
L1155         # トレンドテンプレート合格
L1156         trend_template_ok = (px[g_universe] > ma50) & (px[g_universe] > ma150) & (px[g_universe] > ma200) \
L1157                             & (ma150 > ma200) & (ma200.diff() > 0)
L1158
L1159         # 汎用ピボット：直近65営業日の高値（当日除外）
L1160         pivot_price = hi[g_universe].rolling(65).max().shift(1)
L1161
L1162         # 相対力：年内高値更新
L1163         bench_aligned = bench.reindex(px.index).ffill()
L1164         rs = px[g_universe].div(bench_aligned, axis=0)
L1165         rs_high = rs.rolling(252).max().shift(1)
L1166
L1167         # ブレイクアウト「発生日」：条件立ち上がり
L1168         breakout_today = trend_template_ok & (px[g_universe] > pivot_price) \
L1169                          & (vol[g_universe] >= 1.5 * vol50) & (rs > rs_high)
L1170         breakout_event = breakout_today & ~breakout_today.shift(1).fillna(False)
L1171
L1172         # 押し目反発「発生日」：EMA21帯×出来高ドライアップ×前日高値越え×終値EMA21上
L1173         near_ema21_band = px[g_universe].between(ema21 - atr20, ema21 + atr20)
L1174         volume_dryup = (vol20 / vol50) <= 1.0
L1175         pullback_bounce_confirmed = (px[g_universe] > hi[g_universe].shift(1)) & (px[g_universe] > ema21)
L1176         pullback_today = trend_template_ok & near_ema21_band & volume_dryup & pullback_bounce_confirmed
L1177         pullback_event = pullback_today & ~pullback_today.shift(1).fillna(False)
L1178
L1179         # 直近N営業日内の発火 / 最終発生日
L1180         rows = []
L1181         for t in g_universe:
L1182             def _recent_and_date(s, win):
L1183                 sw = s[t].iloc[-win:]
L1184                 if sw.any():
L1185                     d = sw[sw].index[-1]
L1186                     return True, d.strftime("%Y-%m-%d")
L1187                 return False, ""
L1188             br_recent, br_date = _recent_and_date(breakout_event, win_breakout)
L1189             pb_recent, pb_date = _recent_and_date(pullback_event, win_pullback)
L1190             rows.append((t, {
L1191                 "G_BREAKOUT_recent_5d": br_recent,
L1192                 "G_BREAKOUT_last_date": br_date,
L1193                 "G_PULLBACK_recent_5d": pb_recent,
L1194                 "G_PULLBACK_last_date": pb_date,
L1195                 "G_PIVOT_price": float(pivot_price[t].iloc[-1]) if t in pivot_price.columns else float('nan'),
L1196             }))
L1197         flags = pd.DataFrame({k: v for k, v in rows}).T
L1198
L1199         # 列を作成・上書き
L1200         cols = ["G_BREAKOUT_recent_5d","G_BREAKOUT_last_date","G_PULLBACK_recent_5d","G_PULLBACK_last_date","G_PIVOT_price"]
L1201         for c in cols:
L1202             if c not in feature_df.columns:
L1203                 feature_df[c] = np.nan
L1204         feature_df.loc[flags.index, flags.columns] = flags
L1205
L1206     except Exception:
L1207         pass
L1208     return feature_df
L1209
```

## <.github/workflows/weekly-report.yml>
```text
L1 name: Weekly Stock Report
L2
L3 on:
L4   push:
L5     branches: [ main ]
L6     paths-ignore:
L7       - 'CodeForChat/**'
L8   schedule:
L9     - cron: '0 0 * * 6'  # UTC 00:00 → JST 09:00（土）
L10   workflow_dispatch:
L11
L12 jobs:
L13   build-and-report:
L14     runs-on: ubuntu-latest
L15     permissions:
L16       contents: write
L17
L18     steps:
L19       - name: Debug start
L20         run: echo '🚀 DEBUGstarted'
L21               
L22       - name: Checkout repository
L23         uses: actions/checkout@v3
L24
L25       - name: Setup Python
L26         uses: actions/setup-python@v5
L27         with:
L28           python-version: '3.x'
L29           cache: 'pip'
L30           cache-dependency-path: requirements.txt
L31
L32       - name: Install dependencies
L33         run: pip install -r requirements.txt
L34
L35       - name: Prepare results directory
L36         run: mkdir -p results
L37
L38       - name: Run factor & scoring
L39         env:
L40           SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
L41           FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
L42           FIN_THREADS: "8"
L43         run: python factor.py
```

## <documents/README.md>
```text
L1 # 運用ルール
L2
L3 ## 基本構成
L4 - 20銘柄を均等配分（現金を除き1銘柄あたり5%）
L5 - moomoo証券で運用
L6 - **Growth枠 12銘柄 / Defense枠 8銘柄**（NORMAL 基準）
L7
L8 ## Barbell Growth-Defense方針
L9 - Growth枠 **12銘柄**：高成長で乖離源となる攻めの銘柄
L10 - Defense枠 **8銘柄**：低ボラで安定成長し配当を増やす守りの銘柄
L11 - 「猛烈に伸びる攻め × 着実に稼ぐ盾」の組合せで乖離→半戻しプレミアムを狙う
L12
L13 ## レジーム判定（trend_template 合格“本数”で判定）
L14 - 合格本数 = current+candidate 全体のうち、trend_template 条件を満たした銘柄の**本数(C)**（基準 N_G=12）
L15 - しきい値は過去~600営業日の分布から**毎回自動採用**（分位点と運用“床”のmax）
L16   - 緊急入り: `max(q05, 12本)`（= N_G）
L17   - 緊急解除: `max(q20, 18本)`（= ceil(1.5×12)）
L18   - 通常復帰: `max(q60, 36本)`（= 3×N_G）
L19 - ヒステリシス: 前回モードに依存（EMERG→解除は23本以上、CAUTION→通常は45本以上）
L20
L21 ## レジーム別の現金・ドリフト
L22  - **通常(NORMAL)** : 現金 **10%** / ドリフト閾値 **12%**
L23  - **警戒(CAUTION)** : 現金 **12.5%** / ドリフト閾値 **14%**
L24  - **緊急(EMERG)** : 現金 **20%** / **ドリフト売買停止**（20×5%に全戻しのみ）
L25
L26 ## モード別の推奨“保有銘柄数”（MMF≒現金）
L27 *各枠=5%（20銘柄均等）。モード移行時は**Gの枠数のみ**調整し、外した枠は現金として保持。*
L28
L29 - **NORMAL:** G **12** / D **8** / 現金化枠 **0**  
L30 - **CAUTION:** G **10** / D **8** / 現金化枠 **2**（= 10%）  
L31 - **EMERG:** G **8**  / D **8** / 現金化枠 **4**（= 20%）  
L32
L33 > 実運用：⭐️低スコアのGから順に外す。解除時はfactor上位から補充。
L34
L35 ## トレーリングストップ
L36 - **基本TS (モード別):** NORMAL **15%** / CAUTION **13%** / EMERG **10%**
L37 - 含み益が **+30% / +60% / +100%** 到達で、基本から **-3pt / -6pt / -8pt** 引き上げ
L38 - TS発動で減少した銘柄は翌日以降に補充（※緊急モード中は補充しない）
L39
L40 ## 半戻し（リバランス）手順
L41 ドリフトチェックで**アラート**が出た場合（合計|drift| がモード閾値を超過、EMERG除く）、翌営業日の米国寄付きで下記を実施する。
L42
L43 1. **売却（必須）**  
L44    Slackテーブルの **Δqty がマイナスの銘柄を売却** する（寄付き成行推奨）。  
L45    これは「半戻し」計算に基づく過重量の削減を意味する。
L46
L47 2. **購入（任意・半戻し目安）**  
L48    半戻し後の合計|drift|を**シミュレーション値（Slackヘッダに表示）**に近づけることを目安に、  
L49    **任意の銘柄を買い増し**してバランスを取る（Δqtyがプラスの銘柄を優先してもよい）。
L50
L51 3. **トレーリングストップの再設定（必須）**  
L52    すべての保有銘柄について、最新の評価額に合わせてTSを**再発注／更新**する。  
L53    ルールは下記（利益到達で段階的にタイト化）：  
L54    - **基本TS:** -15%  
L55    - **+30% 到達 → TS -12%**  
L56    - **+60% 到達 → TS -9%**  
L57    - **+100% 到達 → TS -7%**  
L58    ※ストップ価格の引き上げは許可、**引き下げは不可**（利益保全の原則）。
L59
L60 4. **例外（EMERGモード）**  
L61    緊急(EMERG)では**ドリフト由来の売買は停止（∞）**。20銘柄×各5%への**全戻し**のみ許容。
L62
L63 5. **実行タイミング**
L64    - 判定：米国市場終値直後
L65    - 執行：翌営業日の米国寄付き成行
L66
L67 ## モード移行の実務手順（超シンプル）
L68 モードが変わったら、**MMF≒現金**として扱い、**Gの枠数だけ**を調整する：
L69 1. **Gを削る**（CAUTION/EMERG）  
L70    - ⭐️低スコアのGから順に外す。  
L71    - **`current_tickers.csv` から外すG銘柄の行を削除**（＝その枠は現金化）。
L72 2. **現金として保持**  
L73    - 外した枠は現金（またはMMF相当）でプール。  
L74 3. **復帰時の補充**（NORMALへ）  
L75    - **`current_tickers.csv` に銘柄を追加**（factor上位から）。  
L76    - 以降は日次ドリフト/TSルールに従う。
L77
L78 > driftは `target_ratio = 1/銘柄数` を自動適用。行数に応じて自動で均等比率が再計算される。
L79
L80 ## 入替銘柄選定
L81 - Oxfordキャピタル／インカム、Alpha Investor、Motley Fool Stock Advisor、moomooスクリーニング等を参考にchatGPTで検討
L82 - 年間NISA枠はGrowth群の中から低ボラ銘柄を選定し利用。長期保持にはこだわらない。
L83
L84 ## 再エントリー（クールダウン）
L85 - TSヒット後の同銘柄再INは **8営業日** のクールダウンを設ける（期間中は再IN禁止）
L86
L87 ## 実行タイミング
L88 - 判定：米国市場終値直後
L89 - 執行：翌営業日の米国寄付き成行
```

## <documents/factor_design.md>
```text
L1 # factor.py 詳細設計書
L2
L3 ## 概要
L4 - 既存ポートフォリオの銘柄と検討中の銘柄群を同時に扱う銘柄選定パイプライン。
L5 - 価格・財務データを取り込みスコアリングとDRRS選定を行うことで、以下のアウトプットを得る。
L6   - 採用銘柄と惜しくも漏れた銘柄のスコア一覧
L7   - IN/OUTのティッカーリストとOUT側の低スコア銘柄
L8   - 新旧ポートフォリオの比較表
L9   - 検討中銘柄の低スコアランキング（整理用）
L10
L11 ## 全体フロー
L12 1. **Input** – `current_tickers.csv`と`candidate_tickers.csv`を読み込み、yfinanceやFinnhubのAPIから価格・財務データを収集して`InputBundle`を整備。
L13 2. **Score Calculation** – Scorerが特徴量を計算し因子スコアを合成して`FeatureBundle`を生成。
L14 3. **Correlation Reduction & Selection** – SelectorがDRRSロジックで相関を抑えつつG/D銘柄を選定し`SelectionBundle`を得る。
L15 4. **Output** – 採用結果と周辺情報を表・Slack通知として出力。
L16
L17 ```mermaid
L18 flowchart LR
L19   A[Input\nAPI & 前処理] --> B[Score Calculation\n特徴量・因子合成]
L20   B --> C[Correlation Reduction\nDRRS選定]
L21   C --> D[Output\nSlack通知]
L22 ```
L23
L24 ## 定数・設定
L25 | 変数 | 内容 | 主な用途 |
L26 | --- | --- | --- |
L27 | `exist` / `cand` | 現行ポートフォリオと検討中銘柄のティッカーリスト | スコア対象ユニバースの構成、候補整理 |
L28 | `bench` | ベンチマークティッカー | 相対強さ・β算出、ポート比較 |
L29 | `CAND_PRICE_MAX` | 候補銘柄の許容価格上限 | 高額銘柄の事前除外 |
L30 | `N_G` / `N_D` | G/D採用枠の件数（**既定: 12 / 8**） | 最終的に選ぶ銘柄数の制約 |
L31 | `g_weights` / `D_weights` | 各因子の重みdict | G/Dスコア合成 |
L32 | `D_BETA_MAX` | Dバケットの許容β上限 | 高β銘柄の除外フィルタ |
L33 | `FILTER_SPEC` | G/Dごとの前処理フィルタ | トレンドマスクやβ上限設定 |
L34 | `corrM` | DRRS初期プールの最大件数 | 相関行列サイズ制御 |
L35 | `DRRS_G` / `DRRS_D` | DRRSパラメータdict | バケット別の相関低減設定 |
L36 | `DRRS_SHRINK` | 残差相関の対角シュリンク率 | `residual_corr`の安定化 |
L37 | `CROSS_MU_GD` | G-D間クロス相関ペナルティμ | 2バケット同時最適化で相関抑制 |
L38 | `RESULTS_DIR` | 選定結果保存ディレクトリ | `_save_sel`/`_load_prev`の入出力 |
L39
L40 選定結果は`results/`配下にJSONとして保存し、次回実行時に`_load_prev`で読み込んで選定条件に反映。
L41
L42 ## DTO/Config
L43 各ステップ間で受け渡すデータ構造と設定値。変数の意味合いと利用箇所を以下に示す。
L44
L45 ### InputBundle（Input → Scorer）
L46 | 変数 | 内容 | 主な用途 |
L47 | --- | --- | --- |
L48 | `cand` | 候補銘柄ティッカーのリスト | OUTテーブルや低スコアランキング対象の母集団 |
L49 | `tickers` | 現行+候補を合わせたティッカー一覧 | 価格・出来高ダウンロード、リターン計算 |
L50 | `bench` | ベンチマークティッカー | 相対強さ・β算出、ポート比較 |
L51 | `data` | yfinanceのダウンロード結果（階層列） | `px`/`spx`/リターン等の基礎データ |
L52 | `px` | `data['Close']`だけを抜き出した価格系列 | 指標計算・リターン生成 |
L53 | `spx` | `data['Close'][bench]` のSeries | `rs`や`calc_beta`の基準指数 |
L54 | `tickers_bulk` | `yf.Tickers`オブジェクト | `info`等の一括取得 |
L55 | `info` | ティッカー別のyfinance情報dict | セクター判定やEPS補完 |
L56 | `eps_df` | EPS TTM/直近EPS等をまとめた表 | 成長指標の算出 |
L57 | `fcf_df` | CFO・CapEx・FCF TTMと情報源フラグ | FCF/EVや配当カバレッジ |
L58 | `returns` | `px.pct_change()`のリターン表 | 相関行列・DRRS計算 |
L59
L60 ### FeatureBundle（Scorer → Selector）
L61 | 変数 | 内容 | 主な用途 |
L62 | --- | --- | --- |
L63 | `df` | 計算済み指標の生値テーブル | デバッグ・出力表示 |
L64 | `df_z` | ウィンザー後Zスコア化した指標表 | 因子スコア合成、選定基準 |
L65 | `g_score` | Gバケット総合スコア | G選定、IN/OUT比較 |
L66 | `d_score_all` | Dバケット総合スコア（全銘柄） | D選定、低スコアランキング |
L67 | `missing_logs` | 欠損指標と補完状況のログ | データ品質チェック |
L68
L69 ### SelectionBundle（Selector → Output）
L70 | 変数 | 内容 | 主な用途 |
L71 | --- | --- | --- |
L72 | `resG` | G選定結果の詳細dict（`tickers`、目的値等） | 結果保存・平均相関などの指標表示 |
L73 | `resD` | D選定結果の詳細dict | 同上 |
L74 | `top_G` | 最終採用Gティッカー | 新ポートフォリオ構築 |
L75 | `top_D` | 最終採用Dティッカー | 同上 |
L76 | `init_G` | DRRS前のG初期候補 | 惜しくも外れた銘柄表示 |
L77 | `init_D` | DRRS前のD初期候補 | 同上 |
L78
L79 ### WeightsConfig
L80 | 変数 | 内容 | 主な用途 |
L81 | --- | --- | --- |
L82 | `g` | G因子（GRW/MOM/VOL）の重みdict | `g_score`合成 |
L83 | `d` | D因子（D_QAL/D_YLD/D_VOL_RAW/D_TRD）の重みdict | `d_score_all`合成 |
L84
L85 ### DRRSParams
L86 | 変数 | 内容 | 主な用途 |
L87 | --- | --- | --- |
L88 | `corrM` | DRRS初期プールの最大件数 | 相関行列サイズ制御 |
L89 | `shrink` | 残差相関のシュリンク率 | `residual_corr`の対角強調 |
L90 | `G` | Gバケット用パラメータdict（`lookback`等） | `select_bucket_drrs`設定 |
L91 | `D` | Dバケット用パラメータdict | 同上 |
L92 | `cross_mu_gd` | G-Dクロス相関ペナルティ係数μ | `select_buckets`の目的関数 |
L93
L94 ### PipelineConfig
L95 | 変数 | 内容 | 主な用途 |
L96 | --- | --- | --- |
L97 | `weights` | `WeightsConfig`のインスタンス | スコア合成の重み参照 |
L98 | `drrs` | `DRRSParams`のインスタンス | 選定ステップの設定値 |
L99 | `price_max` | 候補銘柄の許容価格上限 | Input段階でのフィルタ |
L100
L101 ## 共通ユーティリティ
L102 - `winsorize_s` / `robust_z` : 外れ値処理とZスコア化。
L103 - `_safe_div` / `_safe_last` : 例外を潰した分割・末尾取得。
L104 - `_load_prev` / `_save_sel` : 選定結果の読み書き。
L105
L106 ## クラス設計
L107 ### Step1: Input
L108 `current_tickers.csv`の現行銘柄と`candidate_tickers.csv`の検討中銘柄を起点にデータを集約する。外部I/Oと前処理を担当し、`prepare_data`で`InputBundle`を生成。価格・財務データの取得は**yfinanceを優先し、欠損がある指標のみFinnhub APIで補完**する。
L109 主なメソッド:
L110 - `impute_eps_ttm` : 四半期EPS×4でTTMを推定し欠損時のみ差し替え。
L111 - `fetch_cfo_capex_ttm_yf` : yfinanceの四半期/年次キャッシュフローからCFO・CapEx・FCF TTMを算出。
L112 - `fetch_cfo_capex_ttm_finnhub` : yfinanceで欠けた銘柄のみFinnhub APIで補完。
L113 - `compute_fcf_with_fallback` : yfinance値を基準にFinnhub値で穴埋めし、CFO/CapEx/FCFと情報源フラグを返す。
L114 - `_build_eps_df` : `info`や`quarterly_earnings`からEPS TTMと直近EPSを計算し、`impute_eps_ttm`で補完。
L115 - `prepare_data` :
L116     0. CSVから現行銘柄と候補銘柄のティッカー一覧を読み込む。
L117     1. 候補銘柄の現在値を取得し価格上限でフィルタ。
L118     2. 既存+候補から対象ティッカーを決定し、価格・出来高を一括ダウンロード（yfinance）。
L119     3. yfinance値を基にEPS/FCFテーブルやベンチマーク系列、リターンを構築し、欠損セルはFinnhub呼び出しで穴埋め。
L120     4. 上記を`InputBundle`に格納して返す。
L121
L122 ### Step2: Score Calculation (Scorer)
L123 特徴量計算とスコア合成を担当し、`FeatureBundle`を返す。
L124
L125 #### 補助関数
L126 - `trend(s)` : 50/150/200日移動平均や52週レンジから-0.5〜0.5で構成されたトレンド指標。
L127 - `rs(s,b)` / `tr_str(s)` / `rs_line_slope(s,b,win)` : 相対強さや短期トレンド、RS回帰傾きを算出。
L128 - `ev_fallback` : `enterpriseValue`欠損時に負債・現金からEVを推定。
L129 - `dividend_status` / `div_streak` : 配当未設定状況の判定と増配年数カウント。
L130 - `fetch_finnhub_metrics` : Finnhub APIからEPS成長・ROE・βなど不足指標を取得。
L131 - `calc_beta` : ベンチマークとの共分散からβを算出。
L132 - `spx_to_alpha` : S&P500の位置情報からDRRSで用いるαを推定。
L133 - `soft_cap_effective_scores` / `pick_top_softcap` : セクターソフトキャップ付きスコア調整と上位抽出。
L134
L135 **補助関数と生成指標**
L136
L137 | 補助関数 | 生成指標 | 略称 |
L138 | --- | --- | --- |
L139 | `trend` | トレンド総合値 | `TR` |
L140 | `rs` | 相対強さ | `RS` |
L141 | `tr_str` | 価格と50日線の乖離 | `TR_str` |
L142 | `rs_line_slope` | RS線の回帰傾き | `RS_SLOPE_*` |
L143 | `calc_beta` | β | `BETA` |
L144 | `div_streak` | 連続増配年数 | `DIV_STREAK` |
L145
L146 #### `aggregate_scores` 詳細
L147 1. 各銘柄の価格系列や`info`を基に以下を算出。
L148    - **トレンド/モメンタム**: `TR`、`RS`、`TR_str`、多様な移動平均比、`RS_SLOPE_*`など。
L149    - **リスク**: `BETA`、`DOWNSIDE_DEV`、`MDD_1Y`、`RESID_VOL`、`DOWN_OUTPERF`、`EXT_200`等。
L150    - **配当**: `DIV`、`DIV_TTM_PS`、`DIV_VAR5`、`DIV_YOY`、`DIV_FCF_COVER`、`DIV_STREAK`。
L151    - **財務・成長**: `EPS`、`REV`、`ROE`、`FCF/EV`、`REV_Q_YOY`、`EPS_Q_YOY`、`REV_YOY_ACC`、`REV_YOY_VAR`、`REV_ANN_STREAK`、`RULE40`、`FCF_MGN` 等。
L152    - **安定性/サイズ**: `DEBT2EQ`、`CURR_RATIO`、`MARKET_CAP`、`ADV60_USD`、`EPS_VAR_8Q`など。
L153 2. 指標欠損はFinnhub API等で補完し、未取得項目を`missing_logs`に記録。
L154 3. `winsorize_s`→`robust_z`で標準化し`df_z`へ保存。サイズ・流動性は対数変換。
L155 4. 正規化済指標から因子スコアを合成。
L156    - 各因子の構成と重みは以下の通り。
L157      - **GRW**: 0.30×`REV` + 0.20×`EPS_Q_YOY` + 0.15×`REV_Q_YOY` + 0.15×`REV_YOY_ACC` + 0.10×`RULE40` + 0.10×`FCF_MGN` + 0.10×`REV_ANN_STREAK` − 0.05×`REV_YOY_VAR`。
L158      - **MOM**: 0.40×`RS` + 0.15×`TR_str` + 0.15×`RS_SLOPE_6W` + 0.15×`RS_SLOPE_13W` + 0.10×`MA200_SLOPE_5M` + 0.10×`MA200_UP_STREAK_D`。
L159      - **VOL**: `BETA`単体を使用。
L160      - **QAL**: 0.60×`FCF_W` + 0.40×`ROE_W`で作成。
L161      - **YLD**: 0.30×`DIV` + 0.70×`DIV_STREAK`。
L162      - **D_QAL**: 0.35×`QAL` + 0.20×`FCF` + 0.15×`CURR_RATIO` − 0.15×`DEBT2EQ` − 0.15×`EPS_VAR_8Q`。
L163      - **D_YLD**: 0.45×`DIV` + 0.25×`DIV_STREAK` + 0.20×`DIV_FCF_COVER` − 0.10×`DIV_VAR5`。
L164      - **D_VOL_RAW**: 0.40×`DOWNSIDE_DEV` + 0.22×`RESID_VOL` + 0.18×`MDD_1Y` − 0.10×`DOWN_OUTPERF` − 0.05×`EXT_200` − 0.08×`SIZE` − 0.10×`LIQ` + 0.10×`BETA`。
L165      - **D_TRD**: 0.40×`MA200_SLOPE_5M` − 0.30×`EXT_200` + 0.15×`NEAR_52W_HIGH` + 0.15×`TR`。
L166     - 主な指標の略称と意味:
L167
L168       | 略称 | 補助関数 | 概要 |
L169       | --- | --- | --- |
L170       | TR | `trend` | 50/150/200日移動平均と52週レンジを組み合わせたトレンド総合値 |
L171       | RS | `rs` | ベンチマークに対する相対強さ（12M/1Mリターン差） |
L172       | TR_str | `tr_str` | 価格と50日移動平均の乖離 |
L173       | RS_SLOPE_6W | `rs_line_slope` | 相対強さ線の6週回帰傾き |
L174       | RS_SLOPE_13W | `rs_line_slope` | 相対強さ線の13週回帰傾き |
L175       | MA200_SLOPE_5M | - | 200日移動平均の5か月騰落率 |
L176       | MA200_UP_STREAK_D | - | 200日線が連続で上向いた日数 |
L177       | BETA | `calc_beta` | ベンチマークに対するβ |
L178       | DOWNSIDE_DEV | - | 下方リターンのみの年率化標準偏差 |
L179       | RESID_VOL | - | βで調整した残差リターンの年率化標準偏差 |
L180       | MDD_1Y | - | 過去1年の最大ドローダウン |
L181       | DOWN_OUTPERF | - | 市場下落日に対する平均超過リターン |
L182       | EXT_200 | - | 200日移動平均からの絶対乖離率 |
L183       | NEAR_52W_HIGH | - | 52週高値までの下方距離（0=高値） |
L184       | FCF_W | - | ウィンザー処理後のFCF/EV |
L185       | ROE_W | - | ウィンザー処理後のROE |
L186       | FCF | - | FCF/EV |
L187       | QAL | - | FCF_WとROE_Wを組み合わせた品質スコア |
L188       | CURR_RATIO | - | 流動比率 |
L189       | DEBT2EQ | - | 負債資本倍率 |
L190       | EPS_VAR_8Q | - | EPSの8四半期標準偏差 |
L191       | DIV | - | 年率換算配当利回り |
L192       | DIV_STREAK | `div_streak` | 連続増配年数 |
L193       | DIV_FCF_COVER | - | 配当のFCFカバレッジ |
L194       | DIV_VAR5 | - | 5年配当変動率 |
L195       | DIV_TTM_PS | - | 1株当たりTTM配当 |
L196       | DIV_YOY | - | 前年比配当成長率 |
L197       | REV | - | 売上成長率TTM |
L198       | EPS_Q_YOY | - | 四半期EPSの前年同期比 |
L199       | REV_Q_YOY | - | 四半期売上の前年同期比 |
L200       | REV_YOY_ACC | - | 売上成長率の加速分 |
L201       | RULE40 | - | 売上成長率とFCFマージンの合計 |
L202       | FCF_MGN | - | FCFマージン |
L203       | REV_ANN_STREAK | - | 年次売上成長の連続年数 |
L204       | REV_YOY_VAR | - | 年次売上成長率の変動性 |
L205       | SIZE | - | 時価総額の対数値 |
L206       | LIQ | - | 60日平均出来高ドルの対数値 |
L207    - Gバケット: `GRW`、`MOM`、`VOL`を`cfg.weights.g`（0.40/0.45/-0.15）で加重し`g_score`を得る。
L208    - Dバケット: `D_QAL`、`D_YLD`、`D_VOL_RAW`、`D_TRD`を`cfg.weights.d`（0.15/0.15/-0.45/0.25）で加重し`d_score_all`を算出。
L209    - セクターcapによる`soft_cap_effective_scores`を適用し、G採用銘柄にはトレンドテンプレートフィルタを適用。
L210 5. `_apply_growth_entry_flags`でブレイクアウト/押し目発火状況を付加し、`FeatureBundle`を返す。
L211
L212 ### Step3: Correlation Reduction & Selection (Selector)
L213 DRRSアルゴリズムで相関を抑えた銘柄選定を行い、`SelectionBundle`を返す。`results/`に保存された前回選定（`G_selection.json` / `D_selection.json`）を`_load_prev`で読み込み、目的値が大きく悪化しない限り維持する。新しい採用集合は`_save_sel`でJSONに書き出し次回以降の入力に備える。
L214 主なメソッド:
L215 - `residual_corr` : 収益率行列をZスコア化し、上位主成分を除去した残差から相関行列を求め、平均相関に応じてシュリンク。
L216 - `rrqr_like_det` : スコアを重み付けしたQR分解風の手順で初期候補をk件抽出し、スコアの高い非相関な集合を得る。
L217 - `swap_local_det` / `swap_local_det_cross` : `sum(score) - λ*within_corr - μ*cross_corr`を目的関数として、入れ替え探索で局所的に最適化。
L218 - `select_bucket_drrs` : プール銘柄とスコアから残差相関を計算し、上記2段階(初期選択→入れ替え)でk銘柄を決定。過去採用銘柄との比較で目的値が劣化しなければ維持する。
L219 - `select_buckets` : Gバケットを選定後、その結果を除いた候補からDバケットを選ぶ。D選定時はGとの相関ペナルティμを付与し、両バケットの分散を制御する。
L220
L221 #### 相関低減ロジック詳細
L222 1. **残差相関行列の構築 (`residual_corr`)**
L223    - リターン行列`R`をZスコア化。
L224    - SVDで上位`n_pc`主成分`F`を求め、最小二乗で係数`B`を算出し残差`E = Z - F@B`を得る。
L225    - `E`の相関行列`C`を計算し、平均絶対相関に応じてシュリンク量`shrink_eff`を補正して対角を強調。
L226 2. **初期候補の抽出 (`rrqr_like_det`)**
L227    - スコアを0-1正規化した重み`w`とし、`Z*(1+γw)`で列ノルムを強調。
L228    - 残差ノルム最大の列を逐次選び、QRライクなデフレーションを行って非相関かつ高スコアな`k`銘柄集合`S0`を得る。
L229 3. **局所探索 (`swap_local_det` / `swap_local_det_cross`)**
L230    - 目的関数`Σz_score − λ·within_corr − μ·cross_corr`を最大化。
L231    - 選択集合の各銘柄を他候補と入れ替え、改善がなくなるまでまたは`max_pass`回まで探索。
L232    - `swap_local_det_cross`はGバケットとのクロス相関行列`C_cross`を使用し、ペナルティ`μ`を付与。
L233 4. **過去採用の維持とクロスペナルティ (`select_bucket_drrs` / `select_buckets`)**
L234    - 局所探索結果`S`と過去集合`P`の目的値を比較し、`S`が`P`より`η`未満の改善なら`P`を維持。
L235    - `select_buckets`ではGを先に決定し、D選定時にGとの相関ペナルティ`μ`を加えてクロス分散を抑制。
L236
L237 ### Step4: Output
L238 選定結果を可視化し共有する工程。以下の内容をテーブル化して標準出力とSlackへ送る。
L239 - 採用銘柄と惜しくも選外となった銘柄のスコア一覧
L240 - IN/OUTリストとOUT銘柄のスコア（低得点銘柄を確認しやすく）
L241 - 新旧ポートフォリオの比較表（組入れ・除外、スコア変化）
L242 - 検討中銘柄の低スコアランキング
L243
L244 主なメソッド:
L245 - `display_results` : 上記テーブルに加えパフォーマンス指標や分散化指標を表示。
L246 - `notify_slack` : Slack Webhookへ同内容を送信。
L247 - 補助:`_avg_offdiag`、`_resid_avg_rho`、`_raw_avg_rho`、`_cross_block_raw_rho`。
L248
L249 ## エントリポイント
L250 1. `PipelineConfig`を構築。
L251 2. **Step1** `Input.prepare_data`で`InputBundle`を生成。
L252 3. **Step2** `Scorer.aggregate_scores`で`FeatureBundle`を取得。
L253 4. **Step3** `Selector.select_buckets`で`SelectionBundle`を算出。
L254 5. **Step4** `Output.display_results`と`notify_slack`で結果を出力。
```

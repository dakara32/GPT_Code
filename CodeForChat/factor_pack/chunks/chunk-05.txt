```text
cted12, near_G)
L846     try:
L847         fire_recent = [t for t in guni
L848                        if (str(df.at[t, "G_BREAKOUT_recent_5d"]) == "True") or
L849                           (str(df.at[t, "G_PULLBACK_recent_5d"]) == "True")]
L850     except Exception: fire_recent = []
L851
L852     lines = [
L853         "ã€Gæ ãƒ¬ãƒãƒ¼ãƒˆï½œé€±æ¬¡ãƒ¢ãƒ‹ã‚¿ï¼ˆç›´è¿‘5å–¶æ¥­æ—¥ï¼‰ã€‘",
L854         "ã€å‡¡ä¾‹ã€‘ğŸ”¥=ç›´è¿‘5å–¶æ¥­æ—¥å†…ã«ã€Œãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºå®šã€ã¾ãŸã¯ã€ŒæŠ¼ã—ç›®åç™ºã€ã‚’æ¤œçŸ¥",
L855         f"é¸å®š{N_G}: {', '.join(_fmt_with_fire_mark(selected12, df))}" if selected12 else f"é¸å®š{N_G}: ãªã—",
L856         f"æ¬¡ç‚¹10: {', '.join(_fmt_with_fire_mark(near_G, df))}" if near_G else "æ¬¡ç‚¹10: ãªã—",]
L857
L858     if fire_recent:
L859         fire_list = ", ".join([_label_recent_event(t, df) for t in fire_recent])
L860         lines.append(f"éå»5å–¶æ¥­æ—¥ã®æ¤œçŸ¥: {fire_list}")
L861     else:
L862         lines.append("éå»5å–¶æ¥­æ—¥ã®æ¤œçŸ¥: ãªã—")
L863
L864     try:
L865         webhook = os.environ.get("SLACK_WEBHOOK_URL", "")
L866         if webhook:
L867             requests.post(webhook, json={"text": "\n".join([s for s in lines if s != ""])}, timeout=10)
L868     except Exception:
L869         pass
L870
L871     out = Output()
L872     # è¡¨ç¤ºå´ã‹ã‚‰é¸å®šæ™‚ã®é›†è¨ˆã¸ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†ã«ä¿æŒï¼ˆè¡¨ç¤ºå°‚ç”¨ãƒ»å‰¯ä½œç”¨ãªã—ï¼‰
L873     try: out._sc = sc
L874     except Exception: pass
L875     if hasattr(sc, "_feat"):
L876         try:
L877             fb = sc._feat
L878             out.miss_df = fb.missing_logs
L879             out.display_results(
L880                 exist=exist,
L881                 bench=bench,
L882                 df_z=fb.df_z,
L883                 g_score=fb.g_score,
L884                 d_score_all=fb.d_score_all,
L885                 init_G=top_G,
L886                 init_D=top_D,
L887                 top_G=top_G,
L888                 top_D=top_D,
L889                 df_full_z=getattr(fb, "df_full_z", None),
L890                 prev_G=getattr(sc, "_prev_G", exist),
L891                 prev_D=getattr(sc, "_prev_D", exist),
L892             )
L893         except Exception:
L894             pass
L895     out.notify_slack()
L896     sb = SelectionBundle(resG={"tickers": top_G, "avg_res_corr": avgG,
L897               "sum_score": sumG, "objective": objG},
L898         resD={"tickers": top_D, "avg_res_corr": avgD,
L899               "sum_score": sumD, "objective": objD},
L900         top_G=top_G, top_D=top_D, init_G=top_G, init_D=top_D)
L901
L902     # --- Low Score Candidates (GSC+DSC bottom 10) : send before debug dump ---
L903     try:
L904         _low_df = (pd.DataFrame({"GSC": fb.g_score, "DSC": fb.d_score_all})
L905               .assign(G_plus_D=lambda x: x["GSC"] + x["DSC"])
L906               .sort_values("G_plus_D")
L907               .head(10)
L908               .round(3))
L909         low_msg = "Low Score Candidates (GSC+DSC bottom 10)\n" + _low_df.to_string(index=True, index_names=False)
L910         _post_slack({"text": f"```{low_msg}```"})
L911     except Exception as _e:
L912         _post_slack({"text": f"```Low Score Candidates: ä½œæˆå¤±æ•—: {_e}```"})
L913
L914     return sb
L915
L916 if __name__ == "__main__":
L917     run_pipeline()
```

## <scorer.py>
```text
L1 # scorer.py
L2 # kawatest
L3 # =============================================================================
L4 # Scorer: ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼/æŒ‡æ¨™ã®ç”Ÿæˆã¨åˆæˆã‚¹ã‚³ã‚¢ç®—å‡ºã‚’æ‹…ã†ç´”ç²‹å±¤
L5 #
L6 # ã€ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã ã‘èª­ã‚ã°åˆ†ã‹ã‚‹ãƒã‚¤ãƒ³ãƒˆã€‘
L7 # - å…¥åŠ›(InputBundle)ã¯ã€Œä¾¡æ ¼/å‡ºæ¥é«˜/ãƒ™ãƒ³ãƒ/åŸºæœ¬æƒ…å ±/EPS/FCF/ãƒªã‚¿ãƒ¼ãƒ³ã€ã‚’å«ã‚€DTO
L8 # - å‡ºåŠ›(FeatureBundle)ã¯ã€Œrawç‰¹å¾´é‡ dfã€ã€Œæ¨™æº–åŒ– df_zã€ã€ŒG/D ã‚¹ã‚³ã‚¢ã€ã€Œæ¬ æãƒ­ã‚°ã€
L9 # - é‡ã¿ç­‰ã®ã‚³ãƒ³ãƒ•ã‚£ã‚°(PipelineConfig)ã¯ factor ã‹ã‚‰æ¸¡ã™ï¼ˆcfg å¿…é ˆï¼‰
L10 # - æ—§ã‚«ãƒ©ãƒ åã¯ Scorer å†…ã§è‡ªå‹•ãƒªãƒãƒ¼ãƒ ã—ã¦å—ã‘å…¥ã‚Œï¼ˆå¾Œæ–¹äº’æ›ï¼‰
L11 #   ä¾‹) eps_ttm -> EPS_TTM, eps_q_recent -> EPS_Q_LastQ, fcf_ttm -> FCF_TTM
L12 #
L13 # ã€I/Oå¥‘ç´„ï¼ˆScorerãŒå‚ç…§ã™ã‚‹InputBundleãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰ã€‘
L14 #   - cand: List[str]    â€¦ å€™è£œéŠ˜æŸ„ï¼ˆå˜ä½“å®Ÿè¡Œã§ã¯æœªä½¿ç”¨ï¼‰
L15 #   - tickers: List[str] â€¦ å¯¾è±¡éŠ˜æŸ„ãƒªã‚¹ãƒˆ
L16 #   - bench: str         â€¦ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚£ãƒƒã‚«ãƒ¼ï¼ˆä¾‹ '^GSPC'ï¼‰
L17 #   - data: pd.DataFrame â€¦ yfinance downloadçµæœ ('Close','Volume' ç­‰ã®éšå±¤åˆ—)
L18 #   - px: pd.DataFrame   â€¦ data['Close'] ç›¸å½“ï¼ˆçµ‚å€¤ï¼‰
L19 #   - spx: pd.Series     â€¦ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®çµ‚å€¤
L20 #   - tickers_bulk: object         â€¦ yfinance.Tickers
L21 #   - info: Dict[str, dict]        â€¦ yfinance info per ticker
L22 #   - eps_df: pd.DataFrame         â€¦ å¿…é ˆåˆ—: EPS_TTM, EPS_Q_LastQï¼ˆæ—§åã‚‚å¯ï¼‰
L23 #   - fcf_df: pd.DataFrame         â€¦ å¿…é ˆåˆ—: FCF_TTMï¼ˆæ—§åã‚‚å¯ï¼‰
L24 #   - returns: pd.DataFrame        â€¦ px[tickers].pct_change() ç›¸å½“
L25 #
L26 # â€»å…¥å‡ºåŠ›ã®å½¢å¼ãƒ»ä¾‹å¤–æ–‡è¨€ã¯æ—¢å­˜å®Ÿè£…ã‚’å¤‰ãˆã¾ã›ã‚“ï¼ˆå®‰å…¨ãªçŸ­ç¸®ã®ã¿ï¼‰
L27 # =============================================================================
L28
L29 import logging
L30 import math
L31 import os, sys, warnings
L32 import requests
L33 import numpy as np
L34 import pandas as pd
L35 import yfinance as yf
L36 from typing import Any, TYPE_CHECKING
L37 from scipy.stats import zscore
L38
L39 if TYPE_CHECKING:
L40     from factor import PipelineConfig  # type: ignore  # å®Ÿè¡Œæ™‚importãªã—ï¼ˆå¾ªç’°å›é¿ï¼‰
L41
L42 logger = logging.getLogger(__name__)
L43
L44 # ---- Dividend Helpers -------------------------------------------------------
L45 def _last_close(t, price_map=None):
L46     if price_map and (c := price_map.get(t)) is not None: return float(c)
L47     try:
L48         h = yf.Ticker(t).history(period="5d")["Close"].dropna()
L49         return float(h.iloc[-1]) if len(h) else np.nan
L50     except Exception:
L51         return np.nan
L52
L53 def _ttm_div_sum(t, lookback_days=400):
L54     try:
L55         div = yf.Ticker(t).dividends
L56         if div is None or len(div) == 0: return 0.0
L57         cutoff = pd.Timestamp.utcnow().tz_localize(None) - pd.Timedelta(days=lookback_days)
L58         ttm = float(div[div.index.tz_localize(None) >= cutoff].sum())
L59         return ttm if ttm > 0 else float(div.tail(4).sum())
L60     except Exception:
L61         return 0.0
L62
L63 def ttm_div_yield_portfolio(tickers, price_map=None):
L64     ys = [(lambda c, s: (s/c) if (np.isfinite(c) and c>0 and s>0) else 0.0)(_last_close(t, price_map), _ttm_div_sum(t)) for t in tickers]
L65     return float(np.mean(ys)) if ys else 0.0
L66
L67 # ---- ç°¡æ˜“ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆå®‰å…¨ãªçŸ­ç¸®ã®ã¿ï¼‰ -----------------------------------
L68 def winsorize_s(s: pd.Series, p=0.02):
L69     if s is None or s.dropna().empty: return s
L70     lo, hi = np.nanpercentile(s.astype(float), [100*p, 100*(1-p)]); return s.clip(lo, hi)
L71
L72 def robust_z(s: pd.Series, p=0.02):
L73     s2 = winsorize_s(s,p); return np.nan_to_num(zscore(s2.fillna(s2.mean())))
L74
L75 def robust_z_keepnan(s: pd.Series) -> pd.Series:
L76     """robust_z variant that preserves NaNs and falls back to rank-z when needed."""
L77     if s is None:
L78         return pd.Series(dtype=float)
L79     v = pd.to_numeric(s, errors="coerce")
L80     m = np.nanmedian(v)
L81     mad = np.nanmedian(np.abs(v - m))
L82     z = (v - m) / (1.4826 * mad + 1e-9)
L83     if np.nanstd(z) < 1e-9:
L84         r = v.rank(method="average", na_option="keep")
L85         z = (r - np.nanmean(r)) / (np.nanstd(r) + 1e-9)
L86     return pd.Series(z, index=v.index, dtype=float)
L87
L88
L89 def _fetch_revenue_quarterly_via_finnhub(symbol: str):
L90     """
L91     Finnhub 'Financials As Reported' ã‹ã‚‰å››åŠæœŸå£²ä¸Šã‚’ Series ã§è¿”ã™ã€‚
L92     ã‚¹ã‚­ãƒ¼ãƒæºã‚Œã«å¯¾å¿œ: income statement(ic)é…ä¸‹ã®è¤‡æ•°ã‚­ãƒ¼å€™è£œã‚’é †æ¢ç´¢ã€‚
L93     è¿”å´: pandas.Series(index="YYYYQn", name="Revenue") or None
L94     """
L95
L96     api_key = os.getenv("FINNHUB_API_KEY", "").strip()
L97     if not api_key:
L98         return None
L99
L100     try:
L101         url = "https://finnhub.io/api/v1/stock/financials-reported"
L102         r = requests.get(url, params={"symbol": symbol, "token": api_key}, timeout=10)
L103         r.raise_for_status()
L104         payload = r.json() or {}
L105         data = payload.get("data", []) or []
L106
L107         CAND_KEYS = [
L108             "revenue",
L109             "totalRevenue",
L110             "netSales",
L111             "salesRevenueNet",
L112             "operatingRevenue",
L113             "totalOperatingRevenue",
L114             "totalRevenueFromOperations",
L115             "turnover",
L116         ]
L117
L118         def _first_number(v):
L119             try:
L120                 if isinstance(v, dict):
L121                     v = v.get("value")
L122                 if v is None:
L123                     return None
L124                 x = float(v)
L125                 if math.isfinite(x):
L126                     return x
L127             except Exception:
L128                 return None
L129             return None
L130
L131         rows = []
L132         for item in data:
L133             rep = (item or {}).get("report", {})
L134             ic = rep.get("ic", {}) if isinstance(rep, dict) else {}
L135
L136             if isinstance(ic, list):
L137                 ic_dict = {}
L138                 for entry in ic:
L139                     if isinstance(entry, dict) and "concept" in entry:
L140                         ic_dict[entry["concept"]] = {"value": entry.get("value")}
L141                 ic = ic_dict
L142
L143             rev_val = None
L144             if isinstance(ic, dict):
L145                 for key in CAND_KEYS:
L146                     if key in ic:
L147                         rev_val = _first_number(ic.get(key))
L148                         if rev_val is not None:
L149                             break
L150             if rev_val is None:
L151                 continue
L152
L153             year = item.get("year")
L154             quarter = item.get("quarter")
L155             if year is None or quarter is None:
L156                 continue
L157
L158             rows.append((int(year), int(quarter), rev_val))
L159
L160         if not rows:
L161             return None
L162
L163         rows.sort(key=lambda x: (x[0], x[1]))
L164         idx = [f"{y}Q{q}" for y, q, _ in rows]
L165         vals = [v for _, _, v in rows]
L166         s = pd.Series(vals, index=idx, name="Revenue")
L167         if s.dropna().empty:
L168             return None
L169         return s
L170     except Exception:
L171         return None
L172
L173
L174 def _dump_dfz(df_z: pd.DataFrame, debug_mode: bool, max_rows: int = 400, ndigits: int = 3) -> None:
L175     """df_z ã‚’ System log(INFO) ã¸ãƒ€ãƒ³ãƒ—ã™ã‚‹ç°¡æ½”ãªãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£."""
L176     if not debug_mode:
L177         return
L178     try:
L179         view = df_z.copy()
L180         view = view.apply(
L181             lambda s: s.round(ndigits)
L182             if getattr(getattr(s, "dtype", None), "kind", "") in ("f", "i")
L183             else s
L184         )
L185         if len(view) > max_rows:
L186             view = view.iloc[:max_rows]
L187
L188         # === NaNã‚µãƒãƒªï¼ˆåˆ—ã”ã¨ã®æ¬ æä»¶æ•° ä¸Šä½20ï¼‰ ===
L189         try:
L190             nan_counts = df_z.isna().sum().sort_values(ascending=False)
L191             top_nan = nan_counts[nan_counts > 0].head(20)
L192             if len(top_nan) > 0:
L193                 logger.info("NaN columns (top20):\n%s", top_nan.to_string())
L194             else:
L195                 logger.info("NaN columns: none")
L196         except Exception as exc:
L197             logger.warning("nan summary failed: %s", exc)
L198
L199         # === Zeroã‚µãƒãƒªï¼ˆåˆ—ã”ã¨ã®ã‚¼ãƒ­æ¯”ç‡ ä¸Šä½20ï¼‰ ===
L200         try:
L201             zero_counts = ((df_z == 0) & (~df_z.isna())).sum()
L202             nonnull_counts = (~df_z.isna()).sum()
L203             zero_ratio = (zero_counts / nonnull_counts).sort_values(ascending=False)
L204             top_zero = zero_ratio[zero_ratio > 0].head(20)
L205             if len(top_zero) > 0:
L206                 logger.info(
L207                     "Zero-dominated columns (top20):\n%s",
L208                     top_zero.to_string(float_format=lambda x: f"{x:.2%}"),
L209                 )
L210             else:
L211                 logger.info("Zero-dominated columns: none")
L212         except Exception as exc:
L213             logger.warning("zero summary failed: %s", exc)
L214
L215         logger.info("===== DF_Z DUMP START =====")
L216         logger.info("\n%s", view.to_string(max_rows=None, max_cols=None))
L217         logger.info("===== DF_Z DUMP END =====")
L218     except Exception as exc:
L219         logger.warning("df_z dump failed: %s", exc)
L220
L221 def _safe_div(a, b):
L222     try: return np.nan if (b is None or float(b
```
```text
cted12, near_G)
L846     try:
L847         fire_recent = [t for t in guni
L848                        if (str(df.at[t, "G_BREAKOUT_recent_5d"]) == "True") or
L849                           (str(df.at[t, "G_PULLBACK_recent_5d"]) == "True")]
L850     except Exception: fire_recent = []
L851
L852     lines = [
L853         "【G枠レポート｜週次モニタ（直近5営業日）】",
L854         "【凡例】🔥=直近5営業日内に「ブレイクアウト確定」または「押し目反発」を検知",
L855         f"選定{N_G}: {', '.join(_fmt_with_fire_mark(selected12, df))}" if selected12 else f"選定{N_G}: なし",
L856         f"次点10: {', '.join(_fmt_with_fire_mark(near_G, df))}" if near_G else "次点10: なし",]
L857
L858     if fire_recent:
L859         fire_list = ", ".join([_label_recent_event(t, df) for t in fire_recent])
L860         lines.append(f"過去5営業日の検知: {fire_list}")
L861     else:
L862         lines.append("過去5営業日の検知: なし")
L863
L864     try:
L865         webhook = os.environ.get("SLACK_WEBHOOK_URL", "")
L866         if webhook:
L867             requests.post(webhook, json={"text": "\n".join([s for s in lines if s != ""])}, timeout=10)
L868     except Exception:
L869         pass
L870
L871     out = Output()
L872     # 表示側から選定時の集計へアクセスできるように保持（表示専用・副作用なし）
L873     try: out._sc = sc
L874     except Exception: pass
L875     if hasattr(sc, "_feat"):
L876         try:
L877             fb = sc._feat
L878             out.miss_df = fb.missing_logs
L879             out.display_results(
L880                 exist=exist,
L881                 bench=bench,
L882                 df_z=fb.df_z,
L883                 g_score=fb.g_score,
L884                 d_score_all=fb.d_score_all,
L885                 init_G=top_G,
L886                 init_D=top_D,
L887                 top_G=top_G,
L888                 top_D=top_D,
L889                 df_full_z=getattr(fb, "df_full_z", None),
L890                 prev_G=getattr(sc, "_prev_G", exist),
L891                 prev_D=getattr(sc, "_prev_D", exist),
L892             )
L893         except Exception:
L894             pass
L895     out.notify_slack()
L896     sb = SelectionBundle(resG={"tickers": top_G, "avg_res_corr": avgG,
L897               "sum_score": sumG, "objective": objG},
L898         resD={"tickers": top_D, "avg_res_corr": avgD,
L899               "sum_score": sumD, "objective": objD},
L900         top_G=top_G, top_D=top_D, init_G=top_G, init_D=top_D)
L901
L902     # --- Low Score Candidates (GSC+DSC bottom 10) : send before debug dump ---
L903     try:
L904         _low_df = (pd.DataFrame({"GSC": fb.g_score, "DSC": fb.d_score_all})
L905               .assign(G_plus_D=lambda x: x["GSC"] + x["DSC"])
L906               .sort_values("G_plus_D")
L907               .head(10)
L908               .round(3))
L909         low_msg = "Low Score Candidates (GSC+DSC bottom 10)\n" + _low_df.to_string(index=True, index_names=False)
L910         _post_slack({"text": f"```{low_msg}```"})
L911     except Exception as _e:
L912         _post_slack({"text": f"```Low Score Candidates: 作成失敗: {_e}```"})
L913
L914     return sb
L915
L916 if __name__ == "__main__":
L917     run_pipeline()
```

## <scorer.py>
```text
L1 # scorer.py
L2 # kawatest
L3 # =============================================================================
L4 # Scorer: ファクター/指標の生成と合成スコア算出を担う純粋層
L5 #
L6 # 【このファイルだけ読めば分かるポイント】
L7 # - 入力(InputBundle)は「価格/出来高/ベンチ/基本情報/EPS/FCF/リターン」を含むDTO
L8 # - 出力(FeatureBundle)は「raw特徴量 df」「標準化 df_z」「G/D スコア」「欠損ログ」
L9 # - 重み等のコンフィグ(PipelineConfig)は factor から渡す（cfg 必須）
L10 # - 旧カラム名は Scorer 内で自動リネームして受け入れ（後方互換）
L11 #   例) eps_ttm -> EPS_TTM, eps_q_recent -> EPS_Q_LastQ, fcf_ttm -> FCF_TTM
L12 #
L13 # 【I/O契約（Scorerが参照するInputBundleフィールド）】
L14 #   - cand: List[str]    … 候補銘柄（単体実行では未使用）
L15 #   - tickers: List[str] … 対象銘柄リスト
L16 #   - bench: str         … ベンチマークティッカー（例 '^GSPC'）
L17 #   - data: pd.DataFrame … yfinance download結果 ('Close','Volume' 等の階層列)
L18 #   - px: pd.DataFrame   … data['Close'] 相当（終値）
L19 #   - spx: pd.Series     … ベンチマークの終値
L20 #   - tickers_bulk: object         … yfinance.Tickers
L21 #   - info: Dict[str, dict]        … yfinance info per ticker
L22 #   - eps_df: pd.DataFrame         … 必須列: EPS_TTM, EPS_Q_LastQ（旧名も可）
L23 #   - fcf_df: pd.DataFrame         … 必須列: FCF_TTM（旧名も可）
L24 #   - returns: pd.DataFrame        … px[tickers].pct_change() 相当
L25 #
L26 # ※入出力の形式・例外文言は既存実装を変えません（安全な短縮のみ）
L27 # =============================================================================
L28
L29 import logging
L30 import math
L31 import os, sys, warnings
L32 import requests
L33 import numpy as np
L34 import pandas as pd
L35 import yfinance as yf
L36 from typing import Any, TYPE_CHECKING
L37 from scipy.stats import zscore
L38
L39 if TYPE_CHECKING:
L40     from factor import PipelineConfig  # type: ignore  # 実行時importなし（循環回避）
L41
L42 logger = logging.getLogger(__name__)
L43
L44 # ---- Dividend Helpers -------------------------------------------------------
L45 def _last_close(t, price_map=None):
L46     if price_map and (c := price_map.get(t)) is not None: return float(c)
L47     try:
L48         h = yf.Ticker(t).history(period="5d")["Close"].dropna()
L49         return float(h.iloc[-1]) if len(h) else np.nan
L50     except Exception:
L51         return np.nan
L52
L53 def _ttm_div_sum(t, lookback_days=400):
L54     try:
L55         div = yf.Ticker(t).dividends
L56         if div is None or len(div) == 0: return 0.0
L57         cutoff = pd.Timestamp.utcnow().tz_localize(None) - pd.Timedelta(days=lookback_days)
L58         ttm = float(div[div.index.tz_localize(None) >= cutoff].sum())
L59         return ttm if ttm > 0 else float(div.tail(4).sum())
L60     except Exception:
L61         return 0.0
L62
L63 def ttm_div_yield_portfolio(tickers, price_map=None):
L64     ys = [(lambda c, s: (s/c) if (np.isfinite(c) and c>0 and s>0) else 0.0)(_last_close(t, price_map), _ttm_div_sum(t)) for t in tickers]
L65     return float(np.mean(ys)) if ys else 0.0
L66
L67 # ---- 簡易ユーティリティ（安全な短縮のみ） -----------------------------------
L68 def winsorize_s(s: pd.Series, p=0.02):
L69     if s is None or s.dropna().empty: return s
L70     lo, hi = np.nanpercentile(s.astype(float), [100*p, 100*(1-p)]); return s.clip(lo, hi)
L71
L72 def robust_z(s: pd.Series, p=0.02):
L73     s2 = winsorize_s(s,p); return np.nan_to_num(zscore(s2.fillna(s2.mean())))
L74
L75 def robust_z_keepnan(s: pd.Series) -> pd.Series:
L76     """robust_z variant that preserves NaNs and falls back to rank-z when needed."""
L77     if s is None:
L78         return pd.Series(dtype=float)
L79     v = pd.to_numeric(s, errors="coerce")
L80     m = np.nanmedian(v)
L81     mad = np.nanmedian(np.abs(v - m))
L82     z = (v - m) / (1.4826 * mad + 1e-9)
L83     if np.nanstd(z) < 1e-9:
L84         r = v.rank(method="average", na_option="keep")
L85         z = (r - np.nanmean(r)) / (np.nanstd(r) + 1e-9)
L86     return pd.Series(z, index=v.index, dtype=float)
L87
L88
L89 def _fetch_revenue_quarterly_via_finnhub(symbol: str):
L90     """Finnhub financials-reported endpoint から四半期売上を取得する."""
L91
L92     api_key = os.getenv("FINNHUB_API_KEY", "").strip()
L93     if not api_key:
L94         return None
L95
L96     try:
L97         url = "https://finnhub.io/api/v1/stock/financials-reported"
L98         r = requests.get(url, params={"symbol": symbol, "token": api_key}, timeout=10)
L99         r.raise_for_status()
L100         data = (r.json() or {}).get("data", []) or []
L101
L102         cand_keys = [
L103             "revenue",
L104             "totalRevenue",
L105             "netSales",
L106             "salesRevenueNet",
L107             "operatingRevenue",
L108             "totalOperatingRevenue",
L109             "totalRevenueFromOperations",
L110             "turnover",
L111         ]
L112
L113         def _num(v):
L114             try:
L115                 if isinstance(v, dict):
L116                     v = v.get("value")
L117                 if v is None:
L118                     return None
L119                 x = float(v)
L120                 return x if math.isfinite(x) else None
L121             except Exception:
L122                 return None
L123
L124         rows = []
L125         for item in data:
L126             rep = (item or {}).get("report", {})
L127             ic = rep.get("ic", {}) if isinstance(rep, dict) else {}
L128             if isinstance(ic, list):
L129                 ic = {
L130                     u.get("concept"): {"value": u.get("value")}
L131                     for u in ic
L132                     if isinstance(u, dict) and "concept" in u
L133                 }
L134
L135             val = None
L136             if isinstance(ic, dict):
L137                 for key in cand_keys:
L138                     if key in ic:
L139                         val = _num(ic.get(key))
L140                         if val is not None:
L141                             break
L142
L143             year = item.get("year")
L144             quarter = item.get("quarter")
L145             if val is None or year is None or quarter is None:
L146                 continue
L147
L148             try:
L149                 rows.append((int(year), int(quarter), float(val)))
L150             except Exception:
L151                 continue
L152
L153         if not rows:
L154             return None
L155
L156         rows.sort(key=lambda x: (x[0], x[1]))
L157         idx = [f"{y}Q{q}" for y, q, _ in rows]
L158         vals = [v for _, _, v in rows]
L159         s = pd.Series(vals, index=idx, name="Revenue")
L160         return None if s.dropna().empty else s
L161     except Exception:
L162         return None
L163
L164
L165 def _dump_dfz(df_z: pd.DataFrame, debug_mode: bool, max_rows: int = 400, ndigits: int = 3) -> None:
L166     """df_z を System log(INFO) へダンプする簡潔なユーティリティ."""
L167     if not debug_mode:
L168         return
L169     try:
L170         view = df_z.copy()
L171         view = view.apply(
L172             lambda s: s.round(ndigits)
L173             if getattr(getattr(s, "dtype", None), "kind", "") in ("f", "i")
L174             else s
L175         )
L176         if len(view) > max_rows:
L177             view = view.iloc[:max_rows]
L178
L179         # === NaNサマリ（列ごとの欠損件数 上位20） ===
L180         try:
L181             nan_counts = df_z.isna().sum().sort_values(ascending=False)
L182             top_nan = nan_counts[nan_counts > 0].head(20)
L183             if len(top_nan) > 0:
L184                 logger.info("NaN columns (top20):\n%s", top_nan.to_string())
L185             else:
L186                 logger.info("NaN columns: none")
L187         except Exception as exc:
L188             logger.warning("nan summary failed: %s", exc)
L189
L190         # === Zeroサマリ（列ごとのゼロ比率 上位20） ===
L191         try:
L192             zero_counts = ((df_z == 0) & (~df_z.isna())).sum()
L193             nonnull_counts = (~df_z.isna()).sum()
L194             zero_ratio = (zero_counts / nonnull_counts).sort_values(ascending=False)
L195             top_zero = zero_ratio[zero_ratio > 0].head(20)
L196             if len(top_zero) > 0:
L197                 logger.info(
L198                     "Zero-dominated columns (top20):\n%s",
L199                     top_zero.to_string(float_format=lambda x: f"{x:.2%}"),
L200                 )
L201             else:
L202                 logger.info("Zero-dominated columns: none")
L203         except Exception as exc:
L204             logger.warning("zero summary failed: %s", exc)
L205
L206         logger.info("===== DF_Z DUMP START =====")
L207         logger.info("\n%s", view.to_string(max_rows=None, max_cols=None))
L208         logger.info("===== DF_Z DUMP END =====")
L209     except Exception as exc:
L210         logger.warning("df_z dump failed: %s", exc)
L211
L212 def _safe_div(a, b):
L213     try: return np.nan if (b is None or float(b)==0 or pd.isna(b)) else float(a)/float(b)
L214     except Exception: return np.nan
L215
L216 def _safe_last(series: pd.Series, default=np.nan):
L217     try: return float(series.iloc[-1])
L218     except Exception: return default
L219
L220 D_WEIGHTS_EFF = None  # 出力表示互換のため
L221
L222 # ---
```
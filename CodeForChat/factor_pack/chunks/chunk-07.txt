```text
sc, "D_WEIGHTS_EFF", {}).keys():
L88             if k in df_z.columns:
L89                 used.add(k)
L90     except Exception:
L91         pass
L92
L93     for k in [
L94         "GROWTH_F",
L95         "MOM",
L96         "TRD",
L97         "VOL",
L98         "D_QAL",
L99         "D_YLD",
L100         "D_VOL_RAW",
L101         "D_TRD",
L102     ]:
L103         if k in df_z.columns:
L104             used.add(k)
L105
L106     grw_cols = [
L107         "GRW_PATH",
L108         "GRW_FLEX_SCORE",
L109         "GROWTH_F",
L110         "GRW_REV_YOY_Q",
L111         "GRW_REV_ACC_Q",
L112         "GRW_REV_QOQ",
L113         "GRW_REV_TTM2",
L114         "GRW_REV_YOY_Y",
L115         "GRW_PRICE_PROXY",
L116     ]
L117     for k in grw_cols:
L118         if (k in df.columns) or (k in df_z.columns):
L119             used.add(k)
L120
L121     for c in df_z.columns:
L122         if isinstance(c, str) and c.startswith("D_"):
L123             used.add(c)
L124
L125     num = df_z.select_dtypes(include=["number"])
L126     if not num.empty:
L127         var_top = num.var().sort_values(ascending=False).head(20).index.tolist()
L128         used.update(var_top)
L129
L130     return sorted(used)
L131
L132
L133 def _reorder_for_debug(df, df_z, factor_cols=FACTOR_COLUMNS):
L134     cols: list[str] = []
L135     for fac in ["GRW", "MOM", "VOL", "QUAL", "VAL"]:
L136         for c in factor_cols.get(fac, []):
L137             if c in getattr(df_z, "columns", []) or c in getattr(df, "columns", []):
L138                 cols.append(c)
L139     seen: set[str] = set()
L140     ordered: list[str] = []
L141     for c in cols:
L142         if c not in seen:
L143             ordered.append(c)
L144             seen.add(c)
L145     return ordered
L146
L147
L148 def dump_dfz_scoped(df, df_z, *, topk=20, logger=None):
L149     import numpy as np, pandas as pd, logging
L150
L151     lg = logger or logging.getLogger(__name__)
L152
L153     if not DEBUG_SCOPE_STRICT:
L154         dfz = df_z.copy()
L155         lg.info("DEBUG scope: disabled (showing ALL %d columns).", dfz.shape[1])
L156     else:
L157         rel = _detect_used_cols(df, df_z)
L158         dfz = df_z[[c for c in rel if c in df_z.columns]].copy()
L159         if dfz.shape[1] < 15:
L160             num = df_z.select_dtypes(include=["number"])
L161             add = []
L162             if not num.empty:
L163                 add = [
L164                     c
L165                     for c in num.var().sort_values(ascending=False).head(30).index
L166                     if c not in dfz.columns
L167                 ]
L168                 if dfz.shape[1] < 15:
L169                     add = add[: 15 - dfz.shape[1]]
L170                 else:
L171                     add = []
L172             dfz = pd.concat([dfz, df_z[add]], axis=1)
L173             lg.info("DEBUG scope too small → fallback add %d cols", len(add))
L174         excluded = [c for c in df_z.columns if c not in dfz.columns]
L175         lg.info(
L176             "DEBUG scope: %d relevant cols kept, %d excluded.",
L177             dfz.shape[1],
L178             len(excluded),
L179         )
L180
L181     nan_top = dfz.isna().sum().sort_values(ascending=False).head(topk)
L182     lg.info("scorer:NaN columns (top%d):", topk)
L183     for c, n in nan_top.items():
L184         lg.info("%s\t%d", c, int(n))
L185
L186     num_dfz = dfz.select_dtypes(include=["number"])
L187     if not num_dfz.empty:
L188         ztop = (num_dfz == 0).mean().sort_values(ascending=False).head(topk)
L189         lg.info("scorer:Zero-dominated columns (top%d):", topk)
L190         for c, r in ztop.items():
L191             lg.info("%s\t%.2f%%", c, 100.0 * float(r))
L192
L193     return dfz
L194
L195
L196 def save_factor_debug_csv(df, df_z, path="out/factor_debug_latest.csv"):
L197     import os, pandas as pd, logging
L198
L199     lg = logging.getLogger(__name__)
L200     try:
L201         cols = _reorder_for_debug(df, df_z)
L202         dump = pd.DataFrame(index=df.index)
L203         for c in cols:
L204             if c in getattr(df, "columns", []):
L205                 dump[c] = df[c]
L206             if c in getattr(df_z, "columns", []):
L207                 dump[c] = df_z[c]
L208         dump.reset_index(names=["symbol"], inplace=True)
L209         if path:
L210             dirpath = os.path.dirname(path) or "."
L211             os.makedirs(dirpath, exist_ok=True)
L212             dump.to_csv(path, index=False)
L213         lg.info(
L214             "factor debug CSV saved: %s (cols=%d rows=%d)",
L215             path,
L216             dump.shape[1],
L217             dump.shape[0],
L218         )
L219     except Exception as e:
L220         lg.warning("factor debug CSV failed: %s", e)
L221
L222
L223 def log_grw_stats(df, df_z, logger):
L224     import numpy as np, pandas as pd
L225
L226     try:
L227         s = pd.to_numeric(df.get("GRW_FLEX_SCORE", pd.Series(dtype=float)), errors="coerce")
L228         z = pd.to_numeric(df_z.get("GROWTH_F", pd.Series(dtype=float)), errors="coerce")
L229         if s.size:
L230             logger.info(
L231                 "GRW raw stats: n=%d, median=%.3f, mad=%.3f, std=%.3f",
L232                 s.count(),
L233                 np.nanmedian(s),
L234                 np.nanmedian(np.abs(s - np.nanmedian(s))),
L235                 np.nanstd(s),
L236             )
L237         if z.size and not z.dropna().empty:
L238             clip_hi = float((z >= 2.95).mean() * 100.0)
L239             clip_lo = float((z <= -2.95).mean() * 100.0)
L240             logger.info(
L241                 "GRW z stats: min=%.2f, p25=%.2f, med=%.2f, p75=%.2f, max=%.2f, clipped_hi=%.1f%%, clipped_lo=%.1f%%",
L242                 np.nanmin(z),
L243                 np.nanpercentile(z.dropna(), 25),
L244                 np.nanmedian(z),
L245                 np.nanpercentile(z.dropna(), 75),
L246                 np.nanmax(z),
L247                 clip_hi,
L248                 clip_lo,
L249             )
L250         if "GRW_PATH" in getattr(df, "columns", []):
L251             logger.info(
L252                 "GRW path breakdown: %s",
L253                 df["GRW_PATH"].value_counts(dropna=False).to_dict(),
L254             )
L255     except Exception as e:
L256         logger.warning("GRW stats log failed: %s", e)
L257
L258
L259 def _grw_record_to_df(t: str, info_t: dict, df):
L260     if not isinstance(df, pd.DataFrame):
L261         return
L262     raw_parts = info_t.get("DEBUG_GRW_PARTS") if isinstance(info_t, dict) else None
L263     parts: dict[str, Any] = {}
L264     if isinstance(raw_parts, str):
L265         try:
L266             parts = json.loads(raw_parts)
L267         except Exception:
L268             parts = {}
L269     elif isinstance(raw_parts, dict):
L270         parts = raw_parts
L271     path = info_t.get("DEBUG_GRW_PATH") if isinstance(info_t, dict) else None
L272     score = info_t.get("GRW_SCORE") if isinstance(info_t, dict) else None
L273
L274     def _part_value(key: str):
L275         value = parts.get(key) if isinstance(parts, dict) else None
L276         if value is None:
L277             return np.nan
L278         try:
L279             return float(value)
L280         except Exception:
L281             return np.nan
L282
L283     df.loc[t, "GRW_PATH"] = path
L284     df.loc[t, "GRW_FLEX_SCORE"] = np.nan if score is None else float(score)
L285     df.loc[t, "GRW_REV_YOY_Q"] = _part_value("rev_yoy_q")
L286     df.loc[t, "GRW_REV_ACC_Q"] = _part_value("rev_acc_q")
L287     df.loc[t, "GRW_REV_QOQ"] = _part_value("rev_qoq")
L288     df.loc[t, "GRW_REV_TTM2"] = _part_value("rev_ttm2")
L289     df.loc[t, "GRW_REV_YOY_Y"] = _part_value("rev_yoy_y")
L290     df.loc[t, "GRW_PRICE_PROXY"] = _part_value("price_proxy")
L291
L292 # ---- Dividend Helpers -------------------------------------------------------
L293 def _last_close(t, price_map=None):
L294     if price_map and (c := price_map.get(t)) is not None: return float(c)
L295     try:
L296         h = yf.Ticker(t).history(period="5d")["Close"].dropna()
L297         return float(h.iloc[-1]) if len(h) else np.nan
L298     except Exception:
L299         return np.nan
L300
L301 def _ttm_div_sum(t, lookback_days=400):
L302     try:
L303         div = yf.Ticker(t).dividends
L304         if div is None or len(div) == 0: return 0.0
L305         cutoff = pd.Timestamp.utcnow().tz_localize(None) - pd.Timedelta(days=lookback_days)
L306         ttm = float(div[div.index.tz_localize(None) >= cutoff].sum())
L307         return ttm if ttm > 0 else float(div.tail(4).sum())
L308     except Exception:
L309         return 0.0
L310
L311 def ttm_div_yield_portfolio(tickers, price_map=None):
L312     ys = [(lambda c, s: (s/c) if (np.isfinite(c) and c>0 and s>0) else 0.0)(_last_close(t, price_map), _ttm_div_sum(t)) for t in tickers]
L313     return float(np.mean(ys)) if ys else 0.0
L314
L315 # ---- 簡易ユーティリティ（安全な短縮のみ） -----------------------------------
L316 def winsorize_s(s: pd.Series, p=0.02):
L317     if s is None or s.dropna().empty: return s
L318     lo, hi = np.nanpercentile(s.astype(float), [100*p, 100*(1-p)]); return s.clip(lo, hi)
L319
L320 def robust_z(s: pd.Series, p=0.02):
L321     s2 = winsorize_s(s,p); return np.nan_to_num(zscore(s2.fillna(s2.mean())))
L322
L323 def robust_z_keepnan(s: pd.Series) -> pd.Series:
L324     """robust_z variant that preserves NaNs and falls back to rank-z when needed."""
L325     if s is None:
L326         return pd.Series(dtype=float)
L327     v = pd.to_numeric(s, errors="coerce")
L328     m = np.nanmedian(v)
L329     mad = np.nanmedian(np.abs(v - m))
L330     z = (v - m) / (1.4826 * mad + 1e-9)
L331     if np.nanstd(z) < 1e-9:
L332         r = v.rank(method="average", na_option="keep")
L333         z = (r - np.nanmean(r)) / (np.nanstd(r) + 1e-9)
L334     return pd.Series(z, index=v.index, dtype=float)
L335
L336
L337 def _dump_dfz(
L338     df: pd.DataFrame,
L339     df_z: pd.DataFrame,
L340     debug_mode: bool,
L341     max_rows: int = 400,
L342     ndigits: int = 3,
L343 ) -> None:
L344     """df_z を System log(INFO) へダンプする簡潔なユーティリティ."""
L345
L346     if not debug_mode:
L347         return
L348     try:
L349         dfz_scoped = dump_dfz_scoped(df, df_z, topk=20, logger=logger)
L350         ordered = _reorder_for_debug(df, df_z)
L351         rel_set = set(dfz_scoped.columns)
L352         view_cols = [c for c in ordered if c in rel_set]
L353         if not view_cols:
L354             view_cols = list(dfz_scoped.columns)
L355         view = dfz_scoped[view_cols].copy()
L356         view = view.apply(
L357             lambda s: s.round(ndigits)
L358             if getattr(getattr(s, "dtype", None), "kind", "") in ("f", "i")
L359             else s
L360         )
L361         if len(view) > max_rows:
L362             view = view.iloc[:max_rows]
L363
L364         logger.info("===== DF_Z DUMP START =====")
L365         logger.info("\n%s", view.to_string(max_rows=None, max_cols=None))
L366         logger.info("===== DF_Z DUMP END =====")
L367     except Exception as exc:
L368         logger.warning("df_z dump failed: %s", exc)
L369
L370 def _safe_div(a, b):
L371     try: return np.nan if (b is None or float(b)==0 or pd.isna(b)) else float(a)/float(b)
L372     except Exception: return np.nan
L373
L374 def _safe_last(series: pd.Series, default=np.nan):
L375     try: return float(series.iloc[-1])
L376     except Exception: return default
L377
L378
L379 def _ensure_series(x):
L380     if x is None:
L381         return pd.Series(dtype=float)
L382     if isinstance(x, pd.Series):
L383         return x.dropna()
L384     if isinstance(x, (list, tuple)):
L385         if len(x) and isinstance(x[0], (tuple, list)) and len(x[0]) == 2:
L386             dt = pd.to_datetime([d for d, _ in x], errors="coerce")
L387             v = pd.to_numeric([_v for _, _v in x], errors="coerce")
L388             return pd.Series(v, index=dt).dropna()
L389         return pd.Series(pd.to_numeric(list(x), errors="coerce")).dropna()
L390     try:
L391   
```
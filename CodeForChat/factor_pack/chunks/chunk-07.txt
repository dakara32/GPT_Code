```text
ollect_relevant_cols(df, df_z, *, weight_dict=None):
L72     use: set[str] = set()
L73     if weight_dict:
L74         try:
L75             items = weight_dict.items() if hasattr(weight_dict, "items") else []
L76         except Exception:
L77             items = []
L78         for col, w in items:
L79             try:
L80                 weight = float(w or 0)
L81             except Exception:
L82                 weight = 0.0
L83             if abs(weight) > 0 and col in getattr(df_z, "columns", []):
L84                 use.add(col)
L85     for fac in ("GRW",):
L86         for c in FACTOR_COLUMNS.get(fac, []):
L87             if c in getattr(df, "columns", []) or c in getattr(df_z, "columns", []):
L88                 use.add(c)
L89     for fac in ["MOM", "VOL", "QUAL", "VAL"]:
L90         for c in FACTOR_COLUMNS.get(fac, []):
L91             if c in getattr(df, "columns", []) or c in getattr(df_z, "columns", []):
L92                 use.add(c)
L93     return sorted(use)
L94
L95
L96 def _reorder_for_debug(df, df_z, factor_cols=FACTOR_COLUMNS):
L97     cols: list[str] = []
L98     for fac in ["GRW", "MOM", "VOL", "QUAL", "VAL"]:
L99         for c in factor_cols.get(fac, []):
L100             if c in getattr(df_z, "columns", []) or c in getattr(df, "columns", []):
L101                 cols.append(c)
L102     seen: set[str] = set()
L103     ordered: list[str] = []
L104     for c in cols:
L105         if c not in seen:
L106             ordered.append(c)
L107             seen.add(c)
L108     return ordered
L109
L110
L111 def dump_dfz_scoped(df, df_z, *, weight_dict=None, topk=20, logger=None):
L112     import pandas as pd, logging
L113
L114     logger = logger or logging.getLogger(__name__)
L115     rel = _collect_relevant_cols(df, df_z, weight_dict=weight_dict)
L116     rel_df = [c for c in rel if c in getattr(df_z, "columns", [])]
L117     dfz = df_z[rel_df].copy() if rel_df else df_z.copy()
L118     logger.info("DEBUG scope: %d relevant cols (of %d total).", dfz.shape[1], df_z.shape[1])
L119     nan_top = dfz.isna().sum().sort_values(ascending=False).head(topk)
L120     logger.info("scorer:NaN columns (top%d):", topk)
L121     for c, n in nan_top.items():
L122         logger.info("%s\t%d", c, int(n))
L123     num_dfz = dfz.select_dtypes(include=["number"])
L124     if not num_dfz.empty:
L125         ztop = (num_dfz == 0).mean().sort_values(ascending=False).head(topk)
L126         logger.info("scorer:Zero-dominated columns (top%d):", topk)
L127         for c, r in ztop.items():
L128             logger.info("%s\t%.2f%%", c, 100.0 * float(r))
L129
L130
L131 def save_factor_debug_csv(df, df_z, path="out/factor_debug_latest.csv"):
L132     import os, pandas as pd, logging
L133
L134     lg = logging.getLogger(__name__)
L135     try:
L136         cols = _reorder_for_debug(df, df_z)
L137         dump = pd.DataFrame(index=df.index)
L138         for c in cols:
L139             if c in getattr(df, "columns", []):
L140                 dump[c] = df[c]
L141             if c in getattr(df_z, "columns", []):
L142                 dump[c] = df_z[c]
L143         dump.reset_index(names=["symbol"], inplace=True)
L144         if path:
L145             dirpath = os.path.dirname(path) or "."
L146             os.makedirs(dirpath, exist_ok=True)
L147             dump.to_csv(path, index=False)
L148         lg.info(
L149             "factor debug CSV saved: %s (cols=%d rows=%d)",
L150             path,
L151             dump.shape[1],
L152             dump.shape[0],
L153         )
L154     except Exception as e:
L155         lg.warning("factor debug CSV failed: %s", e)
L156
L157
L158 def log_grw_stats(df, df_z, logger):
L159     import numpy as np, pandas as pd
L160
L161     try:
L162         s = pd.to_numeric(df.get("GRW_FLEX_SCORE", pd.Series(dtype=float)), errors="coerce")
L163         z = pd.to_numeric(df_z.get("GROWTH_F", pd.Series(dtype=float)), errors="coerce")
L164         if s.size:
L165             logger.info(
L166                 "GRW raw stats: n=%d, median=%.3f, mad=%.3f, std=%.3f",
L167                 s.count(),
L168                 np.nanmedian(s),
L169                 np.nanmedian(np.abs(s - np.nanmedian(s))),
L170                 np.nanstd(s),
L171             )
L172         if z.size and not z.dropna().empty:
L173             clip_hi = float((z >= 2.95).mean() * 100.0)
L174             clip_lo = float((z <= -2.95).mean() * 100.0)
L175             logger.info(
L176                 "GRW z stats: min=%.2f, p25=%.2f, med=%.2f, p75=%.2f, max=%.2f, clipped_hi=%.1f%%, clipped_lo=%.1f%%",
L177                 np.nanmin(z),
L178                 np.nanpercentile(z.dropna(), 25),
L179                 np.nanmedian(z),
L180                 np.nanpercentile(z.dropna(), 75),
L181                 np.nanmax(z),
L182                 clip_hi,
L183                 clip_lo,
L184             )
L185         if "GRW_PATH" in getattr(df, "columns", []):
L186             logger.info(
L187                 "GRW path breakdown: %s",
L188                 df["GRW_PATH"].value_counts(dropna=False).to_dict(),
L189             )
L190     except Exception as e:
L191         logger.warning("GRW stats log failed: %s", e)
L192
L193
L194 def _grw_record_to_df(t: str, info_t: dict, df):
L195     if not isinstance(df, pd.DataFrame):
L196         return
L197     raw_parts = info_t.get("DEBUG_GRW_PARTS") if isinstance(info_t, dict) else None
L198     parts: dict[str, Any] = {}
L199     if isinstance(raw_parts, str):
L200         try:
L201             parts = json.loads(raw_parts)
L202         except Exception:
L203             parts = {}
L204     elif isinstance(raw_parts, dict):
L205         parts = raw_parts
L206     path = info_t.get("DEBUG_GRW_PATH") if isinstance(info_t, dict) else None
L207     score = info_t.get("GRW_SCORE") if isinstance(info_t, dict) else None
L208
L209     def _part_value(key: str):
L210         value = parts.get(key) if isinstance(parts, dict) else None
L211         if value is None:
L212             return np.nan
L213         try:
L214             return float(value)
L215         except Exception:
L216             return np.nan
L217
L218     df.loc[t, "GRW_PATH"] = path
L219     df.loc[t, "GRW_FLEX_SCORE"] = np.nan if score is None else float(score)
L220     df.loc[t, "GRW_REV_YOY_Q"] = _part_value("rev_yoy_q")
L221     df.loc[t, "GRW_REV_ACC_Q"] = _part_value("rev_acc_q")
L222     df.loc[t, "GRW_REV_QOQ"] = _part_value("rev_qoq")
L223     df.loc[t, "GRW_REV_TTM2"] = _part_value("rev_ttm2")
L224     df.loc[t, "GRW_REV_YOY_Y"] = _part_value("rev_yoy_y")
L225     df.loc[t, "GRW_PRICE_PROXY"] = _part_value("price_proxy")
L226
L227 # ---- Dividend Helpers -------------------------------------------------------
L228 def _last_close(t, price_map=None):
L229     if price_map and (c := price_map.get(t)) is not None: return float(c)
L230     try:
L231         h = yf.Ticker(t).history(period="5d")["Close"].dropna()
L232         return float(h.iloc[-1]) if len(h) else np.nan
L233     except Exception:
L234         return np.nan
L235
L236 def _ttm_div_sum(t, lookback_days=400):
L237     try:
L238         div = yf.Ticker(t).dividends
L239         if div is None or len(div) == 0: return 0.0
L240         cutoff = pd.Timestamp.utcnow().tz_localize(None) - pd.Timedelta(days=lookback_days)
L241         ttm = float(div[div.index.tz_localize(None) >= cutoff].sum())
L242         return ttm if ttm > 0 else float(div.tail(4).sum())
L243     except Exception:
L244         return 0.0
L245
L246 def ttm_div_yield_portfolio(tickers, price_map=None):
L247     ys = [(lambda c, s: (s/c) if (np.isfinite(c) and c>0 and s>0) else 0.0)(_last_close(t, price_map), _ttm_div_sum(t)) for t in tickers]
L248     return float(np.mean(ys)) if ys else 0.0
L249
L250 # ---- 簡易ユーティリティ（安全な短縮のみ） -----------------------------------
L251 def winsorize_s(s: pd.Series, p=0.02):
L252     if s is None or s.dropna().empty: return s
L253     lo, hi = np.nanpercentile(s.astype(float), [100*p, 100*(1-p)]); return s.clip(lo, hi)
L254
L255 def robust_z(s: pd.Series, p=0.02):
L256     s2 = winsorize_s(s,p); return np.nan_to_num(zscore(s2.fillna(s2.mean())))
L257
L258 def robust_z_keepnan(s: pd.Series) -> pd.Series:
L259     """robust_z variant that preserves NaNs and falls back to rank-z when needed."""
L260     if s is None:
L261         return pd.Series(dtype=float)
L262     v = pd.to_numeric(s, errors="coerce")
L263     m = np.nanmedian(v)
L264     mad = np.nanmedian(np.abs(v - m))
L265     z = (v - m) / (1.4826 * mad + 1e-9)
L266     if np.nanstd(z) < 1e-9:
L267         r = v.rank(method="average", na_option="keep")
L268         z = (r - np.nanmean(r)) / (np.nanstd(r) + 1e-9)
L269     return pd.Series(z, index=v.index, dtype=float)
L270
L271
L272 def _dump_dfz(
L273     df: pd.DataFrame,
L274     df_z: pd.DataFrame,
L275     debug_mode: bool,
L276     max_rows: int = 400,
L277     ndigits: int = 3,
L278     *,
L279     weight_dict=None,
L280 ) -> None:
L281     """df_z を System log(INFO) へダンプする簡潔なユーティリティ."""
L282
L283     if not debug_mode:
L284         return
L285     try:
L286         rel = _collect_relevant_cols(df, df_z, weight_dict=weight_dict)
L287         ordered = _reorder_for_debug(df, df_z)
L288         rel_set = set(rel)
L289         view_cols = [c for c in ordered if c in rel_set and c in df_z.columns]
L290         if not view_cols:
L291             view_cols = list(df_z.columns)
L292         view = df_z[view_cols].copy()
L293         view = view.apply(
L294             lambda s: s.round(ndigits)
L295             if getattr(getattr(s, "dtype", None), "kind", "") in ("f", "i")
L296             else s
L297         )
L298         if len(view) > max_rows:
L299             view = view.iloc[:max_rows]
L300
L301         dump_dfz_scoped(df, df_z, weight_dict=weight_dict, topk=20, logger=logger)
L302
L303         logger.info("===== DF_Z DUMP START =====")
L304         logger.info("\n%s", view.to_string(max_rows=None, max_cols=None))
L305         logger.info("===== DF_Z DUMP END =====")
L306     except Exception as exc:
L307         logger.warning("df_z dump failed: %s", exc)
L308
L309 def _safe_div(a, b):
L310     try: return np.nan if (b is None or float(b)==0 or pd.isna(b)) else float(a)/float(b)
L311     except Exception: return np.nan
L312
L313 def _safe_last(series: pd.Series, default=np.nan):
L314     try: return float(series.iloc[-1])
L315     except Exception: return default
L316
L317
L318 def _ensure_series(x):
L319     if x is None:
L320         return pd.Series(dtype=float)
L321     if isinstance(x, pd.Series):
L322         return x.dropna()
L323     if isinstance(x, (list, tuple)):
L324         if len(x) and isinstance(x[0], (tuple, list)) and len(x[0]) == 2:
L325             dt = pd.to_datetime([d for d, _ in x], errors="coerce")
L326             v = pd.to_numeric([_v for _, _v in x], errors="coerce")
L327             return pd.Series(v, index=dt).dropna()
L328         return pd.Series(pd.to_numeric(list(x), errors="coerce")).dropna()
L329     try:
L330         return pd.Series(x).dropna()
L331     except Exception:
L332         return pd.Series(dtype=float)
L333
L334
L335 def _to_quarterly(s: pd.Series) -> pd.Series:
L336     if s.empty or not isinstance(s.index, pd.DatetimeIndex):
L337         return s
L338     return s.resample("Q").last().dropna()
L339
L340
L341 def _ttm_yoy_from_quarterly(qs: pd.Series) -> pd.Series:
L342     if qs is None or qs.empty:
L343         return pd.Series(dtype=float)
L344     ttm = qs.rolling(4, min_periods=2).sum()
L345     yoy = ttm.pct_change(4)
L346     return yoy
L347
L348
L349 def _nz(x) -> float:
L350     if x is None:
L351         return 0.0
L352     try:
L353         value = float(x)
L354     except Exception:
L355         return 0.0
L356     if not np.isfinite(value):
L357         return 0.0
L358     return value
L359
L360
L361 def _winsor(x, lo=-2.0, hi=2.0) -> float:
L362     v = _nz(x)
L363     if v < lo:
L364         return float(lo)
L365     if v > hi:
L366         return float(hi)
L367    
```
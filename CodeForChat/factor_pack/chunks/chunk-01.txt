```text
# === Chat Paste Pack ===
# Repo: dakara32/GPT_Code @ main
# Files: factor.py, scorer.py, .github/workflows/weekly-report.yml, documents/README.md, documents/factor_design.md
# ä½¿ã„æ–¹: ä¸‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’é †ã«è²¼ã‚Œã°ã“ã®ãƒãƒ£ãƒƒãƒˆã§å…¨ä½“æŠŠæ¡ã§ãã¾ã™ã€‚
# æ³¨è¨˜: å„ãƒ•ã‚¡ã‚¤ãƒ«ã¯å€‹åˆ¥ã« L1.. ã§è¡Œç•ªå·ä»˜ä¸ã€‚
---

## <factor.py>
```text
L1 '''ROLE: Orchestration ONLYï¼ˆå¤–éƒ¨I/Oãƒ»SSOTãƒ»Slackå‡ºåŠ›ï¼‰, è¨ˆç®—ã¯ scorer.py'''
L2 # === NOTE: æ©Ÿèƒ½ãƒ»å…¥å‡ºåŠ›ãƒ»ãƒ­ã‚°æ–‡è¨€ãƒ»ä¾‹å¤–æŒ™å‹•ã¯ä¸å¤‰ã€‚å®‰å…¨ãªçŸ­ç¸®ï¼ˆimportçµ±åˆ/è¤‡æ•°ä»£å…¥/å†…åŒ…è¡¨è¨˜/ãƒ¡ã‚½ãƒƒãƒ‰ãƒã‚§ãƒ¼ãƒ³/ä¸€è¡ŒåŒ–/ç©ºè¡Œåœ§ç¸®ãªã©ï¼‰ã®ã¿é©ç”¨ ===
L3 BONUS_COEFF = 0.4   # æ”»ã‚=0.3 / ä¸­åº¸=0.4 / å®ˆã‚Š=0.5
L4 import os, json, time, requests
L5 from time import perf_counter
L6 from dataclasses import dataclass
L7 from typing import Dict, List
L8 from concurrent.futures import ThreadPoolExecutor
L9 import numpy as np
L10 import pandas as pd
L11 import yfinance as yf
L12 from scipy.stats import zscore  # used via scorer
L13 from scorer import Scorer, ttm_div_yield_portfolio
L14
L15 class T:
L16     t = perf_counter()
L17     log = staticmethod(lambda tag: (lambda now=perf_counter(): (print(f"[T] {tag}: {now - T.t:.2f}s"), setattr(T, "t", now))[-1])())
L18
L19 T.log("start")
L20
L21 # === ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã¨å®šæ•°ï¼ˆå†’é ­ã«å›ºå®šï¼‰ ===
L22 exist, cand = [pd.read_csv(f, header=None)[0].tolist() for f in ("current_tickers.csv","candidate_tickers.csv")]
L23 T.log(f"csv loaded: exist={len(exist)} cand={len(cand)}")
L24 CAND_PRICE_MAX, bench = 450, '^GSPC'  # ä¾¡æ ¼ä¸Šé™ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
L25 N_G, N_D = 12, 13  # G/Dæ ã‚µã‚¤ã‚º
L26 g_weights = {'GRW':0.40,'MOM':0.45,'VOL':-0.15}
L27 D_BETA_MAX = float(os.environ.get("D_BETA_MAX", "0.8"))
L28 FILTER_SPEC = {"G":{"pre_mask":["trend_template"]},"D":{"pre_filter":{"beta_max":D_BETA_MAX}}}
L29 D_weights = {'QAL':0.15,'YLD':0.15,'VOL':-0.45,'TRD':0.25}
L30 _fmt_w = lambda w: " ".join(f"{k}{int(v*100)}" for k, v in w.items())
L31
L32 # DRRS åˆæœŸãƒ—ãƒ¼ãƒ«ãƒ»å„ç¨®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
L33 corrM = 45
L34 DRRS_G, DRRS_D = dict(lookback=252,n_pc=3,gamma=1.2,lam=0.68,eta=0.8), dict(lookback=504,n_pc=4,gamma=0.8,lam=0.85,eta=0.5)
L35 DRRS_SHRINK = 0.10  # æ®‹å·®ç›¸é–¢ã®å¯¾è§’ã‚·ãƒ¥ãƒªãƒ³ã‚¯ï¼ˆåŸºç¤ï¼‰
L36
L37 # ã‚¯ãƒ­ã‚¹ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆæœªå®šç¾©ãªã‚‰è¨­å®šï¼‰
L38 try: CROSS_MU_GD
L39 except NameError: CROSS_MU_GD = 0.40  # æ¨å¥¨ 0.35â€“0.45ï¼ˆlam=0.85æƒ³å®šï¼‰
L40
L41 # å‡ºåŠ›é–¢é€£
L42 RESULTS_DIR = "results"
L43 os.makedirs(RESULTS_DIR, exist_ok=True)
L44
L45 # ãã®ä»–
L46 debug_mode, FINNHUB_API_KEY = False, os.environ.get("FINNHUB_API_KEY")
L47
L48 # === å…±æœ‰DTOï¼ˆã‚¯ãƒ©ã‚¹é–“I/Oå¥‘ç´„ï¼‰ï¼‹ Config ===
L49 @dataclass(frozen=True)
L50 class InputBundle:
L51     # Input â†’ Scorer ã§å—ã‘æ¸¡ã™ç´ æï¼ˆI/Oç¦æ­¢ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ï¼‰
L52     cand: List[str]
L53     tickers: List[str]
L54     bench: str
L55     data: pd.DataFrame              # yfinance downloadçµæœï¼ˆ'Close','Volume'ç­‰ã®éšå±¤åˆ—ï¼‰
L56     px: pd.DataFrame                # data['Close']
L57     spx: pd.Series                  # data['Close'][bench]
L58     tickers_bulk: object            # yfinance.Tickers
L59     info: Dict[str, dict]           # yfinance info per ticker
L60     eps_df: pd.DataFrame            # ['eps_ttm','eps_q_recent',...]
L61     fcf_df: pd.DataFrame            # ['fcf_ttm', ...]
L62     returns: pd.DataFrame           # px[tickers].pct_change()
L63
L64 @dataclass(frozen=True)
L65 class FeatureBundle:
L66     df: pd.DataFrame
L67     df_z: pd.DataFrame
L68     g_score: pd.Series
L69     d_score_all: pd.Series
L70     missing_logs: pd.DataFrame
L71
L72 @dataclass(frozen=True)
L73 class SelectionBundle:
L74     resG: dict
L75     resD: dict
L76     top_G: List[str]
L77     top_D: List[str]
L78     init_G: List[str]
L79     init_D: List[str]
L80
L81 @dataclass(frozen=True)
L82 class WeightsConfig:
L83     g: Dict[str,float]
L84     d: Dict[str,float]
L85
L86 @dataclass(frozen=True)
L87 class DRRSParams:
L88     corrM: int
L89     shrink: float
L90     G: Dict[str,float]   # lookback, n_pc, gamma, lam, eta
L91     D: Dict[str,float]
L92     cross_mu_gd: float
L93
L94 @dataclass(frozen=True)
L95 class PipelineConfig:
L96     weights: WeightsConfig
L97     drrs: DRRSParams
L98     price_max: float
L99
L100 # === å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆè¤‡æ•°ã‚¯ãƒ©ã‚¹ã§ä½¿ç”¨ï¼‰ ===
L101 # (unused local utils removed â€“ use scorer.py versions if needed)
L102
L103 _env_true = lambda name, default=False: (os.getenv(name) or str(default)).strip().lower() == "true"
L104
L105 def _post_slack(payload: dict):
L106     url = os.getenv("SLACK_WEBHOOK_URL")
L107     if not url: print("âš ï¸ SLACK_WEBHOOK_URL æœªè¨­å®š"); return
L108     try:
L109         requests.post(url, json=payload).raise_for_status()
L110     except Exception as e:
L111         print(f"âš ï¸ Slacké€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
L112
L113 _slack = lambda message, code=False: _post_slack({"text": f"```{message}```" if code else message})
L114
L115 def _slack_debug(text: str, chunk=2800):
L116     i = 0
L117     while i < len(text):
L118         j = min(len(text), i + chunk)
L119         k = text.rfind("\n", i, j)
L120         j = k if k > i + 100 else j
L121         _post_slack({"blocks":[{"type":"section","text":{"type":"mrkdwn","text":f"```{text[i:j]}```"}}]})
L122         i = j
L123
L124 def _compact_debug(fb, sb, prevG, prevD, max_rows=140):
L125     want=["TR","EPS","REV","ROE","BETA_RAW","FCF","RS","TR_str","DIV_STREAK","DSC"]
L126     all_cols = _env_true("DEBUG_ALL_COLS", False)
L127     cols = list(fb.df_z.columns if all_cols else [c for c in want if c in fb.df_z.columns])
L128
L129     Gp, Dp = set(prevG or []), set(prevD or [])
L130     g_new=[t for t in (sb.top_G or []) if t not in Gp]; g_out=[t for t in Gp if t not in (sb.top_G or [])]
L131     d_new=[t for t in (sb.top_D or []) if t not in Dp]; d_out=[t for t in Dp if t not in (sb.top_D or [])]
L132
L133     show_near = _env_true("DEBUG_NEAR5", True)
L134     gs, ds = getattr(fb,"g_score",None), getattr(fb,"d_score_all",None)
L135     gs = (gs.sort_values(ascending=False) if show_near and hasattr(gs,"sort_values") else None)
L136     ds = (ds.sort_values(ascending=False) if show_near and hasattr(ds,"sort_values") else None)
L137     g_miss = ([t for t in gs.index if t not in (sb.top_G or [])][:10]) if gs is not None else []
L138     d_excl = set((sb.top_G or [])+(sb.top_D or []))
L139     d_miss = ([t for t in ds.index if t not in d_excl][:10]) if ds is not None else []
L140
L141     all_rows = _env_true("DEBUG_ALL_ROWS", False)
L142     focus = list(fb.df_z.index) if all_rows else sorted(set(g_new+g_out+d_new+d_out+(sb.top_G or [])+(sb.top_D or [])+g_miss+d_miss))[:max_rows]
L143
L144     def _fmt_near(lbl, ser, lst):
L145         if ser is None: return f"{lbl}: off"
L146         g = ser.get
L147         parts=[f"{t}:{g(t,float('nan')):.3f}" if pd.notna(g(t)) else f"{t}:nan" for t in lst]
L148         return f"{lbl}: " + (", ".join(parts) if parts else "-")
L149
L150     head=[f"G new/out: {len(g_new)}/{len(g_out)}  D new/out: {len(d_new)}/{len(d_out)}",
L151           _fmt_near("G near10", gs, g_miss),
L152           _fmt_near("D near10", ds, d_miss),
L153           f"Filters: G pre_mask=['trend_template'], D pre_filter={{'beta_max': {D_BETA_MAX}}}",
L154           f"Cols={'ALL' if all_cols else 'MIN'}  Rows={'ALL' if all_rows else 'SUBSET'}"]
L155
L156     tbl="(df_z or columns not available)"
L157     if not fb.df_z.empty and cols:
L158         idx=[t for t in focus if t in fb.df_z.index]
L159         tbl=fb.df_z.loc[idx, cols].round(3).to_string(max_rows=None, max_cols=None)
L160
L161     miss_txt=""
L162     if _env_true("DEBUG_MISSING_LOGS", False):
L163         miss=getattr(fb,"missing_logs",None)
L164         if miss is not None and not miss.empty:
L165             miss_txt="\nMissing data (head)\n"+miss.head(10).to_string(index=False)
L166
L167     return "\n".join(head+["\nChanged/Selected (+ Near Miss)", tbl])+miss_txt
L168
L169 def _disjoint_keepG(top_G, top_D, poolD):
L170     """Gé‡è¤‡ã‚’Dã‹ã‚‰é™¤å»ã—ã€poolDã§é †æ¬¡è£œå……ï¼ˆæ¯æ¸‡æ™‚ã¯å…ƒéŠ˜æŸ„ç¶­æŒï¼‰ã€‚"""
L171     used, D, i = set(top_G), list(top_D), 0
L172     for j, t in enumerate(D):
L173         if t in used:
L174             while i < len(poolD) and (poolD[i] in used or poolD[i] in D):
L175                 i += 1
L176             if i < len(poolD):
L177                 D[j] = poolD[i]; used.add(D[j]); i += 1
L178     return top_G, D
L179
L180 _state_file = lambda: os.path.join(RESULTS_DIR, "breadth_state.json")
L181 def load_mode(default: str="NORMAL") -> str:
L182     try:
L183         m = json.loads(open(_state_file()).read()).get("mode", default)
L184         return m if m in ("EMERG","CAUTION","NORMAL") else default
L185     except Exception:
L186         return default
L187 def save_mode(mode: str):
L188     try:
L189         open(_state_file(),"w").write(json.dumps({"mode": mode}))
L190     except Exception:
L191         pass
L192
L193 # --- Breadthâ†’è‡ªå‹•ã—ãã„å€¤â†’ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹â†’Slackå…ˆé ­è¡Œã‚’ä½œæˆ ---
L194 def _build_breadth_lead_lines(inb) -> tuple[list[str], str]:
L195     win = int(os.getenv("BREADTH_CALIB_WIN_DAYS", "600"))
L196     C_ts = Scorer.trend_template_breadth_series(inb.px[inb.tickers], inb.spx, win_days=win)
L197     if C_ts.empty: raise RuntimeError("breadth series empty")
L198     warmup=int(os.getenv("BREADTH_WARMUP_DAYS","252")); base=C_ts.iloc[warmup:] if len(C_ts)>warmup else C_ts; C_full=int(C_ts.iloc[-1])
L199     q05 = int(np.nan_to_num(base.quantile(float(os.getenv("BREADTH_Q_EMERG_IN",  "0.05"))), nan=0.0))
L200     q20 = int(np.nan_to_num(base.quantile(float(os.getenv("BREADTH_Q_EMERG_OUT", "0.20"))), nan=0.0))
L201     q60 = int(np.nan_to_num(base.quantile(float(os.getenv("BREADTH_Q_WARN_OUT",  "0.60"))), nan=0.0))
L202     th_in_rec, th_out_rec, th_norm_rec = max(N_G, q05), max(int(np.ceil(1.5*N_G)), q20), max(3*N_G, q60)
L203     use_calib = os.getenv("BREADTH_USE_CALIB", "true").strip().lower() == "true"
L204     th_in, th_out, th_norm, th_src = (th_in_rec, th_out_rec, th_norm_rec, "è‡ªå‹•") if use_calib else (int(os.getenv("GTT_EMERG_IN",str(N_G))), int(os.getenv("GTT_EMERG_OUT",str(int(1.5*N_G)))), int(os.getenv("GTT_CAUTION_OUT",str(3*N_G))), "æ‰‹å‹•")
L205     prev = load_mode("NORMAL")
L206     if   prev == "EMERG":  mode = "EMERG" if (C_full < th_out) else ("CAUTION" if (C_full < th_norm) else "NORMAL")
L207     elif prev == "CAUTION": mode = "CAUTION" if (C_full < th_norm) else "NORMAL"
L208     else:                   mode = "EMERG" if (C_full < th_in) else ("CAUTION" if (C_full < th_norm) else "NORMAL")
L209     save_mode(mode)
L210     _MODE_JA={"EMERG":"ç·Šæ€¥","CAUTION":"è­¦æˆ’","NORMAL":"é€šå¸¸"}; _MODE_EMOJI={"EMERG":"ğŸš¨","CAUTION":"âš ï¸","NORMAL":"ğŸŸ¢"}
L211     mode_ja,emoji,eff_days=_MODE_JA.get(mode,mode),_MODE_EMOJI.get(mode,"â„¹ï¸"),len(base)
L212     lead_lines = [f"{emoji} *ç¾åœ¨ãƒ¢ãƒ¼ãƒ‰: {mode_ja}*", f"ãƒ†ãƒ³ãƒ—ãƒ¬åˆæ ¼æœ¬æ•°: *{C_full}æœ¬*", "ã—ãã„å€¤ï¼ˆ{0}ï¼‰".format(th_src),
L213         f"  ãƒ»ç·Šæ€¥å…¥ã‚Š: <{th_in}æœ¬", f"  ãƒ»ç·Šæ€¥è§£é™¤: â‰¥{th_out}æœ¬", f"  ãƒ»é€šå¸¸å¾©å¸°: â‰¥{th_norm}æœ¬",
L214         f"å‚è€ƒæŒ‡æ¨™ï¼ˆéå»~{win}å–¶æ¥­æ—¥, æœ‰åŠ¹={eff_days}æ—¥ï¼‰",
L215         f"  ãƒ»ä¸‹ä½5%: {q05}æœ¬", f"  ãƒ»ä¸‹ä½20%: {q20}æœ¬", f"  ãƒ»60%åˆ†ä½: {q60}æœ¬",]
L216     return lead_lines, mode
L217
L218 # === Inputï¼šå¤–éƒ¨I/Oã¨å‰å‡¦ç†ï¼ˆCSV/APIãƒ»æ¬ æè£œå®Œï¼‰ ===
L219 class Input:
L220     def __init__(self, cand, exist, bench, price_max, finnhub_api_key=None):
L221         self.cand, self.exist, self.bench, self.price_max = cand, exist, bench, price_max
L222         self.api_key = finnhub_api_key or os.environ.get("FINNHUB_API_KEY")
L223
L224     # ---- ï¼ˆInputå°‚ç”¨ï¼‰EPSè£œå®Œãƒ»FCFç®—å‡ºç³» ----
L225     @staticmethod
L226     def impute_eps_ttm(df: pd.DataFrame, ttm_col: str="eps_ttm", q_col: str="eps_q_recent", out_col: str|None=None) -> pd.DataFrame:
L227         out_col = out_col or ttm_col; df = df.copy(); df["eps_imputed"] = False
L228         cand = df[q_col]*4; ok = df[ttm_col].isna() & cand.replace([np.inf,-np.inf], np.nan).notna()
L229         df.loc[ok, out_col], df.loc[ok,"eps_imputed"] = cand[ok], True; return df
L230
L231     _CF_ALIASES = {"cfo":["Operating Cash Flow","Total Cash From Operating Activities"], "capex":["Capital Expenditure","Capital Expenditures"]}
L232
L233     @staticmethod
L234     def _pick_row(df: pd.DataFrame, names: list[str]) -> pd.Series|None:
L235         if df is None or df.empty: return None
L236         idx_lower={str(i).lower():i for i in df.index}
L237         for n in names:
L238             k=n.lower()
L239             if k in idx_lower: return df.loc[idx_lower[k]]
L240         return None
L241
L242     @staticmethod
L243     def _sum_l
```
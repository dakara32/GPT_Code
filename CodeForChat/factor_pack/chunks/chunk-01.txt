```text
# === Chat Paste Pack ===
# Repo: dakara32/GPT_Code @ main
# Files: config.py, factor.py, scorer.py, .github/workflows/weekly-report.yml, documents/README.md, documents/factor_design.md
# 使い方: 下のチャンクを順に貼ればこのチャットで全体把握できます。
# 注記: 各ファイルは個別に L1.. で行番号付与。
---

## <config.py>
```text
L1 # 共通設定（factor / drift から参照）
L2 from dataclasses import dataclass
L3
L4 TOTAL_TARGETS = 20
L5
L6 # 基準のバケット数（NORMAL）
L7 COUNTS_BASE = {"G": 12, "D": 8}
L8
L9 # モード別の推奨バケット数
L10 COUNTS_BY_MODE = {
L11     "NORMAL": {"G": 12, "D": 8},
L12     "CAUTION": {"G": 10, "D": 8},
L13     "EMERG": {"G": 8,  "D": 8},
L14 }
L15
L16 # モード別のドリフト閾値（%）
L17 DRIFT_THRESHOLD_BY_MODE = {"NORMAL": 12, "CAUTION": 14, "EMERG": float("inf")}
L18
L19 # モード別のTS（基本幅, 小数=割合）
L20 TS_BASE_BY_MODE = {"NORMAL": 0.15, "CAUTION": 0.13, "EMERG": 0.10}
L21 # 利益到達(+30/+60/+100%)時の段階タイト化（ポイント差）
L22 TS_STEP_DELTAS_PT = (3, 6, 8)
L23
L24 # Breadthの校正は N_G に連動（緊急解除=ceil(1.5*N_G), 通常復帰=3*N_G）
L25 N_G = COUNTS_BASE["G"]
L26 N_D = COUNTS_BASE["D"]
L27
```

## <factor.py>
```text
L1 '''ROLE: Orchestration ONLY（外部I/O・SSOT・Slack出力）, 計算は scorer.py'''
L2 # === NOTE: 機能・入出力・ログ文言・例外挙動は不変。安全な短縮（import統合/複数代入/内包表記/メソッドチェーン/一行化/空行圧縮など）のみ適用 ===
L3 BONUS_COEFF = 0.55  # 推奨: 攻め=0.45 / 中庸=0.55 / 守り=0.65
L4 SWAP_DELTA_Z = 0.15   # 僅差判定: σの15%。(緩め=0.10 / 標準=0.15 / 固め=0.20)
L5 SWAP_KEEP_BUFFER = 3  # n_target+この順位以内の現行は保持。(粘り弱=2 / 標準=3 / 粘り強=4〜5)
L6 import os, time, requests
L7 from time import perf_counter
L8 from dataclasses import dataclass
L9 from typing import Dict, List
L10 from concurrent.futures import ThreadPoolExecutor
L11 import numpy as np
L12 import pandas as pd
L13 import yfinance as yf
L14 from scipy.stats import zscore  # used via scorer
L15 from scorer import Scorer, ttm_div_yield_portfolio
L16 import config
L17
L18 class T:
L19     t = perf_counter()
L20     log = staticmethod(lambda tag: (lambda now=perf_counter(): (print(f"[T] {tag}: {now - T.t:.2f}s"), setattr(T, "t", now))[-1])())
L21
L22 T.log("start")
L23
L24 # === ユニバースと定数（冒頭に固定） ===
L25 exist, cand = [pd.read_csv(f, header=None)[0].tolist() for f in ("current_tickers.csv","candidate_tickers.csv")]
L26 T.log(f"csv loaded: exist={len(exist)} cand={len(cand)}")
L27 CAND_PRICE_MAX, bench = 450, '^GSPC'  # 価格上限・ベンチマーク
L28 N_G, N_D = config.N_G, config.N_D  # G/D枠サイズ（NORMAL基準: G12/D8）
L29 g_weights = {'GROWTH_F':0.30,'MOM':0.55,'VOL':-0.15}
L30 D_BETA_MAX = float(os.environ.get("D_BETA_MAX", "0.8"))
L31 FILTER_SPEC = {"G":{"pre_mask":["trend_template"]},"D":{"pre_filter":{"beta_max":D_BETA_MAX}}}
L32 D_weights = {'QAL':0.1,'YLD':0.3,'VOL':-0.5,'TRD':0.1}
L33 _fmt_w = lambda w: " ".join(f"{k}{int(v*100)}" for k, v in w.items())
L34
L35 # DRRS 初期プール・各種パラメータ
L36 corrM = 45
L37 DRRS_G, DRRS_D = dict(lookback=252,n_pc=3,gamma=1.2,lam=0.68,eta=0.8), dict(lookback=504,n_pc=4,gamma=0.8,lam=0.85,eta=0.5)
L38 DRRS_SHRINK = 0.10  # 残差相関の対角シュリンク（基礎）
L39
L40 # クロス相関ペナルティ（未定義なら設定）
L41 try: CROSS_MU_GD
L42 except NameError: CROSS_MU_GD = 0.40  # 推奨 0.35–0.45（lam=0.85想定）
L43
L44 # 出力関連
L45 RESULTS_DIR = "results"
L46 os.makedirs(RESULTS_DIR, exist_ok=True)
L47
L48 # その他
L49 debug_mode, FINNHUB_API_KEY = True, os.environ.get("FINNHUB_API_KEY")
L50
L51 # === 共有DTO（クラス間I/O契約）＋ Config ===
L52 @dataclass(frozen=True)
L53 class InputBundle:
L54     # Input → Scorer で受け渡す素材（I/O禁止の生データ）
L55     cand: List[str]
L56     tickers: List[str]
L57     bench: str
L58     data: pd.DataFrame              # yfinance download結果（'Close','Volume'等の階層列）
L59     px: pd.DataFrame                # data['Close']
L60     spx: pd.Series                  # data['Close'][bench]
L61     tickers_bulk: object            # yfinance.Tickers
L62     info: Dict[str, dict]           # yfinance info per ticker
L63     eps_df: pd.DataFrame            # ['eps_ttm','eps_q_recent',...]
L64     fcf_df: pd.DataFrame            # ['fcf_ttm', ...]
L65     returns: pd.DataFrame           # px[tickers].pct_change()
L66
L67 @dataclass(frozen=True)
L68 class FeatureBundle:
L69     df: pd.DataFrame
L70     df_z: pd.DataFrame
L71     g_score: pd.Series
L72     d_score_all: pd.Series
L73     missing_logs: pd.DataFrame
L74
L75 @dataclass(frozen=True)
L76 class SelectionBundle:
L77     resG: dict
L78     resD: dict
L79     top_G: List[str]
L80     top_D: List[str]
L81     init_G: List[str]
L82     init_D: List[str]
L83
L84 @dataclass(frozen=True)
L85 class WeightsConfig:
L86     g: Dict[str,float]
L87     d: Dict[str,float]
L88
L89 @dataclass(frozen=True)
L90 class DRRSParams:
L91     corrM: int
L92     shrink: float
L93     G: Dict[str,float]   # lookback, n_pc, gamma, lam, eta
L94     D: Dict[str,float]
L95     cross_mu_gd: float
L96
L97 @dataclass(frozen=True)
L98 class PipelineConfig:
L99     weights: WeightsConfig
L100     drrs: DRRSParams
L101     price_max: float
L102
L103 # === 共通ユーティリティ（複数クラスで使用） ===
L104 # (unused local utils removed – use scorer.py versions if needed)
L105
L106 _env_true = lambda name, default=False: (os.getenv(name) or str(default)).strip().lower() == "true"
L107
L108 _DEBUG_COL_ALIAS = {
L109     "GROWTH_F": "GRW",
L110     "GROWTH_F_RAW": "GRW_RAW",
L111     "TREND_SLOPE_EPS": "TR_EPS",
L112     "TREND_SLOPE_EPS_RAW": "TR_EPS_RAW",
L113     "TREND_SLOPE_REV": "TR_REV",
L114     "TREND_SLOPE_REV_RAW": "TR_REV_RAW",
L115     "TREND_SLOPE_EPS_YR": "TR_EPS_YR",
L116     "TREND_SLOPE_EPS_YR_RAW": "TR_EPS_YR_RAW",
L117     "TREND_SLOPE_REV_YR": "TR_REV_YR",
L118     "TREND_SLOPE_REV_YR_RAW": "TR_REV_YR_RAW",
L119     "BETA": "BETA_RAW",
L120 }
L121
L122 _DEBUG_COL_ORDER = [
L123     "GRW", "GRW_RAW",
L124     "TR_EPS", "TR_EPS_RAW", "TR_REV", "TR_REV_RAW",
L125     "TR_EPS_YR", "TR_EPS_YR_RAW", "TR_REV_YR", "TR_REV_YR_RAW",
L126     "EPS_Q_YOY", "EPS_Q_YOY_RAW", "EPS_YOY", "EPS_YOY_RAW",
L127     "REV_Q_YOY", "REV_Q_YOY_RAW", "REV_YOY", "REV_YOY_RAW",
L128     "REV_YOY_ACC", "REV_YOY_ACC_RAW", "REV_YOY_VAR", "REV_YOY_VAR_RAW", "REV_ANN_STREAK",
L129     "RULE40", "RULE40_RAW", "FCF_MGN", "FCF_MGN_RAW",
L130     "FCF", "ROE", "RS", "TR_str", "BETA_RAW", "DIV_STREAK",
L131 ]
L132
L133 def _post_slack(payload: dict):
L134     url = os.getenv("SLACK_WEBHOOK_URL")
L135     if not url: print("⚠️ SLACK_WEBHOOK_URL 未設定"); return
L136     try:
L137         requests.post(url, json=payload).raise_for_status()
L138     except Exception as e:
L139         print(f"⚠️ Slack通知エラー: {e}")
L140
L141 _slack = lambda message, code=False: _post_slack({"text": f"```{message}```" if code else message})
L142
L143 def _slack_debug(text: str, chunk: int = 2800, fenced: bool = True) -> None:
L144     """Slackへデバッグテキストをコードブロックで送信（行ベース分割）。"""
L145     body = str(text or "").rstrip("\n")
L146     if not body:
L147         return
L148
L149     def _send(lines: List[str]) -> None:
L150         if not lines:
L151             return
L152         payload = "```\n" + "\n".join(lines) + "\n```" if fenced else "\n".join(lines)
L153         _post_slack({"blocks": [{"type": "section", "text": {"type": "mrkdwn", "text": payload}}]})
L154
L155     block, block_len = [], 0
L156     for raw in body.splitlines():
L157         line = raw or ""
L158         while len(line) > chunk:
L159             head, line = line[:chunk], line[chunk:]
L160             if block:
L161                 _send(block)
L162                 block, block_len = [], 0
L163             _send([head])
L164         add_len = len(line) if not block else len(line) + 1
L165         if block and block_len + add_len > chunk:
L166             _send(block)
L167             block, block_len = [], 0
L168             add_len = len(line)
L169         block.append(line)
L170         block_len += add_len
L171     _send(block)
L172
L173 def _compact_debug(fb, sb, prevG, prevD, max_rows=140):
L174     df_z = getattr(fb, "df_z", None)
L175     if not isinstance(df_z, pd.DataFrame) or df_z.empty:
L176         return "df_z not available"
L177
L178     df_show = df_z.rename(columns={k: v for k, v in _DEBUG_COL_ALIAS.items() if k in df_z.columns})
L179     all_cols = _env_true("DEBUG_ALL_COLS", False)
L180     cols = list(df_show.columns) if all_cols else [c for c in _DEBUG_COL_ORDER if c in df_show.columns]
L181     if not cols:
L182         cols = [c for c in df_show.columns if c not in ("GSC", "DSC")]
L183
L184     g_series = getattr(fb, "g_score", None)
L185     d_series = getattr(fb, "d_score_all", None)
L186     show_near = _env_true("DEBUG_NEAR5", True)
L187     g_sorted = g_series.sort_values(ascending=False) if show_near and hasattr(g_series, "sort_values") else None
L188     d_sorted = d_series.sort_values(ascending=False) if show_near and hasattr(d_series, "sort_values") else None
L189
L190     g_pick = list(sb.top_G or [])
L191     d_pick = list(sb.top_D or [])
L192     Gp, Dp = set(prevG or []), set(prevD or [])
L193     g_new = [t for t in g_pick if t not in Gp]
L194     g_out = [t for t in Gp if t not in g_pick]
L195     d_new = [t for t in d_pick if t not in Dp]
L196     d_out = [t for t in Dp if t not in d_pick]
L197
L198     g_miss = [t for t in (g_sorted.index if g_sorted is not None else []) if t not in g_pick][:10]
L199     used_d = set(g_pick + d_pick)
L200     d_miss = [t for t in (d_sorted.index if d_sorted is not None else []) if t not in used_d][:10]
L201
L202     all_rows = _env_true("DEBUG_ALL_ROWS", False)
L203
L204     def _merge_rows(*seqs):
L205         seen, out = set(), []
L206         for seq in seqs:
L207             for t in seq or []:
L208                 if t in df_show.index and t not in seen:
L209                     seen.add(t)
L210                     out.append(t)
L211         return out
L212
L213     focus = df_show.index.tolist() if all_rows else _merge_rows(g_pick + d_pick, [t for t in (exist or [])], g_miss, d_miss)
L214     if not focus:
L215         focus = df_show.index.tolist()
L216     focus = focus[:max_rows]
L217
L218     def _fmt_near(lbl, ser, lst):
L219         if ser is None:
L220             return f"{lbl}: off"
L221         get = ser.get
L222         parts = []
L223         for t in lst:
L224             val = get(t, np.nan)
L225             parts.append(f"{t}:{val:.3f}" if pd.notna(val) else f"{t}:nan")
L226         return f"{lbl}: " + (", ".join(parts) if parts else "-")
L227
L228     head = [
L229         f"G new/out: {len(g_new)}/{len(g_out)}  D new/out: {len(d_new)}/{len(d_out)}",
L230         _fmt_near("G near10", g_sorted, g_miss),
L231         _fmt_near("D near10", d_sorted, d_miss),
L232         f"Filters: G pre_mask=['trend_template'], D pre_filter={{'beta_max': {D_BETA_MAX}}}",
L233         f"Cols={'ALL' if all_cols else 'MIN'}  Rows={'ALL' if all_rows else 'SELECTED+CURRENT+NEAR'}",
L234     ]
L235
L236     if not focus or not cols:
L237         body = "(no rows or columns to display)"
L238     else:
L239         base = df_show.loc[focus, cols].copy()
L240         if "GSC" not in base.columns and g_series is not None:
L241             base["GSC"] = [g_series.get(t, np.nan) if hasattr(g_series, "get") else np.nan for t in base.index]
L242         if "DSC" not in base.columns and d_series is not None:
L243             base["DSC"] = [d_series.get(t, np.nan) if hasattr(d_series, "get") else np.nan for t in base.index]
L244         out_cols = cols + [c for c in ("GSC", "DSC") if c not in cols]
L245         body = base.reindex(columns=out_cols).round(3).to_string(max_rows=None, max_cols=None, na_rep="nan")
L246
L247     miss_txt = ""
L248     if _env_true("DEBUG_MISSING_LOGS", False):
L249         miss = getattr(fb, "missing_logs", None)
L250         if miss is not None and not miss.empty:
L251             miss_txt = "\nMissing data (head)\n" + miss.head(10).to_string(index=False)
L252
L253     return "\n".join(head + ["\nChanged/Selected (+ Near Miss + Current)", body]) + miss_txt
L254
L255 def _disjoint_keepG(top_G, top_D, poolD):
L256     """G重複をDから除去し、poolDで順次補充（枯渇時は元銘柄維持）。"""
L257     used, D, i = set(top_G), list(top_D), 0
L258     for j, t in enumerate(D):
L259         if t in used:
L260             while i < len(poolD) and (poolD[i] in used or poolD[i] in 
```
```text
# === Chat Paste Pack ===
# Repo: dakara32/GPT_Code @ main
# Files: factor.py, scorer.py, .github/workflows/weekly-report.yml, documents/README.md, documents/factor_design.md
# ä½¿ã„æ–¹: ä¸‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’é †ã«è²¼ã‚Œã°ã“ã®ãƒãƒ£ãƒƒãƒˆã§å…¨ä½“æŠŠæ¡ã§ãã¾ã™ã€‚
# æ³¨è¨˜: å„ãƒ•ã‚¡ã‚¤ãƒ«ã¯å€‹åˆ¥ã« L1.. ã§è¡Œç•ªå·ä»˜ä¸ã€‚
---

## <factor.py>
```text
L1 '''ROLE: Orchestration ONLYï¼ˆå¤–éƒ¨I/Oãƒ»SSOTãƒ»Slackå‡ºåŠ›ï¼‰, è¨ˆç®—ã¯ scorer.py'''
L2 # === NOTE: æ©Ÿèƒ½ãƒ»å…¥å‡ºåŠ›ãƒ»ãƒ­ã‚°æ–‡è¨€ãƒ»ä¾‹å¤–æŒ™å‹•ã¯ä¸å¤‰ã€‚å®‰å…¨ãªçŸ­ç¸®ï¼ˆimportçµ±åˆ/è¤‡æ•°ä»£å…¥/å†…åŒ…è¡¨è¨˜/ãƒ¡ã‚½ãƒƒãƒ‰ãƒã‚§ãƒ¼ãƒ³/ä¸€è¡ŒåŒ–/ç©ºè¡Œåœ§ç¸®ãªã©ï¼‰ã®ã¿é©ç”¨ ===
L3 BONUS_COEFF = 0.4   # æ”»ã‚=0.3 / ä¸­åº¸=0.4 / å®ˆã‚Š=0.5
L4 import os, json, time, requests
L5 from time import perf_counter
L6 from dataclasses import dataclass
L7 from typing import Dict, List
L8 from concurrent.futures import ThreadPoolExecutor
L9 import numpy as np
L10 import pandas as pd
L11 import yfinance as yf
L12 from scipy.stats import zscore  # used via scorer
L13 from scorer import Scorer, ttm_div_yield_portfolio
L14
L15
L16 class T:
L17     t = perf_counter()
L18     log = staticmethod(lambda tag: (lambda now=perf_counter(): (print(f"[T] {tag}: {now - T.t:.2f}s"), setattr(T, "t", now))[-1])())
L19
L20
L21 T.log("start")
L22
L23 # === ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹ã¨å®šæ•°ï¼ˆå†’é ­ã«å›ºå®šï¼‰ ===
L24 exist, cand = [pd.read_csv(f, header=None)[0].tolist() for f in ("current_tickers.csv","candidate_tickers.csv")]
L25 T.log(f"csv loaded: exist={len(exist)} cand={len(cand)}")
L26 CAND_PRICE_MAX, bench = 450, '^GSPC'  # ä¾¡æ ¼ä¸Šé™ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
L27 N_G, N_D = 12, 13  # G/Dæ ã‚µã‚¤ã‚º
L28 g_weights = {'GRW':0.40,'MOM':0.45,'VOL':-0.15}
L29 D_BETA_MAX = float(os.environ.get("D_BETA_MAX", "0.8"))
L30 FILTER_SPEC = {"G":{"pre_mask":["trend_template"]},"D":{"pre_filter":{"beta_max":D_BETA_MAX}}}
L31 D_weights = {'QAL':0.15,'YLD':0.15,'VOL':-0.45,'TRD':0.25}
L32 _fmt_w = lambda w: " ".join(f"{k}{int(v*100)}" for k, v in w.items())
L33
L34 # DRRS åˆæœŸãƒ—ãƒ¼ãƒ«ãƒ»å„ç¨®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
L35 corrM = 45
L36 DRRS_G, DRRS_D = dict(lookback=252,n_pc=3,gamma=1.2,lam=0.68,eta=0.8), dict(lookback=504,n_pc=4,gamma=0.8,lam=0.85,eta=0.5)
L37 DRRS_SHRINK = 0.10  # æ®‹å·®ç›¸é–¢ã®å¯¾è§’ã‚·ãƒ¥ãƒªãƒ³ã‚¯ï¼ˆåŸºç¤ï¼‰
L38
L39 # ã‚¯ãƒ­ã‚¹ç›¸é–¢ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆæœªå®šç¾©ãªã‚‰è¨­å®šï¼‰
L40 try: CROSS_MU_GD
L41 except NameError: CROSS_MU_GD = 0.40  # æ¨å¥¨ 0.35â€“0.45ï¼ˆlam=0.85æƒ³å®šï¼‰
L42
L43 # å‡ºåŠ›é–¢é€£
L44 RESULTS_DIR = "results"
L45 os.makedirs(RESULTS_DIR, exist_ok=True)
L46
L47 # ãã®ä»–
L48 debug_mode, FINNHUB_API_KEY = False, os.environ.get("FINNHUB_API_KEY")
L49
L50
L51 # === å…±æœ‰DTOï¼ˆã‚¯ãƒ©ã‚¹é–“I/Oå¥‘ç´„ï¼‰ï¼‹ Config ===
L52 @dataclass(frozen=True)
L53 class InputBundle:
L54     # Input â†’ Scorer ã§å—ã‘æ¸¡ã™ç´ æï¼ˆI/Oç¦æ­¢ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ï¼‰
L55     cand: List[str]
L56     tickers: List[str]
L57     bench: str
L58     data: pd.DataFrame              # yfinance downloadçµæœï¼ˆ'Close','Volume'ç­‰ã®éšå±¤åˆ—ï¼‰
L59     px: pd.DataFrame                # data['Close']
L60     spx: pd.Series                  # data['Close'][bench]
L61     tickers_bulk: object            # yfinance.Tickers
L62     info: Dict[str, dict]           # yfinance info per ticker
L63     eps_df: pd.DataFrame            # ['eps_ttm','eps_q_recent',...]
L64     fcf_df: pd.DataFrame            # ['fcf_ttm', ...]
L65     returns: pd.DataFrame           # px[tickers].pct_change()
L66
L67 @dataclass(frozen=True)
L68 class FeatureBundle:
L69     df: pd.DataFrame
L70     df_z: pd.DataFrame
L71     g_score: pd.Series
L72     d_score_all: pd.Series
L73     missing_logs: pd.DataFrame
L74
L75 @dataclass(frozen=True)
L76 class SelectionBundle:
L77     resG: dict
L78     resD: dict
L79     top_G: List[str]
L80     top_D: List[str]
L81     init_G: List[str]
L82     init_D: List[str]
L83
L84 @dataclass(frozen=True)
L85 class WeightsConfig:
L86     g: Dict[str,float]
L87     d: Dict[str,float]
L88
L89 @dataclass(frozen=True)
L90 class DRRSParams:
L91     corrM: int
L92     shrink: float
L93     G: Dict[str,float]   # lookback, n_pc, gamma, lam, eta
L94     D: Dict[str,float]
L95     cross_mu_gd: float
L96
L97 @dataclass(frozen=True)
L98 class PipelineConfig:
L99     weights: WeightsConfig
L100     drrs: DRRSParams
L101     price_max: float
L102
L103
L104 # === å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆè¤‡æ•°ã‚¯ãƒ©ã‚¹ã§ä½¿ç”¨ï¼‰ ===
L105 # (unused local utils removed â€“ use scorer.py versions if needed)
L106
L107 _env_true = lambda name, default=False: (os.getenv(name) or str(default)).strip().lower() == "true"
L108
L109 def _post_slack(payload: dict):
L110     url = os.getenv("SLACK_WEBHOOK_URL")
L111     if not url: print("âš ï¸ SLACK_WEBHOOK_URL æœªè¨­å®š"); return
L112     try:
L113         requests.post(url, json=payload).raise_for_status()
L114     except Exception as e:
L115         print(f"âš ï¸ Slacké€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
L116
L117 _slack = lambda message, code=False: _post_slack({"text": f"```{message}```" if code else message})
L118
L119 def _slack_debug(text: str, chunk=2800):
L120     i=0
L121     while i<len(text):
L122         j=min(len(text), i+chunk); k=text.rfind("\n", i, j); j=k if k>i+100 else j
L123         _post_slack({"blocks":[{"type":"section","text":{"type":"mrkdwn","text":f"```{text[i:j]}```"}}]}); i=j
L124
L125 def _compact_debug(fb, sb, prevG, prevD, max_rows=140):
L126     want=["TR","EPS","REV","ROE","BETA_RAW","FCF","RS","TR_str","DIV_STREAK","DSC"]
L127     all_cols = _env_true("DEBUG_ALL_COLS", False)
L128     cols = list(fb.df_z.columns if all_cols else [c for c in want if c in fb.df_z.columns])
L129
L130     Gp, Dp = set(prevG or []), set(prevD or [])
L131     g_new=[t for t in (sb.top_G or []) if t not in Gp]; g_out=[t for t in Gp if t not in (sb.top_G or [])]
L132     d_new=[t for t in (sb.top_D or []) if t not in Dp]; d_out=[t for t in Dp if t not in (sb.top_D or [])]
L133
L134     show_near = _env_true("DEBUG_NEAR5", True)
L135     gs, ds = getattr(fb,"g_score",None), getattr(fb,"d_score_all",None)
L136     gs = (gs.sort_values(ascending=False) if show_near and hasattr(gs,"sort_values") else None)
L137     ds = (ds.sort_values(ascending=False) if show_near and hasattr(ds,"sort_values") else None)
L138     g_miss = ([t for t in gs.index if t not in (sb.top_G or [])][:10]) if gs is not None else []
L139     d_excl = set((sb.top_G or [])+(sb.top_D or []))
L140     d_miss = ([t for t in ds.index if t not in d_excl][:10]) if ds is not None else []
L141
L142     all_rows = _env_true("DEBUG_ALL_ROWS", False)
L143     focus = list(fb.df_z.index) if all_rows else sorted(set(g_new+g_out+d_new+d_out+(sb.top_G or [])+(sb.top_D or [])+g_miss+d_miss))[:max_rows]
L144
L145     def _fmt_near(lbl, ser, lst):
L146         if ser is None: return f"{lbl}: off"
L147         g = ser.get
L148         parts=[f"{t}:{g(t,float('nan')):.3f}" if pd.notna(g(t)) else f"{t}:nan" for t in lst]
L149         return f"{lbl}: " + (", ".join(parts) if parts else "-")
L150
L151     head=[f"G new/out: {len(g_new)}/{len(g_out)}  D new/out: {len(d_new)}/{len(d_out)}",
L152           _fmt_near("G near10", gs, g_miss),
L153           _fmt_near("D near10", ds, d_miss),
L154           f"Filters: G pre_mask=['trend_template'], D pre_filter={{'beta_max': {D_BETA_MAX}}}",
L155           f"Cols={'ALL' if all_cols else 'MIN'}  Rows={'ALL' if all_rows else 'SUBSET'}"]
L156
L157     tbl="(df_z or columns not available)"
L158     if not fb.df_z.empty and cols:
L159         idx=[t for t in focus if t in fb.df_z.index]
L160         tbl=fb.df_z.loc[idx, cols].round(3).to_string(max_rows=None, max_cols=None)
L161
L162     miss_txt=""
L163     if _env_true("DEBUG_MISSING_LOGS", False):
L164         miss=getattr(fb,"missing_logs",None)
L165         if miss is not None and not miss.empty:
L166             miss_txt="\nMissing data (head)\n"+miss.head(10).to_string(index=False)
L167
L168     return "\n".join(head+["\nChanged/Selected (+ Near Miss)", tbl])+miss_txt
L169
L170 def _disjoint_keepG(top_G, top_D, poolD):
L171     """
L172     Gã«å«ã¾ã‚Œã‚‹éŠ˜æŸ„ã‚’Dã‹ã‚‰é™¤å»ã—ã€Dã¯poolDï¼ˆæ¬¡ç‚¹ï¼‰ã§è£œå……ã™ã‚‹ã€‚
L173     - å¼•æ•°:
L174         top_G: List[str]  â€¦ Gæœ€çµ‚12éŠ˜æŸ„
L175         top_D: List[str]  â€¦ Dæœ€çµ‚13éŠ˜æŸ„ï¼ˆé‡è¤‡ã‚’å«ã‚€å¯èƒ½æ€§ã‚ã‚Šï¼‰
L176         poolD: List[str]  â€¦ Då€™è£œã®é †ä½ãƒªã‚¹ãƒˆï¼ˆtop_Dã‚’å«ã‚€ä¸Šä½æ‹¡å¼µï¼‰
L177     - æˆ»ã‚Šå€¤: (top_G, top_D_disjoint)
L178     - æŒ™å‹•:
L179         1) Dã«Gé‡è¤‡ãŒã‚ã‚Œã°é †ã«ç½®æ›
L180         2) ç½®æ›å€™è£œã¯ poolD ã‹ã‚‰ã€æ—¢ä½¿ç”¨(GâˆªD)ã‚’é¿ã‘ã¦å‰ã‹ã‚‰æ¡ç”¨
L181         3) è£œå……åˆ†ãŒå°½ããŸå ´åˆã¯å…ƒã®éŠ˜æŸ„ã‚’æ®‹ã™ï¼ˆå®‰å…¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
L182     """
L183     used, D, i = set(top_G), list(top_D), 0
L184     for j, t in enumerate(D):
L185         if t in used:
L186             while i<len(poolD) and (poolD[i] in used or poolD[i] in D): i+=1
L187             if i < len(poolD): D[j] = poolD[i]; used.add(D[j]); i += 1
L188     return top_G, D
L189
L190 _state_file = lambda: os.path.join(RESULTS_DIR, "breadth_state.json")
L191 def load_mode(default: str="NORMAL") -> str:
L192     try: m=json.loads(open(_state_file()).read()).get("mode", default); return m if m in ("EMERG","CAUTION","NORMAL") else default
L193     except Exception: return default
L194 def save_mode(mode: str):
L195     try: open(_state_file(),"w").write(json.dumps({"mode": mode}))
L196     except Exception: pass
L197
L198 # --- Breadthâ†’è‡ªå‹•ã—ãã„å€¤â†’ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹â†’Slackå…ˆé ­è¡Œã‚’ä½œæˆ ---
L199 def _build_breadth_lead_lines(inb) -> tuple[list[str], str]:
L200     win = int(os.getenv("BREADTH_CALIB_WIN_DAYS", "600"))
L201     C_ts = Scorer.trend_template_breadth_series(inb.px[inb.tickers], inb.spx, win_days=win)
L202     if C_ts.empty: raise RuntimeError("breadth series empty")
L203     warmup=int(os.getenv("BREADTH_WARMUP_DAYS","252")); base=C_ts.iloc[warmup:] if len(C_ts)>warmup else C_ts; C_full=int(C_ts.iloc[-1])
L204     q05 = int(np.nan_to_num(base.quantile(float(os.getenv("BREADTH_Q_EMERG_IN",  "0.05"))), nan=0.0))
L205     q20 = int(np.nan_to_num(base.quantile(float(os.getenv("BREADTH_Q_EMERG_OUT", "0.20"))), nan=0.0))
L206     q60 = int(np.nan_to_num(base.quantile(float(os.getenv("BREADTH_Q_WARN_OUT",  "0.60"))), nan=0.0))
L207     th_in_rec, th_out_rec, th_norm_rec = max(N_G, q05), max(int(np.ceil(1.5*N_G)), q20), max(3*N_G, q60)
L208     use_calib = os.getenv("BREADTH_USE_CALIB", "true").strip().lower() == "true"
L209     th_in, th_out, th_norm, th_src = (th_in_rec, th_out_rec, th_norm_rec, "è‡ªå‹•") if use_calib else (int(os.getenv("GTT_EMERG_IN",str(N_G))), int(os.getenv("GTT_EMERG_OUT",str(int(1.5*N_G)))), int(os.getenv("GTT_CAUTION_OUT",str(3*N_G))), "æ‰‹å‹•")
L210     prev = load_mode("NORMAL")
L211     if   prev == "EMERG":  mode = "EMERG" if (C_full < th_out) else ("CAUTION" if (C_full < th_norm) else "NORMAL")
L212     elif prev == "CAUTION": mode = "CAUTION" if (C_full < th_norm) else "NORMAL"
L213     else:                   mode = "EMERG" if (C_full < th_in) else ("CAUTION" if (C_full < th_norm) else "NORMAL")
L214     save_mode(mode)
L215     _MODE_JA={"EMERG":"ç·Šæ€¥","CAUTION":"è­¦æˆ’","NORMAL":"é€šå¸¸"}; _MODE_EMOJI={"EMERG":"ğŸš¨","CAUTION":"âš ï¸","NORMAL":"ğŸŸ¢"}
L216     mode_ja,emoji,eff_days=_MODE_JA.get(mode,mode),_MODE_EMOJI.get(mode,"â„¹ï¸"),len(base)
L217     lead_lines = [
L218         f"{emoji} *ç¾åœ¨ãƒ¢ãƒ¼ãƒ‰: {mode_ja}*", f"ãƒ†ãƒ³ãƒ—ãƒ¬åˆæ ¼æœ¬æ•°: *{C_full}æœ¬*", "ã—ãã„å€¤ï¼ˆ{0}ï¼‰".format(th_src),
L219         f"  ãƒ»ç·Šæ€¥å…¥ã‚Š: <{th_in}æœ¬", f"  ãƒ»ç·Šæ€¥è§£é™¤: â‰¥{th_out}æœ¬", f"  ãƒ»é€šå¸¸å¾©å¸°: â‰¥{th_norm}æœ¬",
L220         f"å‚è€ƒæŒ‡æ¨™ï¼ˆéå»~{win}å–¶æ¥­æ—¥, æœ‰åŠ¹={eff_days}æ—¥ï¼‰",
L221         f"  ãƒ»ä¸‹ä½5%: {q05}æœ¬", f"  ãƒ»ä¸‹ä½20%: {q20}æœ¬", f"  ãƒ»60%åˆ†ä½: {q60}æœ¬",
L222     ]
L223     return lead_lines, mode
L224
L225
L226 # === Inputï¼šå¤–éƒ¨I/Oã¨å‰å‡¦ç†ï¼ˆCSV/APIãƒ»æ¬ æè£œå®Œï¼‰ ===
L227 class Input:
L228     def __init__(self, cand, exist, bench, price_max, finnhub_api_key=None):
L229         self.cand, self.exist, self.bench, self.price_max = cand, exist, bench, price_max
L230         self.api_key = finnhub_api_key or os.environ.get("FINNHUB_API_KEY")
L231
L232     # ---- ï¼ˆInputå°‚ç”¨ï¼‰EPSè£œå®Œãƒ»FCFç®—å‡ºç³» ----
L233     @staticmethod
L234     def impute_eps_ttm(df: pd.DataFrame, ttm_col: str="eps_ttm", q_col: str="eps_q_recent", out_col: str|None=None) -> pd.DataFrame:
L235         out_col = out_col or ttm_col; df = df.copy(); df["eps_imputed"] = False
L236         cand = df[q_col]*4; ok = df[ttm_col].isna() & cand.replace([np.inf,-np.inf], np.nan).notna()
L237         df.loc[ok, out_col], df.loc[ok,"eps_imputed"] = cand[ok], True; return df
L238
L239     _CF_ALIASES = {"cfo":["Operating Cash Flow","Total Cash From Operating Activities"], "capex":["Capital Expenditure","Capital Expenditures"]}
L240
L241     @staticmethod
L242     def _pick_row(df: pd.DataFrame, names: list[str]) -> pd.Series|None:
L243         if df is None or df.empty: return None
L244  
```